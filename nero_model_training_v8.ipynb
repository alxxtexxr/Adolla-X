{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "!fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from typing import Optional, Literal, Union, List\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download, create_repo, upload_folder\n",
    "from safetensors.torch import load_file, save_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import cuml\n",
    "from cuml.decomposition import PCA as cuPCA\n",
    "from cuml.manifold import UMAP as cuUMAP\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(\n",
    "        repo_id: str, \n",
    "        checkpoint: Optional[int], \n",
    "        max_checkpoints: str = 10_000, \n",
    "        checkpoint_steps: str = 25,\n",
    "    ):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_steps) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dir = os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir, checkpoint_dir\n",
    "\n",
    "def check_loss_and_grad_norm(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt=\"Paris is the capital of\",\n",
    "):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "        print(\"Loss 1:\", outputs.loss)\n",
    "        print(\"Loss 2:\", outputs[1])\n",
    "    else:\n",
    "        print(\"Loss:\", outputs.loss)\n",
    "\n",
    "    # Backward pass\n",
    "    if outputs.loss.grad_fn is None:\n",
    "        print(\"Gradient norm:\", None)\n",
    "        return\n",
    "\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "\n",
    "def check_parameter(n, p):\n",
    "    print(f\"- {'name':<8}:\", n)\n",
    "    print(f\"- {'device':<8}:\", p.device)\n",
    "    print(f\"- {'dtype':<8}:\", p.dtype)\n",
    "    print(f\"- {'mean':<8}:\", p.mean().item())\n",
    "    print(f\"- {'min':<8}:\", p.min().item())\n",
    "    print(f\"- {'max':<8}:\", p.max().item())\n",
    "\n",
    "def check_lora_parameters(model, prefix=None):\n",
    "    prefix = 'lora.' + prefix if prefix != None else 'lora'\n",
    "    for n, p in model.named_parameters():\n",
    "        if prefix in n:\n",
    "            check_parameter(n, p)\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def get_task_and_lang_from_repo_id(repo_id: str):\n",
    "    task, lang, _ = repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "    return task, lang\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id: str,\n",
    "    train_size: int = 5000,\n",
    "    test_size: int = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang = get_task_and_lang_from_repo_id(lora_repo_id)\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    \n",
    "    # Load dataset using streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_grad_norm(params):\n",
    "    grad_norm = 0.0\n",
    "    for p in params:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def compute_named_grad_norm(named_params):\n",
    "    grad_norm = 0.0\n",
    "    for n, p in named_params.items():\n",
    "        if p.grad is not None:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            print(f\"{n}/p_grad_norm:\", p_grad_norm)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "        else:\n",
    "            print(f\"[WARN] No gradient for {n}\")\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def format_float(v):\n",
    "    if abs(v) < 0.0001 or abs(v) >= 10000:\n",
    "        return f\"{v:.4e}\"\n",
    "    else:\n",
    "        return f\"{v:.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d524b1cb176a4705b7acb17d264eb67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789e0e18dc0043efb42d9909928331ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "- L1T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650/adapter_model.safetensors\n",
      "- L2T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650/adapter_model.safetensors\n",
      "\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "device = 'auto'\n",
    "\n",
    "# Data configuration\n",
    "hf_data_id = 'alxxtexxr/Nero-XLT-Dataset'\n",
    "hf_data_dir = 'gsm8k_en_5K_1K_1K_512'\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 1.0\n",
    "lr = 2e-4\n",
    "warmup_ratio = 0.1\n",
    "# num_warmup_steps = 100\n",
    "checkpoint_steps = 10\n",
    "push_to_hf = False\n",
    "generate_steps = 10\n",
    "L1T2_sample_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question:\n",
    "Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns are there total?\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "L2T2_sample_prompt = \"\"\"### 命令:\n",
    "次の数学の問題を段階的に解いてください。\n",
    "\n",
    "### 質問:\n",
    "ダンは3本のバラの灌木を植えました。各バラの灌木には25本のバラがあります。各バラには8本のトゲがあります。合計で何本のトゲがあるでしょうか？\n",
    "\n",
    "### 答え:\n",
    "\"\"\"\n",
    "\n",
    "# Model configurations\n",
    "model_configs = {\n",
    "    # L1T1 (Source Language - Source Task)\n",
    "    'L1T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L2T1 (Target Language - Source Task)\n",
    "    'L2T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L1T2 (Source Language - Target Task)\n",
    "    # 'L1T2': {\n",
    "    #     'hf_lora_id': 'alxxtexxr/L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457',\n",
    "    #     'checkpoint': 1875,\n",
    "    # },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    _, lora_dir = download_hf_model(config['hf_lora_id'], config['checkpoint'])\n",
    "    model_configs[key]['lora_dir'] = lora_dir\n",
    "    model_configs[key]['lora_path'] = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "    model_configs[key]['lora_config'] = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "for key, config in model_configs.items():\n",
    "    print(f\"- {key}:\")\n",
    "    for config_name, config_value in config.items():\n",
    "        if config_name == 'lora_config':\n",
    "            continue\n",
    "        print(f\"{'-':>3} {config_name:<10}: {config_value}\")\n",
    "print()\n",
    "\n",
    "assert (\n",
    "    model_configs['L1T1']['lora_config'].base_model_name_or_path == \n",
    "    model_configs['L2T1']['lora_config'].base_model_name_or_path\n",
    "), \"Base models must be the same\"\n",
    "base_model_name = model_configs['L1T1']['lora_config'].base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating Nero directory: L3.1-8B-gsm8k-en-5K-1K-1K-512-Nero-v20250824121713\n",
      "[INFO] Nero directory created!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face configuration\n",
    "hf_nero_id = None\n",
    "resume_step = 0\n",
    "\n",
    "if hf_nero_id is not None and resume_step > 0:\n",
    "    print(f\"[INFO] Downloading Nero checkpoint at step {resume_step} from Hugging Face repository:\", hf_nero_id)\n",
    "    nero_dir, _ = download_hf_model(hf_nero_id, resume_step)\n",
    "    print(f\"[INFO] Nero checkpoint downloaded successfully!\")\n",
    "else:\n",
    "    hf_username = 'alxxtexxr'\n",
    "    nero_dir = f'L3.1-8B-{hf_data_dir.replace(\"_\", \"-\")}-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    print(f\"[INFO] Creating Nero directory:\", nero_dir)\n",
    "    hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "    os.makedirs(nero_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Nero directory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self, dim, residual, rank, alpha, beta, use_rslora=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.beta = beta\n",
    "        self.A = nn.Linear(dim, rank, bias=False)\n",
    "        self.B = nn.Linear(rank, dim, bias=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.A.weight, 0.0, 1 / math.sqrt(rank))\n",
    "        nn.init.zeros_(self.B.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        act_out = self.B(self.act(self.A(x))) * self.scaling * self.beta\n",
    "        if self.residual:\n",
    "            return x + act_out\n",
    "        else:\n",
    "            return act_out\n",
    "\n",
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer,\n",
    "                 \n",
    "                 # LoRA parameters\n",
    "                 L1T1_lora_params, \n",
    "                 L2T1_lora_params,\n",
    "                 L2T2_lora_params,\n",
    "                 lora_beta,\n",
    "                 train_L2T2_lora_and_g=True,\n",
    "                 eval_L1T2_lora=False,\n",
    "                 eval_L2T2_lora=False,\n",
    "                 return_layer_loss=True,\n",
    "                 \n",
    "                 # Debugging parameters\n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.lora_beta = lora_beta\n",
    "        self._train_L2T2_lora_and_g = train_L2T2_lora_and_g\n",
    "        self._eval_L1T2_lora = eval_L1T2_lora\n",
    "        self._eval_L2T2_lora = eval_L2T2_lora\n",
    "        self._return_layer_loss = return_layer_loss\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "\n",
    "        # Initialize LoRA layers\n",
    "        self.lora = nn.ModuleDict({\n",
    "            'L1T1': self._init_lora_layer('L1T1', in_features, out_features, **L1T1_lora_params, device=self.device), # frozen\n",
    "            'L2T1': self._init_lora_layer('L2T1', in_features, out_features, **L2T1_lora_params, device=self.device), # frozen\n",
    "            'L2T2': self._init_lora_layer('L2T2', in_features, out_features, **L2T2_lora_params, device=self.device), # trainable\n",
    "        })\n",
    "        \n",
    "        # Initialize nonlinear function layer\n",
    "        g_rank = L2T2_lora_params['rank'] #// 2 # WARN: Hardcoded rank for g layer, can be adjusted\n",
    "        self.g = G(\n",
    "            out_features, \n",
    "            residual=True, \n",
    "            rank=g_rank, \n",
    "            alpha=L2T2_lora_params['alpha'],\n",
    "            beta=self.lora_beta,\n",
    "        ).to(self.device) # trainable\n",
    "        \n",
    "    def _init_lora_layer(self, name, in_features, out_features, rank, alpha, dropout=0.0, bias=True, use_rslora=False, device=None):\n",
    "        scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        layer = nn.ModuleDict({\n",
    "            'A': nn.Linear(in_features, rank, bias=bias, device=device),\n",
    "            'B': nn.Linear(rank, out_features, bias=bias, device=device),\n",
    "            'dropout': dropout_layer\n",
    "        })\n",
    "        layer.scaling = scaling\n",
    "        layer.bias_flag = bias\n",
    "        if name == 'L2T2':\n",
    "            nn.init.zeros_(layer.A.weight)\n",
    "        else:\n",
    "            nn.init.normal_(layer.A.weight, 0.0, 1 / math.sqrt(rank))\n",
    "        nn.init.zeros_(layer.B.weight)\n",
    "        return layer\n",
    "    \n",
    "    def set_lora_beta(self, lora_beta, verbose: bool=False):\n",
    "        self.lora_beta = lora_beta\n",
    "        self.g.beta = lora_beta\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] LoRA beta on {self.module_name} layer is set to {lora_beta}!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"================================================================\")\n",
    "        print(self.module_name)\n",
    "        print(\"================================================================\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # Base Layer\n",
    "        # ================================================================\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            # TODO: Add assertion to check if all weights have the same dtype\n",
    "            x = x.to(self.lora.L2T2.A.weight.dtype)\n",
    "            \n",
    "        # ================================================================\n",
    "        # L2T2 LoRA Layer (Trainable)\n",
    "        # ================================================================\n",
    "        # L2T2_lora_out = self.lora.L2T2.B(self.lora.L2T2.A(self.lora.L2T2.dropout(x))) #* self.lora.L2T2.scaling\n",
    "        L2T2_lora_dropout_out = self.lora.L2T2.dropout(x)\n",
    "        L2T2_lora_A_out = self.lora.L2T2.A(L2T2_lora_dropout_out)\n",
    "        L2T2_lora_B_out = self.lora.L2T2.B(L2T2_lora_A_out)\n",
    "        L2T2_lora_out = L2T2_lora_B_out * self.lora.L2T2.scaling * self.lora_beta\n",
    "        \n",
    "        if self._eval_L2T2_lora:\n",
    "            if requires_conversion:\n",
    "                L2T2_lora_out = L2T2_lora_out.to(base_out.dtype)\n",
    "            return base_out + L2T2_lora_out\n",
    "\n",
    "        # ================================================================\n",
    "        # L1T1 LoRA Layer (Frozen)\n",
    "        # ================================================================\n",
    "        # L1T1_lora_out = self.lora.L1T1.B(self.lora.L1T1.A(self.lora.L1T1.dropout(x))) #* self.lora.L1T1.scaling\n",
    "        L1T1_lora_dropout_out = self.lora.L1T1.dropout(x)\n",
    "        L1T1_lora_A_out = self.lora.L1T1.A(L1T1_lora_dropout_out)\n",
    "        L1T1_lora_B_out = self.lora.L1T1.B(L1T1_lora_A_out)\n",
    "        L1T1_lora_out = L1T1_lora_B_out * self.lora.L1T1.scaling * self.lora_beta\n",
    "        \n",
    "        # ================================================================\n",
    "        # L2T1 LoRA Layer (Frozen)\n",
    "        # ================================================================\n",
    "        # L2T1_lora_out = self.lora.L2T1.B(self.lora.L2T1.A(self.lora.L2T1.dropout(x))) #* self.lora.L2T1.scaling\n",
    "        L2T1_lora_dropout_out = self.lora.L2T1.dropout(x)\n",
    "        L2T1_lora_A_out = self.lora.L2T1.A(L2T1_lora_dropout_out)\n",
    "        L2T1_lora_B_out = self.lora.L2T1.B(L2T1_lora_A_out)\n",
    "        L2T1_lora_out = L2T1_lora_B_out * self.lora.L2T1.scaling * self.lora_beta\n",
    "        \n",
    "        # ================================================================\n",
    "        # L1T2 Output\n",
    "        # ================================================================\n",
    "        # Assume linear shift:\n",
    "        # king - queen = man - woman\n",
    "        # L1T2 - L2T2 = L1T1 - L2T1\n",
    "        \n",
    "        # Assume difference-based nonlinear shift (with free L2T2):\n",
    "        # king - queen = man - woman\n",
    "        # L1T2 - L2T2 = L1T1 - L2T1\n",
    "        # f(L1T2 - L2T2) = f(L1T1 - L2T1)\n",
    "        # L1T2 - L2T2 = f^-1(f(L1T1 - L2T1))\n",
    "        # L1T2 = L2T2 + f^-1(f(L1T1 - L2T1))\n",
    "        # L1T2 = L2T2 + g(L1T1 - L2T1)\n",
    "        g_out = self.g(L1T1_lora_out - L2T1_lora_out)\n",
    "        L1T2_out = L2T2_lora_out + g_out\n",
    "        # Note:\n",
    "        # L2T2 weights should be initialized from L2T1 weights\n",
    "        # so that during initialization: L1T2 = L2T2 + g(L1T1 - L2T1) => L1T2 = L2T1 + 0 => L1T2 = L2T1\n",
    "        # This ensures the model behaves like sequential fine-tuning from L2T1 to L1T2,\n",
    "        # with g(L1T1 - L2T1) acting as a constraint.\n",
    "        \n",
    "        # Assume difference-based nonlinear shift (without free L2T2):\n",
    "        # king - queen = man - woman\n",
    "        # king - man = queen - woman\n",
    "        # L1T2 - L1T1 = L2T2 - L2T1\n",
    "        # f(L1T2 - L1T1) = f(L2T2 - L2T1)\n",
    "        # L1T2 - L1T1 = f^-1(f(L2T2 - L2T1))\n",
    "        # L1T2 = L1T1 + f^-1(f(L2T2 - L2T1))\n",
    "        # L1T2 = L1T1 + g(L2T2 - L2T1)\n",
    "        # g_out = self.g(L2T2_lora_out - L2T1_lora_out)\n",
    "        # L1T2_out = L1T1_lora_out + g_out\n",
    "        \n",
    "        # if self.debug:\n",
    "        # with torch.no_grad():\n",
    "        #     print(\"L1T1_lora_B_out:\", (L1T1_lora_B_out.max().item(), L1T1_lora_B_out.mean().item(), L1T1_lora_B_out.min().item()))\n",
    "        #     # print(\"L1T1/scaling:\", self.lora.L1T1.scaling)\n",
    "        #     print(\"L1T1_lora_out:\", (L1T1_lora_out.max().item(), L1T1_lora_out.mean().item(), L1T1_lora_out.min().item()))\n",
    "        #     print()\n",
    "        #     print(\"L2T1_lora_B_out:\", (L2T1_lora_B_out.max().item(), L2T1_lora_B_out.mean().item(), L2T1_lora_B_out.min().item()))\n",
    "        #     # # print(\"L2T1/scaling:\", self.lora.L2T1.scaling)\n",
    "        #     print(\"L2T1_lora_out:\", (L2T1_lora_out.max().item(), L2T1_lora_out.mean().item(), L2T1_lora_out.min().item()))\n",
    "        #     print()\n",
    "        #     print(\"L2T2_lora_B_out:\", (L2T2_lora_B_out.max().item(), L2T2_lora_B_out.mean().item(), L2T2_lora_B_out.min().item()))\n",
    "        #     # # print(\"L2T2/scaling:\", self.lora.L2T2.scaling)\n",
    "        #     print(\"L2T2_lora_out:\", (L2T2_lora_out.max().item(), L2T2_lora_out.mean().item(), L2T2_lora_out.min().item()))\n",
    "        #     print()\n",
    "        #     # # print(\"g/scaling:\", self.g.scaling)\n",
    "        #     print(\"g_out:\", (g_out.max().item(), g_out.mean().item(), g_out.min().item())) # must be zero at init\n",
    "        #     print(\"L1T2_out:\", (L1T2_out.max().item(), L1T2_out.mean().item(), L1T2_out.min().item())) # must be zero at init\n",
    "        #     print()\n",
    "        \n",
    "        # ================================================================\n",
    "        # Loss\n",
    "        # ================================================================\n",
    "        loss_fn = 'mse' # WARN: Hardcoded loss function\n",
    "        if loss_fn is not None:\n",
    "            if loss_fn == 'mse':\n",
    "                loss = F.mse_loss(L2T2_lora_out, , reduction='mean')\n",
    "            elif loss_fn == 'kl':\n",
    "                temperature = 2.0 # WARN: Hardcoded temperature for KL divergence\n",
    "                L1T2_out_detached = L1T2_out.detach()\n",
    "                loss = F.kl_div(\n",
    "                    F.log_softmax(L2T2_lora_out / temperature, dim=-1),\n",
    "                    F.softmax(L1T2_out_detached / temperature, dim=-1),\n",
    "                    reduction='batchmean'\n",
    "                ) * (temperature ** 2)\n",
    "            elif loss_fn == 'cosine':\n",
    "                L1T2_out_detached = L1T2_out.detach()\n",
    "                loss = 1 - F.cosine_similarity(L2T2_lora_out, L1T2_out_detached, dim=-1).mean()\n",
    "            \n",
    "            if self._return_layer_loss:\n",
    "                if requires_conversion:\n",
    "                    L1T2_out = L1T2_out.to(base_out.dtype)\n",
    "                return base_out + L1T2_out, loss\n",
    "            \n",
    "        # ================================================================\n",
    "        if requires_conversion:\n",
    "            L1T2_out = L1T2_out.to(base_out.dtype)\n",
    "        return base_out + L1T2_out\n",
    "\n",
    "    def load_lora_params(self, mode: Literal['L1T1', 'L2T1', 'L2T2'], state_dict, prefix: str):\n",
    "        self.lora[mode].A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora[mode].B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora[mode].bias_flag:\n",
    "            self.lora[mode].A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora[mode].B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "        if mode in ['L1T1', 'L2T1']:\n",
    "            self.lora['L2T2'].A.weight.data += state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "            self.lora['L2T2'].B.weight.data += state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "            if self.lora['L2T2'].bias_flag:\n",
    "                self.lora['L2T2'].A.bias.data += state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "                self.lora['L2T2'].B.bias.data += state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "            \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 base_model: nn.Module, \n",
    "                 lora_beta: float,\n",
    "                 L1T1_lora_config: LoraConfig, \n",
    "                 L2T1_lora_config: LoraConfig,\n",
    "                 train_L2T2_lora_and_g: bool=True,\n",
    "                 eval_L1T2_lora: bool=False,\n",
    "                 eval_L2T2_lora: bool=False,\n",
    "                 return_layer_loss: bool=True,\n",
    "                 debug: bool=False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.lora_beta = lora_beta\n",
    "        self._train_L2T2_lora_and_g = train_L2T2_lora_and_g\n",
    "        self._eval_L1T2_lora = eval_L1T2_lora\n",
    "        self._eval_L2T2_lora = eval_L2T2_lora\n",
    "        self._return_layer_loss = return_layer_loss\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self._wrap_target_layers(L1T1_lora_config, L2T1_lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, L1T1_lora_config, L2T1_lora_config):\n",
    "        assert L1T1_lora_config.target_modules == L2T1_lora_config.target_modules, \"[ERROR] L1T1 and L2T1 LoRA configurations must have the same target modules.\"\n",
    "\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in L1T1_lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    base_layer=module,\n",
    "                    eval_L1T2_lora=self._eval_L1T2_lora,\n",
    "                    eval_L2T2_lora=self._eval_L2T2_lora,\n",
    "                    return_layer_loss=self._return_layer_loss,\n",
    "\n",
    "                    # L1T1 LoRA parameters\n",
    "                    L1T1_lora_params={\n",
    "                        'rank': L1T1_lora_config.r, \n",
    "                        'alpha': L1T1_lora_config.lora_alpha, \n",
    "                        'dropout': L1T1_lora_config.lora_dropout,\n",
    "                        'bias': L1T1_lora_config.lora_bias,\n",
    "                        'use_rslora': L1T1_lora_config.use_rslora,\n",
    "                    },\n",
    "                \n",
    "                    # L2T1 LoRA parameters\n",
    "                    L2T1_lora_params={\n",
    "                        'rank': L2T1_lora_config.r, \n",
    "                        'alpha': L2T1_lora_config.lora_alpha, \n",
    "                        'dropout': L2T1_lora_config.lora_dropout,\n",
    "                        'bias': L2T1_lora_config.lora_bias,\n",
    "                        'use_rslora': L2T1_lora_config.use_rslora,\n",
    "                    },\n",
    "\n",
    "                    # L2T2 parameters (for temporary, use L2T1 LoRA parameters)\n",
    "                    L2T2_lora_params={\n",
    "                        'rank': L2T1_lora_config.r, \n",
    "                        'alpha': L2T1_lora_config.lora_alpha, \n",
    "                        'dropout': L2T1_lora_config.lora_dropout,\n",
    "                        'bias': L2T1_lora_config.lora_bias,\n",
    "                        'use_rslora': L2T1_lora_config.use_rslora,\n",
    "                    },\n",
    "                    lora_beta=self.lora_beta,\n",
    "                    \n",
    "                    # Debugging parameters\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        \n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "            \n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_debug(self, debug: bool=False, verbose: bool=False):\n",
    "        self.debug = debug\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            nero_layer.debug = debug\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Debugging is {'enabled' if debug else 'disabled'}!\")\n",
    "    \n",
    "    def set_lora_beta(self, lora_beta: bool=False, verbose: bool=False):\n",
    "        self.lora_beta = lora_beta\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            nero_layer.set_lora_beta(lora_beta)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] LoRA beta is set to {lora_beta}.\")\n",
    "    \n",
    "    def set_return_layer_loss(self, return_layer_loss: bool=True, verbose: bool=False):\n",
    "        self._return_layer_loss = return_layer_loss\n",
    "        \n",
    "        for layer in self.nero_layers.values():\n",
    "            layer._return_layer_loss = return_layer_loss\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Return layer loss set to '{return_layer_loss}'.\")\n",
    "    \n",
    "    def train_L2T2_lora_and_g(self, verbose: bool=False):\n",
    "        self.freeze_all_except_L2T2_lora_and_g()\n",
    "        \n",
    "        self._train_L2T2_lora_and_g = True\n",
    "        self._eval_L1T2_lora = False\n",
    "        self._eval_L2T2_lora = False\n",
    "        # self._return_layer_loss = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            nero_layer._eval_L1T2_lora = False\n",
    "            nero_layer._eval_L2T2_lora = False\n",
    "            # nero_layer._return_layer_loss = True\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Training L2T2 LoRA and g layers!\")\n",
    "    \n",
    "    def eval_L1T2_lora(self, verbose: bool=False):\n",
    "        self.freeze_all()\n",
    "        \n",
    "        self._train_L2T2_lora_and_g = False\n",
    "        self._eval_L1T2_lora = True\n",
    "        self._eval_L2T2_lora = False\n",
    "        self._return_layer_loss = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            nero_layer._eval_L1T2_lora = True\n",
    "            nero_layer._eval_L2T2_lora = False\n",
    "            nero_layer._return_layer_loss = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Evaluating L1T2 LoRA!\")\n",
    "            \n",
    "    def eval_L2T2_lora(self, verbose: bool=False):\n",
    "        self.freeze_all()\n",
    "        \n",
    "        self._train_L2T2_lora_and_g = False\n",
    "        self._eval_L1T2_lora = False\n",
    "        self._eval_L2T2_lora = True\n",
    "        self._return_layer_loss = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            nero_layer._eval_L1T2_lora = False\n",
    "            nero_layer._eval_L2T2_lora = True\n",
    "            nero_layer._return_layer_loss = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Evaluating L2T2 LoRA!\")\n",
    "            \n",
    "    def freeze_all(self, verbose: bool=False):\n",
    "        for p in self.base_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen!\")\n",
    "    \n",
    "    def freeze_all_except_L2T2_lora_and_g(self, verbose=False):\n",
    "        self.freeze_all(verbose=verbose)\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for n, p in nero_layer.named_parameters():\n",
    "                if 'lora.L2T2.' in n or 'g.' in n:\n",
    "                    p.requires_grad = True\n",
    "                else:\n",
    "                    p.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen except L2T2 LoRA and g layers!\")\n",
    "    \n",
    "    def load_lora_params(self, mode: Literal['L1T1', 'L2T1', 'L2T2'], lora_path: str):\n",
    "        if not os.path.exists(lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] LoRA file not found:\", lora_path)\n",
    "        \n",
    "        if lora_path.endswith('.safetensors'):\n",
    "            state_dict = load_file(lora_path)\n",
    "        else:\n",
    "            state_dict = torch.load(lora_path, map_location='cpu')\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(mode, state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(f\"[INFO] {mode} LoRA parameters loaded successfully!\")\n",
    "        if mode == 'L2T1':\n",
    "            print(f\"[INFO] L2T2 LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self._return_layer_loss:\n",
    "            layer_losses = []\n",
    "            # L2T2_lora_outs = {}\n",
    "            # L2T2_tgts = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                assert isinstance(_out, tuple) and len(_out) == 2\n",
    "                layer_out, layer_loss = _out\n",
    "                layer_losses.append(layer_loss)\n",
    "                return layer_out # Return only `layer_out` to avoid breaking model flow\n",
    "                \n",
    "                # assert isinstance(_out, tuple) and len(_out) == 3\n",
    "                # layer_out, L2T2_lora_out, L2T2_tgt = _out\n",
    "                # L2T2_lora_outs[layer_name] = L2T2_lora_out\n",
    "                # L2T2_tgts[layer_name] = L2T2_tgt\n",
    "                # return layer_out # Return only `layer_out` to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract hidden_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                outputs = self.base_model(*args, **kwargs)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "                    \n",
    "            # Move all `layer_losses` to the same device\n",
    "            layer_losses = [t.to(layer_losses[0].device) for t in layer_losses]\n",
    "            \n",
    "            # Average `layer_losses`\n",
    "            layer_loss = torch.stack(layer_losses).mean()\n",
    "            return outputs, layer_loss\n",
    "            \n",
    "            # return outputs, L2T2_lora_outs, L2T2_tgts\n",
    "        \n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    device_map=device,\n",
    ")\n",
    "model = NeroModel(\n",
    "    base_model, \n",
    "    L1T1_lora_config=model_configs['L1T1']['lora_config'], \n",
    "    L2T1_lora_config=model_configs['L2T1']['lora_config'], \n",
    "    lora_beta=1.0,\n",
    "    return_layer_loss=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded L1T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L1T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 0.002062347484752536\n",
      "- min     : -1.3849756717681885\n",
      "- max     : 1.4044522047042847\n",
      "\n",
      "[INFO] L1T1 LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded L1T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L1T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 6.287686119321734e-05\n",
      "- min     : -0.04176201671361923\n",
      "- max     : 0.04242725297808647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded L1T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L1T1')\n",
    "print()\n",
    "\n",
    "model.load_lora_params('L1T1', model_configs['L1T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L1T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L1T1')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded L2T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L2T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : -0.00022871472174301744\n",
      "- min     : -1.4289981126785278\n",
      "- max     : 1.4715492725372314\n",
      "\n",
      "Check unloaded L2T2 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L2T2.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 6.287686119321734e-05\n",
      "- min     : -0.04176201671361923\n",
      "- max     : 0.04242725297808647\n",
      "\n",
      "[INFO] L2T1 LoRA parameters loaded successfully!\n",
      "[INFO] L2T2 LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded L2T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L2T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 2.2152138626552187e-05\n",
      "- min     : -0.06327299773693085\n",
      "- max     : 0.0625513345003128\n",
      "\n",
      "Check loaded L2T2 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L2T2.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 8.502899436280131e-05\n",
      "- min     : -0.07848924398422241\n",
      "- max     : 0.06970184296369553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded L2T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L2T1')\n",
    "print()\n",
    "\n",
    "print(\"Check unloaded L2T2 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L2T2')\n",
    "print()\n",
    "\n",
    "model.load_lora_params('L2T1', model_configs['L2T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L2T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L2T1')\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L2T2 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L2T2')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.freeze_all_except_L2T2_lora_and_g()\n",
    "print()\n",
    "\n",
    "# check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Debugging is disabled!\n"
     ]
    }
   ],
   "source": [
    "model.set_debug(False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "model.layers.0.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05553344264626503, 1.6210209651035257e-05, -0.04610259830951691)\n",
      "L1T1_lora_out: (0.11106688529253006, 3.242041930207051e-05, -0.09220519661903381)\n",
      "\n",
      "L2T1_lora_B_out: (0.09217894822359085, 0.0002234637358924374, -0.09333131462335587)\n",
      "L2T1_lora_out: (0.1843578964471817, 0.0004469274717848748, -0.18666262924671173)\n",
      "\n",
      "L2T2_lora_B_out: (0.19206136465072632, 0.00032500174711458385, -0.18994779884815216)\n",
      "L2T2_lora_out: (0.38412272930145264, 0.0006500034942291677, -0.3798955976963043)\n",
      "\n",
      "g_out: (0.24636539816856384, 0.000203075964236632, -0.225641667842865)\n",
      "L1T2_out: (0.3418782949447632, 0.0002354963799007237, -0.3017212748527527)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021090906113386154, 2.7187588784727268e-05, -0.023282183334231377)\n",
      "L1T1_lora_out: (0.04218181222677231, 5.4375177569454536e-05, -0.04656436666846275)\n",
      "\n",
      "L2T1_lora_B_out: (0.04138234630227089, -0.0003364249423611909, -0.056755971163511276)\n",
      "L2T1_lora_out: (0.08276469260454178, -0.0006728498847223818, -0.11351194232702255)\n",
      "\n",
      "L2T2_lora_B_out: (0.07309944927692413, -0.0002238736196886748, -0.08251675963401794)\n",
      "L2T2_lora_out: (0.14619889855384827, -0.0004477472393773496, -0.1650335192680359)\n",
      "\n",
      "g_out: (0.08270559459924698, 0.00022510260168928653, -0.11090990155935287)\n",
      "L1T2_out: (0.11278905719518661, 0.00027947782655246556, -0.15747426450252533)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.008716757409274578, -2.8442867915146053e-05, -0.007715492509305477)\n",
      "L1T1_lora_out: (0.017433514818549156, -5.6885735830292106e-05, -0.015430985018610954)\n",
      "\n",
      "L2T1_lora_B_out: (0.005308718420565128, -6.5582985371293034e-06, -0.0052671232260763645)\n",
      "L2T1_lora_out: (0.010617436841130257, -1.3116597074258607e-05, -0.010534246452152729)\n",
      "\n",
      "L2T2_lora_B_out: (0.018929829820990562, -5.26731091667898e-05, -0.018674097955226898)\n",
      "L2T2_lora_out: (0.037859659641981125, -0.0001053462183335796, -0.037348195910453796)\n",
      "\n",
      "g_out: (0.034400779753923416, -9.22296239878051e-05, -0.030437486246228218)\n",
      "L1T2_out: (0.05115383863449097, -0.0001491153525421396, -0.042768776416778564)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.002039884915575385, 2.7272873737160808e-08, -0.002300991676747799)\n",
      "L1T1_lora_out: (0.00407976983115077, 5.4545747474321615e-08, -0.004601983353495598)\n",
      "\n",
      "L2T1_lora_B_out: (0.001961570465937257, 3.2663743354532926e-07, -0.001588845276273787)\n",
      "L2T1_lora_out: (0.003923140931874514, 6.532748670906585e-07, -0.003177690552547574)\n",
      "\n",
      "L2T2_lora_B_out: (0.004872211720794439, 3.788465619436465e-06, -0.005229265429079533)\n",
      "L2T2_lora_out: (0.009744423441588879, 7.57693123887293e-06, -0.010458530858159065)\n",
      "\n",
      "g_out: (0.007881018333137035, 6.923656201252015e-06, -0.00896497629582882)\n",
      "L1T2_out: (0.011398258619010448, 6.978203145990847e-06, -0.013566959649324417)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01898428238928318, 1.4771856513107195e-05, -0.025295084342360497)\n",
      "L1T1_lora_out: (0.03796856477856636, 2.954371302621439e-05, -0.05059016868472099)\n",
      "\n",
      "L2T1_lora_B_out: (0.037945691496133804, 1.9915483790100552e-05, -0.041048843413591385)\n",
      "L2T1_lora_out: (0.07589138299226761, 3.9830967580201104e-05, -0.08209768682718277)\n",
      "\n",
      "L2T2_lora_B_out: (0.0719948336482048, 6.351585761876777e-05, -0.06940541416406631)\n",
      "L2T2_lora_out: (0.1439896672964096, 0.00012703171523753554, -0.13881082832813263)\n",
      "\n",
      "g_out: (0.09989137947559357, 8.720075129531324e-05, -0.11089575290679932)\n",
      "L1T2_out: (0.13428789377212524, 0.00011674446432152763, -0.1614859253168106)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.013516903854906559, 6.480955107690534e-06, -0.013282574713230133)\n",
      "L1T1_lora_out: (0.027033807709813118, 1.2961910215381067e-05, -0.026565149426460266)\n",
      "\n",
      "L2T1_lora_B_out: (0.02384313754737377, -2.433785311950487e-06, -0.033879444003105164)\n",
      "L2T1_lora_out: (0.04768627509474754, -4.867570623900974e-06, -0.06775888800621033)\n",
      "\n",
      "L2T2_lora_B_out: (0.05428127944469452, 5.5916066230565775e-06, -0.05375473201274872)\n",
      "L2T2_lora_out: (0.10856255888938904, 1.1183213246113155e-05, -0.10750946402549744)\n",
      "\n",
      "g_out: (0.07174541056156158, 1.6050786143750884e-05, -0.07475323975086212)\n",
      "L1T2_out: (0.09552304446697235, 2.9012691811658442e-05, -0.10131838917732239)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0016304466407746077, -7.98870019025344e-08, -0.001823857077397406)\n",
      "L1T1_lora_out: (0.0032608932815492153, -1.597740038050688e-07, -0.003647714154794812)\n",
      "\n",
      "L2T1_lora_B_out: (0.0024912874214351177, -8.421272355008114e-07, -0.0027416571974754333)\n",
      "L2T1_lora_out: (0.0049825748428702354, -1.6842544710016227e-06, -0.005483314394950867)\n",
      "\n",
      "L2T2_lora_B_out: (0.005282466299831867, -1.0687510894058505e-06, -0.005711251869797707)\n",
      "L2T2_lora_out: (0.010564932599663734, -2.137502178811701e-06, -0.011422503739595413)\n",
      "\n",
      "g_out: (0.00815303809940815, -4.532473667495651e-07, -0.00850951112806797)\n",
      "L1T2_out: (0.01062263734638691, -6.130214842414716e-07, -0.011884959414601326)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01172013208270073, -8.953748874773737e-06, -0.011408695951104164)\n",
      "L1T1_lora_out: (0.02344026416540146, -1.7907497749547474e-05, -0.022817391902208328)\n",
      "\n",
      "L2T1_lora_B_out: (0.01825328730046749, 6.764401405234821e-06, -0.022329207509756088)\n",
      "L2T1_lora_out: (0.03650657460093498, 1.3528802810469642e-05, -0.044658415019512177)\n",
      "\n",
      "L2T2_lora_B_out: (0.0354950986802578, -2.080786920544142e-08, -0.04150627553462982)\n",
      "L2T2_lora_out: (0.0709901973605156, -4.161573841088284e-08, -0.08301255106925964)\n",
      "\n",
      "g_out: (0.04256168007850647, -1.3570419469033368e-05, -0.048187315464019775)\n",
      "L1T2_out: (0.06593942642211914, -3.147791721858084e-05, -0.06514548510313034)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01800074428319931, 1.1051200090150815e-05, -0.015257058665156364)\n",
      "L1T1_lora_out: (0.03600148856639862, 2.210240018030163e-05, -0.03051411733031273)\n",
      "\n",
      "L2T1_lora_B_out: (0.019778097048401833, -1.565043203299865e-05, -0.01837131194770336)\n",
      "L2T1_lora_out: (0.039556194096803665, -3.13008640659973e-05, -0.03674262389540672)\n",
      "\n",
      "L2T2_lora_B_out: (0.03995342180132866, 5.557259555644123e-06, -0.038232725113630295)\n",
      "L2T2_lora_out: (0.07990684360265732, 1.1114519111288246e-05, -0.07646545022726059)\n",
      "\n",
      "g_out: (0.05741911754012108, 4.2415416828589514e-05, -0.051309458911418915)\n",
      "L1T2_out: (0.09342060983181, 6.451778608607128e-05, -0.07573879510164261)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.011923294514417648, 1.1659920346573927e-05, -0.010881830006837845)\n",
      "L1T1_lora_out: (0.023846589028835297, 2.3319840693147853e-05, -0.02176366001367569)\n",
      "\n",
      "L2T1_lora_B_out: (0.01029124204069376, -4.8349189455620944e-05, -0.01491522416472435)\n",
      "L2T1_lora_out: (0.02058248408138752, -9.669837891124189e-05, -0.0298304483294487)\n",
      "\n",
      "L2T2_lora_B_out: (0.03230058774352074, -2.616176425362937e-05, -0.031559545546770096)\n",
      "L2T2_lora_out: (0.06460117548704147, -5.232352850725874e-05, -0.06311909109354019)\n",
      "\n",
      "g_out: (0.05251679569482803, 4.437484676600434e-05, -0.05130849778652191)\n",
      "L1T2_out: (0.07211045920848846, 6.769468018319458e-05, -0.0730721578001976)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.001594821223989129, -6.733234840794466e-07, -0.0016941074281930923)\n",
      "L1T1_lora_out: (0.003189642447978258, -1.3466469681588933e-06, -0.0033882148563861847)\n",
      "\n",
      "L2T1_lora_B_out: (0.0018925113836303353, 5.032142325944733e-06, -0.0018324304837733507)\n",
      "L2T1_lora_out: (0.0037850227672606707, 1.0064284651889466e-05, -0.0036648609675467014)\n",
      "\n",
      "L2T2_lora_B_out: (0.0042836847715079784, 5.64731362828752e-06, -0.004938152618706226)\n",
      "L2T2_lora_out: (0.008567369543015957, 1.129462725657504e-05, -0.009876305237412453)\n",
      "\n",
      "g_out: (0.005823333747684956, 1.2303413541303598e-06, -0.006981409154832363)\n",
      "L1T2_out: (0.008948098868131638, -1.1630541507656744e-07, -0.00982762686908245)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.035438280552625656, -5.623134711640887e-06, -0.04187031835317612)\n",
      "L1T1_lora_out: (0.07087656110525131, -1.1246269423281774e-05, -0.08374063670635223)\n",
      "\n",
      "L2T1_lora_B_out: (0.17216792702674866, -4.641195118892938e-06, -0.06731774657964706)\n",
      "L2T1_lora_out: (0.3443358540534973, -9.282390237785876e-06, -0.13463549315929413)\n",
      "\n",
      "L2T2_lora_B_out: (0.33747398853302, 1.0429742360429373e-05, -0.1092776209115982)\n",
      "L2T2_lora_out: (0.67494797706604, 2.0859484720858745e-05, -0.2185552418231964)\n",
      "\n",
      "g_out: (0.3306121230125427, 3.0141871320665814e-05, -0.1337558776140213)\n",
      "L1T2_out: (0.40148869156837463, 1.889560189738404e-05, -0.19835582375526428)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07986161857843399, 8.499137038597837e-06, -0.08805125951766968)\n",
      "L1T1_lora_out: (0.15972323715686798, 1.6998274077195674e-05, -0.17610251903533936)\n",
      "\n",
      "L2T1_lora_B_out: (0.08858165144920349, 9.146458978648297e-06, -0.22149918973445892)\n",
      "L2T1_lora_out: (0.17716330289840698, 1.8292917957296595e-05, -0.44299837946891785)\n",
      "\n",
      "L2T2_lora_B_out: (0.24899643659591675, 2.7653863071464002e-05, -0.5718643069267273)\n",
      "L2T2_lora_out: (0.4979928731918335, 5.5307726142928004e-05, -1.1437286138534546)\n",
      "\n",
      "g_out: (0.44782760739326477, 3.701480818563141e-05, -0.710547924041748)\n",
      "L1T2_out: (0.607550859451294, 5.40130895387847e-05, -0.8866504430770874)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01803511567413807, -7.443724570066479e-08, -0.014425413683056831)\n",
      "L1T1_lora_out: (0.03607023134827614, -1.4887449140132958e-07, -0.028850827366113663)\n",
      "\n",
      "L2T1_lora_B_out: (0.02432060055434704, -7.674114499423013e-08, -0.030159344896674156)\n",
      "L2T1_lora_out: (0.04864120110869408, -1.5348228998846025e-07, -0.06031868979334831)\n",
      "\n",
      "L2T2_lora_B_out: (0.02953084371984005, -1.4882260757076438e-06, -0.027911733835935593)\n",
      "L2T2_lora_out: (0.0590616874396801, -2.9764521514152875e-06, -0.055823467671871185)\n",
      "\n",
      "g_out: (0.05270865559577942, -2.822970373017597e-06, -0.052616558969020844)\n",
      "L1T2_out: (0.06425995379686356, -2.971844196508755e-06, -0.05955110490322113)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02885890007019043, -4.0512808482162654e-05, -0.038135964423418045)\n",
      "L1T1_lora_out: (0.05771780014038086, -8.102561696432531e-05, -0.07627192884683609)\n",
      "\n",
      "L2T1_lora_B_out: (0.03840123116970062, 2.7079393476014957e-05, -0.035472799092531204)\n",
      "L2T1_lora_out: (0.07680246233940125, 5.4158786952029914e-05, -0.07094559818506241)\n",
      "\n",
      "L2T2_lora_B_out: (0.08475414663553238, 4.129891749471426e-06, -0.08840376138687134)\n",
      "L2T2_lora_out: (0.16950829327106476, 8.259783498942852e-06, -0.17680752277374268)\n",
      "\n",
      "g_out: (0.1352316290140152, -4.5898999815108255e-05, -0.1504439264535904)\n",
      "L1T2_out: (0.19196389615535736, -0.00012692461314145476, -0.22671586275100708)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02790571190416813, 0.00010092651791637763, -0.029255373403429985)\n",
      "L1T1_lora_out: (0.05581142380833626, 0.00020185303583275527, -0.05851074680685997)\n",
      "\n",
      "L2T1_lora_B_out: (0.04924214631319046, 6.907882925588638e-05, -0.03798004239797592)\n",
      "L2T1_lora_out: (0.09848429262638092, 0.00013815765851177275, -0.07596008479595184)\n",
      "\n",
      "L2T2_lora_B_out: (0.09615590423345566, 0.00022929815168026835, -0.08720651268959045)\n",
      "L2T2_lora_out: (0.19231180846691132, 0.0004585963033605367, -0.1744130253791809)\n",
      "\n",
      "g_out: (0.11940746009349823, 0.00032043864484876394, -0.13924981653690338)\n",
      "L1T2_out: (0.16816513240337372, 0.0005222918116487563, -0.19776056706905365)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03775758668780327, 8.192328823497519e-05, -0.04027705639600754)\n",
      "L1T1_lora_out: (0.07551517337560654, 0.00016384657646995038, -0.08055411279201508)\n",
      "\n",
      "L2T1_lora_B_out: (0.03467440977692604, -0.00010649744945112616, -0.03735525906085968)\n",
      "L2T1_lora_out: (0.06934881955385208, -0.00021299489890225232, -0.07471051812171936)\n",
      "\n",
      "L2T2_lora_B_out: (0.09747166931629181, 5.9568928918452e-06, -0.09949380904436111)\n",
      "L2T2_lora_out: (0.19494333863258362, 1.19137857836904e-05, -0.19898761808872223)\n",
      "\n",
      "g_out: (0.1685420572757721, 0.00022490871197078377, -0.16623875498771667)\n",
      "L1T2_out: (0.24129310250282288, 0.0003887552593369037, -0.24478179216384888)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.002554930280894041, -1.6231209656325518e-06, -0.0024968914221972227)\n",
      "L1T1_lora_out: (0.005109860561788082, -3.2462419312651036e-06, -0.004993782844394445)\n",
      "\n",
      "L2T1_lora_B_out: (0.0021600532345473766, -1.4671767303298111e-06, -0.002435449045151472)\n",
      "L2T1_lora_out: (0.004320106469094753, -2.9343534606596222e-06, -0.004870898090302944)\n",
      "\n",
      "L2T2_lora_B_out: (0.006884487345814705, -6.2252479438029695e-06, -0.006809838116168976)\n",
      "L2T2_lora_out: (0.01376897469162941, -1.2450495887605939e-05, -0.013619676232337952)\n",
      "\n",
      "g_out: (0.01151085365563631, -9.51614129007794e-06, -0.009978268295526505)\n",
      "L1T2_out: (0.016460511833429337, -1.2762383448716719e-05, -0.014972051605582237)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05426162853837013, -9.061553100764286e-06, -0.0495341457426548)\n",
      "L1T1_lora_out: (0.10852325707674026, -1.812310620152857e-05, -0.0990682914853096)\n",
      "\n",
      "L2T1_lora_B_out: (0.04190773144364357, -3.6811488826060668e-06, -0.039410550147295)\n",
      "L2T1_lora_out: (0.08381546288728714, -7.3622977652121335e-06, -0.07882110029459)\n",
      "\n",
      "L2T2_lora_B_out: (0.09696614742279053, -5.8045452533406205e-06, -0.11384453624486923)\n",
      "L2T2_lora_out: (0.19393229484558105, -1.1609090506681241e-05, -0.22768907248973846)\n",
      "\n",
      "g_out: (0.15752077102661133, -4.246795015205862e-06, -0.17707513272762299)\n",
      "L1T2_out: (0.266044020652771, -2.236989894299768e-05, -0.2683793604373932)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02938619814813137, 1.2589806829055306e-05, -0.026104476302862167)\n",
      "L1T1_lora_out: (0.05877239629626274, 2.517961365811061e-05, -0.052208952605724335)\n",
      "\n",
      "L2T1_lora_B_out: (0.04281468689441681, 1.234574392583454e-05, -0.04821781441569328)\n",
      "L2T1_lora_out: (0.08562937378883362, 2.469148785166908e-05, -0.09643562883138657)\n",
      "\n",
      "L2T2_lora_B_out: (0.11592598259449005, 5.579359276453033e-05, -0.118136465549469)\n",
      "L2T2_lora_out: (0.2318519651889801, 0.00011158718552906066, -0.236272931098938)\n",
      "\n",
      "g_out: (0.15518134832382202, 8.689569949638098e-05, -0.13983729481697083)\n",
      "L1T2_out: (0.21329645812511444, 0.000112075314973481, -0.18233099579811096)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0022202979307621717, 2.7153265591550735e-07, -0.0018959525041282177)\n",
      "L1T1_lora_out: (0.0044405958615243435, 5.430653118310147e-07, -0.0037919050082564354)\n",
      "\n",
      "L2T1_lora_B_out: (0.0025689497124403715, -2.0587790459103417e-06, -0.002546723699197173)\n",
      "L2T1_lora_out: (0.005137899424880743, -4.117558091820683e-06, -0.005093447398394346)\n",
      "\n",
      "L2T2_lora_B_out: (0.0050721364095807076, -2.773687356238952e-06, -0.005068464204668999)\n",
      "L2T2_lora_out: (0.010144272819161415, -5.547374712477904e-06, -0.010136928409337997)\n",
      "\n",
      "g_out: (0.006449396722018719, -1.4298165069703828e-06, -0.006841098889708519)\n",
      "L1T2_out: (0.009945565834641457, -8.867507403920172e-07, -0.008799361065030098)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02471054345369339, -6.066600235499209e-06, -0.02435089834034443)\n",
      "L1T1_lora_out: (0.04942108690738678, -1.2133200470998418e-05, -0.04870179668068886)\n",
      "\n",
      "L2T1_lora_B_out: (0.032883863896131516, -3.802396895480342e-05, -0.029929010197520256)\n",
      "L2T1_lora_out: (0.06576772779226303, -7.604793790960684e-05, -0.05985802039504051)\n",
      "\n",
      "L2T2_lora_B_out: (0.061466023325920105, 1.8004659068537876e-05, -0.06568270176649094)\n",
      "L2T2_lora_out: (0.12293204665184021, 3.600931813707575e-05, -0.13136540353298187)\n",
      "\n",
      "g_out: (0.08197910338640213, 0.00011205726332264021, -0.08588510751724243)\n",
      "L1T2_out: (0.1188594251871109, 9.992405830416828e-05, -0.1153445616364479)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020814595744013786, -0.00010258167458232492, -0.02180551365017891)\n",
      "L1T1_lora_out: (0.04162919148802757, -0.00020516334916464984, -0.04361102730035782)\n",
      "\n",
      "L2T1_lora_B_out: (0.026277340948581696, 2.3960046746651642e-05, -0.023542653769254684)\n",
      "L2T1_lora_out: (0.05255468189716339, 4.7920093493303284e-05, -0.04708530753850937)\n",
      "\n",
      "L2T2_lora_B_out: (0.04833308607339859, -0.00022413568513002247, -0.048787813633680344)\n",
      "L2T2_lora_out: (0.09666617214679718, -0.00044827137026004493, -0.09757562726736069)\n",
      "\n",
      "g_out: (0.0790085420012474, -0.0004961914382874966, -0.07878389209508896)\n",
      "L1T2_out: (0.12063772976398468, -0.0007013548165559769, -0.10935775935649872)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.032817043364048004, -1.861005694081541e-05, -0.0286996029317379)\n",
      "L1T1_lora_out: (0.06563408672809601, -3.722011388163082e-05, -0.0573992058634758)\n",
      "\n",
      "L2T1_lora_B_out: (0.028056424111127853, -7.023370562819764e-05, -0.02467294968664646)\n",
      "L2T1_lora_out: (0.05611284822225571, -0.00014046741125639528, -0.04934589937329292)\n",
      "\n",
      "L2T2_lora_B_out: (0.07943899184465408, -0.00017659513105172664, -0.06639089435338974)\n",
      "L2T2_lora_out: (0.15887798368930817, -0.0003531902621034533, -0.13278178870677948)\n",
      "\n",
      "g_out: (0.11417675018310547, -0.00021272293815854937, -0.1129448413848877)\n",
      "L1T2_out: (0.1583009958267212, -0.00024994314298965037, -0.1703440546989441)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0025632288306951523, 4.919336333841784e-06, -0.0025384079199284315)\n",
      "L1T1_lora_out: (0.0051264576613903046, 9.838672667683568e-06, -0.005076815839856863)\n",
      "\n",
      "L2T1_lora_B_out: (0.002943758387118578, -1.1819944347735145e-06, -0.002677879063412547)\n",
      "L2T1_lora_out: (0.005887516774237156, -2.363988869547029e-06, -0.005355758126825094)\n",
      "\n",
      "L2T2_lora_B_out: (0.00561037752777338, 3.4819584016076988e-06, -0.005996878258883953)\n",
      "L2T2_lora_out: (0.01122075505554676, 6.9639168032153975e-06, -0.011993756517767906)\n",
      "\n",
      "g_out: (0.009478750638663769, 9.3279049906414e-06, -0.009083757176995277)\n",
      "L1T2_out: (0.013523129746317863, 1.916657856781967e-05, -0.01299552246928215)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.036799583584070206, -1.9880473701050505e-05, -0.032204143702983856)\n",
      "L1T1_lora_out: (0.07359916716814041, -3.976094740210101e-05, -0.06440828740596771)\n",
      "\n",
      "L2T1_lora_B_out: (0.04628213122487068, -4.549774985207478e-06, -0.042609330266714096)\n",
      "L2T1_lora_out: (0.09256426244974136, -9.099549970414955e-06, -0.08521866053342819)\n",
      "\n",
      "L2T2_lora_B_out: (0.09218225628137589, -8.699504178366624e-06, -0.08862316608428955)\n",
      "L2T2_lora_out: (0.18436451256275177, -1.7399008356733248e-05, -0.1772463321685791)\n",
      "\n",
      "g_out: (0.1377122849225998, -8.299458386318292e-06, -0.12608155608177185)\n",
      "L1T2_out: (0.20851537585258484, -4.80604030599352e-05, -0.18078115582466125)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.027906198054552078, -3.793894393311348e-06, -0.027828045189380646)\n",
      "L1T1_lora_out: (0.055812396109104156, -7.587788786622696e-06, -0.05565609037876129)\n",
      "\n",
      "L2T1_lora_B_out: (0.051507368683815, 5.584398877545027e-06, -0.034547071903944016)\n",
      "L2T1_lora_out: (0.10301473736763, 1.1168797755090054e-05, -0.06909414380788803)\n",
      "\n",
      "L2T2_lora_B_out: (0.10175148397684097, 2.99136604553496e-06, -0.101727694272995)\n",
      "L2T2_lora_out: (0.20350296795368195, 5.98273209106992e-06, -0.20345538854599)\n",
      "\n",
      "g_out: (0.15460476279258728, -5.186062026041327e-06, -0.14947262406349182)\n",
      "L1T2_out: (0.21041715145111084, -1.2773851267411374e-05, -0.1951562911272049)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0022081234492361546, 7.798648766765837e-07, -0.0021584213245660067)\n",
      "L1T1_lora_out: (0.004416246898472309, 1.5597297533531673e-06, -0.004316842649132013)\n",
      "\n",
      "L2T1_lora_B_out: (0.003591955406591296, 1.1202797622900107e-06, -0.003844198305159807)\n",
      "L2T1_lora_out: (0.007183910813182592, 2.2405595245800214e-06, -0.007688396610319614)\n",
      "\n",
      "L2T2_lora_B_out: (0.006061262916773558, 5.211863481235923e-06, -0.007046704180538654)\n",
      "L2T2_lora_out: (0.012122525833547115, 1.0423726962471846e-05, -0.014093408361077309)\n",
      "\n",
      "g_out: (0.008192740380764008, 8.183166755770799e-06, -0.008824345655739307)\n",
      "L1T2_out: (0.011687502264976501, 9.742897418618668e-06, -0.012016136199235916)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023910028859972954, -1.7598933482076973e-05, -0.026727454736828804)\n",
      "L1T1_lora_out: (0.04782005771994591, -3.5197866964153945e-05, -0.05345490947365761)\n",
      "\n",
      "L2T1_lora_B_out: (0.02477695234119892, -1.214111671288265e-05, -0.022603504359722137)\n",
      "L2T1_lora_out: (0.04955390468239784, -2.42822334257653e-05, -0.045207008719444275)\n",
      "\n",
      "L2T2_lora_B_out: (0.049397923052310944, -6.778675015084445e-05, -0.06324946880340576)\n",
      "L2T2_lora_out: (0.09879584610462189, -0.0001355735003016889, -0.12649893760681152)\n",
      "\n",
      "g_out: (0.08874224126338959, -0.00011129127233289182, -0.10348605364561081)\n",
      "L1T2_out: (0.131626695394516, -0.00014648913929704577, -0.14463716745376587)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03009955585002899, 7.146139978431165e-05, -0.02690894342958927)\n",
      "L1T1_lora_out: (0.06019911170005798, 0.0001429227995686233, -0.05381788685917854)\n",
      "\n",
      "L2T1_lora_B_out: (0.03599642589688301, 0.00010010396363213658, -0.03846089541912079)\n",
      "L2T1_lora_out: (0.07199285179376602, 0.00020020792726427317, -0.07692179083824158)\n",
      "\n",
      "L2T2_lora_B_out: (0.10795967280864716, 0.00027593839331530035, -0.08776675164699554)\n",
      "L2T2_lora_out: (0.2159193456172943, 0.0005518767866306007, -0.1755335032939911)\n",
      "\n",
      "g_out: (0.16004078090190887, 0.0003516687429510057, -0.11956740915775299)\n",
      "L1T2_out: (0.21880941092967987, 0.0004945914843119681, -0.16219519078731537)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.025258636102080345, -9.922750905388966e-06, -0.024559805169701576)\n",
      "L1T1_lora_out: (0.05051727220416069, -1.9845501810777932e-05, -0.04911961033940315)\n",
      "\n",
      "L2T1_lora_B_out: (0.029761089012026787, 1.1168842320330441e-05, -0.026272663846611977)\n",
      "L2T1_lora_out: (0.059522178024053574, 2.2337684640660882e-05, -0.05254532769322395)\n",
      "\n",
      "L2T2_lora_B_out: (0.08811594545841217, 1.4301876944955438e-05, -0.06850438565015793)\n",
      "L2T2_lora_out: (0.17623189091682434, 2.8603753889910877e-05, -0.13700877130031586)\n",
      "\n",
      "g_out: (0.12559090554714203, 6.266053333092714e-06, -0.09685736894607544)\n",
      "L1T2_out: (0.16602849960327148, -1.3579439837485552e-05, -0.1308833658695221)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0034384429454803467, 7.948518032208085e-06, -0.003593114670366049)\n",
      "L1T1_lora_out: (0.006876885890960693, 1.589703606441617e-05, -0.007186229340732098)\n",
      "\n",
      "L2T1_lora_B_out: (0.0033825028222054243, -1.193597199744545e-06, -0.003419499611482024)\n",
      "L2T1_lora_out: (0.006765005644410849, -2.38719439948909e-06, -0.006838999222964048)\n",
      "\n",
      "L2T2_lora_B_out: (0.008324059657752514, 2.5781516796996584e-06, -0.006870736368000507)\n",
      "L2T2_lora_out: (0.016648119315505028, 5.156303359399317e-06, -0.013741472736001015)\n",
      "\n",
      "g_out: (0.012142607942223549, 7.543497758888407e-06, -0.01232888363301754)\n",
      "L1T2_out: (0.016342831775546074, 2.3440532459062524e-05, -0.017729276791214943)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.11618562042713165, -2.9502349207177758e-05, -0.08777782320976257)\n",
      "L1T1_lora_out: (0.2323712408542633, -5.9004698414355516e-05, -0.17555564641952515)\n",
      "\n",
      "L2T1_lora_B_out: (0.10700102150440216, 4.938275014865212e-05, -0.08624250441789627)\n",
      "L2T1_lora_out: (0.21400204300880432, 9.876550029730424e-05, -0.17248500883579254)\n",
      "\n",
      "L2T2_lora_B_out: (0.29848188161849976, 3.323000782984309e-05, -0.18792881071567535)\n",
      "L2T2_lora_out: (0.5969637632369995, 6.646001565968618e-05, -0.3758576214313507)\n",
      "\n",
      "g_out: (0.4559113383293152, -3.230549191357568e-05, -0.32911044359207153)\n",
      "L1T2_out: (0.6609867811203003, -9.13101903279312e-05, -0.5046660900115967)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.035594869405031204, -1.3161400602257345e-05, -0.04927007108926773)\n",
      "L1T1_lora_out: (0.07118973881006241, -2.632280120451469e-05, -0.09854014217853546)\n",
      "\n",
      "L2T1_lora_B_out: (0.05311351642012596, -2.534919804020319e-05, -0.044858839362859726)\n",
      "L2T1_lora_out: (0.10622703284025192, -5.069839608040638e-05, -0.08971767872571945)\n",
      "\n",
      "L2T2_lora_B_out: (0.09253702312707901, -3.338147871545516e-05, -0.08786191046237946)\n",
      "L2T2_lora_out: (0.18507404625415802, -6.676295743091032e-05, -0.1757238209247589)\n",
      "\n",
      "g_out: (0.12494123727083206, -1.606455771252513e-05, -0.1537759155035019)\n",
      "L1T2_out: (0.187025785446167, -4.2387357098050416e-05, -0.25231605768203735)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003448296105489135, -1.188795408779697e-06, -0.003914764150977135)\n",
      "L1T1_lora_out: (0.00689659221097827, -2.377590817559394e-06, -0.00782952830195427)\n",
      "\n",
      "L2T1_lora_B_out: (0.0049591888673603535, -1.0510142601560801e-05, -0.005708671174943447)\n",
      "L2T1_lora_out: (0.009918377734720707, -2.1020285203121603e-05, -0.011417342349886894)\n",
      "\n",
      "L2T2_lora_B_out: (0.007301786914467812, -1.0916303835983854e-05, -0.007426594849675894)\n",
      "L2T2_lora_out: (0.014603573828935623, -2.1832607671967708e-05, -0.014853189699351788)\n",
      "\n",
      "g_out: (0.01166580617427826, -8.123236057144823e-07, -0.012213291600346565)\n",
      "L1T2_out: (0.018133169040083885, -3.1899155601422535e-06, -0.020042819902300835)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.030331317335367203, 3.768722081076703e-06, -0.03614136204123497)\n",
      "L1T1_lora_out: (0.060662634670734406, 7.537444162153406e-06, -0.07228272408246994)\n",
      "\n",
      "L2T1_lora_B_out: (0.03567682206630707, 7.0263076850096695e-06, -0.043993301689624786)\n",
      "L2T1_lora_out: (0.07135364413261414, 1.4052615370019339e-05, -0.08798660337924957)\n",
      "\n",
      "L2T2_lora_B_out: (0.10558407008647919, 3.688222204800695e-05, -0.10695276409387589)\n",
      "L2T2_lora_out: (0.21116814017295837, 7.37644440960139e-05, -0.21390552818775177)\n",
      "\n",
      "g_out: (0.15180525183677673, 5.971181963104755e-05, -0.15742096304893494)\n",
      "L1T2_out: (0.20505934953689575, 6.724926788592711e-05, -0.22063136100769043)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.036121249198913574, -0.00017376702453475446, -0.04287801682949066)\n",
      "L1T1_lora_out: (0.07224249839782715, -0.0003475340490695089, -0.08575603365898132)\n",
      "\n",
      "L2T1_lora_B_out: (0.03648152947425842, 8.697164594195783e-05, -0.03426592797040939)\n",
      "L2T1_lora_out: (0.07296305894851685, 0.00017394329188391566, -0.06853185594081879)\n",
      "\n",
      "L2T2_lora_B_out: (0.11745434254407883, -0.00038914085598662496, -0.11399972438812256)\n",
      "L2T2_lora_out: (0.23490868508815765, -0.0007782817119732499, -0.22799944877624512)\n",
      "\n",
      "g_out: (0.1831335425376892, -0.0009522248874418437, -0.18599915504455566)\n",
      "L1T2_out: (0.2550676465034485, -0.001299759023822844, -0.271755188703537)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028515901416540146, 2.466104160703253e-05, -0.02854142151772976)\n",
      "L1T1_lora_out: (0.05703180283308029, 4.932208321406506e-05, -0.05708284303545952)\n",
      "\n",
      "L2T1_lora_B_out: (0.044097043573856354, -8.805830475466792e-06, -0.04114227369427681)\n",
      "L2T1_lora_out: (0.08819408714771271, -1.7611660950933583e-05, -0.08228454738855362)\n",
      "\n",
      "L2T2_lora_B_out: (0.0849551409482956, -5.1031744078500196e-05, -0.07653236389160156)\n",
      "L2T2_lora_out: (0.1699102818965912, -0.00010206348815700039, -0.15306472778320312)\n",
      "\n",
      "g_out: (0.1307809054851532, -8.445182902505621e-05, -0.10076433420181274)\n",
      "L1T2_out: (0.16763874888420105, -3.512974944896996e-05, -0.12834316492080688)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0027056096587330103, -2.6497682483750395e-06, -0.0030488602351397276)\n",
      "L1T1_lora_out: (0.005411219317466021, -5.299536496750079e-06, -0.006097720470279455)\n",
      "\n",
      "L2T1_lora_B_out: (0.004196904134005308, -1.7483248484495562e-06, -0.0040130154229700565)\n",
      "L2T1_lora_out: (0.008393808268010616, -3.4966496968991123e-06, -0.008026030845940113)\n",
      "\n",
      "L2T2_lora_B_out: (0.006485216319561005, -4.7981893658288755e-06, -0.006424025632441044)\n",
      "L2T2_lora_out: (0.01297043263912201, -9.596378731657751e-06, -0.012848051264882088)\n",
      "\n",
      "g_out: (0.010480694472789764, -6.099729034758639e-06, -0.010227756574749947)\n",
      "L1T2_out: (0.014550930820405483, -1.139926644100342e-05, -0.014842242002487183)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04669376462697983, 2.7313428745401325e-06, -0.03961700573563576)\n",
      "L1T1_lora_out: (0.09338752925395966, 5.462685749080265e-06, -0.07923401147127151)\n",
      "\n",
      "L2T1_lora_B_out: (0.088868647813797, -5.854717528563924e-05, -0.09086057543754578)\n",
      "L2T1_lora_out: (0.177737295627594, -0.00011709435057127848, -0.18172115087509155)\n",
      "\n",
      "L2T2_lora_B_out: (0.2219446450471878, -4.499729402596131e-05, -0.2023497223854065)\n",
      "L2T2_lora_out: (0.4438892900943756, -8.999458805192262e-05, -0.404699444770813)\n",
      "\n",
      "g_out: (0.3134869635105133, 2.709977161430288e-05, -0.24380649626255035)\n",
      "L1T2_out: (0.40606003999710083, 3.256244963267818e-05, -0.3099049925804138)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04601989686489105, 4.2000151552201714e-06, -0.03610925003886223)\n",
      "L1T1_lora_out: (0.0920397937297821, 8.400030310440343e-06, -0.07221850007772446)\n",
      "\n",
      "L2T1_lora_B_out: (0.050089240074157715, 3.227060733479448e-05, -0.05228159576654434)\n",
      "L2T1_lora_out: (0.10017848014831543, 6.454121466958895e-05, -0.10456319153308868)\n",
      "\n",
      "L2T2_lora_B_out: (0.11793769896030426, 1.7877102436614223e-05, -0.1138438731431961)\n",
      "L2T2_lora_out: (0.23587539792060852, 3.5754204873228446e-05, -0.2276877462863922)\n",
      "\n",
      "g_out: (0.19602476060390472, -2.87870061583817e-05, -0.15226256847381592)\n",
      "L1T2_out: (0.28806453943252563, -2.0386973119457252e-05, -0.21980372071266174)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004928827751427889, -1.5200104144241777e-06, -0.004682313650846481)\n",
      "L1T1_lora_out: (0.009857655502855778, -3.0400208288483554e-06, -0.009364627301692963)\n",
      "\n",
      "L2T1_lora_B_out: (0.006687183398753405, -5.711239282391034e-06, -0.005919489543884993)\n",
      "L2T1_lora_out: (0.01337436679750681, -1.1422478564782068e-05, -0.011838979087769985)\n",
      "\n",
      "L2T2_lora_B_out: (0.0082941809669137, -1.375893407384865e-05, -0.00930382777005434)\n",
      "L2T2_lora_out: (0.0165883619338274, -2.75178681476973e-05, -0.01860765554010868)\n",
      "\n",
      "g_out: (0.013122850097715855, -1.6095391401904635e-05, -0.01436952780932188)\n",
      "L1T2_out: (0.020307162776589394, -1.9135412003379315e-05, -0.022091485559940338)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02420334331691265, 5.941828203503974e-05, -0.027100272476673126)\n",
      "L1T1_lora_out: (0.0484066866338253, 0.00011883656407007948, -0.05420054495334625)\n",
      "\n",
      "L2T1_lora_B_out: (0.03366856276988983, 1.5140180039452389e-05, -0.03191910311579704)\n",
      "L2T1_lora_out: (0.06733712553977966, 3.0280360078904778e-05, -0.06383820623159409)\n",
      "\n",
      "L2T2_lora_B_out: (0.06727517396211624, 8.311576675623655e-05, -0.07457327842712402)\n",
      "L2T2_lora_out: (0.13455034792423248, 0.0001662315335124731, -0.14914655685424805)\n",
      "\n",
      "g_out: (0.10773380100727081, 0.00013595116615761071, -0.10995949804782867)\n",
      "L1T2_out: (0.1448088139295578, 0.0002547877375036478, -0.14512252807617188)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04454682022333145, -0.00015358635573647916, -0.05297957733273506)\n",
      "L1T1_lora_out: (0.0890936404466629, -0.0003071727114729583, -0.10595915466547012)\n",
      "\n",
      "L2T1_lora_B_out: (0.06481614708900452, 0.00033987528877332807, -0.07621068507432938)\n",
      "L2T1_lora_out: (0.12963229417800903, 0.0006797505775466561, -0.15242137014865875)\n",
      "\n",
      "L2T2_lora_B_out: (0.12132171541452408, 0.00038500953814946115, -0.1489587426185608)\n",
      "L2T2_lora_out: (0.24264343082904816, 0.0007700190762989223, -0.2979174852371216)\n",
      "\n",
      "g_out: (0.18598034977912903, 9.026858606375754e-05, -0.19734537601470947)\n",
      "L1T2_out: (0.26176226139068604, -0.00021690389257855713, -0.30135631561279297)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02685813419520855, 1.5511419405811466e-05, -0.02318452298641205)\n",
      "L1T1_lora_out: (0.0537162683904171, 3.102283881162293e-05, -0.0463690459728241)\n",
      "\n",
      "L2T1_lora_B_out: (0.034195877611637115, 0.00014859635848551989, -0.029120925813913345)\n",
      "L2T1_lora_out: (0.06839175522327423, 0.00029719271697103977, -0.05824185162782669)\n",
      "\n",
      "L2T2_lora_B_out: (0.07457640767097473, 0.00017493218183517456, -0.06430435925722122)\n",
      "L2T2_lora_out: (0.14915281534194946, 0.0003498643636703491, -0.12860871851444244)\n",
      "\n",
      "g_out: (0.11080627143383026, 5.267159576760605e-05, -0.0948953926563263)\n",
      "L1T2_out: (0.14660651981830597, 8.369443094125018e-05, -0.1336406171321869)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006194094195961952, 9.23356680004872e-08, -0.005985099822282791)\n",
      "L1T1_lora_out: (0.012388188391923904, 1.846713360009744e-07, -0.011970199644565582)\n",
      "\n",
      "L2T1_lora_B_out: (0.005754801910370588, -2.2981755591899855e-06, -0.005560401361435652)\n",
      "L2T1_lora_out: (0.011509603820741177, -4.596351118379971e-06, -0.011120802722871304)\n",
      "\n",
      "L2T2_lora_B_out: (0.01605919934809208, -6.6505767790658865e-06, -0.015538199804723263)\n",
      "L2T2_lora_out: (0.03211839869618416, -1.3301153558131773e-05, -0.031076399609446526)\n",
      "\n",
      "g_out: (0.025792382657527924, -8.704800166015048e-06, -0.026357652619481087)\n",
      "L1T2_out: (0.03714117035269737, -8.520129085809458e-06, -0.037800028920173645)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06188548356294632, -6.098126323195174e-05, -0.051097236573696136)\n",
      "L1T1_lora_out: (0.12377096712589264, -0.00012196252646390349, -0.10219447314739227)\n",
      "\n",
      "L2T1_lora_B_out: (0.06920517235994339, -0.00017211795784533024, -0.08457256853580475)\n",
      "L2T1_lora_out: (0.13841034471988678, -0.0003442359156906605, -0.1691451370716095)\n",
      "\n",
      "L2T2_lora_B_out: (0.1339404582977295, -0.00029493545298464596, -0.16667243838310242)\n",
      "L2T2_lora_out: (0.267880916595459, -0.0005898709059692919, -0.33334487676620483)\n",
      "\n",
      "g_out: (0.23537084460258484, -0.0002456350193824619, -0.19427403807640076)\n",
      "L1T2_out: (0.35203468799591064, -0.0003675975603982806, -0.29394957423210144)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05449675768613815, 1.1893120245076716e-05, -0.04774424806237221)\n",
      "L1T1_lora_out: (0.1089935153722763, 2.3786240490153432e-05, -0.09548849612474442)\n",
      "\n",
      "L2T1_lora_B_out: (0.03519328683614731, 2.495611806807574e-05, -0.031811803579330444)\n",
      "L2T1_lora_out: (0.07038657367229462, 4.991223613615148e-05, -0.06362360715866089)\n",
      "\n",
      "L2T2_lora_B_out: (0.12898889183998108, 7.816345168976113e-05, -0.1456470489501953)\n",
      "L2T2_lora_out: (0.25797778367996216, 0.00015632690337952226, -0.2912940979003906)\n",
      "\n",
      "g_out: (0.19159863889217377, 0.00010641467815730721, -0.22896882891654968)\n",
      "L1T2_out: (0.2915005385875702, 0.00013020091864746064, -0.3223862051963806)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004632718861103058, 2.7165938263351563e-06, -0.004591894801706076)\n",
      "L1T1_lora_out: (0.009265437722206116, 5.433187652670313e-06, -0.009183789603412151)\n",
      "\n",
      "L2T1_lora_B_out: (0.007162030320614576, 1.2715511275018798e-06, -0.006899242289364338)\n",
      "L2T1_lora_out: (0.014324060641229153, 2.5431022550037596e-06, -0.013798484578728676)\n",
      "\n",
      "L2T2_lora_B_out: (0.012347200885415077, -1.7544107322464697e-06, -0.013750044628977776)\n",
      "L2T2_lora_out: (0.024694401770830154, -3.5088214644929394e-06, -0.02750008925795555)\n",
      "\n",
      "g_out: (0.016982553526759148, -6.051923264749348e-06, -0.014275936409831047)\n",
      "L1T2_out: (0.02560775727033615, -6.187342478369828e-07, -0.021212756633758545)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.022107265889644623, -2.6772133423946798e-05, -0.021921567618846893)\n",
      "L1T1_lora_out: (0.044214531779289246, -5.3544266847893596e-05, -0.04384313523769379)\n",
      "\n",
      "L2T1_lora_B_out: (0.038948334753513336, 4.0519324102206156e-05, -0.03296399861574173)\n",
      "L2T1_lora_out: (0.07789666950702667, 8.103864820441231e-05, -0.06592799723148346)\n",
      "\n",
      "L2T2_lora_B_out: (0.06274008750915527, -3.3549972613400314e-06, -0.06654779613018036)\n",
      "L2T2_lora_out: (0.12548017501831055, -6.709994522680063e-06, -0.13309559226036072)\n",
      "\n",
      "g_out: (0.08100569248199463, -8.774863817961887e-05, -0.10166339576244354)\n",
      "L1T2_out: (0.10631400346755981, -0.00014129291230347008, -0.13419172167778015)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.034883007407188416, -3.8323664739436936e-06, -0.03303055837750435)\n",
      "L1T1_lora_out: (0.06976601481437683, -7.664732947887387e-06, -0.0660611167550087)\n",
      "\n",
      "L2T1_lora_B_out: (0.042216673493385315, -0.00017554369696881622, -0.0405874103307724)\n",
      "L2T1_lora_out: (0.08443334698677063, -0.00035108739393763244, -0.0811748206615448)\n",
      "\n",
      "L2T2_lora_B_out: (0.07995783537626266, -2.17390861507738e-05, -0.07821432501077652)\n",
      "L2T2_lora_out: (0.15991567075252533, -4.34781723015476e-05, -0.15642865002155304)\n",
      "\n",
      "g_out: (0.1398344486951828, 0.00030760932713747025, -0.12739120423793793)\n",
      "L1T2_out: (0.20960046350955963, 0.0002999446587637067, -0.1837008148431778)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.025343535467982292, -6.624609522987157e-05, -0.024778245016932487)\n",
      "L1T1_lora_out: (0.050687070935964584, -0.00013249219045974314, -0.049556490033864975)\n",
      "\n",
      "L2T1_lora_B_out: (0.021769922226667404, -7.195227954071015e-05, -0.022848745808005333)\n",
      "L2T1_lora_out: (0.04353984445333481, -0.0001439045590814203, -0.045697491616010666)\n",
      "\n",
      "L2T2_lora_B_out: (0.05072982609272003, -0.0003628023259807378, -0.04626934602856636)\n",
      "L2T2_lora_out: (0.10145965218544006, -0.0007256046519614756, -0.09253869205713272)\n",
      "\n",
      "g_out: (0.0875234454870224, -0.0005817001219838858, -0.07685451954603195)\n",
      "L1T2_out: (0.13821052014827728, -0.0007141922833397985, -0.12641100585460663)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005217822268605232, -1.0102052328875288e-05, -0.005276219453662634)\n",
      "L1T1_lora_out: (0.010435644537210464, -2.0204104657750577e-05, -0.010552438907325268)\n",
      "\n",
      "L2T1_lora_B_out: (0.004839321598410606, 2.415099515928887e-06, -0.004758988507091999)\n",
      "L2T1_lora_out: (0.009678643196821213, 4.830199031857774e-06, -0.009517977014183998)\n",
      "\n",
      "L2T2_lora_B_out: (0.014050892554223537, -1.2135013093939051e-05, -0.016488857567310333)\n",
      "L2T2_lora_out: (0.028101785108447075, -2.4270026187878102e-05, -0.032977715134620667)\n",
      "\n",
      "g_out: (0.023612357676029205, -2.910022703872528e-05, -0.026919057592749596)\n",
      "L1T2_out: (0.03404800221323967, -4.9304326239507645e-05, -0.03747149556875229)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08080175518989563, -0.00012304347183089703, -0.06934843957424164)\n",
      "L1T1_lora_out: (0.16160351037979126, -0.00024608694366179407, -0.13869687914848328)\n",
      "\n",
      "L2T1_lora_B_out: (0.14568357169628143, -0.0001650170743232593, -0.07796310633420944)\n",
      "L2T1_lora_out: (0.29136714339256287, -0.0003300341486465186, -0.15592621266841888)\n",
      "\n",
      "L2T2_lora_B_out: (0.28511151671409607, -0.00048525293823331594, -0.2350790947675705)\n",
      "L2T2_lora_out: (0.5702230334281921, -0.0009705058764666319, -0.470158189535141)\n",
      "\n",
      "g_out: (0.38623905181884766, -0.0006404717569239438, -0.3373225927352905)\n",
      "L1T2_out: (0.5163752436637878, -0.0008865586714819074, -0.4567711353302002)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02134857326745987, 2.0283061530790292e-05, -0.02086424082517624)\n",
      "L1T1_lora_out: (0.04269714653491974, 4.0566123061580583e-05, -0.04172848165035248)\n",
      "\n",
      "L2T1_lora_B_out: (0.05190253257751465, 2.0652098555729026e-06, -0.03937901183962822)\n",
      "L2T1_lora_out: (0.1038050651550293, 4.130419711145805e-06, -0.07875802367925644)\n",
      "\n",
      "L2T2_lora_B_out: (0.08159142732620239, 5.9654306824086234e-05, -0.06311306357383728)\n",
      "L2T2_lora_out: (0.16318285465240479, 0.00011930861364817247, -0.12622612714767456)\n",
      "\n",
      "g_out: (0.08935408294200897, 0.0001151781907537952, -0.08391240984201431)\n",
      "L1T2_out: (0.1197446659207344, 0.00015574433200526983, -0.11492182314395905)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00935362558811903, -6.647497343692521e-07, -0.009664230048656464)\n",
      "L1T1_lora_out: (0.01870725117623806, -1.3294994687385042e-06, -0.019328460097312927)\n",
      "\n",
      "L2T1_lora_B_out: (0.009406883269548416, 1.6946906953307916e-06, -0.008595718070864677)\n",
      "L2T1_lora_out: (0.018813766539096832, 3.3893813906615833e-06, -0.017191436141729355)\n",
      "\n",
      "L2T2_lora_B_out: (0.016334502026438713, 5.268642780720256e-06, -0.019948754459619522)\n",
      "L2T2_lora_out: (0.032669004052877426, 1.0537285561440513e-05, -0.039897508919239044)\n",
      "\n",
      "g_out: (0.02702859230339527, 7.147903488657903e-06, -0.032679907977581024)\n",
      "L1T2_out: (0.04528086632490158, 5.8184036788588855e-06, -0.05200836807489395)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.031201424077153206, -6.543848030560184e-06, -0.02680147998034954)\n",
      "L1T1_lora_out: (0.06240284815430641, -1.3087696061120369e-05, -0.05360295996069908)\n",
      "\n",
      "L2T1_lora_B_out: (0.03561747819185257, 2.2461998014478013e-05, -0.03043636679649353)\n",
      "L2T1_lora_out: (0.07123495638370514, 4.4923996028956026e-05, -0.06087273359298706)\n",
      "\n",
      "L2T2_lora_B_out: (0.0577031709253788, 5.1124799938406795e-05, -0.06299883127212524)\n",
      "L2T2_lora_out: (0.1154063418507576, 0.00010224959987681359, -0.1259976625442505)\n",
      "\n",
      "g_out: (0.10400328785181046, 5.732560020987876e-05, -0.09594520181417465)\n",
      "L1T2_out: (0.16007184982299805, 4.423790596774779e-05, -0.14862176775932312)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0536048598587513, 0.0001951325248228386, -0.05533554404973984)\n",
      "L1T1_lora_out: (0.1072097197175026, 0.0003902650496456772, -0.11067108809947968)\n",
      "\n",
      "L2T1_lora_B_out: (0.050952546298503876, 0.00036246937816031277, -0.05250927433371544)\n",
      "L2T1_lora_out: (0.10190509259700775, 0.0007249387563206255, -0.10501854866743088)\n",
      "\n",
      "L2T2_lora_B_out: (0.1291014403104782, 0.0009011983056552708, -0.11732889711856842)\n",
      "L2T2_lora_out: (0.2582028806209564, 0.0018023966113105416, -0.23465779423713684)\n",
      "\n",
      "g_out: (0.20796842873096466, 0.0010774577967822552, -0.18559888005256653)\n",
      "L1T2_out: (0.3009154200553894, 0.0014677231665700674, -0.2884405553340912)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.026675676926970482, -4.6163851948222145e-05, -0.02217327430844307)\n",
      "L1T1_lora_out: (0.053351353853940964, -9.232770389644429e-05, -0.04434654861688614)\n",
      "\n",
      "L2T1_lora_B_out: (0.02777557447552681, 4.1484463508822955e-06, -0.031076548621058464)\n",
      "L2T1_lora_out: (0.05555114895105362, 8.296892701764591e-06, -0.06215309724211693)\n",
      "\n",
      "L2T2_lora_B_out: (0.06100962683558464, -6.64948092889972e-05, -0.057592976838350296)\n",
      "L2T2_lora_out: (0.12201925367116928, -0.0001329896185779944, -0.11518595367670059)\n",
      "\n",
      "g_out: (0.08877614140510559, -0.00014128649490885437, -0.09156370908021927)\n",
      "L1T2_out: (0.12561243772506714, -0.00023361419152934104, -0.13384591042995453)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0044923038221895695, 2.598466380732134e-06, -0.00442891102284193)\n",
      "L1T1_lora_out: (0.008984607644379139, 5.196932761464268e-06, -0.00885782204568386)\n",
      "\n",
      "L2T1_lora_B_out: (0.007168498821556568, 4.859149385083583e-07, -0.008076092228293419)\n",
      "L2T1_lora_out: (0.014336997643113136, 9.718298770167166e-07, -0.016152184456586838)\n",
      "\n",
      "L2T2_lora_B_out: (0.01476274710148573, 7.53777430873015e-06, -0.015231112949550152)\n",
      "L2T2_lora_out: (0.02952549420297146, 1.50755486174603e-05, -0.030462225899100304)\n",
      "\n",
      "g_out: (0.025305993854999542, 1.4103716239333153e-05, -0.024599451571702957)\n",
      "L1T2_out: (0.030041664838790894, 1.9300650819786824e-05, -0.03126627951860428)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07448075711727142, -2.461113217577804e-05, -0.12082526832818985)\n",
      "L1T1_lora_out: (0.14896151423454285, -4.922226435155608e-05, -0.2416505366563797)\n",
      "\n",
      "L2T1_lora_B_out: (0.11659680306911469, -0.00013844325440004468, -0.09639643132686615)\n",
      "L2T1_lora_out: (0.23319360613822937, -0.00027688650880008936, -0.1927928626537323)\n",
      "\n",
      "L2T2_lora_B_out: (0.11525382846593857, -0.0001755625708028674, -0.1297798454761505)\n",
      "L2T2_lora_out: (0.23050765693187714, -0.0003511251416057348, -0.259559690952301)\n",
      "\n",
      "g_out: (0.15004874765872955, -7.423863280564547e-05, -0.1508825719356537)\n",
      "L1T2_out: (0.2295634001493454, -0.00012346089351922274, -0.30473846197128296)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04874177277088165, -2.9685488698305562e-05, -0.04425317049026489)\n",
      "L1T1_lora_out: (0.0974835455417633, -5.9370977396611124e-05, -0.08850634098052979)\n",
      "\n",
      "L2T1_lora_B_out: (0.03156506270170212, 1.9221426555304788e-05, -0.03276633098721504)\n",
      "L2T1_lora_out: (0.06313012540340424, 3.8442853110609576e-05, -0.06553266197443008)\n",
      "\n",
      "L2T2_lora_B_out: (0.12365081906318665, -1.9361659724381752e-05, -0.10269826650619507)\n",
      "L2T2_lora_out: (0.2473016381263733, -3.8723319448763505e-05, -0.20539653301239014)\n",
      "\n",
      "g_out: (0.20541918277740479, -7.716617255937308e-05, -0.17688415944576263)\n",
      "L1T2_out: (0.30005162954330444, -0.00013653715723194182, -0.25147774815559387)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.007352892775088549, -9.628829502617009e-06, -0.009127439931035042)\n",
      "L1T1_lora_out: (0.014705785550177097, -1.9257659005234018e-05, -0.018254879862070084)\n",
      "\n",
      "L2T1_lora_B_out: (0.006626942194998264, 6.520331226056442e-06, -0.007083873264491558)\n",
      "L2T1_lora_out: (0.013253884389996529, 1.3040662452112883e-05, -0.014167746528983116)\n",
      "\n",
      "L2T2_lora_B_out: (0.015278383158147335, -7.137865054573922e-07, -0.017493002116680145)\n",
      "L2T2_lora_out: (0.03055676631629467, -1.4275730109147844e-06, -0.03498600423336029)\n",
      "\n",
      "g_out: (0.023317014798521996, -1.4468234439846128e-05, -0.028485016897320747)\n",
      "L1T2_out: (0.03689715266227722, -3.3725893445080146e-05, -0.04673989862203598)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023221146315336227, 3.920333620044403e-05, -0.02384421043097973)\n",
      "L1T1_lora_out: (0.046442292630672455, 7.840667240088806e-05, -0.04768842086195946)\n",
      "\n",
      "L2T1_lora_B_out: (0.03497634455561638, 2.706685791054042e-06, -0.029194300994277)\n",
      "L2T1_lora_out: (0.06995268911123276, 5.413371582108084e-06, -0.058388601988554)\n",
      "\n",
      "L2T2_lora_B_out: (0.06569202989339828, 6.368241156451404e-05, -0.0578715018928051)\n",
      "L2T2_lora_out: (0.13138405978679657, 0.00012736482312902808, -0.1157430037856102)\n",
      "\n",
      "g_out: (0.10961571335792542, 0.00012195145973237231, -0.09530849754810333)\n",
      "L1T2_out: (0.15244720876216888, 0.0002003581466851756, -0.12730009853839874)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07561103999614716, -0.001308368518948555, -0.08162420988082886)\n",
      "L1T1_lora_out: (0.1512220799922943, -0.00261673703789711, -0.16324841976165771)\n",
      "\n",
      "L2T1_lora_B_out: (0.0484723299741745, -0.00035808520624414086, -0.05501113086938858)\n",
      "L2T1_lora_out: (0.096944659948349, -0.0007161704124882817, -0.11002226173877716)\n",
      "\n",
      "L2T2_lora_B_out: (0.1654880940914154, -0.002351700095459819, -0.19777975976467133)\n",
      "L2T2_lora_out: (0.3309761881828308, -0.004703400190919638, -0.39555951952934265)\n",
      "\n",
      "g_out: (0.271760493516922, -0.0039872294291853905, -0.3356364965438843)\n",
      "L1T2_out: (0.4023272693157196, -0.006603966932743788, -0.498884916305542)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.034258149564266205, 0.00012003663141513243, -0.03203266113996506)\n",
      "L1T1_lora_out: (0.06851629912853241, 0.00024007326283026487, -0.06406532227993011)\n",
      "\n",
      "L2T1_lora_B_out: (0.02820584364235401, 6.383552681654692e-05, -0.024827294051647186)\n",
      "L2T1_lora_out: (0.05641168728470802, 0.00012767105363309383, -0.04965458810329437)\n",
      "\n",
      "L2T2_lora_B_out: (0.06921287626028061, 0.0002679722383618355, -0.06140770763158798)\n",
      "L2T2_lora_out: (0.13842575252056122, 0.000535944476723671, -0.12281541526317596)\n",
      "\n",
      "g_out: (0.09922041743993759, 0.00040827333577908576, -0.1108439564704895)\n",
      "L1T2_out: (0.15853966772556305, 0.0006483465549536049, -0.15065491199493408)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00540597178041935, -4.576660558086587e-06, -0.004769422113895416)\n",
      "L1T1_lora_out: (0.0108119435608387, -9.153321116173174e-06, -0.009538844227790833)\n",
      "\n",
      "L2T1_lora_B_out: (0.00616677338257432, 4.397703833092237e-06, -0.006570597644895315)\n",
      "L2T1_lora_out: (0.01233354676514864, 8.795407666184474e-06, -0.01314119528979063)\n",
      "\n",
      "L2T2_lora_B_out: (0.015183378010988235, 1.4192085473041516e-05, -0.013368924148380756)\n",
      "L2T2_lora_out: (0.03036675602197647, 2.838417094608303e-05, -0.026737848296761513)\n",
      "\n",
      "g_out: (0.020670419558882713, 1.9588766008382663e-05, -0.02076929621398449)\n",
      "L1T2_out: (0.02760685794055462, 1.0435442163725384e-05, -0.0276511050760746)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07825888693332672, -4.103761602891609e-05, -0.10712944716215134)\n",
      "L1T1_lora_out: (0.15651777386665344, -8.207523205783218e-05, -0.21425889432430267)\n",
      "\n",
      "L2T1_lora_B_out: (0.17225958406925201, -0.00019474081636872143, -0.10197509825229645)\n",
      "L2T1_lora_out: (0.34451916813850403, -0.00038948163273744285, -0.2039501965045929)\n",
      "\n",
      "L2T2_lora_B_out: (0.3147418200969696, -0.0004142125544603914, -0.35853976011276245)\n",
      "L2T2_lora_out: (0.6294836401939392, -0.0008284251089207828, -0.7170795202255249)\n",
      "\n",
      "g_out: (0.3453606069087982, -0.0004389433888718486, -0.5722788572311401)\n",
      "L1T2_out: (0.49822431802749634, -0.000521018635481596, -0.786537766456604)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.040012434124946594, -2.120486351486761e-05, -0.03262348100543022)\n",
      "L1T1_lora_out: (0.08002486824989319, -4.240972702973522e-05, -0.06524696201086044)\n",
      "\n",
      "L2T1_lora_B_out: (0.04330480098724365, 1.401098143105628e-05, -0.03980090096592903)\n",
      "L2T1_lora_out: (0.0866096019744873, 2.802196286211256e-05, -0.07960180193185806)\n",
      "\n",
      "L2T2_lora_B_out: (0.13524916768074036, -1.3222976122051477e-05, -0.10377398133277893)\n",
      "L2T2_lora_out: (0.2704983353614807, -2.6445952244102955e-05, -0.20754796266555786)\n",
      "\n",
      "g_out: (0.21546325087547302, -5.446791328722611e-05, -0.14651253819465637)\n",
      "L1T2_out: (0.2954881191253662, -9.687763667898253e-05, -0.20335417985916138)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004896487109363079, -3.084240063344623e-07, -0.005122439004480839)\n",
      "L1T1_lora_out: (0.009792974218726158, -6.168480126689246e-07, -0.010244878008961678)\n",
      "\n",
      "L2T1_lora_B_out: (0.005726476665586233, 1.0266639037581626e-05, -0.007005862891674042)\n",
      "L2T1_lora_out: (0.011452953331172466, 2.0533278075163253e-05, -0.014011725783348083)\n",
      "\n",
      "L2T2_lora_B_out: (0.014647071249783039, 1.3179020243114792e-05, -0.017510682344436646)\n",
      "L2T2_lora_out: (0.029294142499566078, 2.6358040486229584e-05, -0.03502136468887329)\n",
      "\n",
      "g_out: (0.021373199298977852, 5.824763320561033e-06, -0.02113978937268257)\n",
      "L1T2_out: (0.030316878110170364, 5.207915364735527e-06, -0.028555069118738174)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03436427563428879, 6.628113169426797e-06, -0.02789800800383091)\n",
      "L1T1_lora_out: (0.06872855126857758, 1.3256226338853594e-05, -0.05579601600766182)\n",
      "\n",
      "L2T1_lora_B_out: (0.04675653576850891, -0.00014655203267466277, -0.05085670202970505)\n",
      "L2T1_lora_out: (0.09351307153701782, -0.00029310406534932554, -0.1017134040594101)\n",
      "\n",
      "L2T2_lora_B_out: (0.10187902301549911, -0.0001857914903666824, -0.09369159489870071)\n",
      "L2T2_lora_out: (0.20375804603099823, -0.0003715829807333648, -0.18738318979740143)\n",
      "\n",
      "g_out: (0.16523881256580353, -7.847892993595451e-05, -0.14562895894050598)\n",
      "L1T2_out: (0.22838130593299866, -6.522270268760622e-05, -0.1893180012702942)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05049016326665878, -9.987196790461894e-06, -0.05360396206378937)\n",
      "L1T1_lora_out: (0.10098032653331757, -1.9974393580923788e-05, -0.10720792412757874)\n",
      "\n",
      "L2T1_lora_B_out: (0.07769642770290375, 0.0002993993985000998, -0.04863567277789116)\n",
      "L2T1_lora_out: (0.1553928554058075, 0.0005987987970001996, -0.09727134555578232)\n",
      "\n",
      "L2T2_lora_B_out: (0.16392917931079865, 0.0005814484320580959, -0.1248617172241211)\n",
      "L2T2_lora_out: (0.3278583586215973, 0.0011628968641161919, -0.2497234344482422)\n",
      "\n",
      "g_out: (0.2138325572013855, 0.000564098241738975, -0.21490052342414856)\n",
      "L1T2_out: (0.30398523807525635, 0.0005441235261969268, -0.3221084475517273)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020618168637156487, -2.0147247425938986e-07, -0.02187984623014927)\n",
      "L1T1_lora_out: (0.04123633727431297, -4.029449485187797e-07, -0.04375969246029854)\n",
      "\n",
      "L2T1_lora_B_out: (0.025810392573475838, 4.239507325110026e-05, -0.03505733236670494)\n",
      "L2T1_lora_out: (0.051620785146951675, 8.479014650220051e-05, -0.07011466473340988)\n",
      "\n",
      "L2T2_lora_B_out: (0.04819038137793541, 0.00014810339780524373, -0.06410836428403854)\n",
      "L2T2_lora_out: (0.09638076275587082, 0.00029620679561048746, -0.1282167285680771)\n",
      "\n",
      "g_out: (0.08796501904726028, 0.00021141659817658365, -0.09872384369373322)\n",
      "L1T2_out: (0.12786664068698883, 0.00021101368474774063, -0.13691040873527527)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005771771539002657, 5.033904926676769e-06, -0.006956573110073805)\n",
      "L1T1_lora_out: (0.011543543078005314, 1.0067809853353538e-05, -0.01391314622014761)\n",
      "\n",
      "L2T1_lora_B_out: (0.0065839276649057865, -1.0458213637321023e-06, -0.006213848479092121)\n",
      "L2T1_lora_out: (0.013167855329811573, -2.0916427274642047e-06, -0.012427696958184242)\n",
      "\n",
      "L2T2_lora_B_out: (0.01173882745206356, 5.7469287639833055e-06, -0.012656599283218384)\n",
      "L2T2_lora_out: (0.02347765490412712, 1.1493857527966611e-05, -0.025313198566436768)\n",
      "\n",
      "g_out: (0.020811215043067932, 1.3585501619672868e-05, -0.018408725038170815)\n",
      "L1T2_out: (0.029576348140835762, 2.36533087445423e-05, -0.029783785343170166)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0774712860584259, -0.00014668716175947338, -0.08706516772508621)\n",
      "L1T1_lora_out: (0.1549425721168518, -0.00029337432351894677, -0.17413033545017242)\n",
      "\n",
      "L2T1_lora_B_out: (0.07956113666296005, -0.00025140208890661597, -0.08160490542650223)\n",
      "L2T1_lora_out: (0.1591222733259201, -0.0005028041778132319, -0.16320981085300446)\n",
      "\n",
      "L2T2_lora_B_out: (0.19308456778526306, -0.0005673614214174449, -0.2424253672361374)\n",
      "L2T2_lora_out: (0.3861691355705261, -0.0011347228428348899, -0.4848507344722748)\n",
      "\n",
      "g_out: (0.3057355582714081, -0.0006319186650216579, -0.34845906496047974)\n",
      "L1T2_out: (0.4425920248031616, -0.0009252930176444352, -0.5156744718551636)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.025696687400341034, 1.113969847210683e-05, -0.024920247495174408)\n",
      "L1T1_lora_out: (0.05139337480068207, 2.227939694421366e-05, -0.049840494990348816)\n",
      "\n",
      "L2T1_lora_B_out: (0.02655324712395668, 1.3112050964991795e-06, -0.027597269043326378)\n",
      "L2T1_lora_out: (0.05310649424791336, 2.622410192998359e-06, -0.055194538086652756)\n",
      "\n",
      "L2T2_lora_B_out: (0.06407637149095535, 2.696916453714948e-05, -0.06376319378614426)\n",
      "L2T2_lora_out: (0.1281527429819107, 5.393832907429896e-05, -0.1275263875722885)\n",
      "\n",
      "g_out: (0.0983346477150917, 5.131591751705855e-05, -0.09532766044139862)\n",
      "L1T2_out: (0.14379334449768066, 7.359531446127221e-05, -0.13776874542236328)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.009716859087347984, 2.8671693144133314e-06, -0.00907098688185215)\n",
      "L1T1_lora_out: (0.01943371817469597, 5.734338628826663e-06, -0.0181419737637043)\n",
      "\n",
      "L2T1_lora_B_out: (0.007542244158685207, 1.2694218639808241e-05, -0.00709028635174036)\n",
      "L2T1_lora_out: (0.015084488317370415, 2.5388437279616483e-05, -0.01418057270348072)\n",
      "\n",
      "L2T2_lora_B_out: (0.02222992666065693, 2.6520961910136975e-05, -0.022241495549678802)\n",
      "L2T2_lora_out: (0.04445985332131386, 5.304192382027395e-05, -0.044482991099357605)\n",
      "\n",
      "g_out: (0.03843070566654205, 2.765348835964687e-05, -0.03417014330625534)\n",
      "L1T2_out: (0.05704636871814728, 3.3387826988473535e-05, -0.05071365833282471)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.038899194449186325, 4.8913883802015334e-05, -0.042669061571359634)\n",
      "L1T1_lora_out: (0.07779838889837265, 9.782776760403067e-05, -0.08533812314271927)\n",
      "\n",
      "L2T1_lora_B_out: (0.0397738516330719, -4.025547968922183e-05, -0.04075002670288086)\n",
      "L2T1_lora_out: (0.0795477032661438, -8.051095937844366e-05, -0.08150005340576172)\n",
      "\n",
      "L2T2_lora_B_out: (0.10580042749643326, 8.150337635015603e-06, -0.10000884532928467)\n",
      "L2T2_lora_out: (0.21160085499286652, 1.6300675270031206e-05, -0.20001769065856934)\n",
      "\n",
      "g_out: (0.1630256474018097, 9.681164374342188e-05, -0.16059692203998566)\n",
      "L1T2_out: (0.23396232724189758, 0.0001946393895195797, -0.22825273871421814)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.061286088079214096, 7.772160643071402e-06, -0.06430673599243164)\n",
      "L1T1_lora_out: (0.12257217615842819, 1.5544321286142804e-05, -0.12861347198486328)\n",
      "\n",
      "L2T1_lora_B_out: (0.06092881038784981, -4.9128539103548974e-05, -0.04873339831829071)\n",
      "L2T1_lora_out: (0.12185762077569962, -9.825707820709795e-05, -0.09746679663658142)\n",
      "\n",
      "L2T2_lora_B_out: (0.15302696824073792, -0.00014671099779661745, -0.1297287791967392)\n",
      "L2T2_lora_out: (0.30605393648147583, -0.0002934219955932349, -0.2594575583934784)\n",
      "\n",
      "g_out: (0.2242041528224945, -0.00019516500469762832, -0.2002631425857544)\n",
      "L1T2_out: (0.3467763364315033, -0.00017962080892175436, -0.31351277232170105)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02196389250457287, 6.055010453565046e-05, -0.023123400285840034)\n",
      "L1T1_lora_out: (0.04392778500914574, 0.00012110020907130092, -0.04624680057168007)\n",
      "\n",
      "L2T1_lora_B_out: (0.028440209105610847, 0.00011519216059241444, -0.023058883845806122)\n",
      "L2T1_lora_out: (0.056880418211221695, 0.00023038432118482888, -0.046117767691612244)\n",
      "\n",
      "L2T2_lora_B_out: (0.05941745638847351, 0.00023518959642387927, -0.07134663313627243)\n",
      "L2T2_lora_out: (0.11883491277694702, 0.00047037919284775853, -0.14269326627254486)\n",
      "\n",
      "g_out: (0.10164958238601685, 0.00023999481345526874, -0.11600776016712189)\n",
      "L1T2_out: (0.14417900145053864, 0.00036109506618231535, -0.15933795273303986)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00393782090395689, 5.277749096421758e-06, -0.004078318830579519)\n",
      "L1T1_lora_out: (0.00787564180791378, 1.0555498192843515e-05, -0.008156637661159039)\n",
      "\n",
      "L2T1_lora_B_out: (0.006128014996647835, -1.8249762661071145e-06, -0.00684894435107708)\n",
      "L2T1_lora_out: (0.01225602999329567, -3.649952532214229e-06, -0.01369788870215416)\n",
      "\n",
      "L2T2_lora_B_out: (0.01400996558368206, 8.993154551717453e-06, -0.012175056152045727)\n",
      "L2T2_lora_out: (0.02801993116736412, 1.7986309103434905e-05, -0.024350112304091454)\n",
      "\n",
      "g_out: (0.020609714090824127, 2.163626231777016e-05, -0.018384991213679314)\n",
      "L1T2_out: (0.02787785790860653, 3.219175778212957e-05, -0.026124585419893265)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06777343899011612, -9.034918912220746e-05, -0.07967348396778107)\n",
      "L1T1_lora_out: (0.13554687798023224, -0.00018069837824441493, -0.15934696793556213)\n",
      "\n",
      "L2T1_lora_B_out: (0.12418418377637863, -0.00019771058578044176, -0.14306151866912842)\n",
      "L2T1_lora_out: (0.24836836755275726, -0.0003954211715608835, -0.28612303733825684)\n",
      "\n",
      "L2T2_lora_B_out: (0.19477972388267517, -0.0004195318615529686, -0.3130550682544708)\n",
      "L2T2_lora_out: (0.38955944776535034, -0.0008390637231059372, -0.6261101365089417)\n",
      "\n",
      "g_out: (0.26735323667526245, -0.00044364252244122326, -0.3493027687072754)\n",
      "L1T2_out: (0.39278584718704224, -0.0006243409006856382, -0.4622591435909271)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03515084087848663, -3.0419399990933016e-05, -0.031378719955682755)\n",
      "L1T1_lora_out: (0.07030168175697327, -6.083879998186603e-05, -0.06275743991136551)\n",
      "\n",
      "L2T1_lora_B_out: (0.029690200462937355, -1.1347907275194302e-05, -0.028305480256676674)\n",
      "L2T1_lora_out: (0.05938040092587471, -2.2695814550388604e-05, -0.05661096051335335)\n",
      "\n",
      "L2T2_lora_B_out: (0.08385861665010452, -8.138123666867614e-05, -0.07083700597286224)\n",
      "L2T2_lora_out: (0.16771723330020905, -0.00016276247333735228, -0.1416740119457245)\n",
      "\n",
      "g_out: (0.13506478071212769, -0.00014006668061483651, -0.1154773086309433)\n",
      "L1T2_out: (0.2016887217760086, -0.00020090548787266016, -0.16562597453594208)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005891887005418539, -6.598427262360929e-07, -0.006403901148587465)\n",
      "L1T1_lora_out: (0.011783774010837078, -1.3196854524721857e-06, -0.01280780229717493)\n",
      "\n",
      "L2T1_lora_B_out: (0.0076455469243228436, 1.9436356524238363e-06, -0.007620552089065313)\n",
      "L2T1_lora_out: (0.015291093848645687, 3.887271304847673e-06, -0.015241104178130627)\n",
      "\n",
      "L2T2_lora_B_out: (0.01567600667476654, 4.3601157813100144e-06, -0.01745690032839775)\n",
      "L2T2_lora_out: (0.03135201334953308, 8.720231562620029e-06, -0.0349138006567955)\n",
      "\n",
      "g_out: (0.023631839081645012, 4.832959803025005e-06, -0.024047069251537323)\n",
      "L1T2_out: (0.035415612161159515, 3.5132748053001706e-06, -0.03538134694099426)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.022751199081540108, -9.676616173237562e-06, -0.02536170743405819)\n",
      "L1T1_lora_out: (0.045502398163080215, -1.9353232346475124e-05, -0.05072341486811638)\n",
      "\n",
      "L2T1_lora_B_out: (0.03859015181660652, -6.274319457588717e-05, -0.048255011439323425)\n",
      "L2T1_lora_out: (0.07718030363321304, -0.00012548638915177435, -0.09651002287864685)\n",
      "\n",
      "L2T2_lora_B_out: (0.0725979208946228, -5.994249659124762e-05, -0.09481025487184525)\n",
      "L2T2_lora_out: (0.1451958417892456, -0.00011988499318249524, -0.1896205097436905)\n",
      "\n",
      "g_out: (0.11649442464113235, 5.601378234132426e-06, -0.12905056774616241)\n",
      "L1T2_out: (0.15878264605998993, -1.3751843653153628e-05, -0.16859301924705505)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03232812508940697, -9.524707274977118e-05, -0.030659615993499756)\n",
      "L1T1_lora_out: (0.06465625017881393, -0.00019049414549954236, -0.06131923198699951)\n",
      "\n",
      "L2T1_lora_B_out: (0.047170452773571014, 0.00014806308900006115, -0.04236684367060661)\n",
      "L2T1_lora_out: (0.09434090554714203, 0.0002961261780001223, -0.08473368734121323)\n",
      "\n",
      "L2T2_lora_B_out: (0.10812412947416306, -9.213907469529659e-05, -0.11223279684782028)\n",
      "L2T2_lora_out: (0.2162482589483261, -0.00018427814939059317, -0.22446559369564056)\n",
      "\n",
      "g_out: (0.15741313993930817, -0.00048040415276773274, -0.14846950769424438)\n",
      "L1T2_out: (0.20000484585762024, -0.0006708982982672751, -0.1973910629749298)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03237485513091087, 1.5175413864199072e-05, -0.02952069416642189)\n",
      "L1T1_lora_out: (0.06474971026182175, 3.0350827728398144e-05, -0.05904138833284378)\n",
      "\n",
      "L2T1_lora_B_out: (0.03455251082777977, -0.00011821220687124878, -0.04988442733883858)\n",
      "L2T1_lora_out: (0.06910502165555954, -0.00023642441374249756, -0.09976885467767715)\n",
      "\n",
      "L2T2_lora_B_out: (0.08047068864107132, -0.00018070952501147985, -0.08211058378219604)\n",
      "L2T2_lora_out: (0.16094137728214264, -0.0003614190500229597, -0.1642211675643921)\n",
      "\n",
      "g_out: (0.13580790162086487, -0.00012499462172854692, -0.1371641606092453)\n",
      "L1T2_out: (0.1976516842842102, -9.4643808552064e-05, -0.19620555639266968)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005838945508003235, 6.959964139241492e-06, -0.0060257636941969395)\n",
      "L1T1_lora_out: (0.01167789101600647, 1.3919928278482985e-05, -0.012051527388393879)\n",
      "\n",
      "L2T1_lora_B_out: (0.00548790767788887, -1.7582825648787548e-06, -0.00622054748237133)\n",
      "L2T1_lora_out: (0.01097581535577774, -3.5165651297575096e-06, -0.01244109496474266)\n",
      "\n",
      "L2T2_lora_B_out: (0.016646966338157654, 1.59056689881254e-05, -0.014456454664468765)\n",
      "L2T2_lora_out: (0.03329393267631531, 3.18113379762508e-05, -0.02891290932893753)\n",
      "\n",
      "g_out: (0.026113295927643776, 3.532790287863463e-05, -0.02476886287331581)\n",
      "L1T2_out: (0.037791185081005096, 4.9247835704591125e-05, -0.03504472225904465)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06969105452299118, -0.00012310082092881203, -0.09204519540071487)\n",
      "L1T1_lora_out: (0.13938210904598236, -0.00024620164185762405, -0.18409039080142975)\n",
      "\n",
      "L2T1_lora_B_out: (0.11245428770780563, -0.00012118110316805542, -0.15451104938983917)\n",
      "L2T1_lora_out: (0.22490857541561127, -0.00024236220633611083, -0.30902209877967834)\n",
      "\n",
      "L2T2_lora_B_out: (0.20991051197052002, -0.0004071058356203139, -0.3775065839290619)\n",
      "L2T2_lora_out: (0.41982102394104004, -0.0008142116712406278, -0.7550131678581238)\n",
      "\n",
      "g_out: (0.2689077854156494, -0.0005718494649045169, -0.45443788170814514)\n",
      "L1T2_out: (0.4006958603858948, -0.0008180511649698019, -0.6385282874107361)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03157389536499977, 9.625850907468703e-06, -0.041269391775131226)\n",
      "L1T1_lora_out: (0.06314779072999954, 1.9251701814937405e-05, -0.08253878355026245)\n",
      "\n",
      "L2T1_lora_B_out: (0.03519580140709877, 1.4343869224830996e-05, -0.03565334528684616)\n",
      "L2T1_lora_out: (0.07039160281419754, 2.8687738449661992e-05, -0.07130669057369232)\n",
      "\n",
      "L2T2_lora_B_out: (0.09239031374454498, 3.5609315091278404e-05, -0.13059070706367493)\n",
      "L2T2_lora_out: (0.18478062748908997, 7.121863018255681e-05, -0.26118141412734985)\n",
      "\n",
      "g_out: (0.14099915325641632, 4.253089355188422e-05, -0.20541512966156006)\n",
      "L1T2_out: (0.1992245614528656, 6.178259354783222e-05, -0.2879539132118225)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00700082303956151, -3.348948439452215e-06, -0.006739738862961531)\n",
      "L1T1_lora_out: (0.01400164607912302, -6.69789687890443e-06, -0.013479477725923061)\n",
      "\n",
      "L2T1_lora_B_out: (0.00858134776353836, 1.1400893527024891e-05, -0.00859854556620121)\n",
      "L2T1_lora_out: (0.01716269552707672, 2.2801787054049782e-05, -0.01719709113240242)\n",
      "\n",
      "L2T2_lora_B_out: (0.016818823292851448, 7.87164390203543e-06, -0.01830383948981762)\n",
      "L2T2_lora_out: (0.033637646585702896, 1.574328780407086e-05, -0.03660767897963524)\n",
      "\n",
      "g_out: (0.025058550760149956, -7.05849788573687e-06, -0.027971994131803513)\n",
      "L1T2_out: (0.03805515915155411, -1.37563947646413e-05, -0.04073159024119377)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.055162493139505386, 8.285730291390792e-05, -0.05486874282360077)\n",
      "L1T1_lora_out: (0.11032498627901077, 0.00016571460582781583, -0.10973748564720154)\n",
      "\n",
      "L2T1_lora_B_out: (0.049630627036094666, 6.885125003464054e-06, -0.04050077125430107)\n",
      "L2T1_lora_out: (0.09926125407218933, 1.3770250006928109e-05, -0.08100154250860214)\n",
      "\n",
      "L2T2_lora_B_out: (0.13499033451080322, 0.00016172083269339055, -0.13022257387638092)\n",
      "L2T2_lora_out: (0.26998066902160645, 0.0003234416653867811, -0.26044514775276184)\n",
      "\n",
      "g_out: (0.22454865276813507, 0.00030967139173299074, -0.1979455053806305)\n",
      "L1T2_out: (0.33487364649772644, 0.0004753860121127218, -0.2956418991088867)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04354619234800339, -0.00023765656806062907, -0.04443749040365219)\n",
      "L1T1_lora_out: (0.08709238469600677, -0.00047531313612125814, -0.08887498080730438)\n",
      "\n",
      "L2T1_lora_B_out: (0.051103025674819946, -0.00016121167573146522, -0.056918662041425705)\n",
      "L2T1_lora_out: (0.10220605134963989, -0.00032242335146293044, -0.11383732408285141)\n",
      "\n",
      "L2T2_lora_B_out: (0.10810677707195282, -0.0007735900580883026, -0.12915346026420593)\n",
      "L2T2_lora_out: (0.21621355414390564, -0.0015471801161766052, -0.25830692052841187)\n",
      "\n",
      "g_out: (0.1758110374212265, -0.0012247564736753702, -0.18700405955314636)\n",
      "L1T2_out: (0.24851401150226593, -0.0017000698717311025, -0.2494504600763321)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01964421756565571, 5.3626252338290215e-05, -0.02355058677494526)\n",
      "L1T1_lora_out: (0.03928843513131142, 0.00010725250467658043, -0.04710117354989052)\n",
      "\n",
      "L2T1_lora_B_out: (0.02728685736656189, -8.73706358106574e-06, -0.027346735820174217)\n",
      "L2T1_lora_out: (0.05457371473312378, -1.747412716213148e-05, -0.054693471640348434)\n",
      "\n",
      "L2T2_lora_B_out: (0.05430940166115761, -9.06509521882981e-05, -0.058390457183122635)\n",
      "L2T2_lora_out: (0.10861880332231522, -0.0001813019043765962, -0.11678091436624527)\n",
      "\n",
      "g_out: (0.08241481333971024, -0.00016382777539547533, -0.08865637332201004)\n",
      "L1T2_out: (0.11752952635288239, -5.657530709868297e-05, -0.13170452415943146)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006878471467643976, -1.293467676077853e-06, -0.0058573344722390175)\n",
      "L1T1_lora_out: (0.013756942935287952, -2.586935352155706e-06, -0.011714668944478035)\n",
      "\n",
      "L2T1_lora_B_out: (0.006117557641118765, 5.980857622489566e-06, -0.005676980130374432)\n",
      "L2T1_lora_out: (0.01223511528223753, 1.1961715244979132e-05, -0.011353960260748863)\n",
      "\n",
      "L2T2_lora_B_out: (0.01367214135825634, 1.8920391084975563e-05, -0.013462497852742672)\n",
      "L2T2_lora_out: (0.02734428271651268, 3.7840782169951126e-05, -0.026924995705485344)\n",
      "\n",
      "g_out: (0.020049670711159706, 2.5879066015477292e-05, -0.021511003375053406)\n",
      "L1T2_out: (0.029130540788173676, 2.329213202756364e-05, -0.03322567045688629)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0744287371635437, -0.00016332502127625048, -0.07551068067550659)\n",
      "L1T1_lora_out: (0.1488574743270874, -0.00032665004255250096, -0.15102136135101318)\n",
      "\n",
      "L2T1_lora_B_out: (0.11278444528579712, -0.00021594758436549455, -0.11473270505666733)\n",
      "L2T1_lora_out: (0.22556889057159424, -0.0004318951687309891, -0.22946541011333466)\n",
      "\n",
      "L2T2_lora_B_out: (0.26309865713119507, -0.0007095244945958257, -0.2707006335258484)\n",
      "L2T2_lora_out: (0.5261973142623901, -0.0014190489891916513, -0.5414012670516968)\n",
      "\n",
      "g_out: (0.34496429562568665, -0.0009871538495644927, -0.3869079649448395)\n",
      "L1T2_out: (0.49382176995277405, -0.0013138038339093328, -0.5297249555587769)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.033984310925006866, 1.5277490092557855e-05, -0.03055965155363083)\n",
      "L1T1_lora_out: (0.06796862185001373, 3.055498018511571e-05, -0.06111930310726166)\n",
      "\n",
      "L2T1_lora_B_out: (0.042068712413311005, -9.63912043516757e-08, -0.04000796377658844)\n",
      "L2T1_lora_out: (0.08413742482662201, -1.927824087033514e-07, -0.08001592755317688)\n",
      "\n",
      "L2T2_lora_B_out: (0.09040825814008713, 1.2519423762569204e-05, -0.09448950737714767)\n",
      "L2T2_lora_out: (0.18081651628017426, 2.5038847525138408e-05, -0.18897901475429535)\n",
      "\n",
      "g_out: (0.14841479063034058, 2.52316258411156e-05, -0.12702420353889465)\n",
      "L1T2_out: (0.20838114619255066, 5.578660420724191e-05, -0.184401273727417)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.012600206770002842, -6.490984105766984e-06, -0.011766246519982815)\n",
      "L1T1_lora_out: (0.025200413540005684, -1.2981968211533967e-05, -0.02353249303996563)\n",
      "\n",
      "L2T1_lora_B_out: (0.007479615043848753, 7.387056939478498e-06, -0.007285475730895996)\n",
      "L2T1_lora_out: (0.014959230087697506, 1.4774113878956996e-05, -0.014570951461791992)\n",
      "\n",
      "L2T2_lora_B_out: (0.0247667133808136, -1.099339897336904e-05, -0.029752632603049278)\n",
      "L2T2_lora_out: (0.0495334267616272, -2.198679794673808e-05, -0.059505265206098557)\n",
      "\n",
      "g_out: (0.039661701768636703, -3.676091364468448e-05, -0.04934561625123024)\n",
      "L1T2_out: (0.0600835420191288, -4.974287730874494e-05, -0.0699482411146164)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03449656069278717, 2.172821587009821e-05, -0.03756231069564819)\n",
      "L1T1_lora_out: (0.06899312138557434, 4.345643174019642e-05, -0.07512462139129639)\n",
      "\n",
      "L2T1_lora_B_out: (0.03811687231063843, 2.4004610168049112e-05, -0.048822272568941116)\n",
      "L2T1_lora_out: (0.07623374462127686, 4.8009220336098224e-05, -0.09764454513788223)\n",
      "\n",
      "L2T2_lora_B_out: (0.09851336479187012, 8.445733692497015e-05, -0.09672750532627106)\n",
      "L2T2_lora_out: (0.19702672958374023, 0.0001689146738499403, -0.19345501065254211)\n",
      "\n",
      "g_out: (0.1756349503993988, 0.00012090544623788446, -0.16136091947555542)\n",
      "L1T2_out: (0.24462807178497314, 0.00016436187434010208, -0.2364855408668518)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03767727315425873, 7.483267836505547e-05, -0.041040997952222824)\n",
      "L1T1_lora_out: (0.07535454630851746, 0.00014966535673011094, -0.08208199590444565)\n",
      "\n",
      "L2T1_lora_B_out: (0.03957885131239891, -2.924307409557514e-05, -0.03988852724432945)\n",
      "L2T1_lora_out: (0.07915770262479782, -5.848614819115028e-05, -0.0797770544886589)\n",
      "\n",
      "L2T2_lora_B_out: (0.08286513388156891, 0.00018687645206227899, -0.07783105224370956)\n",
      "L2T2_lora_out: (0.16573026776313782, 0.00037375290412455797, -0.15566210448741913)\n",
      "\n",
      "g_out: (0.11597787588834763, 0.0004322391177993268, -0.13308203220367432)\n",
      "L1T2_out: (0.17104169726371765, 0.000581904489081353, -0.17260365188121796)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028446462005376816, 0.00014469998131971806, -0.029878776520490646)\n",
      "L1T1_lora_out: (0.05689292401075363, 0.0002893999626394361, -0.05975755304098129)\n",
      "\n",
      "L2T1_lora_B_out: (0.02638350985944271, -1.8893521769314248e-07, -0.02708032727241516)\n",
      "L2T1_lora_out: (0.05276701971888542, -3.7787043538628495e-07, -0.05416065454483032)\n",
      "\n",
      "L2T2_lora_B_out: (0.07066649943590164, 0.0006174437585286796, -0.06661836057901382)\n",
      "L2T2_lora_out: (0.14133299887180328, 0.0012348875170573592, -0.13323672115802765)\n",
      "\n",
      "g_out: (0.10999830812215805, 0.0012352654011920094, -0.11420322209596634)\n",
      "L1T2_out: (0.16019050776958466, 0.0015246655093505979, -0.16519980132579803)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006252526771277189, 1.038446407619631e-05, -0.006414559204131365)\n",
      "L1T1_lora_out: (0.012505053542554379, 2.076892815239262e-05, -0.01282911840826273)\n",
      "\n",
      "L2T1_lora_B_out: (0.006990201771259308, 4.24465997639345e-06, -0.006658792495727539)\n",
      "L2T1_lora_out: (0.013980403542518616, 8.4893199527869e-06, -0.013317584991455078)\n",
      "\n",
      "L2T2_lora_B_out: (0.01320627797394991, 7.44234284866252e-07, -0.013464311137795448)\n",
      "L2T2_lora_out: (0.02641255594789982, 1.488468569732504e-06, -0.026928622275590897)\n",
      "\n",
      "g_out: (0.024200549349188805, -7.000852747296449e-06, -0.022620387375354767)\n",
      "L1T2_out: (0.036388251930475235, 1.3768074495601468e-05, -0.03281768783926964)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08759627491235733, -7.836490840418264e-05, -0.08604758232831955)\n",
      "L1T1_lora_out: (0.17519254982471466, -0.00015672981680836529, -0.1720951646566391)\n",
      "\n",
      "L2T1_lora_B_out: (0.12076888978481293, -0.00019213595078326762, -0.10347967594861984)\n",
      "L2T1_lora_out: (0.24153777956962585, -0.00038427190156653523, -0.20695935189723969)\n",
      "\n",
      "L2T2_lora_B_out: (0.23533128201961517, -0.0004738428979180753, -0.2761395573616028)\n",
      "L2T2_lora_out: (0.47066256403923035, -0.0009476857958361506, -0.5522791147232056)\n",
      "\n",
      "g_out: (0.30885377526283264, -0.0005634139524772763, -0.3453197479248047)\n",
      "L1T2_out: (0.46866804361343384, -0.0007201438420452178, -0.47804784774780273)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04096720740199089, 1.4271467080106959e-05, -0.04315730929374695)\n",
      "L1T1_lora_out: (0.08193441480398178, 2.8542934160213917e-05, -0.0863146185874939)\n",
      "\n",
      "L2T1_lora_B_out: (0.041649073362350464, -1.9534723833203316e-05, -0.040931060910224915)\n",
      "L2T1_lora_out: (0.08329814672470093, -3.906944766640663e-05, -0.08186212182044983)\n",
      "\n",
      "L2T2_lora_B_out: (0.09434645622968674, -3.7908645026618615e-05, -0.10370325297117233)\n",
      "L2T2_lora_out: (0.18869291245937347, -7.581729005323723e-05, -0.20740650594234467)\n",
      "\n",
      "g_out: (0.14540785551071167, -3.6747846024809405e-05, -0.14705979824066162)\n",
      "L1T2_out: (0.20973613858222961, -8.204910955100786e-06, -0.23337441682815552)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00840197317302227, -7.880373232183047e-06, -0.008059566840529442)\n",
      "L1T1_lora_out: (0.01680394634604454, -1.5760746464366093e-05, -0.016119133681058884)\n",
      "\n",
      "L2T1_lora_B_out: (0.007231435738503933, -7.2005591391643975e-06, -0.007542087696492672)\n",
      "L2T1_lora_out: (0.014462871477007866, -1.4401118278328795e-05, -0.015084175392985344)\n",
      "\n",
      "L2T2_lora_B_out: (0.01885102316737175, -2.8856004064437002e-05, -0.021431835368275642)\n",
      "L2T2_lora_out: (0.0377020463347435, -5.7712008128874004e-05, -0.042863670736551285)\n",
      "\n",
      "g_out: (0.03178384527564049, -4.3310887122061104e-05, -0.03359905257821083)\n",
      "L1T2_out: (0.047093264758586884, -5.90716335864272e-05, -0.04727756604552269)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05586066097021103, -1.538543074275367e-05, -0.04759128391742706)\n",
      "L1T1_lora_out: (0.11172132194042206, -3.077086148550734e-05, -0.09518256783485413)\n",
      "\n",
      "L2T1_lora_B_out: (0.05550208315253258, 6.979433965170756e-05, -0.06330893933773041)\n",
      "L2T1_lora_out: (0.11100416630506516, 0.00013958867930341512, -0.12661787867546082)\n",
      "\n",
      "L2T2_lora_B_out: (0.14399519562721252, 0.0002514378575142473, -0.12225035578012466)\n",
      "L2T2_lora_out: (0.28799039125442505, 0.0005028757150284946, -0.24450071156024933)\n",
      "\n",
      "g_out: (0.19980338215827942, 0.0003632869920693338, -0.20537404716014862)\n",
      "L1T2_out: (0.29197463393211365, 0.0003325161524116993, -0.2953910827636719)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04832790419459343, -0.0002462920965626836, -0.03322569653391838)\n",
      "L1T1_lora_out: (0.09665580838918686, -0.0004925841931253672, -0.06645139306783676)\n",
      "\n",
      "L2T1_lora_B_out: (0.05910775437951088, 0.00021107749489601701, -0.053904641419649124)\n",
      "L2T1_lora_out: (0.11821550875902176, 0.00042215498979203403, -0.10780928283929825)\n",
      "\n",
      "L2T2_lora_B_out: (0.10249579697847366, -3.5615292290458456e-05, -0.1362171471118927)\n",
      "L2T2_lora_out: (0.20499159395694733, -7.123058458091691e-05, -0.2724342942237854)\n",
      "\n",
      "g_out: (0.1702541559934616, -0.0004933855379931629, -0.18412166833877563)\n",
      "L1T2_out: (0.24724188446998596, -0.0009859696729108691, -0.22565387189388275)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023417053744196892, -0.000143663099152036, -0.022092219442129135)\n",
      "L1T1_lora_out: (0.046834107488393784, -0.000287326198304072, -0.04418443888425827)\n",
      "\n",
      "L2T1_lora_B_out: (0.04656943306326866, 0.00021114121773280203, -0.03968566656112671)\n",
      "L2T1_lora_out: (0.09313886612653732, 0.00042228243546560407, -0.07937133312225342)\n",
      "\n",
      "L2T2_lora_B_out: (0.08396854996681213, -3.188784830854274e-05, -0.07314667105674744)\n",
      "L2T2_lora_out: (0.16793709993362427, -6.377569661708549e-05, -0.14629334211349487)\n",
      "\n",
      "g_out: (0.132258340716362, -0.0004860582121182233, -0.11203394830226898)\n",
      "L1T2_out: (0.16113272309303284, -0.0007733844104222953, -0.14352215826511383)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006845970172435045, 2.8354040750855347e-06, -0.007273105438798666)\n",
      "L1T1_lora_out: (0.01369194034487009, 5.6708081501710694e-06, -0.014546210877597332)\n",
      "\n",
      "L2T1_lora_B_out: (0.006377961952239275, -7.720810572209302e-06, -0.0074576400220394135)\n",
      "L2T1_lora_out: (0.01275592390447855, -1.5441621144418605e-05, -0.014915280044078827)\n",
      "\n",
      "L2T2_lora_B_out: (0.020225705578923225, -1.9303166482131928e-05, -0.015440099872648716)\n",
      "L2T2_lora_out: (0.04045141115784645, -3.8606332964263856e-05, -0.030880199745297432)\n",
      "\n",
      "g_out: (0.0318978950381279, -2.316471545782406e-05, -0.02701633982360363)\n",
      "L1T2_out: (0.04378113895654678, -1.749390685290564e-05, -0.03632492572069168)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1831042617559433, -9.977189620258287e-05, -0.1466914266347885)\n",
      "L1T1_lora_out: (0.3662085235118866, -0.00019954379240516573, -0.293382853269577)\n",
      "\n",
      "L2T1_lora_B_out: (0.17296452820301056, -0.0002294094447279349, -0.23224133253097534)\n",
      "L2T1_lora_out: (0.3459290564060211, -0.0004588188894558698, -0.4644826650619507)\n",
      "\n",
      "L2T2_lora_B_out: (0.43101081252098083, -0.0005506386514753103, -0.5257394313812256)\n",
      "L2T2_lora_out: (0.8620216250419617, -0.0011012773029506207, -1.0514788627624512)\n",
      "\n",
      "g_out: (0.7432271242141724, -0.0006424583843909204, -0.7364584803581238)\n",
      "L1T2_out: (1.1094356775283813, -0.0008420022786594927, -1.0261423587799072)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.037139784544706345, -8.697170414961874e-06, -0.03254409506917)\n",
      "L1T1_lora_out: (0.07427956908941269, -1.739434082992375e-05, -0.06508819013834)\n",
      "\n",
      "L2T1_lora_B_out: (0.03503166139125824, -6.431851033994462e-06, -0.03602885827422142)\n",
      "L2T1_lora_out: (0.07006332278251648, -1.2863702067988925e-05, -0.07205771654844284)\n",
      "\n",
      "L2T2_lora_B_out: (0.07613065093755722, -5.259031695459271e-06, -0.07038964331150055)\n",
      "L2T2_lora_out: (0.15226130187511444, -1.0518063390918542e-05, -0.1407792866230011)\n",
      "\n",
      "g_out: (0.12441414594650269, 2.345641178180813e-06, -0.12328033894300461)\n",
      "L1T2_out: (0.19287730753421783, -1.5048704881337471e-05, -0.17702485620975494)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0073355743661522865, 1.4883005405863514e-06, -0.010428634472191334)\n",
      "L1T1_lora_out: (0.014671148732304573, 2.976601081172703e-06, -0.020857268944382668)\n",
      "\n",
      "L2T1_lora_B_out: (0.008976106531918049, -2.7903352020075545e-06, -0.00911961030215025)\n",
      "L2T1_lora_out: (0.017952213063836098, -5.580670404015109e-06, -0.0182392206043005)\n",
      "\n",
      "L2T2_lora_B_out: (0.017368968576192856, -6.033075123923481e-07, -0.01748591475188732)\n",
      "L2T2_lora_out: (0.03473793715238571, -1.2066150247846963e-06, -0.03497182950377464)\n",
      "\n",
      "g_out: (0.023645557463169098, 4.374053332867334e-06, -0.026049891486763954)\n",
      "L1T2_out: (0.03568638861179352, 7.350653959292686e-06, -0.04690716043114662)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03698159381747246, 3.1120605854084715e-05, -0.035758305341005325)\n",
      "L1T1_lora_out: (0.07396318763494492, 6.224121170816943e-05, -0.07151661068201065)\n",
      "\n",
      "L2T1_lora_B_out: (0.03497973084449768, -1.570720996824093e-05, -0.04138626903295517)\n",
      "L2T1_lora_out: (0.06995946168899536, -3.141441993648186e-05, -0.08277253806591034)\n",
      "\n",
      "L2T2_lora_B_out: (0.08812247961759567, 3.776541416300461e-05, -0.08419715613126755)\n",
      "L2T2_lora_out: (0.17624495923519135, 7.553082832600921e-05, -0.1683943122625351)\n",
      "\n",
      "g_out: (0.13095860183238983, 0.00010694524098653346, -0.13576707243919373)\n",
      "L1T2_out: (0.1831316351890564, 0.0001691864599706605, -0.20122191309928894)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.030120426788926125, 8.810059807728976e-05, -0.02293279394507408)\n",
      "L1T1_lora_out: (0.06024085357785225, 0.00017620119615457952, -0.04586558789014816)\n",
      "\n",
      "L2T1_lora_B_out: (0.059727128595113754, 5.32170342921745e-05, -0.044737257063388824)\n",
      "L2T1_lora_out: (0.11945425719022751, 0.000106434068584349, -0.08947451412677765)\n",
      "\n",
      "L2T2_lora_B_out: (0.09172336012125015, 0.00021572900004684925, -0.07088951021432877)\n",
      "L2T2_lora_out: (0.1834467202425003, 0.0004314580000936985, -0.14177902042865753)\n",
      "\n",
      "g_out: (0.14476019144058228, 0.0003250238951295614, -0.11248299479484558)\n",
      "L1T2_out: (0.19316117465496063, 0.0005012251785956323, -0.15225329995155334)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.041120707988739014, 2.905911060224753e-05, -0.0285671167075634)\n",
      "L1T1_lora_out: (0.08224141597747803, 5.811822120449506e-05, -0.0571342334151268)\n",
      "\n",
      "L2T1_lora_B_out: (0.034957144409418106, 5.61514389119111e-05, -0.039456237107515335)\n",
      "L2T1_lora_out: (0.06991428881883621, 0.0001123028778238222, -0.07891247421503067)\n",
      "\n",
      "L2T2_lora_B_out: (0.09572931379079819, 7.742232992313802e-05, -0.08736075460910797)\n",
      "L2T2_lora_out: (0.19145862758159637, 0.00015484465984627604, -0.17472150921821594)\n",
      "\n",
      "g_out: (0.15256348252296448, 4.25417092628777e-05, -0.11423392593860626)\n",
      "L1T2_out: (0.21299120783805847, 0.00010065997776109725, -0.15247125923633575)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00717319268733263, -7.067981186992256e-06, -0.008382746949791908)\n",
      "L1T1_lora_out: (0.01434638537466526, -1.4135962373984512e-05, -0.016765493899583817)\n",
      "\n",
      "L2T1_lora_B_out: (0.00927506573498249, -1.3610185305878986e-05, -0.008757526986300945)\n",
      "L2T1_lora_out: (0.01855013146996498, -2.722037061175797e-05, -0.01751505397260189)\n",
      "\n",
      "L2T2_lora_B_out: (0.01981428824365139, -2.2736383471055888e-05, -0.019237207248806953)\n",
      "L2T2_lora_out: (0.03962857648730278, -4.5472766942111775e-05, -0.03847441449761391)\n",
      "\n",
      "g_out: (0.032879579812288284, -1.8252396330353804e-05, -0.034622520208358765)\n",
      "L1T2_out: (0.043914295732975006, -3.2388357794843614e-05, -0.05138801410794258)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1039554551243782, -7.332762470468879e-05, -0.13453109562397003)\n",
      "L1T1_lora_out: (0.2079109102487564, -0.00014665524940937757, -0.26906219124794006)\n",
      "\n",
      "L2T1_lora_B_out: (0.24222631752490997, -0.00039369231672026217, -0.22014842927455902)\n",
      "L2T1_lora_out: (0.48445263504981995, -0.0007873846334405243, -0.44029685854911804)\n",
      "\n",
      "L2T2_lora_B_out: (0.46566760540008545, -0.0007023127982392907, -0.37339749932289124)\n",
      "L2T2_lora_out: (0.9313352108001709, -0.0014046255964785814, -0.7467949986457825)\n",
      "\n",
      "g_out: (0.44688257575035095, -0.0006172409630380571, -0.5181924700737)\n",
      "L1T2_out: (0.6458055377006531, -0.0007638962124474347, -0.7872546911239624)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08562920242547989, 2.468664570187684e-05, -0.04878928139805794)\n",
      "L1T1_lora_out: (0.17125840485095978, 4.937329140375368e-05, -0.09757856279611588)\n",
      "\n",
      "L2T1_lora_B_out: (0.05556311830878258, 2.3836175387259573e-05, -0.0537942498922348)\n",
      "L2T1_lora_out: (0.11112623661756516, 4.7672350774519145e-05, -0.1075884997844696)\n",
      "\n",
      "L2T2_lora_B_out: (0.1378639191389084, 8.17557011032477e-05, -0.11700920760631561)\n",
      "L2T2_lora_out: (0.2757278382778168, 0.0001635114022064954, -0.23401841521263123)\n",
      "\n",
      "g_out: (0.22767439484596252, 0.00011583905143197626, -0.1707591712474823)\n",
      "L1T2_out: (0.3989328145980835, 0.00016521233192179352, -0.2601410150527954)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.009192965924739838, 1.5575045608784421e-06, -0.010743762366473675)\n",
      "L1T1_lora_out: (0.018385931849479675, 3.1150091217568843e-06, -0.02148752473294735)\n",
      "\n",
      "L2T1_lora_B_out: (0.008105375804007053, 8.357724254892673e-06, -0.007958795875310898)\n",
      "L2T1_lora_out: (0.016210751608014107, 1.6715448509785347e-05, -0.015917591750621796)\n",
      "\n",
      "L2T2_lora_B_out: (0.01928751729428768, 2.8282436687732115e-05, -0.017435071989893913)\n",
      "L2T2_lora_out: (0.03857503458857536, 5.656487337546423e-05, -0.03487014397978783)\n",
      "\n",
      "g_out: (0.03475718945264816, 3.9849419408710673e-05, -0.02851913496851921)\n",
      "L1T2_out: (0.05314312130212784, 4.2964424210367724e-05, -0.049260545521974564)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.031008021906018257, -3.243322862545028e-05, -0.031864482909440994)\n",
      "L1T1_lora_out: (0.062016043812036514, -6.486645725090057e-05, -0.06372896581888199)\n",
      "\n",
      "L2T1_lora_B_out: (0.0486474484205246, -3.843484591925517e-06, -0.04936794564127922)\n",
      "L2T1_lora_out: (0.0972948968410492, -7.686969183851033e-06, -0.09873589128255844)\n",
      "\n",
      "L2T2_lora_B_out: (0.07893359661102295, -2.081556522171013e-05, -0.09135358780622482)\n",
      "L2T2_lora_out: (0.1578671932220459, -4.163113044342026e-05, -0.18270717561244965)\n",
      "\n",
      "g_out: (0.103260837495327, -3.3944153983611614e-05, -0.12759298086166382)\n",
      "L1T2_out: (0.14517469704151154, -9.881061123451218e-05, -0.17106589674949646)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.051206719130277634, 0.00014968506002333015, -0.03895488753914833)\n",
      "L1T1_lora_out: (0.10241343826055527, 0.0002993701200466603, -0.07790977507829666)\n",
      "\n",
      "L2T1_lora_B_out: (0.07506762444972992, -0.0001273836096515879, -0.07428374886512756)\n",
      "L2T1_lora_out: (0.15013524889945984, -0.0002547672193031758, -0.14856749773025513)\n",
      "\n",
      "L2T2_lora_B_out: (0.13296538591384888, 0.0001097490603569895, -0.11987308412790298)\n",
      "L2T2_lora_out: (0.26593077182769775, 0.000219498120713979, -0.23974616825580597)\n",
      "\n",
      "g_out: (0.23439645767211914, 0.0004742652818094939, -0.17444902658462524)\n",
      "L1T2_out: (0.33154839277267456, 0.0007736353436484933, -0.2523587942123413)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.031562451273202896, -9.409723861608654e-05, -0.0374457947909832)\n",
      "L1T1_lora_out: (0.06312490254640579, -0.00018819447723217309, -0.0748915895819664)\n",
      "\n",
      "L2T1_lora_B_out: (0.035068899393081665, 2.832483187376056e-05, -0.029383106157183647)\n",
      "L2T1_lora_out: (0.07013779878616333, 5.664966374752112e-05, -0.058766212314367294)\n",
      "\n",
      "L2T2_lora_B_out: (0.09428353607654572, -0.00010918729822151363, -0.08866886794567108)\n",
      "L2T2_lora_out: (0.18856707215309143, -0.00021837459644302726, -0.17733773589134216)\n",
      "\n",
      "g_out: (0.13908430933952332, -0.0002750243293121457, -0.14504337310791016)\n",
      "L1T2_out: (0.18641868233680725, -0.00046321869012899697, -0.21286115050315857)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004152711480855942, -1.0358621693740133e-06, -0.00403333967551589)\n",
      "L1T1_lora_out: (0.008305422961711884, -2.0717243387480266e-06, -0.00806667935103178)\n",
      "\n",
      "L2T1_lora_B_out: (0.0063267601653933525, 5.247382432571612e-06, -0.007625375408679247)\n",
      "L2T1_lora_out: (0.012653520330786705, 1.0494764865143225e-05, -0.015250750817358494)\n",
      "\n",
      "L2T2_lora_B_out: (0.017136722803115845, 1.7992219000007026e-05, -0.015241126529872417)\n",
      "L2T2_lora_out: (0.03427344560623169, 3.598443800001405e-05, -0.030482253059744835)\n",
      "\n",
      "g_out: (0.02268393337726593, 2.548966949689202e-05, -0.02200138196349144)\n",
      "L1T2_out: (0.030691126361489296, 2.34179478866281e-05, -0.02836727723479271)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.13716377317905426, -2.8719134206767194e-05, -0.1427105963230133)\n",
      "L1T1_lora_out: (0.2743275463581085, -5.743826841353439e-05, -0.2854211926460266)\n",
      "\n",
      "L2T1_lora_B_out: (0.24849513173103333, -0.00028814803226850927, -0.26215487718582153)\n",
      "L2T1_lora_out: (0.49699026346206665, -0.0005762960645370185, -0.5243097543716431)\n",
      "\n",
      "L2T2_lora_B_out: (0.36603981256484985, -0.0004609355528373271, -0.49112793803215027)\n",
      "L2T2_lora_out: (0.7320796251296997, -0.0009218711056746542, -0.9822558760643005)\n",
      "\n",
      "g_out: (0.37895873188972473, -0.00034557507024146616, -0.5016739368438721)\n",
      "L1T2_out: (0.6308352947235107, -0.00040301334229297936, -0.7870951294898987)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07497003674507141, 5.681464699591743e-06, -0.0598582923412323)\n",
      "L1T1_lora_out: (0.14994007349014282, 1.1362929399183486e-05, -0.1197165846824646)\n",
      "\n",
      "L2T1_lora_B_out: (0.062043506652116776, 2.1455477963172598e-06, -0.052108123898506165)\n",
      "L2T1_lora_out: (0.12408701330423355, 4.2910955926345196e-06, -0.10421624779701233)\n",
      "\n",
      "L2T2_lora_B_out: (0.16125613451004028, 9.855719326878898e-06, -0.1409371942281723)\n",
      "L2T2_lora_out: (0.32251226902008057, 1.9711438653757796e-05, -0.2818743884563446)\n",
      "\n",
      "g_out: (0.2527695894241333, 1.542034442536533e-05, -0.2213653177022934)\n",
      "L1T2_out: (0.3849192261695862, 2.67832656390965e-05, -0.33035001158714294)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.010358801111578941, -8.34035290608881e-06, -0.012690195813775063)\n",
      "L1T1_lora_out: (0.020717602223157883, -1.668070581217762e-05, -0.025380391627550125)\n",
      "\n",
      "L2T1_lora_B_out: (0.009687657468020916, -1.2790356777259149e-05, -0.010978513397276402)\n",
      "L2T1_lora_out: (0.019375314936041832, -2.5580713554518297e-05, -0.021957026794552803)\n",
      "\n",
      "L2T2_lora_B_out: (0.02050516940653324, -4.615524085238576e-05, -0.021346580237150192)\n",
      "L2T2_lora_out: (0.04101033881306648, -9.231048170477152e-05, -0.042693160474300385)\n",
      "\n",
      "g_out: (0.035642046481370926, -6.67297572363168e-05, -0.037881046533584595)\n",
      "L1T2_out: (0.05358041077852249, -8.341046486748382e-05, -0.059684570878744125)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04690936580300331, 4.449970947462134e-05, -0.04288691654801369)\n",
      "L1T1_lora_out: (0.09381873160600662, 8.899941894924268e-05, -0.08577383309602737)\n",
      "\n",
      "L2T1_lora_B_out: (0.05296265706419945, 0.00010549682338023558, -0.05109001323580742)\n",
      "L2T1_lora_out: (0.1059253141283989, 0.00021099364676047117, -0.10218002647161484)\n",
      "\n",
      "L2T2_lora_B_out: (0.10814516991376877, 0.00026198665727861226, -0.11514212936162949)\n",
      "L2T2_lora_out: (0.21629033982753754, 0.0005239733145572245, -0.23028425872325897)\n",
      "\n",
      "g_out: (0.1694485992193222, 0.00031297971145249903, -0.16940563917160034)\n",
      "L1T2_out: (0.24250644445419312, 0.0004019790794700384, -0.2463541328907013)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04382152482867241, -0.0001229168992722407, -0.04613026976585388)\n",
      "L1T1_lora_out: (0.08764304965734482, -0.0002458337985444814, -0.09226053953170776)\n",
      "\n",
      "L2T1_lora_B_out: (0.042730510234832764, 3.259986260673031e-05, -0.045237354934215546)\n",
      "L2T1_lora_out: (0.08546102046966553, 6.519972521346062e-05, -0.09047470986843109)\n",
      "\n",
      "L2T2_lora_B_out: (0.08203479647636414, 4.6897632273612544e-05, -0.08906455338001251)\n",
      "L2T2_lora_out: (0.16406959295272827, 9.379526454722509e-05, -0.17812910676002502)\n",
      "\n",
      "g_out: (0.11726605147123337, 2.8595504772965796e-05, -0.1375635266304016)\n",
      "L1T2_out: (0.1625363826751709, -0.00021723817917518318, -0.20992179214954376)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04136469215154648, 7.798621481924783e-06, -0.026530321687459946)\n",
      "L1T1_lora_out: (0.08272938430309296, 1.5597242963849567e-05, -0.05306064337491989)\n",
      "\n",
      "L2T1_lora_B_out: (0.028023619204759598, 1.0931232282018755e-05, -0.02928023971617222)\n",
      "L2T1_lora_out: (0.056047238409519196, 2.186246456403751e-05, -0.05856047943234444)\n",
      "\n",
      "L2T2_lora_B_out: (0.07569561153650284, 8.309742042911239e-06, -0.0724080428481102)\n",
      "L2T2_lora_out: (0.15139122307300568, 1.6619484085822478e-05, -0.1448160856962204)\n",
      "\n",
      "g_out: (0.16029416024684906, -5.242995484877611e-06, -0.11190502345561981)\n",
      "L1T2_out: (0.24112743139266968, 1.0354268852097448e-05, -0.1552288830280304)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005356915760785341, 9.566218068357557e-06, -0.005135617684572935)\n",
      "L1T1_lora_out: (0.010713831521570683, 1.9132436136715114e-05, -0.01027123536914587)\n",
      "\n",
      "L2T1_lora_B_out: (0.0065313377417624, -9.310764653491788e-06, -0.005942996591329575)\n",
      "L2T1_lora_out: (0.0130626754835248, -1.8621529306983575e-05, -0.01188599318265915)\n",
      "\n",
      "L2T2_lora_B_out: (0.015234548598527908, -8.455558599962387e-06, -0.014819663017988205)\n",
      "L2T2_lora_out: (0.030469097197055817, -1.6911117199924774e-05, -0.02963932603597641)\n",
      "\n",
      "g_out: (0.022578181698918343, 1.7104143807955552e-06, -0.021950725466012955)\n",
      "L1T2_out: (0.03042323887348175, 2.084285006276332e-05, -0.030553095042705536)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.155442476272583, -0.0001052569059538655, -0.14878767728805542)\n",
      "L1T1_lora_out: (0.310884952545166, -0.000210513811907731, -0.29757535457611084)\n",
      "\n",
      "L2T1_lora_B_out: (0.21349036693572998, -7.902632205514237e-05, -0.20988495647907257)\n",
      "L2T1_lora_out: (0.42698073387145996, -0.00015805264411028475, -0.41976991295814514)\n",
      "\n",
      "L2T2_lora_B_out: (0.47178733348846436, -0.00028104527154937387, -0.38282445073127747)\n",
      "L2T2_lora_out: (0.9435746669769287, -0.0005620905430987477, -0.7656489014625549)\n",
      "\n",
      "g_out: (0.6285440921783447, -0.0004040379135403782, -0.6153807044029236)\n",
      "L1T2_out: (0.9243203997612, -0.000614551710896194, -0.9129560589790344)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08505159616470337, -1.1494928003230598e-05, -0.0848185122013092)\n",
      "L1T1_lora_out: (0.17010319232940674, -2.2989856006461196e-05, -0.1696370244026184)\n",
      "\n",
      "L2T1_lora_B_out: (0.10214702039957047, 5.887935185455717e-05, -0.08711443096399307)\n",
      "L2T1_lora_out: (0.20429404079914093, 0.00011775870370911434, -0.17422886192798615)\n",
      "\n",
      "L2T2_lora_B_out: (0.1879614293575287, 0.0001025386227411218, -0.13214848935604095)\n",
      "L2T2_lora_out: (0.3759228587150574, 0.0002050772454822436, -0.2642969787120819)\n",
      "\n",
      "g_out: (0.19793367385864258, 8.731854177312925e-05, -0.1798201948404312)\n",
      "L1T2_out: (0.3649575114250183, 6.432868394767866e-05, -0.3421943485736847)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.007972975261509418, 5.169921678316314e-06, -0.00831659883260727)\n",
      "L1T1_lora_out: (0.015945950523018837, 1.0339843356632628e-05, -0.01663319766521454)\n",
      "\n",
      "L2T1_lora_B_out: (0.009204188361763954, 1.6518392840225715e-06, -0.009395534172654152)\n",
      "L2T1_lora_out: (0.01840837672352791, 3.303678568045143e-06, -0.018791068345308304)\n",
      "\n",
      "L2T2_lora_B_out: (0.017018798738718033, 2.5960416678572074e-05, -0.01638522371649742)\n",
      "L2T2_lora_out: (0.034037597477436066, 5.192083335714415e-05, -0.03277044743299484)\n",
      "\n",
      "g_out: (0.024063840508461, 4.8617155698593706e-05, -0.023149825632572174)\n",
      "L1T2_out: (0.03690695762634277, 5.8957004512194544e-05, -0.03880607336759567)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03653784841299057, 9.434855746803805e-05, -0.034942399710416794)\n",
      "L1T1_lora_out: (0.07307569682598114, 0.0001886971149360761, -0.06988479942083359)\n",
      "\n",
      "L2T1_lora_B_out: (0.08014289289712906, -8.584395254729316e-05, -0.06827899068593979)\n",
      "L2T1_lora_out: (0.16028578579425812, -0.0001716879050945863, -0.13655798137187958)\n",
      "\n",
      "L2T2_lora_B_out: (0.16832797229290009, 1.5390736734843813e-05, -0.1511639505624771)\n",
      "L2T2_lora_out: (0.33665594458580017, 3.0781473469687626e-05, -0.3023279011249542)\n",
      "\n",
      "g_out: (0.2178160399198532, 0.0002024693530984223, -0.1924709975719452)\n",
      "L1T2_out: (0.27280375361442566, 0.0003911664825864136, -0.25220444798469543)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04244804382324219, -1.9423281116814906e-07, -0.043065380305051804)\n",
      "L1T1_lora_out: (0.08489608764648438, -3.884656223362981e-07, -0.08613076061010361)\n",
      "\n",
      "L2T1_lora_B_out: (0.05951112508773804, 0.00013473289436660707, -0.05606841668486595)\n",
      "L2T1_lora_out: (0.11902225017547607, 0.00026946578873321414, -0.1121368333697319)\n",
      "\n",
      "L2T2_lora_B_out: (0.1061227023601532, -1.5101253666216508e-05, -0.10345132648944855)\n",
      "L2T2_lora_out: (0.2122454047203064, -3.0202507332433015e-05, -0.2069026529788971)\n",
      "\n",
      "g_out: (0.1640504002571106, -0.00029966840520501137, -0.17506642639636993)\n",
      "L1T2_out: (0.24894648790359497, -0.00030005682492628694, -0.26119717955589294)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03879336267709732, 0.00010189844033448026, -0.03970319777727127)\n",
      "L1T1_lora_out: (0.07758672535419464, 0.0002037968806689605, -0.07940639555454254)\n",
      "\n",
      "L2T1_lora_B_out: (0.05020281672477722, -0.0002918715472333133, -0.0585276260972023)\n",
      "L2T1_lora_out: (0.10040563344955444, -0.0005837430944666266, -0.1170552521944046)\n",
      "\n",
      "L2T2_lora_B_out: (0.15317504107952118, -0.0002493712236173451, -0.12364304065704346)\n",
      "L2T2_lora_out: (0.30635008215904236, -0.0004987424472346902, -0.24728608131408691)\n",
      "\n",
      "g_out: (0.23080214858055115, 8.500052354065701e-05, -0.17389050126075745)\n",
      "L1T2_out: (0.30334699153900146, 0.000288797658868134, -0.2336125522851944)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005008353851735592, 9.219912158187071e-07, -0.004622065462172031)\n",
      "L1T1_lora_out: (0.010016707703471184, 1.8439824316374143e-06, -0.009244130924344063)\n",
      "\n",
      "L2T1_lora_B_out: (0.005858436692506075, 2.1290456970746163e-06, -0.005842889659106731)\n",
      "L2T1_lora_out: (0.01171687338501215, 4.258091394149233e-06, -0.011685779318213463)\n",
      "\n",
      "L2T2_lora_B_out: (0.011316156014800072, 8.660285857331473e-07, -0.01348407007753849)\n",
      "L2T2_lora_out: (0.022632312029600143, 1.7320571714662947e-06, -0.02696814015507698)\n",
      "\n",
      "g_out: (0.01860571652650833, -2.5260344500566134e-06, -0.020318076014518738)\n",
      "L1T2_out: (0.028622424229979515, -6.820518478889426e-07, -0.027703747153282166)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1259426772594452, -4.332397656980902e-05, -0.11662087589502335)\n",
      "L1T1_lora_out: (0.2518853545188904, -8.664795313961804e-05, -0.2332417517900467)\n",
      "\n",
      "L2T1_lora_B_out: (0.18285349011421204, -0.0001628555473871529, -0.18156757950782776)\n",
      "L2T1_lora_out: (0.3657069802284241, -0.0003257110947743058, -0.3631351590156555)\n",
      "\n",
      "L2T2_lora_B_out: (0.3445582091808319, -0.00033551789238117635, -0.4127064049243927)\n",
      "L2T2_lora_out: (0.6891164183616638, -0.0006710357847623527, -0.8254128098487854)\n",
      "\n",
      "g_out: (0.4899519085884094, -0.0003453247481957078, -0.5620012283325195)\n",
      "L1T2_out: (0.7371541857719421, -0.00043197270133532584, -0.7503429055213928)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06373637169599533, -1.564718331792392e-05, -0.06617170572280884)\n",
      "L1T1_lora_out: (0.12747274339199066, -3.129436663584784e-05, -0.13234341144561768)\n",
      "\n",
      "L2T1_lora_B_out: (0.08088218420743942, 1.2859197262571342e-08, -0.09400927275419235)\n",
      "L2T1_lora_out: (0.16176436841487885, 2.5718394525142685e-08, -0.1880185455083847)\n",
      "\n",
      "L2T2_lora_B_out: (0.1964600533246994, 4.297571649658494e-05, -0.16356270015239716)\n",
      "L2T2_lora_out: (0.3929201066493988, 8.595143299316987e-05, -0.3271254003047943)\n",
      "\n",
      "g_out: (0.2518576681613922, 8.592570520704612e-05, -0.22472617030143738)\n",
      "L1T2_out: (0.3668595552444458, 5.4631338571198285e-05, -0.3539668619632721)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.009867018088698387, -2.0998031686758623e-06, -0.010560437105596066)\n",
      "L1T1_lora_out: (0.019734036177396774, -4.1996063373517245e-06, -0.02112087421119213)\n",
      "\n",
      "L2T1_lora_B_out: (0.011138735339045525, 2.0521597434708383e-06, -0.010947322472929955)\n",
      "L2T1_lora_out: (0.02227747067809105, 4.104319486941677e-06, -0.02189464494585991)\n",
      "\n",
      "L2T2_lora_B_out: (0.025428544729948044, 2.488508926035138e-06, -0.029939930886030197)\n",
      "L2T2_lora_out: (0.05085708945989609, 4.977017852070276e-06, -0.059879861772060394)\n",
      "\n",
      "g_out: (0.03691529110074043, 8.726985925022746e-07, -0.0448710173368454)\n",
      "L1T2_out: (0.05520777031779289, -3.32690524373902e-06, -0.06560299545526505)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1282341331243515, 1.217343560711015e-05, -0.10116150230169296)\n",
      "L1T1_lora_out: (0.256468266248703, 2.43468712142203e-05, -0.20232300460338593)\n",
      "\n",
      "L2T1_lora_B_out: (0.09282888472080231, -8.496110967826098e-05, -0.08393578231334686)\n",
      "L2T1_lora_out: (0.18565776944160461, -0.00016992221935652196, -0.16787156462669373)\n",
      "\n",
      "L2T2_lora_B_out: (0.25223755836486816, -0.00010736778494901955, -0.1567375808954239)\n",
      "L2T2_lora_out: (0.5044751167297363, -0.0002147355698980391, -0.3134751617908478)\n",
      "\n",
      "g_out: (0.42744317650794983, -4.4813368731411174e-05, -0.2911888659000397)\n",
      "L1T2_out: (0.6839114427566528, -2.0466488422243856e-05, -0.4922032356262207)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0565861351788044, -0.00020688807126134634, -0.05204618349671364)\n",
      "L1T1_lora_out: (0.1131722703576088, -0.0004137761425226927, -0.10409236699342728)\n",
      "\n",
      "L2T1_lora_B_out: (0.047437768429517746, 1.43763436426525e-05, -0.04558807238936424)\n",
      "L2T1_lora_out: (0.09487553685903549, 2.8752687285305e-05, -0.09117614477872849)\n",
      "\n",
      "L2T2_lora_B_out: (0.12544479966163635, -0.0002497566747479141, -0.10473190993070602)\n",
      "L2T2_lora_out: (0.2508895993232727, -0.0004995133494958282, -0.20946381986141205)\n",
      "\n",
      "g_out: (0.21481135487556458, -0.0005282661877572536, -0.18192318081855774)\n",
      "L1T2_out: (0.3026280999183655, -0.0009420422138646245, -0.2727472186088562)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04611342027783394, -0.00010027726966654882, -0.047495413571596146)\n",
      "L1T1_lora_out: (0.09222684055566788, -0.00020055453933309764, -0.09499082714319229)\n",
      "\n",
      "L2T1_lora_B_out: (0.039766576141119, 7.609179010614753e-05, -0.035082388669252396)\n",
      "L2T1_lora_out: (0.079533152282238, 0.00015218358021229506, -0.07016477733850479)\n",
      "\n",
      "L2T2_lora_B_out: (0.07144511491060257, -0.00012779555981978774, -0.06653721630573273)\n",
      "L2T2_lora_out: (0.14289022982120514, -0.0002555911196395755, -0.13307443261146545)\n",
      "\n",
      "g_out: (0.12999409437179565, -0.0004077747871633619, -0.11834158003330231)\n",
      "L1T2_out: (0.20807409286499023, -0.0006083293119445443, -0.20165857672691345)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005029041785746813, -1.082757535186829e-05, -0.004466244485229254)\n",
      "L1T1_lora_out: (0.010058083571493626, -2.165515070373658e-05, -0.008932488970458508)\n",
      "\n",
      "L2T1_lora_B_out: (0.011329603381454945, 6.3149068409984466e-06, -0.009751798585057259)\n",
      "L2T1_lora_out: (0.02265920676290989, 1.2629813681996893e-05, -0.019503597170114517)\n",
      "\n",
      "L2T2_lora_B_out: (0.014772031456232071, -6.21604021944222e-06, -0.01727612316608429)\n",
      "L2T2_lora_out: (0.029544062912464142, -1.243208043888444e-05, -0.03455224633216858)\n",
      "\n",
      "g_out: (0.022492218762636185, -2.5061894120881334e-05, -0.02093026414513588)\n",
      "L1T2_out: (0.03051874414086342, -4.671704664360732e-05, -0.027130089700222015)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.13060836493968964, -6.392316572600976e-05, -0.11556794494390488)\n",
      "L1T1_lora_out: (0.2612167298793793, -0.0001278463314520195, -0.23113588988780975)\n",
      "\n",
      "L2T1_lora_B_out: (0.17665931582450867, -0.000142432123539038, -0.1591508537530899)\n",
      "L2T1_lora_out: (0.35331863164901733, -0.000284864247078076, -0.3183017075061798)\n",
      "\n",
      "L2T2_lora_B_out: (0.2720798850059509, -0.00019924429943785071, -0.2108355462551117)\n",
      "L2T2_lora_out: (0.5441597700119019, -0.00039848859887570143, -0.4216710925102234)\n",
      "\n",
      "g_out: (0.45549678802490234, -0.00011362435907358304, -0.34008896350860596)\n",
      "L1T2_out: (0.716713547706604, -0.0002414707123534754, -0.5306187868118286)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08895990252494812, -1.515815620223293e-05, -0.10908814519643784)\n",
      "L1T1_lora_out: (0.17791980504989624, -3.031631240446586e-05, -0.21817629039287567)\n",
      "\n",
      "L2T1_lora_B_out: (0.058142997324466705, 8.927553608373273e-06, -0.07779484987258911)\n",
      "L2T1_lora_out: (0.11628599464893341, 1.7855107216746546e-05, -0.15558969974517822)\n",
      "\n",
      "L2T2_lora_B_out: (0.19408419728279114, -2.0471094103413634e-05, -0.16055579483509064)\n",
      "L2T2_lora_out: (0.3881683945655823, -4.094218820682727e-05, -0.3211115896701813)\n",
      "\n",
      "g_out: (0.33638548851013184, -5.879730451852083e-05, -0.3014604449272156)\n",
      "L1T2_out: (0.5143052935600281, -8.911360782803968e-05, -0.5196367502212524)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.009617391042411327, 9.65275557973655e-06, -0.011198775842785835)\n",
      "L1T1_lora_out: (0.019234782084822655, 1.93055111594731e-05, -0.02239755168557167)\n",
      "\n",
      "L2T1_lora_B_out: (0.012983023189008236, 1.1435567103035282e-05, -0.012102395296096802)\n",
      "L2T1_lora_out: (0.025966046378016472, 2.2871134206070565e-05, -0.024204790592193604)\n",
      "\n",
      "L2T2_lora_B_out: (0.018539587035775185, 4.713822636404075e-05, -0.020612340420484543)\n",
      "L2T2_lora_out: (0.03707917407155037, 9.42764527280815e-05, -0.041224680840969086)\n",
      "\n",
      "g_out: (0.028716647997498512, 7.140531670302153e-05, -0.02870175801217556)\n",
      "L1T2_out: (0.04476315528154373, 9.071083331946284e-05, -0.05109930783510208)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06315677613019943, 9.588535249349661e-06, -0.07482220977544785)\n",
      "L1T1_lora_out: (0.12631355226039886, 1.9177070498699322e-05, -0.1496444195508957)\n",
      "\n",
      "L2T1_lora_B_out: (0.10367490351200104, -0.00011452342732809484, -0.1080511212348938)\n",
      "L2T1_lora_out: (0.20734980702400208, -0.00022904685465618968, -0.2161022424697876)\n",
      "\n",
      "L2T2_lora_B_out: (0.21086278557777405, -0.00021620930056087673, -0.23147575557231903)\n",
      "L2T2_lora_out: (0.4217255711555481, -0.00043241860112175345, -0.46295151114463806)\n",
      "\n",
      "g_out: (0.2742513120174408, -0.00020337174646556377, -0.38796064257621765)\n",
      "L1T2_out: (0.37914586067199707, -0.00018419470870867372, -0.5376050472259521)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04999835416674614, 0.0001707223418634385, -0.04426098242402077)\n",
      "L1T1_lora_out: (0.09999670833349228, 0.000341444683726877, -0.08852196484804153)\n",
      "\n",
      "L2T1_lora_B_out: (0.06849602609872818, 0.0001441371423425153, -0.05804428830742836)\n",
      "L2T1_lora_out: (0.13699205219745636, 0.0002882742846850306, -0.11608857661485672)\n",
      "\n",
      "L2T2_lora_B_out: (0.12885624170303345, 0.000595071236602962, -0.10400344431400299)\n",
      "L2T2_lora_out: (0.2577124834060669, 0.001190142473205924, -0.20800688862800598)\n",
      "\n",
      "g_out: (0.20312127470970154, 0.0009018683340400457, -0.15624642372131348)\n",
      "L1T2_out: (0.27114105224609375, 0.0012433129595592618, -0.21140846610069275)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02968718484044075, 0.00018204559455625713, -0.026107627898454666)\n",
      "L1T1_lora_out: (0.0593743696808815, 0.00036409118911251426, -0.05221525579690933)\n",
      "\n",
      "L2T1_lora_B_out: (0.0480523556470871, 6.618497718591243e-05, -0.0535455122590065)\n",
      "L2T1_lora_out: (0.0961047112941742, 0.00013236995437182486, -0.107091024518013)\n",
      "\n",
      "L2T2_lora_B_out: (0.10272055864334106, 0.0005261223996058106, -0.11493395268917084)\n",
      "L2T2_lora_out: (0.20544111728668213, 0.0010522447992116213, -0.22986790537834167)\n",
      "\n",
      "g_out: (0.1423264741897583, 0.000919874815735966, -0.15533702075481415)\n",
      "L1T2_out: (0.1896057426929474, 0.0012839657720178366, -0.2032349854707718)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.008118975907564163, -4.365615950518986e-06, -0.008170736022293568)\n",
      "L1T1_lora_out: (0.016237951815128326, -8.731231901037972e-06, -0.016341472044587135)\n",
      "\n",
      "L2T1_lora_B_out: (0.00659285020083189, 4.334547156759072e-06, -0.005915127228945494)\n",
      "L2T1_lora_out: (0.01318570040166378, 8.669094313518144e-06, -0.011830254457890987)\n",
      "\n",
      "L2T2_lora_B_out: (0.013918899931013584, 2.0344135919003747e-05, -0.014197799377143383)\n",
      "L2T2_lora_out: (0.02783779986202717, 4.0688271838007495e-05, -0.028395598754286766)\n",
      "\n",
      "g_out: (0.026263603940606117, 3.2019179343478754e-05, -0.023326119408011436)\n",
      "L1T2_out: (0.03993241861462593, 2.328794107597787e-05, -0.03562980145215988)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.2612250745296478, 2.8135789307270898e-06, -0.129654198884964)\n",
      "L1T1_lora_out: (0.5224501490592957, 5.6271578614541795e-06, -0.259308397769928)\n",
      "\n",
      "L2T1_lora_B_out: (0.14481139183044434, -0.00018956112035084516, -0.1705383062362671)\n",
      "L2T1_lora_out: (0.28962278366088867, -0.0003791222407016903, -0.3410766124725342)\n",
      "\n",
      "L2T2_lora_B_out: (0.5357600450515747, -0.00028164847753942013, -0.2783103287220001)\n",
      "L2T2_lora_out: (1.0715200901031494, -0.0005632969550788403, -0.5566206574440002)\n",
      "\n",
      "g_out: (0.8645440936088562, -0.00018417471437714994, -0.4051421284675598)\n",
      "L1T2_out: (1.3818683624267578, -0.0001785475469660014, -0.631049394607544)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1315521001815796, -1.9932384020648897e-05, -0.11549217998981476)\n",
      "L1T1_lora_out: (0.2631042003631592, -3.9864768041297793e-05, -0.23098435997962952)\n",
      "\n",
      "L2T1_lora_B_out: (0.07466578483581543, 3.2118555282067973e-06, -0.09269081056118011)\n",
      "L2T1_lora_out: (0.14933156967163086, 6.423711056413595e-06, -0.18538162112236023)\n",
      "\n",
      "L2T2_lora_B_out: (0.29202720522880554, 7.455435479641892e-06, -0.2278781682252884)\n",
      "L2T2_lora_out: (0.5840544104576111, 1.4910870959283784e-05, -0.4557563364505768)\n",
      "\n",
      "g_out: (0.480867475271225, 8.487168997817207e-06, -0.34543269872665405)\n",
      "L1T2_out: (0.7439717054367065, -3.1377614504890516e-05, -0.5715001821517944)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.013180219568312168, 4.051536052429583e-06, -0.010998239740729332)\n",
      "L1T1_lora_out: (0.026360439136624336, 8.103072104859166e-06, -0.021996479481458664)\n",
      "\n",
      "L2T1_lora_B_out: (0.014669202268123627, 3.375684798356815e-08, -0.014934527687728405)\n",
      "L2T1_lora_out: (0.029338404536247253, 6.75136959671363e-08, -0.02986905537545681)\n",
      "\n",
      "L2T2_lora_B_out: (0.02713620848953724, 9.184744158119429e-06, -0.02886340022087097)\n",
      "L2T2_lora_out: (0.05427241697907448, 1.8369488316238858e-05, -0.05772680044174194)\n",
      "\n",
      "g_out: (0.040428612381219864, 1.8301978343515657e-05, -0.03793413192033768)\n",
      "L1T2_out: (0.06678905338048935, 2.6405050448374823e-05, -0.05423241853713989)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1030379980802536, 4.699018245446496e-05, -0.10034054517745972)\n",
      "L1T1_lora_out: (0.2060759961605072, 9.398036490892991e-05, -0.20068109035491943)\n",
      "\n",
      "L2T1_lora_B_out: (0.08256735652685165, -9.213701559929177e-05, -0.09593787044286728)\n",
      "L2T1_lora_out: (0.1651347130537033, -0.00018427403119858354, -0.19187574088573456)\n",
      "\n",
      "L2T2_lora_B_out: (0.16643327474594116, -4.885959788225591e-05, -0.15238343179225922)\n",
      "L2T2_lora_out: (0.3328665494918823, -9.771919576451182e-05, -0.30476686358451843)\n",
      "\n",
      "g_out: (0.3264326751232147, 8.655484271002933e-05, -0.29343941807746887)\n",
      "L1T2_out: (0.5325086712837219, 0.00018053522217087448, -0.4941205084323883)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0980428084731102, -0.00020055670756846666, -0.09215454012155533)\n",
      "L1T1_lora_out: (0.1960856169462204, -0.0004011134151369333, -0.18430908024311066)\n",
      "\n",
      "L2T1_lora_B_out: (0.06794297695159912, 9.36769720283337e-05, -0.08373519033193588)\n",
      "L2T1_lora_out: (0.13588595390319824, 0.0001873539440566674, -0.16747038066387177)\n",
      "\n",
      "L2T2_lora_B_out: (0.16028299927711487, -0.0002520509879104793, -0.15633828938007355)\n",
      "L2T2_lora_out: (0.32056599855422974, -0.0005041019758209586, -0.3126765787601471)\n",
      "\n",
      "g_out: (0.2961147129535675, -0.0006914561963640153, -0.2850329577922821)\n",
      "L1T2_out: (0.48433762788772583, -0.001092569320462644, -0.4522712230682373)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.12378406524658203, 0.00016909277474042028, -0.10752899944782257)\n",
      "L1T1_lora_out: (0.24756813049316406, 0.00033818554948084056, -0.21505799889564514)\n",
      "\n",
      "L2T1_lora_B_out: (0.07899555563926697, 8.320440974785015e-05, -0.10948203504085541)\n",
      "L2T1_lora_out: (0.15799111127853394, 0.0001664088194957003, -0.21896407008171082)\n",
      "\n",
      "L2T2_lora_B_out: (0.2649155557155609, 0.00031198246870189905, -0.2829490602016449)\n",
      "L2T2_lora_out: (0.5298311114311218, 0.0006239649374037981, -0.5658981204032898)\n",
      "\n",
      "g_out: (0.433449923992157, 0.00045755624887533486, -0.3984139561653137)\n",
      "L1T2_out: (0.6797590255737305, 0.0007957417401485145, -0.5538527965545654)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006713590119034052, 3.010550244653132e-06, -0.005669587291777134)\n",
      "L1T1_lora_out: (0.013427180238068104, 6.021100489306264e-06, -0.011339174583554268)\n",
      "\n",
      "L2T1_lora_B_out: (0.011150709353387356, -1.971175879589282e-05, -0.010201959870755672)\n",
      "L2T1_lora_out: (0.02230141870677471, -3.942351759178564e-05, -0.020403919741511345)\n",
      "\n",
      "L2T2_lora_B_out: (0.01848096214234829, -1.147760926301089e-07, -0.019005199894309044)\n",
      "L2T2_lora_out: (0.03696192428469658, -2.295521852602178e-07, -0.03801039978861809)\n",
      "\n",
      "g_out: (0.030868208035826683, 3.919396476703696e-05, -0.0265175923705101)\n",
      "L1T2_out: (0.04115205630660057, 4.521506707533263e-05, -0.036255501210689545)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.22424191236495972, 3.5355704312678427e-05, -0.2150401622056961)\n",
      "L1T1_lora_out: (0.44848382472991943, 7.071140862535685e-05, -0.4300803244113922)\n",
      "\n",
      "L2T1_lora_B_out: (0.14830362796783447, -0.00023817560577299446, -0.14528930187225342)\n",
      "L2T1_lora_out: (0.29660725593566895, -0.0004763512115459889, -0.29057860374450684)\n",
      "\n",
      "L2T2_lora_B_out: (0.36598989367485046, -0.0002477356174495071, -0.4342665672302246)\n",
      "L2T2_lora_out: (0.7319797873497009, -0.0004954712348990142, -0.8685331344604492)\n",
      "\n",
      "g_out: (0.6740631461143494, -1.9119976059300825e-05, -0.7449266314506531)\n",
      "L1T2_out: (1.122546911239624, 5.1591447117971256e-05, -1.1750069856643677)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10779078304767609, -2.4271059373859316e-05, -0.11816365271806717)\n",
      "L1T1_lora_out: (0.21558156609535217, -4.854211874771863e-05, -0.23632730543613434)\n",
      "\n",
      "L2T1_lora_B_out: (0.08973191678524017, 1.7706348444335163e-05, -0.07554913312196732)\n",
      "L2T1_lora_out: (0.17946383357048035, 3.5412696888670325e-05, -0.15109826624393463)\n",
      "\n",
      "L2T2_lora_B_out: (0.29351815581321716, -3.6777542845811695e-05, -0.30596256256103516)\n",
      "L2T2_lora_out: (0.5870363116264343, -7.355508569162339e-05, -0.6119251251220703)\n",
      "\n",
      "g_out: (0.4547373652458191, -0.00010896778258029372, -0.4984932541847229)\n",
      "L1T2_out: (0.6703189611434937, -0.00015750990132801235, -0.7314587235450745)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.010632405988872051, -1.441620770492591e-05, -0.011892011389136314)\n",
      "L1T1_lora_out: (0.021264811977744102, -2.883241540985182e-05, -0.02378402277827263)\n",
      "\n",
      "L2T1_lora_B_out: (0.009107602760195732, -6.869064236525446e-06, -0.010007644072175026)\n",
      "L2T1_lora_out: (0.018215205520391464, -1.3738128473050892e-05, -0.020015288144350052)\n",
      "\n",
      "L2T2_lora_B_out: (0.017789946869015694, 5.6382368711638264e-06, -0.018906772136688232)\n",
      "L2T2_lora_out: (0.03557989373803139, 1.1276473742327653e-05, -0.037813544273376465)\n",
      "\n",
      "g_out: (0.03226018697023392, 2.5014602215378545e-05, -0.03484367951750755)\n",
      "L1T2_out: (0.048042621463537216, -3.81781092073652e-06, -0.051708269864320755)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0608973354101181, 7.390172686427832e-05, -0.06802241504192352)\n",
      "L1T1_lora_out: (0.1217946708202362, 0.00014780345372855663, -0.13604483008384705)\n",
      "\n",
      "L2T1_lora_B_out: (0.054145198315382004, 5.1526476454455405e-06, -0.06066393852233887)\n",
      "L2T1_lora_out: (0.10829039663076401, 1.0305295290891081e-05, -0.12132787704467773)\n",
      "\n",
      "L2T2_lora_B_out: (0.11972440779209137, 0.00010896421736106277, -0.11965187638998032)\n",
      "L2T2_lora_out: (0.23944881558418274, 0.00021792843472212553, -0.23930375277996063)\n",
      "\n",
      "g_out: (0.2173181176185608, 0.0002076231176033616, -0.2277209758758545)\n",
      "L1T2_out: (0.339112788438797, 0.0003554266004357487, -0.36376580595970154)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03530360758304596, -0.00018863998411688954, -0.044589824974536896)\n",
      "L1T1_lora_out: (0.07060721516609192, -0.00037727996823377907, -0.08917964994907379)\n",
      "\n",
      "L2T1_lora_B_out: (0.07782069593667984, 0.0001851104461820796, -0.06957710534334183)\n",
      "L2T1_lora_out: (0.15564139187335968, 0.0003702208923641592, -0.13915421068668365)\n",
      "\n",
      "L2T2_lora_B_out: (0.11189284175634384, 3.302853292552754e-05, -0.11378555744886398)\n",
      "L2T2_lora_out: (0.22378568351268768, 6.605706585105509e-05, -0.22757111489772797)\n",
      "\n",
      "g_out: (0.13636431097984314, -0.0003041638119611889, -0.14886386692523956)\n",
      "L1T2_out: (0.17023566365242004, -0.000681443780194968, -0.20890477299690247)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03432194143533707, -0.0001761999592417851, -0.0343933142721653)\n",
      "L1T1_lora_out: (0.06864388287067413, -0.0003523999184835702, -0.0687866285443306)\n",
      "\n",
      "L2T1_lora_B_out: (0.048910584300756454, 3.009004649356939e-05, -0.05253402888774872)\n",
      "L2T1_lora_out: (0.09782116860151291, 6.018009298713878e-05, -0.10506805777549744)\n",
      "\n",
      "L2T2_lora_B_out: (0.08095613121986389, -0.00017708608356770128, -0.0922575443983078)\n",
      "L2T2_lora_out: (0.16191226243972778, -0.00035417216713540256, -0.1845150887966156)\n",
      "\n",
      "g_out: (0.11378185451030731, -0.0004143522528465837, -0.13744303584098816)\n",
      "L1T2_out: (0.1608312427997589, -0.0007667522295378149, -0.17280907928943634)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003958043642342091, -3.2404698231403017e-06, -0.004006220493465662)\n",
      "L1T1_lora_out: (0.007916087284684181, -6.4809396462806035e-06, -0.008012440986931324)\n",
      "\n",
      "L2T1_lora_B_out: (0.006262739654630423, 4.005346454505343e-06, -0.006189826875925064)\n",
      "L2T1_lora_out: (0.012525479309260845, 8.010692909010686e-06, -0.012379653751850128)\n",
      "\n",
      "L2T2_lora_B_out: (0.009228761307895184, 5.489032446348574e-06, -0.009204870089888573)\n",
      "L2T2_lora_out: (0.018457522615790367, 1.0978064892697148e-05, -0.018409740179777145)\n",
      "\n",
      "g_out: (0.018227221444249153, 2.9673717563127866e-06, -0.014496121555566788)\n",
      "L1T2_out: (0.026143308728933334, -3.513567435220466e-06, -0.021000057458877563)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08158589154481888, -4.080465714650927e-06, -0.0763264074921608)\n",
      "L1T1_lora_out: (0.16317178308963776, -8.160931429301854e-06, -0.1526528149843216)\n",
      "\n",
      "L2T1_lora_B_out: (0.17175546288490295, -1.7959560864255764e-05, -0.1634674221277237)\n",
      "L2T1_lora_out: (0.3435109257698059, -3.591912172851153e-05, -0.3269348442554474)\n",
      "\n",
      "L2T2_lora_B_out: (0.22179661691188812, -2.4300657969433814e-05, -0.22575491666793823)\n",
      "L2T2_lora_out: (0.44359323382377625, -4.860131593886763e-05, -0.45150983333587646)\n",
      "\n",
      "g_out: (0.2992788553237915, -1.2682187843893189e-05, -0.2508874833583832)\n",
      "L1T2_out: (0.46245062351226807, -2.0843130187131464e-05, -0.36209723353385925)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1047133356332779, 4.749091658595717e-06, -0.09947410970926285)\n",
      "L1T1_lora_out: (0.2094266712665558, 9.498183317191433e-06, -0.1989482194185257)\n",
      "\n",
      "L2T1_lora_B_out: (0.06697718799114227, -2.01459442905616e-05, -0.06656648218631744)\n",
      "L2T1_lora_out: (0.13395437598228455, -4.02918885811232e-05, -0.1331329643726349)\n",
      "\n",
      "L2T2_lora_B_out: (0.21279685199260712, 4.1946041164919734e-05, -0.28468939661979675)\n",
      "L2T2_lora_out: (0.42559370398521423, 8.389208232983947e-05, -0.5693787932395935)\n",
      "\n",
      "g_out: (0.32459574937820435, 0.00012418397818692029, -0.46815037727355957)\n",
      "L1T2_out: (0.49629831314086914, 0.00013368217332754284, -0.6670985817909241)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.009192417375743389, 6.564472641912289e-06, -0.0095777353271842)\n",
      "L1T1_lora_out: (0.018384834751486778, 1.3128945283824578e-05, -0.0191554706543684)\n",
      "\n",
      "L2T1_lora_B_out: (0.010273558087646961, 3.0540923035005108e-06, -0.009112857282161713)\n",
      "L2T1_lora_out: (0.020547116175293922, 6.1081846070010215e-06, -0.018225714564323425)\n",
      "\n",
      "L2T2_lora_B_out: (0.022390637546777725, 2.317007601959631e-05, -0.021323340013623238)\n",
      "L2T2_lora_out: (0.04478127509355545, 4.634015203919262e-05, -0.042646680027246475)\n",
      "\n",
      "g_out: (0.038215186446905136, 4.0231967432191595e-05, -0.03443833440542221)\n",
      "L1T2_out: (0.05641865357756615, 5.336091635399498e-05, -0.050905004143714905)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06854646652936935, 3.923716940334998e-05, -0.08849005401134491)\n",
      "L1T1_lora_out: (0.1370929330587387, 7.847433880669996e-05, -0.17698010802268982)\n",
      "\n",
      "L2T1_lora_B_out: (0.09251455962657928, 0.00011182177695445716, -0.08831366896629333)\n",
      "L2T1_lora_out: (0.18502911925315857, 0.00022364355390891433, -0.17662733793258667)\n",
      "\n",
      "L2T2_lora_B_out: (0.20235149562358856, 0.0002749726118054241, -0.25269922614097595)\n",
      "L2T2_lora_out: (0.4047029912471771, 0.0005499452236108482, -0.5053984522819519)\n",
      "\n",
      "g_out: (0.32299748063087463, 0.00032630166970193386, -0.4141121506690979)\n",
      "L1T2_out: (0.4585545063018799, 0.00040477601578459144, -0.5910922288894653)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1061694547533989, -0.00039598511648364365, -0.09076079726219177)\n",
      "L1T1_lora_out: (0.2123389095067978, -0.0007919702329672873, -0.18152159452438354)\n",
      "\n",
      "L2T1_lora_B_out: (0.10482490062713623, 0.00014893017942085862, -0.1206313818693161)\n",
      "L2T1_lora_out: (0.20964980125427246, 0.00029786035884171724, -0.2412627637386322)\n",
      "\n",
      "L2T2_lora_B_out: (0.2311776876449585, -0.00057231355458498, -0.2961920499801636)\n",
      "L2T2_lora_out: (0.462355375289917, -0.00114462710916996, -0.5923840999603271)\n",
      "\n",
      "g_out: (0.34257078170776367, -0.0014424873515963554, -0.4164472222328186)\n",
      "L1T2_out: (0.4872756004333496, -0.002234457293525338, -0.5879834890365601)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.19350624084472656, -8.689337118994445e-05, -0.17940008640289307)\n",
      "L1T1_lora_out: (0.3870124816894531, -0.0001737867423798889, -0.35880017280578613)\n",
      "\n",
      "L2T1_lora_B_out: (0.1612272560596466, 6.687275163130835e-05, -0.15304580330848694)\n",
      "L2T1_lora_out: (0.3224545121192932, 0.0001337455032626167, -0.3060916066169739)\n",
      "\n",
      "L2T2_lora_B_out: (0.35071972012519836, 0.00010985552944475785, -0.32197922468185425)\n",
      "L2T2_lora_out: (0.7014394402503967, 0.0002197110588895157, -0.6439584493637085)\n",
      "\n",
      "g_out: (0.5696457624435425, 8.59655046951957e-05, -0.5317928791046143)\n",
      "L1T2_out: (0.9566582441329956, -8.782126678852364e-05, -0.8905930519104004)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020783739164471626, 1.4075645822231309e-06, -0.01947532780468464)\n",
      "L1T1_lora_out: (0.04156747832894325, 2.8151291644462617e-06, -0.03895065560936928)\n",
      "\n",
      "L2T1_lora_B_out: (0.016719525679945946, -1.2827838872908615e-05, -0.021949799731373787)\n",
      "L2T1_lora_out: (0.03343905135989189, -2.565567774581723e-05, -0.043899599462747574)\n",
      "\n",
      "L2T2_lora_B_out: (0.04183614253997803, -1.5722531315987e-05, -0.045447468757629395)\n",
      "L2T2_lora_out: (0.08367228507995605, -3.1445062631974e-05, -0.09089493751525879)\n",
      "\n",
      "g_out: (0.06440822035074234, -5.789382157672662e-06, -0.06895957887172699)\n",
      "L1T2_out: (0.09530527889728546, -2.974254130094778e-06, -0.10647347569465637)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10457804054021835, -8.191081724362448e-05, -0.11249551177024841)\n",
      "L1T1_lora_out: (0.2091560810804367, -0.00016382163448724896, -0.22499102354049683)\n",
      "\n",
      "L2T1_lora_B_out: (0.2007378190755844, -0.00021208965335972607, -0.18006531894207)\n",
      "L2T1_lora_out: (0.4014756381511688, -0.00042417930671945214, -0.36013063788414)\n",
      "\n",
      "L2T2_lora_B_out: (0.4244624078273773, -0.0006352647324092686, -0.3601230978965759)\n",
      "L2T2_lora_out: (0.8489248156547546, -0.0012705294648185372, -0.7202461957931519)\n",
      "\n",
      "g_out: (0.5726886987686157, -0.0008463500998914242, -0.5060714483261108)\n",
      "L1T2_out: (0.75006103515625, -0.001010171719826758, -0.6824561357498169)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.11967921257019043, 3.269380613346584e-05, -0.09708467870950699)\n",
      "L1T1_lora_out: (0.23935842514038086, 6.538761226693168e-05, -0.19416935741901398)\n",
      "\n",
      "L2T1_lora_B_out: (0.06611916422843933, -3.2720319723011926e-05, -0.06871645152568817)\n",
      "L2T1_lora_out: (0.13223832845687866, -6.544063944602385e-05, -0.13743290305137634)\n",
      "\n",
      "L2T2_lora_B_out: (0.13858291506767273, 2.1695279883715557e-06, -0.11311041563749313)\n",
      "L2T2_lora_out: (0.27716583013534546, 4.339055976743111e-06, -0.22622083127498627)\n",
      "\n",
      "g_out: (0.231669083237648, 6.977969314903021e-05, -0.2002025693655014)\n",
      "L1T2_out: (0.4710274934768677, 0.0001351673126919195, -0.3943719267845154)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.011358101852238178, -8.992268703877926e-06, -0.01088621187955141)\n",
      "L1T1_lora_out: (0.022716203704476357, -1.7984537407755852e-05, -0.02177242375910282)\n",
      "\n",
      "L2T1_lora_B_out: (0.011081657372415066, -3.941464001400163e-06, -0.01173661183565855)\n",
      "L2T1_lora_out: (0.02216331474483013, -7.882928002800327e-06, -0.0234732236713171)\n",
      "\n",
      "L2T2_lora_B_out: (0.02516976371407509, -1.690436874923762e-05, -0.027217483147978783)\n",
      "L2T2_lora_out: (0.05033952742815018, -3.380873749847524e-05, -0.054434966295957565)\n",
      "\n",
      "g_out: (0.038005560636520386, -2.5925810405169614e-05, -0.04040491208434105)\n",
      "L1T2_out: (0.05668655410408974, -4.391034599393606e-05, -0.0582265630364418)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06559550762176514, 1.7183645013574278e-06, -0.06345578283071518)\n",
      "L1T1_lora_out: (0.13119101524353027, 3.4367290027148556e-06, -0.12691156566143036)\n",
      "\n",
      "L2T1_lora_B_out: (0.11405732482671738, 0.00014630226360168308, -0.13393175601959229)\n",
      "L2T1_lora_out: (0.22811464965343475, 0.00029260452720336616, -0.26786351203918457)\n",
      "\n",
      "L2T2_lora_B_out: (0.25575053691864014, 6.217808549990878e-05, -0.26151764392852783)\n",
      "L2T2_lora_out: (0.5115010738372803, 0.00012435617099981755, -0.5230352878570557)\n",
      "\n",
      "g_out: (0.3378494679927826, -0.00016824837075546384, -0.32945066690444946)\n",
      "L1T2_out: (0.4532304108142853, -0.0001648116303840652, -0.4452713131904602)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1852187067270279, 9.547077934257686e-05, -0.1290603131055832)\n",
      "L1T1_lora_out: (0.3704374134540558, 0.00019094155868515372, -0.2581206262111664)\n",
      "\n",
      "L2T1_lora_B_out: (0.09692154079675674, -7.37847585696727e-05, -0.14177899062633514)\n",
      "L2T1_lora_out: (0.1938430815935135, -0.0001475695171393454, -0.2835579812526703)\n",
      "\n",
      "L2T2_lora_B_out: (0.3241989016532898, -2.30322821153095e-05, -0.3366965651512146)\n",
      "L2T2_lora_out: (0.6483978033065796, -4.6064564230619e-05, -0.6733931303024292)\n",
      "\n",
      "g_out: (0.6111082434654236, 0.00010150486923521385, -0.5467795133590698)\n",
      "L1T2_out: (0.9815456867218018, 0.0002924464351963252, -0.798781156539917)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10144241154193878, -0.0001424199144821614, -0.09761352092027664)\n",
      "L1T1_lora_out: (0.20288482308387756, -0.0002848398289643228, -0.19522704184055328)\n",
      "\n",
      "L2T1_lora_B_out: (0.12015107274055481, 4.041884676553309e-05, -0.11031021177768707)\n",
      "L2T1_lora_out: (0.24030214548110962, 8.083769353106618e-05, -0.22062042355537415)\n",
      "\n",
      "L2T2_lora_B_out: (0.3771144449710846, -0.00022789779177401215, -0.34455442428588867)\n",
      "L2T2_lora_out: (0.7542288899421692, -0.0004557955835480243, -0.6891088485717773)\n",
      "\n",
      "g_out: (0.5450422763824463, -0.0005366331897675991, -0.4684884250164032)\n",
      "L1T2_out: (0.7479270696640015, -0.0008214732515625656, -0.6525020599365234)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023739539086818695, -1.9927940684283385e-06, -0.02467777393758297)\n",
      "L1T1_lora_out: (0.04747907817363739, -3.985588136856677e-06, -0.04935554787516594)\n",
      "\n",
      "L2T1_lora_B_out: (0.030239958316087723, -5.202283318794798e-06, -0.029975565150380135)\n",
      "L2T1_lora_out: (0.060479916632175446, -1.0404566637589596e-05, -0.05995113030076027)\n",
      "\n",
      "L2T2_lora_B_out: (0.04242955520749092, -1.44623718369985e-05, -0.04072001948952675)\n",
      "L2T2_lora_out: (0.08485911041498184, -2.8924743673997e-05, -0.0814400389790535)\n",
      "\n",
      "g_out: (0.07070697844028473, -1.8520178855396807e-05, -0.06111333146691322)\n",
      "L1T2_out: (0.11818605661392212, -2.250576471851673e-05, -0.09628774225711823)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.14406272768974304, -9.599186887498945e-05, -0.15650488436222076)\n",
      "L1T1_lora_out: (0.2881254553794861, -0.0001919837377499789, -0.31300976872444153)\n",
      "\n",
      "L2T1_lora_B_out: (0.11892260611057281, -0.00024629742256365716, -0.11813880503177643)\n",
      "L2T1_lora_out: (0.23784521222114563, -0.0004925948451273143, -0.23627761006355286)\n",
      "\n",
      "L2T2_lora_B_out: (0.38752639293670654, -0.000503069837577641, -0.3048945963382721)\n",
      "L2T2_lora_out: (0.7750527858734131, -0.001006139675155282, -0.6097891926765442)\n",
      "\n",
      "g_out: (0.5548509359359741, -0.0005135448300279677, -0.5411297082901001)\n",
      "L1T2_out: (0.8073841333389282, -0.0007055285386741161, -0.8541394472122192)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.13149955868721008, 5.066107405582443e-05, -0.11640222370624542)\n",
      "L1T1_lora_out: (0.26299911737442017, 0.00010132214811164886, -0.23280444741249084)\n",
      "\n",
      "L2T1_lora_B_out: (0.08461693674325943, -4.206064477330074e-05, -0.08962617814540863)\n",
      "L2T1_lora_out: (0.16923387348651886, -8.412128954660147e-05, -0.17925235629081726)\n",
      "\n",
      "L2T2_lora_B_out: (0.1774161458015442, 2.5524754164507613e-05, -0.23412129282951355)\n",
      "L2T2_lora_out: (0.3548322916030884, 5.1049508329015225e-05, -0.4682425856590271)\n",
      "\n",
      "g_out: (0.3168877065181732, 0.00013517079059965909, -0.3400002717971802)\n",
      "L1T2_out: (0.579886794090271, 0.00023649292415939271, -0.5540541410446167)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.007846849039196968, 2.273848849654314e-06, -0.009472228586673737)\n",
      "L1T1_lora_out: (0.015693698078393936, 4.547697699308628e-06, -0.018944457173347473)\n",
      "\n",
      "L2T1_lora_B_out: (0.012733940966427326, 1.205330863740528e-05, -0.01215925719588995)\n",
      "L2T1_lora_out: (0.025467881932854652, 2.410661727481056e-05, -0.0243185143917799)\n",
      "\n",
      "L2T2_lora_B_out: (0.026161903515458107, -1.6569789295317605e-05, -0.02918241173028946)\n",
      "L2T2_lora_out: (0.052323807030916214, -3.313957859063521e-05, -0.05836482346057892)\n",
      "\n",
      "g_out: (0.0339854434132576, -5.7246197684435174e-05, -0.03685810789465904)\n",
      "L1T2_out: (0.045682549476623535, -5.2698494982905686e-05, -0.05052223056554794)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0457366518676281, 2.934863732662052e-05, -0.05095982551574707)\n",
      "L1T1_lora_out: (0.0914733037352562, 5.869727465324104e-05, -0.10191965103149414)\n",
      "\n",
      "L2T1_lora_B_out: (0.052561625838279724, 8.99643637239933e-05, -0.05988481268286705)\n",
      "L2T1_lora_out: (0.10512325167655945, 0.0001799287274479866, -0.1197696253657341)\n",
      "\n",
      "L2T2_lora_B_out: (0.11190620809793472, 0.0002354190219193697, -0.16693711280822754)\n",
      "L2T2_lora_out: (0.22381241619586945, 0.0004708380438387394, -0.3338742256164551)\n",
      "\n",
      "g_out: (0.18425492942333221, 0.0002909093163907528, -0.2320249229669571)\n",
      "L1T2_out: (0.2757282257080078, 0.00034960659104399383, -0.32439085841178894)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08019313961267471, 0.00010396599827799946, -0.08145327121019363)\n",
      "L1T1_lora_out: (0.16038627922534943, 0.00020793199655599892, -0.16290654242038727)\n",
      "\n",
      "L2T1_lora_B_out: (0.07417170703411102, 0.0002976521791424602, -0.0649566799402237)\n",
      "L2T1_lora_out: (0.14834341406822205, 0.0005953043582849205, -0.1299133598804474)\n",
      "\n",
      "L2T2_lora_B_out: (0.15819191932678223, 0.0007937569753266871, -0.1328260600566864)\n",
      "L2T2_lora_out: (0.31638383865356445, 0.0015875139506533742, -0.2656521201133728)\n",
      "\n",
      "g_out: (0.23959298431873322, 0.000992209417745471, -0.23707126080989838)\n",
      "L1T2_out: (0.39997926354408264, 0.0012001413851976395, -0.3852846026420593)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.14899134635925293, 6.92657777108252e-05, -0.1460747867822647)\n",
      "L1T1_lora_out: (0.29798269271850586, 0.0001385315554216504, -0.2921495735645294)\n",
      "\n",
      "L2T1_lora_B_out: (0.0804256796836853, 0.0003512331168167293, -0.07324159890413284)\n",
      "L2T1_lora_out: (0.1608513593673706, 0.0007024662336334586, -0.14648319780826569)\n",
      "\n",
      "L2T2_lora_B_out: (0.17010386288166046, 0.0006722195539623499, -0.17333169281482697)\n",
      "L2T2_lora_out: (0.3402077257633209, 0.0013444391079246998, -0.34666338562965393)\n",
      "\n",
      "g_out: (0.26436829566955566, 0.0006419728160835803, -0.3194986581802368)\n",
      "L1T2_out: (0.5539492964744568, 0.0007805044297128916, -0.6116482019424438)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.007627103012055159, 2.9226589504105505e-07, -0.007481657899916172)\n",
      "L1T1_lora_out: (0.015254206024110317, 5.845317900821101e-07, -0.014963315799832344)\n",
      "\n",
      "L2T1_lora_B_out: (0.018414504826068878, 4.866646577283973e-06, -0.01895671896636486)\n",
      "L2T1_lora_out: (0.036829009652137756, 9.733293154567946e-06, -0.03791343793272972)\n",
      "\n",
      "L2T2_lora_B_out: (0.03515935316681862, 9.444427632843144e-06, -0.03381570801138878)\n",
      "L2T2_lora_out: (0.07031870633363724, 1.888885526568629e-05, -0.06763141602277756)\n",
      "\n",
      "g_out: (0.042647331953048706, 9.155561201623641e-06, -0.04268060252070427)\n",
      "L1T2_out: (0.049516621977090836, 9.7400925369584e-06, -0.051844388246536255)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.14831674098968506, -0.00013071771536488086, -0.16445814073085785)\n",
      "L1T1_lora_out: (0.2966334819793701, -0.0002614354307297617, -0.3289162814617157)\n",
      "\n",
      "L2T1_lora_B_out: (0.1873038113117218, -0.00022047477250453085, -0.16072270274162292)\n",
      "L2T1_lora_out: (0.3746076226234436, -0.0004409495450090617, -0.32144540548324585)\n",
      "\n",
      "L2T2_lora_B_out: (0.3992265462875366, -0.0006629019044339657, -0.37034568190574646)\n",
      "L2T2_lora_out: (0.7984530925750732, -0.0013258038088679314, -0.7406913638114929)\n",
      "\n",
      "g_out: (0.5339405536651611, -0.0008848542929627001, -0.5552508234977722)\n",
      "L1T2_out: (0.7447037696838379, -0.0011462896363809705, -0.8597630262374878)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09033884108066559, -2.6162908397964202e-05, -0.16709263622760773)\n",
      "L1T1_lora_out: (0.18067768216133118, -5.2325816795928404e-05, -0.33418527245521545)\n",
      "\n",
      "L2T1_lora_B_out: (0.10814635455608368, 5.88751099712681e-05, -0.08223818987607956)\n",
      "L2T1_lora_out: (0.21629270911216736, 0.0001177502199425362, -0.16447637975215912)\n",
      "\n",
      "L2T2_lora_B_out: (0.22851122915744781, -3.746281436178833e-05, -0.2606934905052185)\n",
      "L2T2_lora_out: (0.45702245831489563, -7.492562872357666e-05, -0.521386981010437)\n",
      "\n",
      "g_out: (0.32659971714019775, -0.00019267584139015526, -0.4671218991279602)\n",
      "L1T2_out: (0.4770742356777191, -0.00024500166182406247, -0.7989850044250488)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.010726566426455975, -7.75317403167719e-06, -0.011741777881979942)\n",
      "L1T1_lora_out: (0.02145313285291195, -1.550634806335438e-05, -0.023483555763959885)\n",
      "\n",
      "L2T1_lora_B_out: (0.012000782415270805, 1.6169031368917786e-05, -0.00991116464138031)\n",
      "L2T1_lora_out: (0.02400156483054161, 3.233806273783557e-05, -0.01982232928276062)\n",
      "\n",
      "L2T2_lora_B_out: (0.022759826853871346, -6.549558293045266e-06, -0.02628832869231701)\n",
      "L2T2_lora_out: (0.04551965370774269, -1.3099116586090531e-05, -0.05257665738463402)\n",
      "\n",
      "g_out: (0.03828724846243858, -4.543718387139961e-05, -0.037347108125686646)\n",
      "L1T2_out: (0.05623890832066536, -6.094353011576459e-05, -0.05456864833831787)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.048691526055336, 4.1335075366077945e-05, -0.04931510239839554)\n",
      "L1T1_lora_out: (0.097383052110672, 8.267015073215589e-05, -0.09863020479679108)\n",
      "\n",
      "L2T1_lora_B_out: (0.05899936333298683, -1.2755222087434959e-05, -0.06628548353910446)\n",
      "L2T1_lora_out: (0.11799872666597366, -2.5510444174869917e-05, -0.13257096707820892)\n",
      "\n",
      "L2T2_lora_B_out: (0.1195797249674797, 3.952243787352927e-05, -0.1275312751531601)\n",
      "L2T2_lora_out: (0.2391594499349594, 7.904487574705854e-05, -0.2550625503063202)\n",
      "\n",
      "g_out: (0.19191092252731323, 0.00010455531446496025, -0.19814230501651764)\n",
      "L1T2_out: (0.2835553288459778, 0.00018722546519711614, -0.28879594802856445)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05543974041938782, 1.5784400602569804e-05, -0.045373935252428055)\n",
      "L1T1_lora_out: (0.11087948083877563, 3.156880120513961e-05, -0.09074787050485611)\n",
      "\n",
      "L2T1_lora_B_out: (0.11294931173324585, 0.0003761326661333442, -0.1044628843665123)\n",
      "L2T1_lora_out: (0.2258986234664917, 0.0007522653322666883, -0.2089257687330246)\n",
      "\n",
      "L2T2_lora_B_out: (0.230390727519989, 0.0004998523509129882, -0.15124428272247314)\n",
      "L2T2_lora_out: (0.460781455039978, 0.0009997047018259764, -0.3024885654449463)\n",
      "\n",
      "g_out: (0.3003500998020172, 0.0002474396023899317, -0.19471518695354462)\n",
      "L1T2_out: (0.41122958064079285, 0.0002790082944557071, -0.2799295485019684)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10153717547655106, -0.00015090724627953023, -0.11398149281740189)\n",
      "L1T1_lora_out: (0.2030743509531021, -0.00030181449255906045, -0.22796298563480377)\n",
      "\n",
      "L2T1_lora_B_out: (0.13318389654159546, -0.0002164082252420485, -0.1324901580810547)\n",
      "L2T1_lora_out: (0.2663677930831909, -0.000432816450484097, -0.2649803161621094)\n",
      "\n",
      "L2T2_lora_B_out: (0.2024381309747696, -0.0006216727197170258, -0.2005670964717865)\n",
      "L2T2_lora_out: (0.4048762619495392, -0.0012433454394340515, -0.401134192943573)\n",
      "\n",
      "g_out: (0.3009355068206787, -0.0008105290471576154, -0.29438310861587524)\n",
      "L1T2_out: (0.46350961923599243, -0.0011123435106128454, -0.4422113299369812)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020284274592995644, -6.45774025542778e-06, -0.026728227734565735)\n",
      "L1T1_lora_out: (0.04056854918599129, -1.291548051085556e-05, -0.05345645546913147)\n",
      "\n",
      "L2T1_lora_B_out: (0.016074353829026222, -1.7483746432844782e-06, -0.014652040787041187)\n",
      "L2T1_lora_out: (0.032148707658052444, -3.4967492865689564e-06, -0.029304081574082375)\n",
      "\n",
      "L2T2_lora_B_out: (0.03830045089125633, 1.3356476301851217e-05, -0.03959597274661064)\n",
      "L2T2_lora_out: (0.07660090178251266, 2.6712952603702433e-05, -0.07919194549322128)\n",
      "\n",
      "g_out: (0.06959201395511627, 3.0209699616534635e-05, -0.0699799656867981)\n",
      "L1T2_out: (0.10756164789199829, 1.7294220015173778e-05, -0.11890445649623871)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.14927485585212708, -0.0002165248733945191, -0.1654452234506607)\n",
      "L1T1_lora_out: (0.29854971170425415, -0.0004330497467890382, -0.3308904469013214)\n",
      "\n",
      "L2T1_lora_B_out: (0.1523866504430771, -0.00018499715952202678, -0.17915894091129303)\n",
      "L2T1_lora_out: (0.3047733008861542, -0.00036999431904405355, -0.35831788182258606)\n",
      "\n",
      "L2T2_lora_B_out: (0.30548346042633057, -0.0004096651100553572, -0.29839372634887695)\n",
      "L2T2_lora_out: (0.6109669208526611, -0.0008193302201107144, -0.5967874526977539)\n",
      "\n",
      "g_out: (0.5044856071472168, -0.00044933593017049134, -0.5202555656433105)\n",
      "L1T2_out: (0.803035318851471, -0.0008823856478556991, -0.8511459827423096)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.16160282492637634, 7.933209417387843e-05, -0.28376370668411255)\n",
      "L1T1_lora_out: (0.3232056498527527, 0.00015866418834775686, -0.5675274133682251)\n",
      "\n",
      "L2T1_lora_B_out: (0.08741121739149094, -0.00011801997607108206, -0.13857290148735046)\n",
      "L2T1_lora_out: (0.17482243478298187, -0.0002360399521421641, -0.2771458029747009)\n",
      "\n",
      "L2T2_lora_B_out: (0.24371811747550964, 3.415285755181685e-05, -0.37026166915893555)\n",
      "L2T2_lora_out: (0.4874362349510193, 6.83057151036337e-05, -0.7405233383178711)\n",
      "\n",
      "g_out: (0.4449184536933899, 0.00030434568179771304, -0.7413043975830078)\n",
      "L1T2_out: (0.7681241035461426, 0.00046300989924930036, -1.308831810951233)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01896136999130249, 2.0672241589636542e-05, -0.01737714186310768)\n",
      "L1T1_lora_out: (0.03792273998260498, 4.1344483179273084e-05, -0.03475428372621536)\n",
      "\n",
      "L2T1_lora_B_out: (0.012914151884615421, 6.595791546715191e-06, -0.014364978298544884)\n",
      "L2T1_lora_out: (0.025828303769230843, 1.3191583093430381e-05, -0.028729956597089767)\n",
      "\n",
      "L2T2_lora_B_out: (0.026025788858532906, 5.1936858653789386e-05, -0.027608538046479225)\n",
      "L2T2_lora_out: (0.05205157771706581, 0.00010387371730757877, -0.05521707609295845)\n",
      "\n",
      "g_out: (0.043027281761169434, 9.06821369426325e-05, -0.049189236015081406)\n",
      "L1T2_out: (0.0711214542388916, 0.00013202661648392677, -0.07691031694412231)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02854202315211296, 1.5128334780456498e-05, -0.031705860048532486)\n",
      "L1T1_lora_out: (0.05708404630422592, 3.0256669560912997e-05, -0.06341172009706497)\n",
      "\n",
      "L2T1_lora_B_out: (0.05236059054732323, 2.0937817680533044e-05, -0.06253296136856079)\n",
      "L2T1_lora_out: (0.10472118109464645, 4.187563536106609e-05, -0.12506592273712158)\n",
      "\n",
      "L2T2_lora_B_out: (0.08905643969774246, 0.00013456687156576663, -0.08809690177440643)\n",
      "L2T2_lora_out: (0.17811287939548492, 0.00026913374313153327, -0.17619380354881287)\n",
      "\n",
      "g_out: (0.12014627456665039, 0.0002272581186844036, -0.1159534752368927)\n",
      "L1T2_out: (0.1573903113603592, 0.00025751482462510467, -0.15527328848838806)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03019093908369541, 0.0001315728441113606, -0.04427759721875191)\n",
      "L1T1_lora_out: (0.06038187816739082, 0.0002631456882227212, -0.08855519443750381)\n",
      "\n",
      "L2T1_lora_B_out: (0.06674881279468536, 0.00035807795939035714, -0.06439905613660812)\n",
      "L2T1_lora_out: (0.13349762558937073, 0.0007161559187807143, -0.12879811227321625)\n",
      "\n",
      "L2T2_lora_B_out: (0.10768316686153412, 0.0004276135587133467, -0.10939453542232513)\n",
      "L2T2_lora_out: (0.21536633372306824, 0.0008552271174266934, -0.21878907084465027)\n",
      "\n",
      "g_out: (0.16456365585327148, 0.00013907109678257257, -0.19358812272548676)\n",
      "L1T2_out: (0.2234240025281906, 0.0004022168868687004, -0.26912856101989746)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0416906364262104, 1.517289365438046e-05, -0.03581719100475311)\n",
      "L1T1_lora_out: (0.0833812728524208, 3.034578730876092e-05, -0.07163438200950623)\n",
      "\n",
      "L2T1_lora_B_out: (0.06570859253406525, 7.911882130429149e-05, -0.06753145903348923)\n",
      "L2T1_lora_out: (0.1314171850681305, 0.00015823764260858297, -0.13506291806697845)\n",
      "\n",
      "L2T2_lora_B_out: (0.09146622568368912, 0.00013463830691762269, -0.1011953130364418)\n",
      "L2T2_lora_out: (0.18293245136737823, 0.00026927661383524537, -0.2023906260728836)\n",
      "\n",
      "g_out: (0.13904796540737152, 0.00011103878932772204, -0.13237527012825012)\n",
      "L1T2_out: (0.2166588008403778, 0.00014138458936940879, -0.20324602723121643)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.014322693459689617, 1.963019349204842e-05, -0.017642436549067497)\n",
      "L1T1_lora_out: (0.028645386919379234, 3.926038698409684e-05, -0.035284873098134995)\n",
      "\n",
      "L2T1_lora_B_out: (0.027234787121415138, -6.81485835229978e-06, -0.0278080515563488)\n",
      "L2T1_lora_out: (0.054469574242830276, -1.362971670459956e-05, -0.0556161031126976)\n",
      "\n",
      "L2T2_lora_B_out: (0.059053853154182434, -2.6843313207791653e-06, -0.07648571580648422)\n",
      "L2T2_lora_out: (0.11810770630836487, -5.3686626415583305e-06, -0.15297143161296844)\n",
      "\n",
      "g_out: (0.08387298136949539, 8.261055882030632e-06, -0.09735532850027084)\n",
      "L1T2_out: (0.11216407269239426, 4.752143649966456e-05, -0.12027393281459808)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.39331281185150146, -0.0003574690781533718, -0.39302563667297363)\n",
      "L1T1_lora_out: (0.7866256237030029, -0.0007149381563067436, -0.7860512733459473)\n",
      "\n",
      "L2T1_lora_B_out: (0.23690524697303772, -0.0003607203543651849, -0.25526562333106995)\n",
      "L2T1_lora_out: (0.47381049394607544, -0.0007214407087303698, -0.5105312466621399)\n",
      "\n",
      "L2T2_lora_B_out: (0.768507182598114, -0.0013104344252496958, -1.1861315965652466)\n",
      "L2T2_lora_out: (1.537014365196228, -0.0026208688504993916, -2.372263193130493)\n",
      "\n",
      "g_out: (1.5335315465927124, -0.0018994281999766827, -1.8620965480804443)\n",
      "L1T2_out: (2.320157051086426, -0.0026143661234527826, -2.6481478214263916)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.25269854068756104, -2.340265564271249e-05, -0.18850450217723846)\n",
      "L1T1_lora_out: (0.5053970813751221, -4.680531128542498e-05, -0.37700900435447693)\n",
      "\n",
      "L2T1_lora_B_out: (0.12526625394821167, 4.6008717617951334e-05, -0.09796148538589478)\n",
      "L2T1_lora_out: (0.25053250789642334, 9.201743523590267e-05, -0.19592297077178955)\n",
      "\n",
      "L2T2_lora_B_out: (0.4722200930118561, 2.8142183509771712e-05, -0.32534363865852356)\n",
      "L2T2_lora_out: (0.9444401860237122, 5.6284367019543424e-05, -0.6506872773170471)\n",
      "\n",
      "g_out: (0.735650897026062, -3.5733079130295664e-05, -0.545720100402832)\n",
      "L1T2_out: (1.2127423286437988, -8.253839769167826e-05, -0.9227291345596313)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02757943980395794, 2.1073488824185915e-05, -0.02478567324578762)\n",
      "L1T1_lora_out: (0.05515887960791588, 4.214697764837183e-05, -0.04957134649157524)\n",
      "\n",
      "L2T1_lora_B_out: (0.017424358054995537, -2.3757628753173776e-08, -0.016816161572933197)\n",
      "L2T1_lora_out: (0.034848716109991074, -4.751525750634755e-08, -0.033632323145866394)\n",
      "\n",
      "L2T2_lora_B_out: (0.04221921041607857, 7.098568312358111e-05, -0.05128171667456627)\n",
      "L2T2_lora_out: (0.08443842083215714, 0.00014197136624716222, -0.10256343334913254)\n",
      "\n",
      "g_out: (0.0736665427684784, 0.00014201887825038284, -0.072902612388134)\n",
      "L1T2_out: (0.12371686100959778, 0.0001841658668126911, -0.12013840675354004)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0337589792907238, -0.00011345581879140809, -0.03888285532593727)\n",
      "L1T1_lora_out: (0.0675179585814476, -0.00022691163758281618, -0.07776571065187454)\n",
      "\n",
      "L2T1_lora_B_out: (0.05216057598590851, 4.705989340436645e-05, -0.046588048338890076)\n",
      "L2T1_lora_out: (0.10432115197181702, 9.41197868087329e-05, -0.09317609667778015)\n",
      "\n",
      "L2T2_lora_B_out: (0.1138959750533104, -0.0001546697021694854, -0.08733633160591125)\n",
      "L2T2_lora_out: (0.2277919501066208, -0.0003093394043389708, -0.1746726632118225)\n",
      "\n",
      "g_out: (0.15557262301445007, -0.0004034592129755765, -0.14171144366264343)\n",
      "L1T2_out: (0.20582956075668335, -0.000630370806902647, -0.1957533210515976)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.11471456289291382, 0.00013937534822616726, -0.13741467893123627)\n",
      "L1T1_lora_out: (0.22942912578582764, 0.0002787506964523345, -0.27482935786247253)\n",
      "\n",
      "L2T1_lora_B_out: (0.07874670624732971, 8.87971036718227e-05, -0.07887166738510132)\n",
      "L2T1_lora_out: (0.15749341249465942, 0.0001775942073436454, -0.15774333477020264)\n",
      "\n",
      "L2T2_lora_B_out: (0.2438363879919052, 0.0006265272386372089, -0.2720656096935272)\n",
      "L2T2_lora_out: (0.4876727759838104, 0.0012530544772744179, -0.5441312193870544)\n",
      "\n",
      "g_out: (0.4031679630279541, 0.0010754602262750268, -0.45027580857276917)\n",
      "L1T2_out: (0.6325970888137817, 0.0013542111264541745, -0.7251051664352417)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.20066671073436737, 0.00010709830530686304, -0.21858243644237518)\n",
      "L1T1_lora_out: (0.40133342146873474, 0.00021419661061372608, -0.43716487288475037)\n",
      "\n",
      "L2T1_lora_B_out: (0.12471431493759155, 0.00022534439631272107, -0.16464805603027344)\n",
      "L2T1_lora_out: (0.2494286298751831, 0.00045068879262544215, -0.3292961120605469)\n",
      "\n",
      "L2T2_lora_B_out: (0.4844106137752533, 0.0007101906812749803, -0.6155028343200684)\n",
      "L2T2_lora_out: (0.9688212275505066, 0.0014203813625499606, -1.2310056686401367)\n",
      "\n",
      "g_out: (0.7213264107704163, 0.0009696926572360098, -0.9017095565795898)\n",
      "L1T2_out: (1.0830929279327393, 0.0011838889913633466, -1.3170679807662964)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.017233040183782578, 1.2646877621591557e-05, -0.020922008901834488)\n",
      "L1T1_lora_out: (0.034466080367565155, 2.5293755243183114e-05, -0.041844017803668976)\n",
      "\n",
      "L2T1_lora_B_out: (0.015232589095830917, -3.32743616127118e-06, -0.013423345051705837)\n",
      "L2T1_lora_out: (0.030465178191661835, -6.65487232254236e-06, -0.026846690103411674)\n",
      "\n",
      "L2T2_lora_B_out: (0.029336409643292427, 3.608105316743604e-06, -0.02678544446825981)\n",
      "L2T2_lora_out: (0.058672819286584854, 7.216210633487208e-06, -0.05357088893651962)\n",
      "\n",
      "g_out: (0.05736486241221428, 1.3871082956029568e-05, -0.05594342574477196)\n",
      "L1T2_out: (0.09160207957029343, 3.916483910870738e-05, -0.09778743982315063)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.5808485746383667, -0.0006124874926172197, -0.587245762348175)\n",
      "L1T1_lora_out: (1.1616971492767334, -0.0012249749852344394, -1.17449152469635)\n",
      "\n",
      "L2T1_lora_B_out: (0.5289615988731384, -0.0005941357812844217, -0.3711993992328644)\n",
      "L2T1_lora_out: (1.0579231977462769, -0.0011882715625688434, -0.7423987984657288)\n",
      "\n",
      "L2T2_lora_B_out: (0.8363975882530212, -0.0013454651925712824, -0.700592577457428)\n",
      "L2T2_lora_out: (1.6727951765060425, -0.0026909303851425648, -1.401185154914856)\n",
      "\n",
      "g_out: (1.3167154788970947, -0.0015026588225737214, -1.2919648885726929)\n",
      "L1T2_out: (2.478412628173828, -0.0027276338078081608, -2.4540767669677734)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.636795163154602, 5.643823533318937e-05, -0.3686976134777069)\n",
      "L1T1_lora_out: (1.273590326309204, 0.00011287647066637874, -0.7373952269554138)\n",
      "\n",
      "L2T1_lora_B_out: (0.6349083185195923, -2.671614129212685e-05, -0.8587326407432556)\n",
      "L2T1_lora_out: (1.2698166370391846, -5.34322825842537e-05, -1.7174652814865112)\n",
      "\n",
      "L2T2_lora_B_out: (1.1431852579116821, 0.0001451417920179665, -1.3651454448699951)\n",
      "L2T2_lora_out: (2.2863705158233643, 0.000290283584035933, -2.7302908897399902)\n",
      "\n",
      "g_out: (1.678675651550293, 0.0003437158593442291, -1.0790003538131714)\n",
      "L1T2_out: (2.9209861755371094, 0.0004565923591144383, -1.709741473197937)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08648800104856491, 0.00010999287042068318, -0.07449093461036682)\n",
      "L1T1_lora_out: (0.17297600209712982, 0.00021998574084136635, -0.14898186922073364)\n",
      "\n",
      "L2T1_lora_B_out: (0.02510647103190422, 3.7882498418184696e-06, -0.025442764163017273)\n",
      "L2T1_lora_out: (0.05021294206380844, 7.576499683636939e-06, -0.050885528326034546)\n",
      "\n",
      "L2T2_lora_B_out: (0.0976705402135849, 0.00011280702892690897, -0.12276887148618698)\n",
      "L2T2_lora_out: (0.1953410804271698, 0.00022561405785381794, -0.24553774297237396)\n",
      "\n",
      "g_out: (0.19232651591300964, 0.00021803755953442305, -0.22869150340557098)\n",
      "L1T2_out: (0.3522113561630249, 0.00043802327127195895, -0.3776733875274658)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0866849273443222, 0.00012039615830872208, -0.07794656604528427)\n",
      "L1T1_lora_out: (0.1733698546886444, 0.00024079231661744416, -0.15589313209056854)\n",
      "\n",
      "L2T1_lora_B_out: (0.09746281057596207, 3.396350803086534e-05, -0.07968348264694214)\n",
      "L2T1_lora_out: (0.19492562115192413, 6.792701606173068e-05, -0.15936696529388428)\n",
      "\n",
      "L2T2_lora_B_out: (0.10221021622419357, 0.00014377730258274823, -0.12367195636034012)\n",
      "L2T2_lora_out: (0.20442043244838715, 0.00028755460516549647, -0.24734391272068024)\n",
      "\n",
      "g_out: (0.19789345562458038, 0.00021962760365568101, -0.19200889766216278)\n",
      "L1T2_out: (0.3596125841140747, 0.00046041994937695563, -0.3285652995109558)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0628909170627594, -0.00020970750483684242, -0.0657186508178711)\n",
      "L1T1_lora_out: (0.1257818341255188, -0.00041941500967368484, -0.1314373016357422)\n",
      "\n",
      "L2T1_lora_B_out: (0.07611223310232162, -8.357869955943897e-05, -0.06486137211322784)\n",
      "L2T1_lora_out: (0.15222446620464325, -0.00016715739911887795, -0.1297227442264557)\n",
      "\n",
      "L2T2_lora_B_out: (0.11782389879226685, -0.00016694754594936967, -0.14063513278961182)\n",
      "L2T2_lora_out: (0.2356477975845337, -0.00033389509189873934, -0.28127026557922363)\n",
      "\n",
      "g_out: (0.20873773097991943, -0.0001667376927798614, -0.19595563411712646)\n",
      "L1T2_out: (0.3088158667087555, -0.0005861527170054615, -0.2956974506378174)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08066122233867645, -9.429188503418118e-05, -0.06869310140609741)\n",
      "L1T1_lora_out: (0.1613224446773529, -0.00018858377006836236, -0.13738620281219482)\n",
      "\n",
      "L2T1_lora_B_out: (0.12658457458019257, 0.00018532344256527722, -0.13928915560245514)\n",
      "L2T1_lora_out: (0.25316914916038513, 0.00037064688513055444, -0.2785783112049103)\n",
      "\n",
      "L2T2_lora_B_out: (0.2960592806339264, 0.0005402155802585185, -0.25206902623176575)\n",
      "L2T2_lora_out: (0.5921185612678528, 0.001080431160517037, -0.5041380524635315)\n",
      "\n",
      "g_out: (0.49080103635787964, 0.0007097838679328561, -0.3118526041507721)\n",
      "L1T2_out: (0.6521234512329102, 0.0005212003015913069, -0.4324946105480194)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03388119861483574, 1.8889213606598787e-05, -0.021822752431035042)\n",
      "L1T1_lora_out: (0.06776239722967148, 3.7778427213197574e-05, -0.043645504862070084)\n",
      "\n",
      "L2T1_lora_B_out: (0.013102391734719276, -1.4268446648202371e-05, -0.01035708375275135)\n",
      "L2T1_lora_out: (0.026204783469438553, -2.8536893296404742e-05, -0.0207141675055027)\n",
      "\n",
      "L2T2_lora_B_out: (0.031488168984651566, 3.826713964372175e-06, -0.027410369366407394)\n",
      "L2T2_lora_out: (0.06297633796930313, 7.65342792874435e-06, -0.05482073873281479)\n",
      "\n",
      "g_out: (0.05527013912796974, 3.61903257726226e-05, -0.06323296576738358)\n",
      "L1T2_out: (0.1092139333486557, 7.396874570986256e-05, -0.09292220324277878)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.7157743573188782, -0.0005045129801146686, -0.8611832857131958)\n",
      "L1T1_lora_out: (1.4315487146377563, -0.0010090259602293372, -1.7223665714263916)\n",
      "\n",
      "L2T1_lora_B_out: (0.4838515520095825, -0.0007643319549970329, -0.4453936815261841)\n",
      "L2T1_lora_out: (0.967703104019165, -0.0015286639099940658, -0.8907873630523682)\n",
      "\n",
      "L2T2_lora_B_out: (0.9833522439002991, -0.00140054349321872, -0.8100734353065491)\n",
      "L2T2_lora_out: (1.9667044878005981, -0.00280108698643744, -1.6201468706130981)\n",
      "\n",
      "g_out: (1.4811270236968994, -0.001272423192858696, -1.6643065214157104)\n",
      "L1T2_out: (2.8948843479156494, -0.0022814490366727114, -3.3866729736328125)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.3675644099712372, 0.0004734984540846199, -0.3390556275844574)\n",
      "L1T1_lora_out: (0.7351288199424744, 0.0009469969081692398, -0.6781112551689148)\n",
      "\n",
      "L2T1_lora_B_out: (0.7509660124778748, -6.546216900460422e-05, -0.5409244298934937)\n",
      "L2T1_lora_out: (1.5019320249557495, -0.00013092433800920844, -1.0818488597869873)\n",
      "\n",
      "L2T2_lora_B_out: (0.9745209813117981, 0.0005099604022689164, -1.1488358974456787)\n",
      "L2T2_lora_out: (1.9490419626235962, 0.0010199208045378327, -2.2976717948913574)\n",
      "\n",
      "g_out: (1.280947208404541, 0.0011508450843393803, -1.2822767496109009)\n",
      "L1T2_out: (1.8713645935058594, 0.0020978418178856373, -1.9233849048614502)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10183142870664597, 2.579722786322236e-05, -0.18695050477981567)\n",
      "L1T1_lora_out: (0.20366285741329193, 5.159445572644472e-05, -0.37390100955963135)\n",
      "\n",
      "L2T1_lora_B_out: (0.23693051934242249, 0.0001437115715816617, -0.24922285974025726)\n",
      "L2T1_lora_out: (0.47386103868484497, 0.0002874231431633234, -0.4984457194805145)\n",
      "\n",
      "L2T2_lora_B_out: (0.43036237359046936, 0.00032089336309581995, -0.43518149852752686)\n",
      "L2T2_lora_out: (0.8607247471809387, 0.0006417867261916399, -0.8703629970550537)\n",
      "\n",
      "g_out: (0.550362229347229, 0.00035436355392448604, -0.45092448592185974)\n",
      "L1T2_out: (0.6782733798027039, 0.0004059580387547612, -0.6752047538757324)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04503466933965683, 8.955577504821122e-05, -0.043203338980674744)\n",
      "L1T1_lora_out: (0.09006933867931366, 0.00017911155009642243, -0.08640667796134949)\n",
      "\n",
      "L2T1_lora_B_out: (0.11752969026565552, 7.769077637931332e-05, -0.1220671758055687)\n",
      "L2T1_lora_out: (0.23505938053131104, 0.00015538155275862664, -0.2441343516111374)\n",
      "\n",
      "L2T2_lora_B_out: (0.185989111661911, 0.0001475007156841457, -0.20976942777633667)\n",
      "L2T2_lora_out: (0.371978223323822, 0.0002950014313682914, -0.41953885555267334)\n",
      "\n",
      "g_out: (0.25001245737075806, 0.00013961987860966474, -0.257565438747406)\n",
      "L1T2_out: (0.3333517909049988, 0.00031873141415417194, -0.3439721167087555)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10036248713731766, 3.645293691079132e-05, -0.1239388957619667)\n",
      "L1T1_lora_out: (0.20072497427463531, 7.290587382158265e-05, -0.2478777915239334)\n",
      "\n",
      "L2T1_lora_B_out: (0.04780245199799538, -0.0004642791172955185, -0.07173091173171997)\n",
      "L2T1_lora_out: (0.09560490399599075, -0.000928558234591037, -0.14346182346343994)\n",
      "\n",
      "L2T2_lora_B_out: (0.15423855185508728, -0.0007835309370420873, -0.16495315730571747)\n",
      "L2T2_lora_out: (0.30847710371017456, -0.0015670618740841746, -0.32990631461143494)\n",
      "\n",
      "g_out: (0.29168304800987244, -0.0006385042797774076, -0.3094222843647003)\n",
      "L1T2_out: (0.49240803718566895, -0.0005655988352373242, -0.49769771099090576)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.15192218124866486, -0.0004804939089808613, -0.14772427082061768)\n",
      "L1T1_lora_out: (0.3038443624973297, -0.0009609878179617226, -0.29544854164123535)\n",
      "\n",
      "L2T1_lora_B_out: (0.11877328902482986, 0.000415333837736398, -0.09384102374315262)\n",
      "L2T1_lora_out: (0.23754657804965973, 0.000830667675472796, -0.18768204748630524)\n",
      "\n",
      "L2T2_lora_B_out: (0.1926380842924118, -0.0006924880435690284, -0.16810090839862823)\n",
      "L2T2_lora_out: (0.3852761685848236, -0.0013849760871380568, -0.33620181679725647)\n",
      "\n",
      "g_out: (0.3141593933105469, -0.0022156438790261745, -0.30545511841773987)\n",
      "L1T2_out: (0.5552866458892822, -0.003176632337272167, -0.563444197177887)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0540422722697258, 2.859655796783045e-05, -0.05587369576096535)\n",
      "L1T1_lora_out: (0.1080845445394516, 5.71931159356609e-05, -0.1117473915219307)\n",
      "\n",
      "L2T1_lora_B_out: (0.09199998527765274, 7.251344504766166e-05, -0.06730596721172333)\n",
      "L2T1_lora_out: (0.18399997055530548, 0.00014502689009532332, -0.13461193442344666)\n",
      "\n",
      "L2T2_lora_B_out: (0.13724419474601746, 0.00013783261238131672, -0.10629641264677048)\n",
      "L2T2_lora_out: (0.2744883894920349, 0.00027566522476263344, -0.21259282529354095)\n",
      "\n",
      "g_out: (0.16889259219169617, 0.0001306383201153949, -0.1672006994485855)\n",
      "L1T2_out: (0.2769771218299866, 0.0001878314360510558, -0.2789480984210968)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.8294233083724976, 0.0001536795898573473, -1.0516035556793213)\n",
      "L1T1_lora_out: (1.6588466167449951, 0.0003073591797146946, -2.1032071113586426)\n",
      "\n",
      "L2T1_lora_B_out: (0.7029286623001099, -0.0008843127288855612, -0.628292977809906)\n",
      "L2T1_lora_out: (1.4058573246002197, -0.0017686254577711225, -1.256585955619812)\n",
      "\n",
      "L2T2_lora_B_out: (2.2414238452911377, -0.0012747624423354864, -2.093027353286743)\n",
      "L2T2_lora_out: (4.482847690582275, -0.002549524884670973, -4.186054706573486)\n",
      "\n",
      "g_out: (3.0769903659820557, -0.0007808994851075113, -3.483468770980835)\n",
      "L1T2_out: (4.08879280090332, -0.0004735402762889862, -5.586675643920898)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.4729008972644806, -1.173171040136367e-05, -0.4060423970222473)\n",
      "L1T1_lora_out: (0.9458017945289612, -2.346342080272734e-05, -0.8120847940444946)\n",
      "\n",
      "L2T1_lora_B_out: (0.38648462295532227, -8.50790529511869e-05, -0.3423182964324951)\n",
      "L2T1_lora_out: (0.7729692459106445, -0.0001701581059023738, -0.6846365928649902)\n",
      "\n",
      "L2T2_lora_B_out: (0.9606515765190125, -0.0002180530718760565, -0.656448483467102)\n",
      "L2T2_lora_out: (1.921303153038025, -0.000436106143752113, -1.312896966934204)\n",
      "\n",
      "g_out: (1.3897625207901, -0.0002659480378497392, -0.9684810042381287)\n",
      "L1T2_out: (2.335564374923706, -0.00028941145865246654, -1.7673091888427734)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (3.717052459716797, -0.0002154930552933365, -6.250217914581299)\n",
      "L1T1_lora_out: (7.434104919433594, -0.000430986110586673, -12.500435829162598)\n",
      "\n",
      "L2T1_lora_B_out: (9.647767066955566, -2.6308967790100724e-05, -8.588513374328613)\n",
      "L2T1_lora_out: (19.295534133911133, -5.261793558020145e-05, -17.177026748657227)\n",
      "\n",
      "L2T2_lora_B_out: (10.490899085998535, -0.0003164995869155973, -12.784456253051758)\n",
      "L2T2_lora_out: (20.98179817199707, -0.0006329991738311946, -25.568912506103516)\n",
      "\n",
      "g_out: (7.9460248947143555, -0.0005803811945952475, -13.506966590881348)\n",
      "L1T2_out: (15.38012981414795, -0.0010113673051819205, -26.007402420043945)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "# Zero gradients manually\n",
    "for p in model.parameters():\n",
    "    p.grad = None\n",
    "\n",
    "# Forward pass\n",
    "inputs = tokenizer(\n",
    "    L2T2_sample_prompt, \n",
    "    return_tensors='pt',\n",
    ").to(next(model.parameters()).device)\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    labels=inputs['input_ids'],\n",
    "    use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3433, device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2T2_lora_outs = outputs[1]\n",
    "# L2T2_tgts = outputs[2]\n",
    "\n",
    "# loss_device = next(iter(L2T2_tgts.values())).device\n",
    "\n",
    "# total_hidden_loss = torch.tensor(0.0, device=loss_device)\n",
    "# for layer_name in L2T2_tgts.keys():\n",
    "#     # print(layer_name)\n",
    "#     L2T2_lora_out = L2T2_lora_outs[layer_name]\n",
    "#     L2T2_tgt = L2T2_tgts[layer_name].detach()\n",
    "    \n",
    "#     # print(L2T2_lora_out.shape, L2T2_tgt.shape)\n",
    "#     # print(L2T2_lora_out.grad_fn, L2T2_tgt.grad_fn)\n",
    "#     hidden_loss = F.mse_loss(L2T2_lora_out, L2T2_tgt, reduction='mean')\n",
    "#     total_hidden_loss += hidden_loss.to(loss_device)\n",
    "    \n",
    "#     # print()\n",
    "# print(total_hidden_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gradient checkpointing enabled!\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05573910102248192, 1.1024943887605332e-05, -0.04592706635594368)\n",
      "L1T1_lora_out: (0.11147820204496384, 2.2049887775210664e-05, -0.09185413271188736)\n",
      "\n",
      "L2T1_lora_B_out: (0.04789327457547188, 0.00019716875976882875, -0.05974280461668968)\n",
      "L2T1_lora_out: (0.09578654915094376, 0.0003943375195376575, -0.11948560923337936)\n",
      "\n",
      "L2T2_lora_B_out: (0.1532118022441864, 0.000313695112708956, -0.16253496706485748)\n",
      "L2T2_lora_out: (0.3064236044883728, 0.000627390225417912, -0.32506993412971497)\n",
      "\n",
      "g_out: (0.24000284075737, 0.00023305288050323725, -0.21710637211799622)\n",
      "L1T2_out: (0.35148105025291443, 0.0002551026991568506, -0.308960497379303)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021090907976031303, -3.678964276332408e-05, -0.023282183334231377)\n",
      "L1T1_lora_out: (0.04218181595206261, -7.357928552664816e-05, -0.04656436666846275)\n",
      "\n",
      "L2T1_lora_B_out: (0.03269142284989357, -0.0001890525163616985, -0.032031938433647156)\n",
      "L2T1_lora_out: (0.06538284569978714, -0.000378105032723397, -0.06406387686729431)\n",
      "\n",
      "L2T2_lora_B_out: (0.04024292156100273, -0.00014135301171336323, -0.0653601810336113)\n",
      "L2T2_lora_out: (0.08048584312200546, -0.00028270602342672646, -0.1307203620672226)\n",
      "\n",
      "g_out: (0.0706072524189949, 9.539897291688249e-05, -0.11090990900993347)\n",
      "L1T2_out: (0.11278906464576721, 2.181966556236148e-05, -0.15747427940368652)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005514121148735285, -2.7487656552693807e-05, -0.006385340355336666)\n",
      "L1T1_lora_out: (0.01102824229747057, -5.497531310538761e-05, -0.012770680710673332)\n",
      "\n",
      "L2T1_lora_B_out: (0.004455916583538055, 6.06398452873691e-06, -0.00437548104673624)\n",
      "L2T1_lora_out: (0.00891183316707611, 1.212796905747382e-05, -0.00875096209347248)\n",
      "\n",
      "L2T2_lora_B_out: (0.014418411999940872, -2.960453821287956e-05, -0.015186331234872341)\n",
      "L2T2_lora_out: (0.028836823999881744, -5.920907642575912e-05, -0.030372662469744682)\n",
      "\n",
      "g_out: (0.024743549525737762, -7.133703911677003e-05, -0.02322794497013092)\n",
      "L1T2_out: (0.03493303060531616, -0.00012631237041205168, -0.03531135991215706)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0006652554147876799, -1.3039052646490745e-06, -0.000710756634362042)\n",
      "L1T1_lora_out: (0.0013305108295753598, -2.607810529298149e-06, -0.001421513268724084)\n",
      "\n",
      "L2T1_lora_B_out: (0.0011847891146317124, 1.5541626225967775e-06, -0.0009297866490669549)\n",
      "L2T1_lora_out: (0.002369578229263425, 3.108325245193555e-06, -0.0018595732981339097)\n",
      "\n",
      "L2T2_lora_B_out: (0.0027238570619374514, 6.774147323085344e-07, -0.002159038558602333)\n",
      "L2T2_lora_out: (0.005447714123874903, 1.3548294646170689e-06, -0.004318077117204666)\n",
      "\n",
      "g_out: (0.003936042543500662, -1.7534943026475958e-06, -0.0035113664343953133)\n",
      "L1T2_out: (0.005159071646630764, -4.3613049456325825e-06, -0.004788490012288094)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01719825342297554, 2.4826416847645305e-05, -0.016772279515862465)\n",
      "L1T1_lora_out: (0.03439650684595108, 4.965283369529061e-05, -0.03354455903172493)\n",
      "\n",
      "L2T1_lora_B_out: (0.02204914763569832, 1.4443315194512252e-05, -0.014421598054468632)\n",
      "L2T1_lora_out: (0.04409829527139664, 2.8886630389024504e-05, -0.028843196108937263)\n",
      "\n",
      "L2T2_lora_B_out: (0.0719948336482048, 6.336949445540085e-05, -0.05591467395424843)\n",
      "L2T2_lora_out: (0.1439896672964096, 0.0001267389889108017, -0.11182934790849686)\n",
      "\n",
      "g_out: (0.09989137202501297, 9.785238216863945e-05, -0.08298615366220474)\n",
      "L1T2_out: (0.13428787887096405, 0.00014750522677786648, -0.11523351073265076)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01294557936489582, 1.4893067600496579e-05, -0.013282570987939835)\n",
      "L1T1_lora_out: (0.02589115872979164, 2.9786135200993158e-05, -0.02656514197587967)\n",
      "\n",
      "L2T1_lora_B_out: (0.02251545898616314, -1.1957946298934985e-05, -0.017174670472741127)\n",
      "L2T1_lora_out: (0.04503091797232628, -2.391589259786997e-05, -0.034349340945482254)\n",
      "\n",
      "L2T2_lora_B_out: (0.05428126081824303, 4.130592969886493e-06, -0.047953568398952484)\n",
      "L2T2_lora_out: (0.10856252163648605, 8.261185939772986e-06, -0.09590713679790497)\n",
      "\n",
      "g_out: (0.06719720363616943, 3.217707489966415e-05, -0.07475320994853973)\n",
      "L1T2_out: (0.08845661580562592, 6.196321919560432e-05, -0.1013183519244194)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003259751247242093, 3.2430623377877055e-06, -0.0027167904190719128)\n",
      "L1T1_lora_out: (0.006519502494484186, 6.486124675575411e-06, -0.0054335808381438255)\n",
      "\n",
      "L2T1_lora_B_out: (0.002130574081093073, -7.72158159634273e-07, -0.0024033645167946815)\n",
      "L2T1_lora_out: (0.004261148162186146, -1.544316319268546e-06, -0.004806729033589363)\n",
      "\n",
      "L2T2_lora_B_out: (0.005282450467348099, 3.842072146653663e-06, -0.005711228586733341)\n",
      "L2T2_lora_out: (0.010564900934696198, 7.684144293307327e-06, -0.011422457173466682)\n",
      "\n",
      "g_out: (0.008152995258569717, 9.228466296917759e-06, -0.008509454317390919)\n",
      "L1T2_out: (0.013849287293851376, 1.5714584151282907e-05, -0.012707127258181572)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.008210139349102974, 4.4754051486961544e-07, -0.0072332522831857204)\n",
      "L1T1_lora_out: (0.016420278698205948, 8.950810297392309e-07, -0.014466504566371441)\n",
      "\n",
      "L2T1_lora_B_out: (0.006062277127057314, -5.045543275628006e-06, -0.006374927703291178)\n",
      "L2T1_lora_out: (0.012124554254114628, -1.0091086551256012e-05, -0.012749855406582355)\n",
      "\n",
      "L2T2_lora_B_out: (0.02030194364488125, -1.0420544640510343e-05, -0.017776360735297203)\n",
      "L2T2_lora_out: (0.0406038872897625, -2.0841089281020686e-05, -0.035552721470594406)\n",
      "\n",
      "g_out: (0.03099501132965088, -1.0750008186732884e-05, -0.027999067679047585)\n",
      "L1T2_out: (0.04417916387319565, -9.85493079497246e-06, -0.03880295902490616)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.009729277342557907, 3.522485201301606e-07, -0.007644969504326582)\n",
      "L1T1_lora_out: (0.019458554685115814, 7.044970402603212e-07, -0.015289939008653164)\n",
      "\n",
      "L2T1_lora_B_out: (0.008091649040579796, -5.788200269307708e-06, -0.010573714971542358)\n",
      "L2T1_lora_out: (0.01618329808115959, -1.1576400538615417e-05, -0.021147429943084717)\n",
      "\n",
      "L2T2_lora_B_out: (0.02107861638069153, -1.3599985322798602e-05, -0.027618005871772766)\n",
      "L2T2_lora_out: (0.04215723276138306, -2.7199970645597205e-05, -0.05523601174354553)\n",
      "\n",
      "g_out: (0.034880176186561584, -1.5623538274667226e-05, -0.038182325661182404)\n",
      "L1T2_out: (0.0543387308716774, -1.4919055502105039e-05, -0.04742109775543213)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005091794766485691, 4.409344455780229e-06, -0.004819472786039114)\n",
      "L1T1_lora_out: (0.010183589532971382, 8.818688911560457e-06, -0.009638945572078228)\n",
      "\n",
      "L2T1_lora_B_out: (0.007443771231919527, -2.3585769667988643e-05, -0.0074156406335532665)\n",
      "L2T1_lora_out: (0.014887542463839054, -4.7171539335977286e-05, -0.014831281267106533)\n",
      "\n",
      "L2T2_lora_B_out: (0.019663071259856224, 1.1209095646336209e-05, -0.019948109984397888)\n",
      "L2T2_lora_out: (0.03932614251971245, 2.2418191292672418e-05, -0.039896219968795776)\n",
      "\n",
      "g_out: (0.030109530314803123, 6.958974699955434e-05, -0.030292121693491936)\n",
      "L1T2_out: (0.039681289345026016, 7.840841863071546e-05, -0.03947548568248749)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0029402347281575203, -8.959324986790307e-06, -0.003149444470182061)\n",
      "L1T1_lora_out: (0.005880469456315041, -1.7918649973580614e-05, -0.006298888940364122)\n",
      "\n",
      "L2T1_lora_B_out: (0.0018293034518137574, 2.3023594621918164e-06, -0.001757293939590454)\n",
      "L2T1_lora_out: (0.003658606903627515, 4.604718924383633e-06, -0.003514587879180908)\n",
      "\n",
      "L2T2_lora_B_out: (0.005407507065683603, -1.610648541827686e-05, -0.0059700608253479)\n",
      "L2T2_lora_out: (0.010815014131367207, -3.221297083655372e-05, -0.0119401216506958)\n",
      "\n",
      "g_out: (0.008914642035961151, -3.6817684303969145e-05, -0.009968291968107224)\n",
      "L1T2_out: (0.014522147364914417, -5.473634155350737e-05, -0.0159786157310009)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0354384146630764, -1.2761555808538105e-05, -0.0525871142745018)\n",
      "L1T1_lora_out: (0.0708768293261528, -2.552311161707621e-05, -0.1051742285490036)\n",
      "\n",
      "L2T1_lora_B_out: (0.1721680462360382, 1.5229039490805008e-05, -0.06731771677732468)\n",
      "L2T1_lora_out: (0.3443360924720764, 3.0458078981610015e-05, -0.13463543355464935)\n",
      "\n",
      "L2T2_lora_B_out: (0.3374742567539215, 3.2765616197139025e-05, -0.10700773447751999)\n",
      "L2T2_lora_out: (0.674948513507843, 6.553123239427805e-05, -0.21401546895503998)\n",
      "\n",
      "g_out: (0.3306124210357666, 3.507316068862565e-05, -0.13375644385814667)\n",
      "L1T2_out: (0.4014892578125, 9.550059075991157e-06, -0.23872233927249908)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0798613578081131, -1.3876509910915047e-05, -0.0880509465932846)\n",
      "L1T1_lora_out: (0.1597227156162262, -2.7753019821830094e-05, -0.1761018931865692)\n",
      "\n",
      "L2T1_lora_B_out: (0.08858136832714081, 3.05989378830418e-05, -0.221498504281044)\n",
      "L2T1_lora_out: (0.17716273665428162, 6.11978757660836e-05, -0.442997008562088)\n",
      "\n",
      "L2T2_lora_B_out: (0.24899567663669586, 3.017476274180808e-06, -0.5718624591827393)\n",
      "L2T2_lora_out: (0.4979913532733917, 6.034952548361616e-06, -1.1437249183654785)\n",
      "\n",
      "g_out: (0.44782620668411255, -5.516289093066007e-05, -0.710545539855957)\n",
      "L1T2_out: (0.6075489521026611, -8.291594713227823e-05, -0.8866474628448486)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.018035108223557472, -5.322756805981044e-06, -0.014425364322960377)\n",
      "L1T1_lora_out: (0.036070216447114944, -1.0645513611962087e-05, -0.028850728645920753)\n",
      "\n",
      "L2T1_lora_B_out: (0.024320751428604126, -1.9916194560209988e-06, -0.030159462243318558)\n",
      "L2T1_lora_out: (0.04864150285720825, -3.9832389120419975e-06, -0.060318924486637115)\n",
      "\n",
      "L2T2_lora_B_out: (0.029530517756938934, -9.55381528910948e-06, -0.027911657467484474)\n",
      "L2T2_lora_out: (0.05906103551387787, -1.910763057821896e-05, -0.05582331493496895)\n",
      "\n",
      "g_out: (0.0527087077498436, -1.512440121587133e-05, -0.05261679366230965)\n",
      "L1T2_out: (0.06425962597131729, -2.576993574621156e-05, -0.05955071747303009)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01806771755218506, 6.105452484916896e-06, -0.016924256458878517)\n",
      "L1T1_lora_out: (0.03613543510437012, 1.2210904969833791e-05, -0.033848512917757034)\n",
      "\n",
      "L2T1_lora_B_out: (0.013497618027031422, -4.037204689666396e-06, -0.012387802824378014)\n",
      "L2T1_lora_out: (0.026995236054062843, -8.074409379332792e-06, -0.024775605648756027)\n",
      "\n",
      "L2T2_lora_B_out: (0.025289364159107208, -1.268797495868057e-05, -0.026121260598301888)\n",
      "L2T2_lora_out: (0.050578728318214417, -2.537594991736114e-05, -0.052242521196603775)\n",
      "\n",
      "g_out: (0.0464409776031971, -1.7301557818427682e-05, -0.04809010401368141)\n",
      "L1T2_out: (0.07576794922351837, -5.090638296678662e-06, -0.08078794181346893)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03644375875592232, 0.0001263038720935583, -0.034529998898506165)\n",
      "L1T1_lora_out: (0.07288751751184464, 0.0002526077441871166, -0.06905999779701233)\n",
      "\n",
      "L2T1_lora_B_out: (0.010133032687008381, -3.0497991247102618e-05, -0.009359166026115417)\n",
      "L2T1_lora_out: (0.020266065374016762, -6.0995982494205236e-05, -0.018718332052230835)\n",
      "\n",
      "L2T2_lora_B_out: (0.08124188333749771, 0.00011055330833187327, -0.07003989815711975)\n",
      "L2T2_lora_out: (0.16248376667499542, 0.00022110661666374654, -0.1400797963142395)\n",
      "\n",
      "g_out: (0.1466459035873413, 0.000282102613709867, -0.12790939211845398)\n",
      "L1T2_out: (0.1990625411272049, 0.0005347104743123055, -0.1969693899154663)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04073552414774895, 2.2345197066897526e-05, -0.04172387719154358)\n",
      "L1T1_lora_out: (0.0814710482954979, 4.469039413379505e-05, -0.08344775438308716)\n",
      "\n",
      "L2T1_lora_B_out: (0.023088505491614342, -2.258350832562428e-05, -0.0208408385515213)\n",
      "L2T1_lora_out: (0.046177010983228683, -4.516701665124856e-05, -0.0416816771030426)\n",
      "\n",
      "L2T2_lora_B_out: (0.09176009148359299, 2.695800139917992e-05, -0.09335771948099136)\n",
      "L2T2_lora_out: (0.18352018296718597, 5.391600279835984e-05, -0.18671543896198273)\n",
      "\n",
      "g_out: (0.16072587668895721, 9.908297215588391e-05, -0.1640452742576599)\n",
      "L1T2_out: (0.2421969175338745, 0.00014377341722138226, -0.24749302864074707)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006093454081565142, -8.929801879276056e-06, -0.006077616475522518)\n",
      "L1T1_lora_out: (0.012186908163130283, -1.785960375855211e-05, -0.012155232951045036)\n",
      "\n",
      "L2T1_lora_B_out: (0.0026763221248984337, -8.962708193394064e-07, -0.002888704650104046)\n",
      "L2T1_lora_out: (0.005352644249796867, -1.7925416386788129e-06, -0.005777409300208092)\n",
      "\n",
      "L2T2_lora_B_out: (0.011920123361051083, -1.4651155652245507e-05, -0.012733908370137215)\n",
      "L2T2_lora_out: (0.023840246722102165, -2.9302311304491013e-05, -0.02546781674027443)\n",
      "\n",
      "g_out: (0.021672947332262993, -2.7509780920809135e-05, -0.02146003395318985)\n",
      "L1T2_out: (0.03385985642671585, -4.536937558441423e-05, -0.03049318492412567)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0707327350974083, -2.6035533664980903e-05, -0.06296531856060028)\n",
      "L1T1_lora_out: (0.1414654701948166, -5.2071067329961807e-05, -0.12593063712120056)\n",
      "\n",
      "L2T1_lora_B_out: (0.029919100925326347, -8.529834190085239e-07, -0.030939800664782524)\n",
      "L2T1_lora_out: (0.059838201850652695, -1.7059668380170478e-06, -0.06187960132956505)\n",
      "\n",
      "L2T2_lora_B_out: (0.11058509349822998, 5.111282916914206e-06, -0.12839700281620026)\n",
      "L2T2_lora_out: (0.22117018699645996, 1.0222565833828412e-05, -0.2567940056324005)\n",
      "\n",
      "g_out: (0.1976257562637329, 1.1928545973205473e-05, -0.21731352806091309)\n",
      "L1T2_out: (0.3390912413597107, -4.014256046502851e-05, -0.34324416518211365)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.019578345119953156, 1.3118064089212567e-05, -0.01749570667743683)\n",
      "L1T1_lora_out: (0.03915669023990631, 2.6236128178425133e-05, -0.03499141335487366)\n",
      "\n",
      "L2T1_lora_B_out: (0.02344536781311035, 1.4581961295334622e-05, -0.028140263631939888)\n",
      "L2T1_lora_out: (0.0468907356262207, 2.9163922590669245e-05, -0.056280527263879776)\n",
      "\n",
      "L2T2_lora_B_out: (0.05784020200371742, 5.624890036415309e-05, -0.05774389207363129)\n",
      "L2T2_lora_out: (0.11568040400743484, 0.00011249780072830617, -0.11548778414726257)\n",
      "\n",
      "g_out: (0.08794348686933517, 8.333387086167932e-05, -0.07207569479942322)\n",
      "L1T2_out: (0.1271001696586609, 0.00010957001359201968, -0.09797632694244385)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.001701591769233346, 4.956362317898311e-06, -0.0015614052535966039)\n",
      "L1T1_lora_out: (0.003403183538466692, 9.912724635796621e-06, -0.0031228105071932077)\n",
      "\n",
      "L2T1_lora_B_out: (0.001603175071068108, -7.253541980389855e-07, -0.0013471790589392185)\n",
      "L2T1_lora_out: (0.003206350142136216, -1.450708396077971e-06, -0.002694358117878437)\n",
      "\n",
      "L2T2_lora_B_out: (0.003321737516671419, 6.268888682825491e-06, -0.003627123311161995)\n",
      "L2T2_lora_out: (0.006643475033342838, 1.2537777365650982e-05, -0.00725424662232399)\n",
      "\n",
      "g_out: (0.004932701587677002, 1.3988485079607926e-05, -0.00502829160541296)\n",
      "L1T2_out: (0.008117308840155602, 2.390120607742574e-05, -0.007284366525709629)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02863040380179882, 9.99440235318616e-05, -0.03127958998084068)\n",
      "L1T1_lora_out: (0.05726080760359764, 0.0001998880470637232, -0.06255917996168137)\n",
      "\n",
      "L2T1_lora_B_out: (0.017718691378831863, -4.344513854448451e-06, -0.01956755854189396)\n",
      "L2T1_lora_out: (0.03543738275766373, -8.689027708896901e-06, -0.03913511708378792)\n",
      "\n",
      "L2T2_lora_B_out: (0.048162732273340225, 0.0002242646150989458, -0.07196137309074402)\n",
      "L2T2_lora_out: (0.09632546454668045, 0.0004485292301978916, -0.14392274618148804)\n",
      "\n",
      "g_out: (0.10089735686779022, 0.0004572183242999017, -0.11971588432788849)\n",
      "L1T2_out: (0.15815816819667816, 0.0006571062840521336, -0.18227505683898926)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.015015986748039722, 4.876801904174499e-05, -0.014609637670218945)\n",
      "L1T1_lora_out: (0.030031973496079445, 9.753603808348998e-05, -0.02921927534043789)\n",
      "\n",
      "L2T1_lora_B_out: (0.01336844451725483, -4.688306580646895e-05, -0.011754806153476238)\n",
      "L2T1_lora_out: (0.02673688903450966, -9.37661316129379e-05, -0.023509612306952477)\n",
      "\n",
      "L2T2_lora_B_out: (0.03473106771707535, 4.4098727812524885e-05, -0.033581774681806564)\n",
      "L2T2_lora_out: (0.0694621354341507, 8.819745562504977e-05, -0.06716354936361313)\n",
      "\n",
      "g_out: (0.053926195949316025, 0.0001819635508581996, -0.06157875061035156)\n",
      "L1T2_out: (0.07686467468738556, 0.00027949962532147765, -0.08376110345125198)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021597763523459435, 1.2078561667294707e-05, -0.02021394856274128)\n",
      "L1T1_lora_out: (0.04319552704691887, 2.4157123334589414e-05, -0.04042789712548256)\n",
      "\n",
      "L2T1_lora_B_out: (0.012218157760798931, -5.124395465827547e-05, -0.010425787419080734)\n",
      "L2T1_lora_out: (0.024436315521597862, -0.00010248790931655094, -0.02085157483816147)\n",
      "\n",
      "L2T2_lora_B_out: (0.04497203230857849, -8.040078682824969e-05, -0.052581001073122025)\n",
      "L2T2_lora_out: (0.08994406461715698, -0.00016080157365649939, -0.10516200214624405)\n",
      "\n",
      "g_out: (0.08040471374988556, -5.831365706399083e-05, -0.08781759440898895)\n",
      "L1T2_out: (0.11758685111999512, -3.4156568290200084e-05, -0.1282454878091812)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00208045425824821, 4.21721506427275e-06, -0.0021323764231055975)\n",
      "L1T1_lora_out: (0.00416090851649642, 8.4344301285455e-06, -0.004264752846211195)\n",
      "\n",
      "L2T1_lora_B_out: (0.0013101111399009824, 3.9453325371141545e-06, -0.0011398288188502192)\n",
      "L2T1_lora_out: (0.0026202222798019648, 7.890665074228309e-06, -0.0022796576377004385)\n",
      "\n",
      "L2T2_lora_B_out: (0.003955925814807415, 1.2254953617230058e-05, -0.0042608496733009815)\n",
      "L2T2_lora_out: (0.00791185162961483, 2.4509907234460115e-05, -0.008521699346601963)\n",
      "\n",
      "g_out: (0.006820155307650566, 1.661924397922121e-05, -0.007467494811862707)\n",
      "L1T2_out: (0.01003161258995533, 2.505367410776671e-05, -0.01105921994894743)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020563311874866486, 4.955757958668983e-06, -0.020826023072004318)\n",
      "L1T1_lora_out: (0.04112662374973297, 9.911515917337965e-06, -0.041652046144008636)\n",
      "\n",
      "L2T1_lora_B_out: (0.023566633462905884, -6.9664883994846605e-06, -0.024705586954951286)\n",
      "L2T1_lora_out: (0.04713326692581177, -1.3932976798969321e-05, -0.04941117390990257)\n",
      "\n",
      "L2T2_lora_B_out: (0.05083231255412102, 3.5555083286453737e-06, -0.05165070667862892)\n",
      "L2T2_lora_out: (0.10166462510824203, 7.111016657290747e-06, -0.10330141335725784)\n",
      "\n",
      "g_out: (0.08302921801805496, 2.1043977540102787e-05, -0.07538190484046936)\n",
      "L1T2_out: (0.12301802635192871, 3.095552165177651e-05, -0.10683439671993256)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01887110248208046, 1.0357184692111332e-05, -0.017004288733005524)\n",
      "L1T1_lora_out: (0.03774220496416092, 2.0714369384222664e-05, -0.03400857746601105)\n",
      "\n",
      "L2T1_lora_B_out: (0.024650059640407562, 1.1528137292771135e-05, -0.019406411796808243)\n",
      "L2T1_lora_out: (0.049300119280815125, 2.305627458554227e-05, -0.038812823593616486)\n",
      "\n",
      "L2T2_lora_B_out: (0.05951937660574913, 4.1330582462251186e-05, -0.06178600341081619)\n",
      "L2T2_lora_out: (0.11903875321149826, 8.266116492450237e-05, -0.12357200682163239)\n",
      "\n",
      "g_out: (0.09329961985349655, 5.960491034784354e-05, -0.09491395950317383)\n",
      "L1T2_out: (0.13104182481765747, 8.0319274275098e-05, -0.12722499668598175)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0023410634603351355, -5.35577498794737e-07, -0.00199713627807796)\n",
      "L1T1_lora_out: (0.004682126920670271, -1.071154997589474e-06, -0.00399427255615592)\n",
      "\n",
      "L2T1_lora_B_out: (0.0023607993498444557, 1.1490286624393775e-06, -0.0027835662476718426)\n",
      "L2T1_lora_out: (0.0047215986996889114, 2.298057324878755e-06, -0.005567132495343685)\n",
      "\n",
      "L2T2_lora_B_out: (0.004361854400485754, -1.2522177712526172e-06, -0.003961226437240839)\n",
      "L2T2_lora_out: (0.008723708800971508, -2.5044355425052345e-06, -0.007922452874481678)\n",
      "\n",
      "g_out: (0.007969294674694538, -4.802493094757665e-06, -0.005923760123550892)\n",
      "L1T2_out: (0.012651421129703522, -5.873649570276029e-06, -0.008278990164399147)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01904020644724369, -1.752052230585832e-05, -0.017266832292079926)\n",
      "L1T1_lora_out: (0.03808041289448738, -3.504104461171664e-05, -0.03453366458415985)\n",
      "\n",
      "L2T1_lora_B_out: (0.019354719668626785, 4.322190216043964e-05, -0.02072816900908947)\n",
      "L2T1_lora_out: (0.03870943933725357, 8.644380432087928e-05, -0.04145633801817894)\n",
      "\n",
      "L2T2_lora_B_out: (0.03382312133908272, 2.8929818654432893e-05, -0.03325323760509491)\n",
      "L2T2_lora_out: (0.06764624267816544, 5.7859637308865786e-05, -0.06650647521018982)\n",
      "\n",
      "g_out: (0.06882838159799576, -2.858412881323602e-05, -0.06655798852443695)\n",
      "L1T2_out: (0.0989021435379982, -6.362518615787849e-05, -0.09543865919113159)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.015456191264092922, -3.418159030843526e-05, -0.01729300059378147)\n",
      "L1T1_lora_out: (0.030912382528185844, -6.836318061687052e-05, -0.03458600118756294)\n",
      "\n",
      "L2T1_lora_B_out: (0.018226258456707, -3.087782533839345e-05, -0.01624462567269802)\n",
      "L2T1_lora_out: (0.036452516913414, -6.17556506767869e-05, -0.03248925134539604)\n",
      "\n",
      "L2T2_lora_B_out: (0.042305637151002884, -9.03788604773581e-05, -0.046899594366550446)\n",
      "L2T2_lora_out: (0.08461127430200577, -0.0001807577209547162, -0.09379918873310089)\n",
      "\n",
      "g_out: (0.06551852822303772, -0.00011900205572601408, -0.07422663271427155)\n",
      "L1T2_out: (0.08360889554023743, -0.00018736526544671506, -0.10881263017654419)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.025556376203894615, 3.9819595258450136e-05, -0.023765835911035538)\n",
      "L1T1_lora_out: (0.05111275240778923, 7.963919051690027e-05, -0.047531671822071075)\n",
      "\n",
      "L2T1_lora_B_out: (0.017548346891999245, 1.297506969422102e-05, -0.01843593642115593)\n",
      "L2T1_lora_out: (0.03509669378399849, 2.595013938844204e-05, -0.03687187284231186)\n",
      "\n",
      "L2T2_lora_B_out: (0.044175632297992706, 9.358444367535412e-05, -0.042061299085617065)\n",
      "L2T2_lora_out: (0.08835126459598541, 0.00018716888735070825, -0.08412259817123413)\n",
      "\n",
      "g_out: (0.0774170309305191, 0.00016121873341035098, -0.07541808485984802)\n",
      "L1T2_out: (0.11310562491416931, 0.00024085798941086978, -0.11515568196773529)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0018003421137109399, 8.700764738023281e-06, -0.0018203537911176682)\n",
      "L1T1_lora_out: (0.0036006842274218798, 1.7401529476046562e-05, -0.0036407075822353363)\n",
      "\n",
      "L2T1_lora_B_out: (0.0012437471887096763, 1.0446278793097008e-06, -0.0011433976469561458)\n",
      "L2T1_lora_out: (0.0024874943774193525, 2.0892557586194016e-06, -0.0022867952939122915)\n",
      "\n",
      "L2T2_lora_B_out: (0.0038781180046498775, 1.0736730473581702e-05, -0.003668013494461775)\n",
      "L2T2_lora_out: (0.007756236009299755, 2.1473460947163403e-05, -0.00733602698892355)\n",
      "\n",
      "g_out: (0.006729216314852238, 1.938420973601751e-05, -0.00631004199385643)\n",
      "L1T2_out: (0.009651292115449905, 3.678574285004288e-05, -0.009448076598346233)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10928814113140106, -5.062659693066962e-05, -0.08148662745952606)\n",
      "L1T1_lora_out: (0.21857628226280212, -0.00010125319386133924, -0.16297325491905212)\n",
      "\n",
      "L2T1_lora_B_out: (0.06608428806066513, 6.662862142547965e-05, -0.04092627763748169)\n",
      "L2T1_lora_out: (0.13216857612133026, 0.0001332572428509593, -0.08185255527496338)\n",
      "\n",
      "L2T2_lora_B_out: (0.20256824791431427, 4.381508188089356e-05, -0.13410405814647675)\n",
      "L2T2_lora_out: (0.40513649582862854, 8.763016376178712e-05, -0.2682081162929535)\n",
      "\n",
      "g_out: (0.33693844079971313, -4.562714457279071e-05, -0.24251624941825867)\n",
      "L1T2_out: (0.5307025909423828, -0.0001468803093302995, -0.4054895043373108)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.014255732297897339, -3.857309366139816e-06, -0.01964458078145981)\n",
      "L1T1_lora_out: (0.028511464595794678, -7.714618732279632e-06, -0.03928916156291962)\n",
      "\n",
      "L2T1_lora_B_out: (0.01772545836865902, 1.0165076673729345e-05, -0.01200275681912899)\n",
      "L2T1_lora_out: (0.03545091673731804, 2.033015334745869e-05, -0.02400551363825798)\n",
      "\n",
      "L2T2_lora_B_out: (0.026072748005390167, 2.1643634681822732e-05, -0.025887517258524895)\n",
      "L2T2_lora_out: (0.052145496010780334, 4.3287269363645464e-05, -0.05177503451704979)\n",
      "\n",
      "g_out: (0.044998899102211, 2.2957110559218563e-05, -0.04360920190811157)\n",
      "L1T2_out: (0.06821854412555695, 1.5242510926327668e-05, -0.08032163977622986)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003655022708699107, -2.2297517716651782e-06, -0.003606556449085474)\n",
      "L1T1_lora_out: (0.007310045417398214, -4.4595035433303565e-06, -0.007213112898170948)\n",
      "\n",
      "L2T1_lora_B_out: (0.0026080214884132147, 7.28378790881834e-06, -0.002722715027630329)\n",
      "L2T1_lora_out: (0.005216042976826429, 1.456757581763668e-05, -0.005445430055260658)\n",
      "\n",
      "L2T2_lora_B_out: (0.007443643640726805, 3.7650625017704442e-06, -0.0062143392860889435)\n",
      "L2T2_lora_out: (0.01488728728145361, 7.5301250035408884e-06, -0.012428678572177887)\n",
      "\n",
      "g_out: (0.010552579537034035, -7.037451723590493e-06, -0.010881825350224972)\n",
      "L1T2_out: (0.01757035404443741, -1.1496957085910253e-05, -0.01805455982685089)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02007506974041462, 3.876711161865387e-06, -0.021719755604863167)\n",
      "L1T1_lora_out: (0.04015013948082924, 7.753422323730774e-06, -0.043439511209726334)\n",
      "\n",
      "L2T1_lora_B_out: (0.02768080122768879, 7.873280992498621e-06, -0.029006710276007652)\n",
      "L2T1_lora_out: (0.05536160245537758, 1.5746561984997243e-05, -0.058013420552015305)\n",
      "\n",
      "L2T2_lora_B_out: (0.07693173736333847, 3.970659599872306e-05, -0.06978750973939896)\n",
      "L2T2_lora_out: (0.15386347472667694, 7.941319199744612e-05, -0.1395750194787979)\n",
      "\n",
      "g_out: (0.09850187599658966, 6.366665184032172e-05, -0.10388664901256561)\n",
      "L1T2_out: (0.12440860271453857, 7.142003596527502e-05, -0.14691928029060364)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028877202421426773, -0.0001578818482812494, -0.025657638907432556)\n",
      "L1T1_lora_out: (0.057754404842853546, -0.0003157636965624988, -0.05131527781486511)\n",
      "\n",
      "L2T1_lora_B_out: (0.01568630523979664, 6.465087062679231e-05, -0.014151358045637608)\n",
      "L2T1_lora_out: (0.03137261047959328, 0.00012930174125358462, -0.028302716091275215)\n",
      "\n",
      "L2T2_lora_B_out: (0.05581699311733246, -0.00016549455176573247, -0.06805513054132462)\n",
      "L2T2_lora_out: (0.11163398623466492, -0.00033098910353146493, -0.13611026108264923)\n",
      "\n",
      "g_out: (0.09364920109510422, -0.0004602906992658973, -0.11645521223545074)\n",
      "L1T2_out: (0.1508631408214569, -0.000776054454036057, -0.16452667117118835)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03441103547811508, -1.515837629995076e-05, -0.044394806027412415)\n",
      "L1T1_lora_out: (0.06882207095623016, -3.031675259990152e-05, -0.08878961205482483)\n",
      "\n",
      "L2T1_lora_B_out: (0.02018480934202671, 0.0001250837231054902, -0.01585506461560726)\n",
      "L2T1_lora_out: (0.04036961868405342, 0.0002501674462109804, -0.03171012923121452)\n",
      "\n",
      "L2T2_lora_B_out: (0.09433558583259583, 7.442622882081196e-05, -0.10014394670724869)\n",
      "L2T2_lora_out: (0.18867117166519165, 0.00014885245764162391, -0.20028789341449738)\n",
      "\n",
      "g_out: (0.15964485704898834, -0.00010131499584531412, -0.18322937190532684)\n",
      "L1T2_out: (0.21918053925037384, -0.00013163175026420504, -0.2720189690589905)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0028081473428756, -1.812136133594322e-06, -0.0033893780782818794)\n",
      "L1T1_lora_out: (0.0056162946857512, -3.624272267188644e-06, -0.006778756156563759)\n",
      "\n",
      "L2T1_lora_B_out: (0.0019339114660397172, -3.6101396290177945e-06, -0.002007710747420788)\n",
      "L2T1_lora_out: (0.0038678229320794344, -7.220279258035589e-06, -0.004015421494841576)\n",
      "\n",
      "L2T2_lora_B_out: (0.006196720991283655, -2.7496960683492944e-05, -0.006106822285801172)\n",
      "L2T2_lora_out: (0.01239344198256731, -5.499392136698589e-05, -0.012213644571602345)\n",
      "\n",
      "g_out: (0.009785226546227932, -4.777363938046619e-05, -0.010129420086741447)\n",
      "L1T2_out: (0.014231942594051361, -5.139790300745517e-05, -0.01684238202869892)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.027411475777626038, -2.795740329020191e-05, -0.030927903950214386)\n",
      "L1T1_lora_out: (0.054822951555252075, -5.591480658040382e-05, -0.06185580790042877)\n",
      "\n",
      "L2T1_lora_B_out: (0.08126167207956314, -0.00010832468251464888, -0.07404813915491104)\n",
      "L2T1_lora_out: (0.16252334415912628, -0.00021664936502929777, -0.14809627830982208)\n",
      "\n",
      "L2T2_lora_B_out: (0.14674915373325348, -0.0001440385531168431, -0.16435375809669495)\n",
      "L2T2_lora_out: (0.29349830746650696, -0.0002880771062336862, -0.3287075161933899)\n",
      "\n",
      "g_out: (0.19367527961730957, -7.142772665247321e-05, -0.19551555812358856)\n",
      "L1T2_out: (0.23576851189136505, -0.00012734254414681345, -0.25737136602401733)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.029140563681721687, 5.507139940164052e-05, -0.028123419731855392)\n",
      "L1T1_lora_out: (0.058281127363443375, 0.00011014279880328104, -0.056246839463710785)\n",
      "\n",
      "L2T1_lora_B_out: (0.04355112835764885, 2.53322741627926e-05, -0.03952259570360184)\n",
      "L2T1_lora_out: (0.0871022567152977, 5.06645483255852e-05, -0.07904519140720367)\n",
      "\n",
      "L2T2_lora_B_out: (0.0989447683095932, 0.00010486340761417523, -0.09427950531244278)\n",
      "L2T2_lora_out: (0.1978895366191864, 0.00020972681522835046, -0.18855901062488556)\n",
      "\n",
      "g_out: (0.1521938443183899, 0.00015906229964457452, -0.1236967146396637)\n",
      "L1T2_out: (0.21047496795654297, 0.0002692050766199827, -0.17524650692939758)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.002436289330944419, -4.6224840843933634e-07, -0.0025115248281508684)\n",
      "L1T1_lora_out: (0.004872578661888838, -9.244968168786727e-07, -0.005023049656301737)\n",
      "\n",
      "L2T1_lora_B_out: (0.0023538931272923946, 2.1780883763256043e-09, -0.00219846167601645)\n",
      "L2T1_lora_out: (0.004707786254584789, 4.3561767526512085e-09, -0.0043969233520329)\n",
      "\n",
      "L2T2_lora_B_out: (0.00485233822837472, 3.5784105421043932e-06, -0.005145143251866102)\n",
      "L2T2_lora_out: (0.00970467645674944, 7.1568210842087865e-06, -0.010290286503732204)\n",
      "\n",
      "g_out: (0.007499918807297945, 7.152469152060803e-06, -0.007581446785479784)\n",
      "L1T2_out: (0.01062553096562624, 6.2279714256874286e-06, -0.01034748088568449)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021487940102815628, 7.046038808766752e-05, -0.023524880409240723)\n",
      "L1T1_lora_out: (0.042975880205631256, 0.00014092077617533505, -0.047049760818481445)\n",
      "\n",
      "L2T1_lora_B_out: (0.016945606097579002, 1.2507322026067413e-05, -0.020322700962424278)\n",
      "L2T1_lora_out: (0.033891212195158005, 2.5014644052134827e-05, -0.040645401924848557)\n",
      "\n",
      "L2T2_lora_B_out: (0.05990399047732353, 0.00014718387683387846, -0.05383993312716484)\n",
      "L2T2_lora_out: (0.11980798095464706, 0.0002943677536677569, -0.10767986625432968)\n",
      "\n",
      "g_out: (0.09202241897583008, 0.0002693531569093466, -0.09537988156080246)\n",
      "L1T2_out: (0.12850825488567352, 0.0004102739039808512, -0.1327882558107376)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03514368459582329, -4.813041232409887e-05, -0.03646579012274742)\n",
      "L1T1_lora_out: (0.07028736919164658, -9.626082464819774e-05, -0.07293158024549484)\n",
      "\n",
      "L2T1_lora_B_out: (0.02824123576283455, -4.0515409637009725e-05, -0.02495616488158703)\n",
      "L2T1_lora_out: (0.0564824715256691, -8.103081927401945e-05, -0.04991232976317406)\n",
      "\n",
      "L2T2_lora_B_out: (0.0752091333270073, 0.00013587175635620952, -0.09142110496759415)\n",
      "L2T2_lora_out: (0.1504182666540146, 0.00027174351271241903, -0.1828422099351883)\n",
      "\n",
      "g_out: (0.12443232536315918, 0.00035277436836622655, -0.14089232683181763)\n",
      "L1T2_out: (0.19104081392288208, 0.0002565135946497321, -0.2079651951789856)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.017406154423952103, 7.827449007891119e-05, -0.017715582624077797)\n",
      "L1T1_lora_out: (0.034812308847904205, 0.00015654898015782237, -0.035431165248155594)\n",
      "\n",
      "L2T1_lora_B_out: (0.016335254535079002, 5.956059249001555e-05, -0.016493557021021843)\n",
      "L2T1_lora_out: (0.032670509070158005, 0.0001191211849800311, -0.032987114042043686)\n",
      "\n",
      "L2T2_lora_B_out: (0.04968991503119469, 0.00019134057220071554, -0.048609256744384766)\n",
      "L2T2_lora_out: (0.09937983006238937, 0.0003826811444014311, -0.09721851348876953)\n",
      "\n",
      "g_out: (0.0835580825805664, 0.00026355989393778145, -0.07000561058521271)\n",
      "L1T2_out: (0.11837039142847061, 0.00042010884499177337, -0.09911125153303146)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005269446410238743, 4.382796760182828e-06, -0.0054239328019320965)\n",
      "L1T1_lora_out: (0.010538892820477486, 8.765593520365655e-06, -0.010847865603864193)\n",
      "\n",
      "L2T1_lora_B_out: (0.0037312933709472418, 2.9072293727949727e-06, -0.003230128902941942)\n",
      "L2T1_lora_out: (0.0074625867418944836, 5.8144587455899455e-06, -0.006460257805883884)\n",
      "\n",
      "L2T2_lora_B_out: (0.008025354705750942, 9.630623026168905e-06, -0.009447813965380192)\n",
      "L2T2_lora_out: (0.016050709411501884, 1.926124605233781e-05, -0.018895627930760384)\n",
      "\n",
      "g_out: (0.014879010617733002, 1.3446780940284953e-05, -0.01752949133515358)\n",
      "L1T2_out: (0.025417903438210487, 2.2212356270756572e-05, -0.028377357870340347)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06158486381173134, -0.00011604427709244192, -0.048949841409921646)\n",
      "L1T1_lora_out: (0.12316972762346268, -0.00023208855418488383, -0.09789968281984329)\n",
      "\n",
      "L2T1_lora_B_out: (0.051977742463350296, -0.0002478279930073768, -0.05962011590600014)\n",
      "L2T1_lora_out: (0.10395548492670059, -0.0004956559860147536, -0.11924023181200027)\n",
      "\n",
      "L2T2_lora_B_out: (0.12220480293035507, -0.0005486978334374726, -0.14256393909454346)\n",
      "L2T2_lora_out: (0.24440960586071014, -0.0010973956668749452, -0.2851278781890869)\n",
      "\n",
      "g_out: (0.2129639983177185, -0.0006017396808601916, -0.17792877554893494)\n",
      "L1T2_out: (0.32527831196784973, -0.0008338281768374145, -0.27582845091819763)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.043952472507953644, 1.6042369679780677e-05, -0.04285642132163048)\n",
      "L1T1_lora_out: (0.08790494501590729, 3.2084739359561354e-05, -0.08571284264326096)\n",
      "\n",
      "L2T1_lora_B_out: (0.023188024759292603, 2.3021271772449836e-05, -0.023781098425388336)\n",
      "L2T1_lora_out: (0.046376049518585205, 4.604254354489967e-05, -0.04756219685077667)\n",
      "\n",
      "L2T2_lora_B_out: (0.0994473323225975, 7.759754225844517e-05, -0.12331004440784454)\n",
      "L2T2_lora_out: (0.198894664645195, 0.00015519508451689035, -0.2466200888156891)\n",
      "\n",
      "g_out: (0.1560177356004715, 0.00010915252642007545, -0.20074757933616638)\n",
      "L1T2_out: (0.24392268061637878, 0.00014123728033155203, -0.28646042943000793)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004285921808332205, 2.4461669454467483e-06, -0.004130812361836433)\n",
      "L1T1_lora_out: (0.00857184361666441, 4.8923338908934966e-06, -0.008261624723672867)\n",
      "\n",
      "L2T1_lora_B_out: (0.0032294520642608404, -3.865898179356009e-06, -0.0029974665958434343)\n",
      "L2T1_lora_out: (0.006458904128521681, -7.731796358712018e-06, -0.005994933191686869)\n",
      "\n",
      "L2T2_lora_B_out: (0.005077263806015253, -1.0143668077944312e-05, -0.005370659753680229)\n",
      "L2T2_lora_out: (0.010154527612030506, -2.0287336155888624e-05, -0.010741319507360458)\n",
      "\n",
      "g_out: (0.00964188203215599, -1.25555361591978e-05, -0.00969672854989767)\n",
      "L1T2_out: (0.01780638098716736, -7.663196811336093e-06, -0.014541548676788807)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.022975610569119453, -9.015308023663238e-06, -0.019208529964089394)\n",
      "L1T1_lora_out: (0.04595122113823891, -1.8030616047326475e-05, -0.03841705992817879)\n",
      "\n",
      "L2T1_lora_B_out: (0.027119241654872894, 2.517922803235706e-05, -0.028315631672739983)\n",
      "L2T1_lora_out: (0.05423848330974579, 5.035845606471412e-05, -0.056631263345479965)\n",
      "\n",
      "L2T2_lora_B_out: (0.0601327009499073, 7.5679590736399405e-06, -0.05181514844298363)\n",
      "L2T2_lora_out: (0.1202654018998146, 1.5135918147279881e-05, -0.10363029688596725)\n",
      "\n",
      "g_out: (0.08263859897851944, -3.522253973642364e-05, -0.07832987606525421)\n",
      "L1T2_out: (0.11695699393749237, -5.325316305970773e-05, -0.1051110178232193)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04184029623866081, 5.003419573768042e-05, -0.03621739521622658)\n",
      "L1T1_lora_out: (0.08368059247732162, 0.00010006839147536084, -0.07243479043245316)\n",
      "\n",
      "L2T1_lora_B_out: (0.02082991786301136, -0.00013020091864746064, -0.022369444370269775)\n",
      "L2T1_lora_out: (0.04165983572602272, -0.0002604018372949213, -0.04473888874053955)\n",
      "\n",
      "L2T2_lora_B_out: (0.06888516247272491, -9.044991747941822e-05, -0.06881631910800934)\n",
      "L2T2_lora_out: (0.13777032494544983, -0.00018089983495883644, -0.13763263821601868)\n",
      "\n",
      "g_out: (0.12275366485118866, 7.950200233608484e-05, -0.12283545732498169)\n",
      "L1T2_out: (0.1876908242702484, 0.00017957028467208147, -0.19127222895622253)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.016869831830263138, 6.389690679498017e-05, -0.01715654879808426)\n",
      "L1T1_lora_out: (0.033739663660526276, 0.00012779381358996034, -0.03431309759616852)\n",
      "\n",
      "L2T1_lora_B_out: (0.019441675394773483, -4.929544047627132e-06, -0.023387819528579712)\n",
      "L2T1_lora_out: (0.03888335078954697, -9.859088095254265e-06, -0.046775639057159424)\n",
      "\n",
      "L2T2_lora_B_out: (0.04139738157391548, 4.954973701387644e-05, -0.04165356233716011)\n",
      "L2T2_lora_out: (0.08279476314783096, 9.909947402775288e-05, -0.08330712467432022)\n",
      "\n",
      "g_out: (0.06376584619283676, 0.00010895851301029325, -0.06765665113925934)\n",
      "L1T2_out: (0.08843272924423218, 0.00023675229749642313, -0.09372128546237946)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004789356142282486, -9.413308362127282e-06, -0.0047788252122700214)\n",
      "L1T1_lora_out: (0.009578712284564972, -1.8826616724254563e-05, -0.009557650424540043)\n",
      "\n",
      "L2T1_lora_B_out: (0.0035274545662105083, 1.550190518173622e-06, -0.003458684775978327)\n",
      "L2T1_lora_out: (0.007054909132421017, 3.100381036347244e-06, -0.006917369551956654)\n",
      "\n",
      "L2T2_lora_B_out: (0.010922874324023724, -1.2779115422745235e-05, -0.011522367596626282)\n",
      "L2T2_lora_out: (0.021845748648047447, -2.555823084549047e-05, -0.023044735193252563)\n",
      "\n",
      "g_out: (0.019026830792427063, -2.865860005840659e-05, -0.02085314877331257)\n",
      "L1T2_out: (0.027443744242191315, -4.748521314468235e-05, -0.030410800129175186)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05756295472383499, -0.00011104383156634867, -0.04676653817296028)\n",
      "L1T1_lora_out: (0.11512590944766998, -0.00022208766313269734, -0.09353307634592056)\n",
      "\n",
      "L2T1_lora_B_out: (0.09632705897092819, -0.00014060278772376478, -0.052985820919275284)\n",
      "L2T1_lora_out: (0.19265411794185638, -0.00028120557544752955, -0.10597164183855057)\n",
      "\n",
      "L2T2_lora_B_out: (0.207667276263237, -0.000388919870601967, -0.17059510946273804)\n",
      "L2T2_lora_out: (0.415334552526474, -0.000777839741203934, -0.3411902189254761)\n",
      "\n",
      "g_out: (0.2857774794101715, -0.0004966342821717262, -0.24979084730148315)\n",
      "L1T2_out: (0.3863447308540344, -0.0007187218870967627, -0.3433239161968231)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020757243037223816, 3.5526041756384075e-05, -0.02117803879082203)\n",
      "L1T1_lora_out: (0.04151448607444763, 7.105208351276815e-05, -0.04235607758164406)\n",
      "\n",
      "L2T1_lora_B_out: (0.02375325933098793, -7.617419669259107e-06, -0.020066246390342712)\n",
      "L2T1_lora_out: (0.04750651866197586, -1.5234839338518213e-05, -0.040132492780685425)\n",
      "\n",
      "L2T2_lora_B_out: (0.0783977210521698, 7.961039955262095e-05, -0.051182474941015244)\n",
      "L2T2_lora_out: (0.1567954421043396, 0.0001592207991052419, -0.10236494988203049)\n",
      "\n",
      "g_out: (0.129734069108963, 0.00017445566481910646, -0.08321435749530792)\n",
      "L1T2_out: (0.16959039866924286, 0.00024550771922804415, -0.12557043135166168)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006335488986223936, 3.875741640513297e-06, -0.00683832261711359)\n",
      "L1T1_lora_out: (0.012670977972447872, 7.751483281026594e-06, -0.01367664523422718)\n",
      "\n",
      "L2T1_lora_B_out: (0.004945374559611082, 9.948274964699522e-06, -0.005215743090957403)\n",
      "L2T1_lora_out: (0.009890749119222164, 1.9896549929399043e-05, -0.010431486181914806)\n",
      "\n",
      "L2T2_lora_B_out: (0.013467550277709961, 7.304347946046619e-06, -0.016977079212665558)\n",
      "L2T2_lora_out: (0.026935100555419922, 1.4608695892093237e-05, -0.033954158425331116)\n",
      "\n",
      "g_out: (0.025186067447066307, -5.287869043968385e-06, -0.026278315111994743)\n",
      "L1T2_out: (0.037857044488191605, 2.463611508574104e-06, -0.03955227881669998)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.022610343992710114, 1.5030156646389514e-05, -0.023972099646925926)\n",
      "L1T1_lora_out: (0.04522068798542023, 3.006031329277903e-05, -0.04794419929385185)\n",
      "\n",
      "L2T1_lora_B_out: (0.032993707805871964, 8.94444456207566e-05, -0.028278296813368797)\n",
      "L2T1_lora_out: (0.06598741561174393, 0.0001788888912415132, -0.056556593626737595)\n",
      "\n",
      "L2T2_lora_B_out: (0.05971141159534454, 0.00012987531954422593, -0.07273777574300766)\n",
      "L2T2_lora_out: (0.11942282319068909, 0.00025975063908845186, -0.14547555148601532)\n",
      "\n",
      "g_out: (0.08825314044952393, 8.086172601906583e-05, -0.10635009407997131)\n",
      "L1T2_out: (0.12839281558990479, 0.00011092205386376008, -0.1472521275281906)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06314335763454437, 0.0003563318168744445, -0.06672868877649307)\n",
      "L1T1_lora_out: (0.12628671526908875, 0.000712663633748889, -0.13345737755298615)\n",
      "\n",
      "L2T1_lora_B_out: (0.03312239050865173, 0.00017842224042396992, -0.026389632374048233)\n",
      "L2T1_lora_out: (0.06624478101730347, 0.00035684448084793985, -0.052779264748096466)\n",
      "\n",
      "L2T2_lora_B_out: (0.11909586936235428, 0.0007314867107197642, -0.12365313619375229)\n",
      "L2T2_lora_out: (0.23819173872470856, 0.0014629734214395285, -0.24730627238750458)\n",
      "\n",
      "g_out: (0.20555379986763, 0.0011061287950724363, -0.22125592827796936)\n",
      "L1T2_out: (0.31587114930152893, 0.0018187923124060035, -0.3464755415916443)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02124049700796604, -3.625527460826561e-05, -0.02030937932431698)\n",
      "L1T1_lora_out: (0.04248099401593208, -7.251054921653122e-05, -0.04061875864863396)\n",
      "\n",
      "L2T1_lora_B_out: (0.020245369523763657, -3.688801371026784e-05, -0.02283569611608982)\n",
      "L2T1_lora_out: (0.04049073904752731, -7.377602742053568e-05, -0.04567139223217964)\n",
      "\n",
      "L2T2_lora_B_out: (0.06384515762329102, -0.00016983864770736545, -0.06222236901521683)\n",
      "L2T2_lora_out: (0.12769031524658203, -0.0003396772954147309, -0.12444473803043365)\n",
      "\n",
      "g_out: (0.09261748194694519, -0.00026590132620185614, -0.0919230505824089)\n",
      "L1T2_out: (0.1342070996761322, -0.0003384117444511503, -0.1302589774131775)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0037736587692052126, -4.430934041010914e-06, -0.004202552139759064)\n",
      "L1T1_lora_out: (0.007547317538410425, -8.861868082021829e-06, -0.008405104279518127)\n",
      "\n",
      "L2T1_lora_B_out: (0.003899791045114398, -7.00558302924037e-06, -0.003552924608811736)\n",
      "L2T1_lora_out: (0.007799582090228796, -1.401116605848074e-05, -0.007105849217623472)\n",
      "\n",
      "L2T2_lora_B_out: (0.011042078025639057, -1.800142490537837e-05, -0.01082761213183403)\n",
      "L2T2_lora_out: (0.022084156051278114, -3.600284981075674e-05, -0.02165522426366806)\n",
      "\n",
      "g_out: (0.016537770628929138, -2.19916819332866e-05, -0.01865304633975029)\n",
      "L1T2_out: (0.022406375035643578, -3.085354910581373e-05, -0.027058150619268417)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.072112537920475, -4.321204687585123e-05, -0.11280635744333267)\n",
      "L1T1_lora_out: (0.14422507584095, -8.642409375170246e-05, -0.22561271488666534)\n",
      "\n",
      "L2T1_lora_B_out: (0.058249324560165405, -7.565740816062316e-05, -0.04121560603380203)\n",
      "L2T1_lora_out: (0.11649864912033081, -0.00015131481632124633, -0.08243121206760406)\n",
      "\n",
      "L2T2_lora_B_out: (0.06505896151065826, -0.00011494844511616975, -0.08263552933931351)\n",
      "L2T2_lora_out: (0.13011792302131653, -0.0002298968902323395, -0.16527105867862701)\n",
      "\n",
      "g_out: (0.12729568779468536, -7.858205935917795e-05, -0.15901508927345276)\n",
      "L1T2_out: (0.2366636097431183, -0.0001650061458349228, -0.33791816234588623)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.026499561965465546, -1.307739603362279e-05, -0.0215771421790123)\n",
      "L1T1_lora_out: (0.05299912393093109, -2.615479206724558e-05, -0.0431542843580246)\n",
      "\n",
      "L2T1_lora_B_out: (0.01797020249068737, 1.7473385582889023e-07, -0.01587679237127304)\n",
      "L2T1_lora_out: (0.03594040498137474, 3.4946771165778046e-07, -0.03175358474254608)\n",
      "\n",
      "L2T2_lora_B_out: (0.06519696861505508, -1.1330650522722863e-05, -0.05451926961541176)\n",
      "L2T2_lora_out: (0.13039393723011017, -2.2661301045445725e-05, -0.10903853923082352)\n",
      "\n",
      "g_out: (0.10716519504785538, -2.3010763470665552e-05, -0.08444319665431976)\n",
      "L1T2_out: (0.15966933965682983, -4.9165551899932325e-05, -0.11965049058198929)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0032357382588088512, -1.5557145161437802e-05, -0.0033335767220705748)\n",
      "L1T1_lora_out: (0.0064714765176177025, -3.1114290322875604e-05, -0.0066671534441411495)\n",
      "\n",
      "L2T1_lora_B_out: (0.0071326387114822865, 2.0733457859023474e-05, -0.006606605369597673)\n",
      "L2T1_lora_out: (0.014265277422964573, 4.146691571804695e-05, -0.013213210739195347)\n",
      "\n",
      "L2T2_lora_B_out: (0.00867057591676712, 2.191121893702075e-05, -0.009614890441298485)\n",
      "L2T2_lora_out: (0.01734115183353424, 4.38224378740415e-05, -0.01922978088259697)\n",
      "\n",
      "g_out: (0.010751658119261265, 2.3555294319521636e-06, -0.013387253507971764)\n",
      "L1T2_out: (0.01515638455748558, -2.8758757252944633e-05, -0.017948776483535767)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.015288535505533218, -3.783523425227031e-05, -0.015327692963182926)\n",
      "L1T1_lora_out: (0.030577071011066437, -7.567046850454062e-05, -0.030655385926365852)\n",
      "\n",
      "L2T1_lora_B_out: (0.020865701138973236, -4.275653191143647e-05, -0.023097701370716095)\n",
      "L2T1_lora_out: (0.04173140227794647, -8.551306382287294e-05, -0.04619540274143219)\n",
      "\n",
      "L2T2_lora_B_out: (0.04315808042883873, -4.364066626294516e-05, -0.03765052556991577)\n",
      "L2T2_lora_out: (0.08631616085767746, -8.728133252589032e-05, -0.07530105113983154)\n",
      "\n",
      "g_out: (0.06612236052751541, -1.7682566522125853e-06, -0.07076368480920792)\n",
      "L1T2_out: (0.09037706255912781, -7.7438737207558e-05, -0.09028077870607376)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04838236793875694, -0.0006970111280679703, -0.053310785442590714)\n",
      "L1T1_lora_out: (0.09676473587751389, -0.0013940222561359406, -0.10662157088518143)\n",
      "\n",
      "L2T1_lora_B_out: (0.026236185804009438, -0.0002858931547962129, -0.023663604632019997)\n",
      "L2T1_lora_out: (0.052472371608018875, -0.0005717863095924258, -0.04732720926403999)\n",
      "\n",
      "L2T2_lora_B_out: (0.09619621932506561, -0.00099681515712291, -0.12832187116146088)\n",
      "L2T2_lora_out: (0.19239243865013123, -0.00199363031424582, -0.25664374232292175)\n",
      "\n",
      "g_out: (0.16996337473392487, -0.0014218440046533942, -0.21129097044467926)\n",
      "L1T2_out: (0.2566431164741516, -0.0028158663772046566, -0.3179125487804413)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01227526180446148, -2.5231653125956655e-05, -0.013408727943897247)\n",
      "L1T1_lora_out: (0.02455052360892296, -5.046330625191331e-05, -0.026817455887794495)\n",
      "\n",
      "L2T1_lora_B_out: (0.014874606393277645, -4.893324145882616e-08, -0.014293772168457508)\n",
      "L2T1_lora_out: (0.02974921278655529, -9.786648291765232e-08, -0.028587544336915016)\n",
      "\n",
      "L2T2_lora_B_out: (0.025431575253605843, -7.634692883584648e-05, -0.030929818749427795)\n",
      "L2T2_lora_out: (0.050863150507211685, -0.00015269385767169297, -0.06185963749885559)\n",
      "\n",
      "g_out: (0.041870906949043274, -0.00015259595238603652, -0.04411475360393524)\n",
      "L1T2_out: (0.06511414051055908, -0.00020305921498220414, -0.06054571270942688)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.002025730675086379, 1.2675167454290204e-05, -0.0021119662560522556)\n",
      "L1T1_lora_out: (0.004051461350172758, 2.5350334908580408e-05, -0.004223932512104511)\n",
      "\n",
      "L2T1_lora_B_out: (0.004503376316279173, 5.413126928033307e-06, -0.0054757678881287575)\n",
      "L2T1_lora_out: (0.009006752632558346, 1.0826253856066614e-05, -0.010951535776257515)\n",
      "\n",
      "L2T2_lora_B_out: (0.011695301160216331, 3.959950845455751e-05, -0.010141356848180294)\n",
      "L2T2_lora_out: (0.023390602320432663, 7.919901690911502e-05, -0.020282713696360588)\n",
      "\n",
      "g_out: (0.01558559201657772, 6.837277032900602e-05, -0.016436750069260597)\n",
      "L1T2_out: (0.019618907943367958, 9.372308704769239e-05, -0.020519331097602844)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03525417670607567, -4.5607150241266936e-05, -0.04879323020577431)\n",
      "L1T1_lora_out: (0.07050835341215134, -9.121430048253387e-05, -0.09758646041154861)\n",
      "\n",
      "L2T1_lora_B_out: (0.07536505162715912, -0.00013484254304785281, -0.045762885361909866)\n",
      "L2T1_lora_out: (0.15073010325431824, -0.00026968508609570563, -0.09152577072381973)\n",
      "\n",
      "L2T2_lora_B_out: (0.13835439085960388, -0.0002974019153043628, -0.17603257298469543)\n",
      "L2T2_lora_out: (0.27670878171920776, -0.0005948038306087255, -0.35206514596939087)\n",
      "\n",
      "g_out: (0.16362139582633972, -0.00032511871540918946, -0.27461516857147217)\n",
      "L1T2_out: (0.23300272226333618, -0.00041633303044363856, -0.3722016215324402)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.026163509115576744, -8.95412995305378e-06, -0.02180728130042553)\n",
      "L1T1_lora_out: (0.05232701823115349, -1.790825990610756e-05, -0.04361456260085106)\n",
      "\n",
      "L2T1_lora_B_out: (0.02589794620871544, -5.971842711005593e-06, -0.02538946643471718)\n",
      "L2T1_lora_out: (0.05179589241743088, -1.1943685422011185e-05, -0.05077893286943436)\n",
      "\n",
      "L2T2_lora_B_out: (0.08549249172210693, -1.9558048734324984e-05, -0.06316487491130829)\n",
      "L2T2_lora_out: (0.17098498344421387, -3.911609746864997e-05, -0.12632974982261658)\n",
      "\n",
      "g_out: (0.13513903319835663, -2.717240022320766e-05, -0.09578201919794083)\n",
      "L1T2_out: (0.18746605515480042, -4.5080647396389395e-05, -0.13301873207092285)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0036251365672796965, -1.6201390735659515e-06, -0.003548017470166087)\n",
      "L1T1_lora_out: (0.007250273134559393, -3.240278147131903e-06, -0.007096034940332174)\n",
      "\n",
      "L2T1_lora_B_out: (0.005828855559229851, 2.7495711037772708e-05, -0.007715532556176186)\n",
      "L2T1_lora_out: (0.011657711118459702, 5.4991422075545415e-05, -0.015431065112352371)\n",
      "\n",
      "L2T2_lora_B_out: (0.014330588281154633, 2.986686013173312e-05, -0.01510639674961567)\n",
      "L2T2_lora_out: (0.028661176562309265, 5.973372026346624e-05, -0.03021279349923134)\n",
      "\n",
      "g_out: (0.01767386496067047, 4.742304554383736e-06, -0.01993601955473423)\n",
      "L1T2_out: (0.02463536337018013, 1.5020244745755917e-06, -0.026815220713615417)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.016561167314648628, 1.107924799725879e-05, -0.015234628692269325)\n",
      "L1T1_lora_out: (0.033122334629297256, 2.215849599451758e-05, -0.03046925738453865)\n",
      "\n",
      "L2T1_lora_B_out: (0.02623562701046467, -3.4184951800853014e-05, -0.0393565371632576)\n",
      "L2T1_lora_out: (0.05247125402092934, -6.836990360170603e-05, -0.0787130743265152)\n",
      "\n",
      "L2T2_lora_B_out: (0.04690863564610481, -5.208434595260769e-05, -0.059015579521656036)\n",
      "L2T2_lora_out: (0.09381727129220963, -0.00010416869190521538, -0.11803115904331207)\n",
      "\n",
      "g_out: (0.07588301599025726, -3.579879557946697e-05, -0.08647963404655457)\n",
      "L1T2_out: (0.1048317700624466, -1.3640276847581845e-05, -0.1129584088921547)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02690211683511734, -8.716238880879246e-06, -0.030064422637224197)\n",
      "L1T1_lora_out: (0.05380423367023468, -1.743247776175849e-05, -0.060128845274448395)\n",
      "\n",
      "L2T1_lora_B_out: (0.08647563308477402, 0.0005825328407809138, -0.05227820575237274)\n",
      "L2T1_lora_out: (0.17295126616954803, 0.0011650656815618277, -0.10455641150474548)\n",
      "\n",
      "L2T2_lora_B_out: (0.12469011545181274, 0.00077378487912938, -0.10622447729110718)\n",
      "L2T2_lora_out: (0.2493802309036255, 0.00154756975825876, -0.21244895458221436)\n",
      "\n",
      "g_out: (0.14627793431282043, 0.00038250413490459323, -0.17231020331382751)\n",
      "L1T2_out: (0.20008216798305511, 0.0003650716971606016, -0.21049685776233673)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.019523847848176956, -0.00013002645573578775, -0.021377597004175186)\n",
      "L1T1_lora_out: (0.03904769569635391, -0.0002600529114715755, -0.04275519400835037)\n",
      "\n",
      "L2T1_lora_B_out: (0.022419866174459457, 7.867180102039129e-05, -0.027859536930918694)\n",
      "L2T1_lora_out: (0.044839732348918915, 0.00015734360204078257, -0.05571907386183739)\n",
      "\n",
      "L2T2_lora_B_out: (0.04567818343639374, -7.50981635064818e-05, -0.050256822258234024)\n",
      "L2T2_lora_out: (0.09135636687278748, -0.0001501963270129636, -0.10051364451646805)\n",
      "\n",
      "g_out: (0.06211321800947189, -0.0003075399436056614, -0.08177965879440308)\n",
      "L1T2_out: (0.09067963063716888, -0.0005675928550772369, -0.11998447775840759)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0035851234570145607, 7.421165719279088e-06, -0.0033416396472603083)\n",
      "L1T1_lora_out: (0.007170246914029121, 1.4842331438558176e-05, -0.0066832792945206165)\n",
      "\n",
      "L2T1_lora_B_out: (0.0046272603794932365, 1.3622715187011636e-06, -0.004133146721869707)\n",
      "L2T1_lora_out: (0.009254520758986473, 2.724543037402327e-06, -0.008266293443739414)\n",
      "\n",
      "L2T2_lora_B_out: (0.010169467888772488, 2.972412039525807e-05, -0.009811654686927795)\n",
      "L2T2_lora_out: (0.020338935777544975, 5.944824079051614e-05, -0.01962330937385559)\n",
      "\n",
      "g_out: (0.015434227883815765, 5.6723707530181855e-05, -0.014720095321536064)\n",
      "L1T2_out: (0.022604474797844887, 7.156604988267645e-05, -0.019601963460445404)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.049832701683044434, -0.00014234328409656882, -0.06114817410707474)\n",
      "L1T1_lora_out: (0.09966540336608887, -0.00028468656819313765, -0.12229634821414948)\n",
      "\n",
      "L2T1_lora_B_out: (0.05000320449471474, -0.00022845265630166978, -0.054030489176511765)\n",
      "L2T1_lora_out: (0.10000640898942947, -0.00045690531260333955, -0.10806097835302353)\n",
      "\n",
      "L2T2_lora_B_out: (0.10624305158853531, -0.00047517396160401404, -0.1369851976633072)\n",
      "L2T2_lora_out: (0.21248610317707062, -0.0009503479232080281, -0.2739703953266144)\n",
      "\n",
      "g_out: (0.16749390959739685, -0.0004934426979161799, -0.19159486889839172)\n",
      "L1T2_out: (0.25180360674858093, -0.0007781292661093175, -0.3138912320137024)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021069742739200592, 3.3592002637305995e-06, -0.02021888457238674)\n",
      "L1T1_lora_out: (0.042139485478401184, 6.718400527461199e-06, -0.04043776914477348)\n",
      "\n",
      "L2T1_lora_B_out: (0.019739538431167603, 2.084954576275777e-05, -0.020671920850872993)\n",
      "L2T1_lora_out: (0.039479076862335205, 4.169909152551554e-05, -0.04134384170174599)\n",
      "\n",
      "L2T2_lora_B_out: (0.04655660316348076, 1.7702810509945266e-05, -0.05046837031841278)\n",
      "L2T2_lora_out: (0.09311320632696152, 3.540562101989053e-05, -0.10093674063682556)\n",
      "\n",
      "g_out: (0.07134661823511124, -6.293512342381291e-06, -0.06668928265571594)\n",
      "L1T2_out: (0.11034876108169556, 4.248826712682785e-07, -0.09492108970880508)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004343308042734861, 1.2324184694989526e-07, -0.0044367071241140366)\n",
      "L1T1_lora_out: (0.008686616085469723, 2.464836938997905e-07, -0.008873414248228073)\n",
      "\n",
      "L2T1_lora_B_out: (0.006641439162194729, 4.4766460632672533e-05, -0.0066939787939190865)\n",
      "L2T1_lora_out: (0.013282878324389458, 8.953292126534507e-05, -0.013387957587838173)\n",
      "\n",
      "L2T2_lora_B_out: (0.014849212020635605, 8.29649725346826e-05, -0.01820230856537819)\n",
      "L2T2_lora_out: (0.02969842404127121, 0.0001659299450693652, -0.03640461713075638)\n",
      "\n",
      "g_out: (0.022176243364810944, 7.639703107997775e-05, -0.02518199011683464)\n",
      "L1T2_out: (0.0306205153465271, 7.664351142011583e-05, -0.03254919499158859)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03093036450445652, -5.922585387452273e-06, -0.029367905110120773)\n",
      "L1T1_lora_out: (0.06186072900891304, -1.1845170774904545e-05, -0.05873581022024155)\n",
      "\n",
      "L2T1_lora_B_out: (0.03450283780694008, -6.390717317117378e-05, -0.03244208171963692)\n",
      "L2T1_lora_out: (0.06900567561388016, -0.00012781434634234756, -0.06488416343927383)\n",
      "\n",
      "L2T2_lora_B_out: (0.0830705463886261, -0.0001303351018577814, -0.07373279333114624)\n",
      "L2T2_lora_out: (0.1661410927772522, -0.0002606702037155628, -0.14746558666229248)\n",
      "\n",
      "g_out: (0.1251811683177948, -0.00013285588647704571, -0.12796054780483246)\n",
      "L1T2_out: (0.182949960231781, -0.0001447010727133602, -0.1823921501636505)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.057128340005874634, -3.662232074930216e-06, -0.05963309109210968)\n",
      "L1T1_lora_out: (0.11425668001174927, -7.324464149860432e-06, -0.11926618218421936)\n",
      "\n",
      "L2T1_lora_B_out: (0.013844734989106655, 7.617396477144212e-05, -0.014793166890740395)\n",
      "L2T1_lora_out: (0.02768946997821331, 0.00015234792954288423, -0.02958633378148079)\n",
      "\n",
      "L2T2_lora_B_out: (0.07964757084846497, -0.0001513849274488166, -0.07343120872974396)\n",
      "L2T2_lora_out: (0.15929514169692993, -0.0003027698548976332, -0.14686241745948792)\n",
      "\n",
      "g_out: (0.14805038273334503, -0.0004551177844405174, -0.13742893934249878)\n",
      "L1T2_out: (0.26046884059906006, -0.00046244222903624177, -0.22233819961547852)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.012504653073847294, 7.293694216059521e-05, -0.012124063447117805)\n",
      "L1T1_lora_out: (0.025009306147694588, 0.00014587388432119042, -0.02424812689423561)\n",
      "\n",
      "L2T1_lora_B_out: (0.020540855824947357, 3.868612111546099e-05, -0.01868928037583828)\n",
      "L2T1_lora_out: (0.041081711649894714, 7.737224223092198e-05, -0.03737856075167656)\n",
      "\n",
      "L2T2_lora_B_out: (0.05463895574212074, 0.00032556382939219475, -0.04809146374464035)\n",
      "L2T2_lora_out: (0.10927791148424149, 0.0006511276587843895, -0.0961829274892807)\n",
      "\n",
      "g_out: (0.06819619983434677, 0.0005737554165534675, -0.060806773602962494)\n",
      "L1T2_out: (0.08486331254243851, 0.0007196292281150818, -0.0784897655248642)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0036353913601487875, 2.4694978492334485e-05, -0.003507426707074046)\n",
      "L1T1_lora_out: (0.007270782720297575, 4.938995698466897e-05, -0.007014853414148092)\n",
      "\n",
      "L2T1_lora_B_out: (0.005097453016787767, 1.0434458090458065e-05, -0.005607915576547384)\n",
      "L2T1_lora_out: (0.010194906033575535, 2.086891618091613e-05, -0.011215831153094769)\n",
      "\n",
      "L2T2_lora_B_out: (0.013093695975840092, 9.039798169396818e-05, -0.012178976088762283)\n",
      "L2T2_lora_out: (0.026187391951680183, 0.00018079596338793635, -0.024357952177524567)\n",
      "\n",
      "g_out: (0.020615609362721443, 0.00015992706175893545, -0.01839011162519455)\n",
      "L1T2_out: (0.027886392548680305, 0.0002093170041916892, -0.02540496550500393)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06347537040710449, -0.00012524712656158954, -0.0784638300538063)\n",
      "L1T1_lora_out: (0.12695074081420898, -0.0002504942531231791, -0.1569276601076126)\n",
      "\n",
      "L2T1_lora_B_out: (0.06365468353033066, -0.0001894957385957241, -0.07158888131380081)\n",
      "L2T1_lora_out: (0.12730936706066132, -0.0003789914771914482, -0.14317776262760162)\n",
      "\n",
      "L2T2_lora_B_out: (0.14526280760765076, -0.0005422625108622015, -0.2307235598564148)\n",
      "L2T2_lora_out: (0.2905256152153015, -0.001084525021724403, -0.4614471197128296)\n",
      "\n",
      "g_out: (0.2214880883693695, -0.0007055335445329547, -0.3212238550186157)\n",
      "L1T2_out: (0.3484388291835785, -0.0009560277685523033, -0.47815150022506714)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02586470916867256, -4.639297185349278e-05, -0.025352735072374344)\n",
      "L1T1_lora_out: (0.05172941833734512, -9.278594370698556e-05, -0.05070547014474869)\n",
      "\n",
      "L2T1_lora_B_out: (0.02551063895225525, -6.682551611447707e-05, -0.023629700765013695)\n",
      "L2T1_lora_out: (0.0510212779045105, -0.00013365103222895414, -0.04725940153002739)\n",
      "\n",
      "L2T2_lora_B_out: (0.05912283807992935, -0.00020121908164583147, -0.06273894011974335)\n",
      "L2T2_lora_out: (0.1182456761598587, -0.00040243816329166293, -0.1254778802394867)\n",
      "\n",
      "g_out: (0.09693396091461182, -0.0002687871747184545, -0.0967366024851799)\n",
      "L1T2_out: (0.14267930388450623, -0.00036157312570139766, -0.14095795154571533)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0037009541410952806, 3.0881897146173287e-06, -0.003944976720958948)\n",
      "L1T1_lora_out: (0.007401908282190561, 6.1763794292346574e-06, -0.007889953441917896)\n",
      "\n",
      "L2T1_lora_B_out: (0.004963574931025505, -4.1491875890642405e-07, -0.0057615963742136955)\n",
      "L2T1_lora_out: (0.00992714986205101, -8.298375178128481e-07, -0.011523192748427391)\n",
      "\n",
      "L2T2_lora_B_out: (0.01153834629803896, -8.646366040920839e-07, -0.010815441608428955)\n",
      "L2T2_lora_out: (0.02307669259607792, -1.7292732081841677e-06, -0.02163088321685791)\n",
      "\n",
      "g_out: (0.016506031155586243, -8.994454105959448e-07, -0.017248712480068207)\n",
      "L1T2_out: (0.02240360900759697, 5.276944193610689e-06, -0.025078359991312027)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020791208371520042, -1.0680355444492307e-05, -0.02117444947361946)\n",
      "L1T1_lora_out: (0.041582416743040085, -2.1360710888984613e-05, -0.04234889894723892)\n",
      "\n",
      "L2T1_lora_B_out: (0.021412819623947144, -2.8470869438024238e-05, -0.021858325228095055)\n",
      "L2T1_lora_out: (0.04282563924789429, -5.6941738876048476e-05, -0.04371665045619011)\n",
      "\n",
      "L2T2_lora_B_out: (0.06151837110519409, -4.743785393657163e-05, -0.07181353867053986)\n",
      "L2T2_lora_out: (0.12303674221038818, -9.487570787314326e-05, -0.1436270773410797)\n",
      "\n",
      "g_out: (0.09967830032110214, -3.793404539464973e-05, -0.10886897146701813)\n",
      "L1T2_out: (0.14126071333885193, -5.9294710808899254e-05, -0.14564675092697144)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03461277484893799, -0.0001716718397801742, -0.028442440554499626)\n",
      "L1T1_lora_out: (0.06922554969787598, -0.0003433436795603484, -0.05688488110899925)\n",
      "\n",
      "L2T1_lora_B_out: (0.030016571283340454, 0.00012911255180370063, -0.03410471975803375)\n",
      "L2T1_lora_out: (0.06003314256668091, 0.00025822510360740125, -0.0682094395160675)\n",
      "\n",
      "L2T2_lora_B_out: (0.08424486219882965, -2.8642089091590606e-05, -0.07649838924407959)\n",
      "L2T2_lora_out: (0.1684897243976593, -5.728417818318121e-05, -0.15299677848815918)\n",
      "\n",
      "g_out: (0.14520154893398285, -0.0003155091544613242, -0.12822261452674866)\n",
      "L1T2_out: (0.21442709863185883, -0.0006588529795408249, -0.18420900404453278)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023933209478855133, -4.851695848628879e-06, -0.02916562370955944)\n",
      "L1T1_lora_out: (0.047866418957710266, -9.703391697257757e-06, -0.05833124741911888)\n",
      "\n",
      "L2T1_lora_B_out: (0.016857784241437912, -0.0002782716474030167, -0.019633354619145393)\n",
      "L2T1_lora_out: (0.033715568482875824, -0.0005565432948060334, -0.03926670923829079)\n",
      "\n",
      "L2T2_lora_B_out: (0.06058108061552048, -0.0006033668178133667, -0.06185612827539444)\n",
      "L2T2_lora_out: (0.12116216123104095, -0.0012067336356267333, -0.12371225655078888)\n",
      "\n",
      "g_out: (0.10068327188491821, -0.0006501903990283608, -0.10561323165893555)\n",
      "L1T2_out: (0.1464899182319641, -0.0006598938489332795, -0.15857131779193878)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0024272622540593147, -5.989978490106296e-06, -0.002671561436727643)\n",
      "L1T1_lora_out: (0.0048545245081186295, -1.1979956980212592e-05, -0.005343122873455286)\n",
      "\n",
      "L2T1_lora_B_out: (0.0034932829439640045, -2.9219022508186754e-06, -0.003484112210571766)\n",
      "L2T1_lora_out: (0.006986565887928009, -5.843804501637351e-06, -0.006968224421143532)\n",
      "\n",
      "L2T2_lora_B_out: (0.007757642772048712, -7.853479473851621e-06, -0.006602066569030285)\n",
      "L2T2_lora_out: (0.015515285544097424, -1.5706958947703242e-05, -0.01320413313806057)\n",
      "\n",
      "g_out: (0.011424597352743149, -9.863152627076488e-06, -0.011714459396898746)\n",
      "L1T2_out: (0.01426306925714016, -2.1843108697794378e-05, -0.014980231411755085)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03617461770772934, -0.00011410502338549122, -0.06493371725082397)\n",
      "L1T1_lora_out: (0.07234923541545868, -0.00022821004677098244, -0.12986743450164795)\n",
      "\n",
      "L2T1_lora_B_out: (0.08800429105758667, -9.271282760892063e-05, -0.11483175307512283)\n",
      "L2T1_lora_out: (0.17600858211517334, -0.00018542565521784127, -0.22966350615024567)\n",
      "\n",
      "L2T2_lora_B_out: (0.13344112038612366, -0.000334591546561569, -0.2724856734275818)\n",
      "L2T2_lora_out: (0.2668822407722473, -0.000669183093123138, -0.5449713468551636)\n",
      "\n",
      "g_out: (0.1803865134716034, -0.0004837574379052967, -0.32477712631225586)\n",
      "L1T2_out: (0.24750252068042755, -0.0007119674701243639, -0.4546445608139038)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.029497181996703148, 3.568138345144689e-05, -0.041176021099090576)\n",
      "L1T1_lora_out: (0.058994363993406296, 7.136276690289378e-05, -0.08235204219818115)\n",
      "\n",
      "L2T1_lora_B_out: (0.03843333572149277, 2.205022974521853e-05, -0.03748521953821182)\n",
      "L2T1_lora_out: (0.07686667144298553, 4.410045949043706e-05, -0.07497043907642365)\n",
      "\n",
      "L2T2_lora_B_out: (0.0998278558254242, 0.00011794755118899047, -0.13707111775875092)\n",
      "L2T2_lora_out: (0.1996557116508484, 0.00023589510237798095, -0.27414223551750183)\n",
      "\n",
      "g_out: (0.15507802367210388, 0.00019179466471541673, -0.21718764305114746)\n",
      "L1T2_out: (0.21407239139080048, 0.00026315744617022574, -0.2995396852493286)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0083096232265234, -1.0471047971805092e-05, -0.009795392863452435)\n",
      "L1T1_lora_out: (0.0166192464530468, -2.0942095943610184e-05, -0.01959078572690487)\n",
      "\n",
      "L2T1_lora_B_out: (0.009058503434062004, -4.340577106631827e-06, -0.008674245327711105)\n",
      "L2T1_lora_out: (0.018117006868124008, -8.681154213263653e-06, -0.01734849065542221)\n",
      "\n",
      "L2T2_lora_B_out: (0.01921517215669155, -2.7812551707029343e-05, -0.021267326548695564)\n",
      "L2T2_lora_out: (0.0384303443133831, -5.5625103414058685e-05, -0.04253465309739113)\n",
      "\n",
      "g_out: (0.034192100167274475, -4.6943951019784436e-05, -0.035425182431936264)\n",
      "L1T2_out: (0.050811346620321274, -6.788607424823567e-05, -0.055015966296195984)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023990025743842125, 2.087960092467256e-05, -0.021207090467214584)\n",
      "L1T1_lora_out: (0.04798005148768425, 4.175920184934512e-05, -0.04241418093442917)\n",
      "\n",
      "L2T1_lora_B_out: (0.027339432388544083, -5.1027977860940155e-06, -0.030542714521288872)\n",
      "L2T1_lora_out: (0.054678864777088165, -1.0205595572188031e-05, -0.061085429042577744)\n",
      "\n",
      "L2T2_lora_B_out: (0.0617302730679512, 8.244984201155603e-05, -0.08240100741386414)\n",
      "L2T2_lora_out: (0.1234605461359024, 0.00016489968402311206, -0.16480201482772827)\n",
      "\n",
      "g_out: (0.09137623012065887, 0.00017510526231490076, -0.11273084580898285)\n",
      "L1T2_out: (0.12757226824760437, 0.00021686448599211872, -0.1511508971452713)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03377281874418259, -7.314200774999335e-05, -0.0312538743019104)\n",
      "L1T1_lora_out: (0.06754563748836517, -0.0001462840154999867, -0.0625077486038208)\n",
      "\n",
      "L2T1_lora_B_out: (0.02008483000099659, -1.6348258213838562e-05, -0.024838948622345924)\n",
      "L2T1_lora_out: (0.04016966000199318, -3.2696516427677125e-05, -0.04967789724469185)\n",
      "\n",
      "L2T2_lora_B_out: (0.07504966855049133, -2.897757076425478e-05, -0.08877547830343246)\n",
      "L2T2_lora_out: (0.15009933710098267, -5.795514152850956e-05, -0.17755095660686493)\n",
      "\n",
      "g_out: (0.11938221752643585, -2.5258637833758257e-05, -0.13991716504096985)\n",
      "L1T2_out: (0.16645345091819763, -0.00017154263332486153, -0.19805744290351868)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.011867628432810307, -3.570041008060798e-05, -0.012852207757532597)\n",
      "L1T1_lora_out: (0.023735256865620613, -7.140082016121596e-05, -0.025704415515065193)\n",
      "\n",
      "L2T1_lora_B_out: (0.019248563796281815, 1.900206189020537e-05, -0.020986497402191162)\n",
      "L2T1_lora_out: (0.03849712759256363, 3.800412378041074e-05, -0.041972994804382324)\n",
      "\n",
      "L2T2_lora_B_out: (0.03365760296583176, -0.00014092130004428327, -0.038617923855781555)\n",
      "L2T2_lora_out: (0.06731520593166351, -0.00028184260008856654, -0.07723584771156311)\n",
      "\n",
      "g_out: (0.0596119724214077, -0.0003198467311449349, -0.05616065859794617)\n",
      "L1T2_out: (0.08334723114967346, -0.0003912475367542356, -0.07725072652101517)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003716842271387577, 4.2149230239374447e-07, -0.0032678351271897554)\n",
      "L1T1_lora_out: (0.007433684542775154, 8.429846047874889e-07, -0.006535670254379511)\n",
      "\n",
      "L2T1_lora_B_out: (0.003683330724015832, 1.0844301868928596e-05, -0.0035669126082211733)\n",
      "L2T1_lora_out: (0.007366661448031664, 2.1688603737857193e-05, -0.007133825216442347)\n",
      "\n",
      "L2T2_lora_B_out: (0.007053771987557411, 2.31372396228835e-05, -0.007747170049697161)\n",
      "L2T2_lora_out: (0.014107543975114822, 4.6274479245767e-05, -0.015494340099394321)\n",
      "\n",
      "g_out: (0.01269006822258234, 2.458589369780384e-05, -0.012339900247752666)\n",
      "L1T2_out: (0.02012375369668007, 2.5428871595067903e-05, -0.01795920729637146)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04257144406437874, -0.00018115843704435974, -0.042375124990940094)\n",
      "L1T1_lora_out: (0.08514288812875748, -0.0003623168740887195, -0.08475024998188019)\n",
      "\n",
      "L2T1_lora_B_out: (0.06339582800865173, -0.0001807322696549818, -0.06366384029388428)\n",
      "L2T1_lora_out: (0.12679165601730347, -0.0003614645393099636, -0.12732768058776855)\n",
      "\n",
      "L2T2_lora_B_out: (0.13687680661678314, -0.0006315671489574015, -0.15470266342163086)\n",
      "L2T2_lora_out: (0.2737536132335663, -0.001263134297914803, -0.3094053268432617)\n",
      "\n",
      "g_out: (0.1811046153306961, -0.0009016696130856872, -0.2112172245979309)\n",
      "L1T2_out: (0.265471488237381, -0.0012639864580705762, -0.2947414517402649)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.024811701849102974, 4.149907545070164e-05, -0.02213355526328087)\n",
      "L1T1_lora_out: (0.04962340369820595, 8.299815090140328e-05, -0.04426711052656174)\n",
      "\n",
      "L2T1_lora_B_out: (0.028300268575549126, -1.0609288437990472e-05, -0.027347123250365257)\n",
      "L2T1_lora_out: (0.05660053715109825, -2.1218576875980943e-05, -0.054694246500730515)\n",
      "\n",
      "L2T2_lora_B_out: (0.06234443187713623, 5.943742871750146e-05, -0.066594697535038)\n",
      "L2T2_lora_out: (0.12468886375427246, 0.00011887485743500292, -0.133189395070076)\n",
      "\n",
      "g_out: (0.10202103853225708, 0.0001400933979311958, -0.10036680102348328)\n",
      "L1T2_out: (0.1457674205303192, 0.00022309154155664146, -0.13657836616039276)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.007739558815956116, -5.049213996244362e-06, -0.008060432970523834)\n",
      "L1T1_lora_out: (0.015479117631912231, -1.0098427992488723e-05, -0.01612086594104767)\n",
      "\n",
      "L2T1_lora_B_out: (0.0069978809915483, 1.796583273971919e-05, -0.007091175299137831)\n",
      "L2T1_lora_out: (0.0139957619830966, 3.593166547943838e-05, -0.014182350598275661)\n",
      "\n",
      "L2T2_lora_B_out: (0.01454103086143732, 9.852918083197437e-06, -0.01759237051010132)\n",
      "L2T2_lora_out: (0.02908206172287464, 1.9705836166394874e-05, -0.03518474102020264)\n",
      "\n",
      "g_out: (0.025909021496772766, -1.622582203708589e-05, -0.03091668337583542)\n",
      "L1T2_out: (0.03962070494890213, -2.6324236387154087e-05, -0.04703754931688309)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04035539552569389, 7.202558481367305e-05, -0.04135074466466904)\n",
      "L1T1_lora_out: (0.08071079105138779, 0.0001440511696273461, -0.08270148932933807)\n",
      "\n",
      "L2T1_lora_B_out: (0.033369410783052444, 3.1857562134973705e-05, -0.043776702135801315)\n",
      "L2T1_lora_out: (0.06673882156610489, 6.371512426994741e-05, -0.08755340427160263)\n",
      "\n",
      "L2T2_lora_B_out: (0.08115722984075546, 0.0001651679922360927, -0.08682887256145477)\n",
      "L2T2_lora_out: (0.16231445968151093, 0.0003303359844721854, -0.17365774512290955)\n",
      "\n",
      "g_out: (0.1534425914287567, 0.0002666208893060684, -0.14132218062877655)\n",
      "L1T2_out: (0.23087544739246368, 0.00041067213169299066, -0.22402366995811462)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.029755230993032455, 0.0001452046853955835, -0.025427132844924927)\n",
      "L1T1_lora_out: (0.05951046198606491, 0.000290409370791167, -0.050854265689849854)\n",
      "\n",
      "L2T1_lora_B_out: (0.03980967774987221, 0.00025325146270915866, -0.03506661579012871)\n",
      "L2T1_lora_out: (0.07961935549974442, 0.0005065029254183173, -0.07013323158025742)\n",
      "\n",
      "L2T2_lora_B_out: (0.07825477421283722, 0.0005804244428873062, -0.0755426213145256)\n",
      "L2T2_lora_out: (0.15650954842567444, 0.0011608488857746124, -0.1510852426290512)\n",
      "\n",
      "g_out: (0.13043653964996338, 0.000654346018563956, -0.10256950557231903)\n",
      "L1T2_out: (0.1899470090866089, 0.000944755389355123, -0.13883385062217712)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.018603302538394928, 0.00013434691936708987, -0.023950349539518356)\n",
      "L1T1_lora_out: (0.037206605076789856, 0.00026869383873417974, -0.04790069907903671)\n",
      "\n",
      "L2T1_lora_B_out: (0.01788289286196232, 9.919345029629767e-05, -0.018035827204585075)\n",
      "L2T1_lora_out: (0.03576578572392464, 0.00019838690059259534, -0.03607165440917015)\n",
      "\n",
      "L2T2_lora_B_out: (0.03770357370376587, 0.0006042271852493286, -0.03892505541443825)\n",
      "L2T2_lora_out: (0.07540714740753174, 0.0012084543704986572, -0.0778501108288765)\n",
      "\n",
      "g_out: (0.06979963183403015, 0.0010100675281137228, -0.06803916394710541)\n",
      "L1T2_out: (0.10070683807134628, 0.0012787613086402416, -0.11227446049451828)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0029585356824100018, 1.8690715251068468e-06, -0.0026994324289262295)\n",
      "L1T1_lora_out: (0.0059170713648200035, 3.7381430502136936e-06, -0.005398864857852459)\n",
      "\n",
      "L2T1_lora_B_out: (0.005591301713138819, 3.0476101528620347e-05, -0.004774678032845259)\n",
      "L2T1_lora_out: (0.011182603426277637, 6.0952203057240695e-05, -0.009549356065690517)\n",
      "\n",
      "L2T2_lora_B_out: (0.011190386489033699, 3.423820453463122e-05, -0.010872717946767807)\n",
      "L2T2_lora_out: (0.022380772978067398, 6.847640906926244e-05, -0.021745435893535614)\n",
      "\n",
      "g_out: (0.01607329398393631, 7.524203738284996e-06, -0.017035074532032013)\n",
      "L1T2_out: (0.021235082298517227, 1.1262338375672698e-05, -0.021853268146514893)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06372813880443573, -0.00011427833669586107, -0.0698331668972969)\n",
      "L1T1_lora_out: (0.12745627760887146, -0.00022855667339172214, -0.1396663337945938)\n",
      "\n",
      "L2T1_lora_B_out: (0.10880617797374725, -0.0002100669516948983, -0.09485074132680893)\n",
      "L2T1_lora_out: (0.2176123559474945, -0.0004201339033897966, -0.18970148265361786)\n",
      "\n",
      "L2T2_lora_B_out: (0.17264094948768616, -0.0005818135105073452, -0.25411051511764526)\n",
      "L2T2_lora_out: (0.3452818989753723, -0.0011636270210146904, -0.5082210302352905)\n",
      "\n",
      "g_out: (0.2627767026424408, -0.0007434930885210633, -0.34090614318847656)\n",
      "L1T2_out: (0.36256521940231323, -0.0009720497764647007, -0.46258968114852905)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03647482022643089, 3.199754064553417e-05, -0.03418475762009621)\n",
      "L1T1_lora_out: (0.07294964045286179, 6.399508129106835e-05, -0.06836951524019241)\n",
      "\n",
      "L2T1_lora_B_out: (0.03173932060599327, -3.834614471998066e-05, -0.034813765436410904)\n",
      "L2T1_lora_out: (0.06347864121198654, -7.669228943996131e-05, -0.06962753087282181)\n",
      "\n",
      "L2T2_lora_B_out: (0.08019398897886276, -8.838816938805394e-06, -0.08792389929294586)\n",
      "L2T2_lora_out: (0.16038797795772552, -1.7677633877610788e-05, -0.17584779858589172)\n",
      "\n",
      "g_out: (0.11940179765224457, 5.9014608268626034e-05, -0.14326734840869904)\n",
      "L1T2_out: (0.18709702789783478, 0.00012300966773182154, -0.21163687109947205)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006387195084244013, 4.845721832680283e-07, -0.005712003447115421)\n",
      "L1T1_lora_out: (0.012774390168488026, 9.691443665360566e-07, -0.011424006894230843)\n",
      "\n",
      "L2T1_lora_B_out: (0.006559907924383879, -1.3316368494997732e-05, -0.007154626306146383)\n",
      "L2T1_lora_out: (0.013119815848767757, -2.6632736989995465e-05, -0.014309252612292767)\n",
      "\n",
      "L2T2_lora_B_out: (0.016800053417682648, -2.0094179490115494e-05, -0.015267510898411274)\n",
      "L2T2_lora_out: (0.033600106835365295, -4.018835898023099e-05, -0.030535021796822548)\n",
      "\n",
      "g_out: (0.03013424016535282, -1.3555620171246119e-05, -0.024872440844774246)\n",
      "L1T2_out: (0.04105005040764809, -1.2586468074005097e-05, -0.03577551990747452)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028224319219589233, 2.3230079023051076e-05, -0.02737293764948845)\n",
      "L1T1_lora_out: (0.05644863843917847, 4.646015804610215e-05, -0.0547458752989769)\n",
      "\n",
      "L2T1_lora_B_out: (0.016180768609046936, -2.6304565835744143e-05, -0.01516177412122488)\n",
      "L2T1_lora_out: (0.03236153721809387, -5.2609131671488285e-05, -0.03032354824244976)\n",
      "\n",
      "L2T2_lora_B_out: (0.04469591751694679, 5.043969940743409e-05, -0.042886946350336075)\n",
      "L2T2_lora_out: (0.08939183503389359, 0.00010087939881486818, -0.08577389270067215)\n",
      "\n",
      "g_out: (0.07242434471845627, 0.00015348856686614454, -0.08686903119087219)\n",
      "L1T2_out: (0.12643592059612274, 0.00019994871399831027, -0.13449932634830475)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02991640567779541, -0.0001627498713787645, -0.026357827708125114)\n",
      "L1T1_lora_out: (0.05983281135559082, -0.000325499742757529, -0.05271565541625023)\n",
      "\n",
      "L2T1_lora_B_out: (0.03020365908741951, -0.00017826768453232944, -0.02825896069407463)\n",
      "L2T1_lora_out: (0.06040731817483902, -0.0003565353690646589, -0.05651792138814926)\n",
      "\n",
      "L2T2_lora_B_out: (0.0585847832262516, -0.0004820935137104243, -0.06295159459114075)\n",
      "L2T2_lora_out: (0.1171695664525032, -0.0009641870274208486, -0.1259031891822815)\n",
      "\n",
      "g_out: (0.07847465574741364, -0.0006076516583561897, -0.09910250455141068)\n",
      "L1T2_out: (0.11329236626625061, -0.0009331514011137187, -0.14917537569999695)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.024805014953017235, 5.409903678810224e-05, -0.020333856344223022)\n",
      "L1T1_lora_out: (0.04961002990603447, 0.00010819807357620448, -0.040667712688446045)\n",
      "\n",
      "L2T1_lora_B_out: (0.037695255130529404, 0.0001623154676053673, -0.02767951600253582)\n",
      "L2T1_lora_out: (0.07539051026105881, 0.0003246309352107346, -0.05535903200507164)\n",
      "\n",
      "L2T2_lora_B_out: (0.06610716879367828, 0.00034591200528666377, -0.07209379971027374)\n",
      "L2T2_lora_out: (0.13221433758735657, 0.0006918240105733275, -0.14418759942054749)\n",
      "\n",
      "g_out: (0.10319259762763977, 0.00036719298805110157, -0.10361413657665253)\n",
      "L1T2_out: (0.14831379055976868, 0.0004753910470753908, -0.1386372447013855)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0023290591780096292, 3.6716553495352855e-06, -0.002143519464880228)\n",
      "L1T1_lora_out: (0.0046581183560192585, 7.343310699070571e-06, -0.004287038929760456)\n",
      "\n",
      "L2T1_lora_B_out: (0.003970754332840443, 6.178578587423544e-06, -0.003993647173047066)\n",
      "L2T1_lora_out: (0.007941508665680885, 1.2357157174847089e-05, -0.007987294346094131)\n",
      "\n",
      "L2T2_lora_B_out: (0.005271630361676216, 6.695641332044033e-06, -0.0049387249164283276)\n",
      "L2T2_lora_out: (0.010543260723352432, 1.3391282664088067e-05, -0.009877449832856655)\n",
      "\n",
      "g_out: (0.008935105986893177, 1.0341282177250832e-06, -0.008889144286513329)\n",
      "L1T2_out: (0.012513081543147564, 8.377435733564198e-06, -0.013176183216273785)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.15685224533081055, -0.0001881476491689682, -0.1279098242521286)\n",
      "L1T1_lora_out: (0.3137044906616211, -0.0003762952983379364, -0.2558196485042572)\n",
      "\n",
      "L2T1_lora_B_out: (0.12574724853038788, -0.00029786425875499845, -0.18147505819797516)\n",
      "L2T1_lora_out: (0.25149449706077576, -0.0005957285175099969, -0.3629501163959503)\n",
      "\n",
      "L2T2_lora_B_out: (0.32710936665534973, -0.0008974673110060394, -0.4140234887599945)\n",
      "L2T2_lora_out: (0.6542187333106995, -0.0017949346220120788, -0.828046977519989)\n",
      "\n",
      "g_out: (0.5907580256462097, -0.0011992058716714382, -0.6138989329338074)\n",
      "L1T2_out: (0.9044625163078308, -0.0015755014028400183, -0.8697185516357422)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02622680738568306, -3.253356408094987e-05, -0.03024320676922798)\n",
      "L1T1_lora_out: (0.05245361477136612, -6.506712816189975e-05, -0.06048641353845596)\n",
      "\n",
      "L2T1_lora_B_out: (0.024433013051748276, -1.072245868272148e-05, -0.028058938682079315)\n",
      "L2T1_lora_out: (0.04886602610349655, -2.144491736544296e-05, -0.05611787736415863)\n",
      "\n",
      "L2T2_lora_B_out: (0.0635782852768898, -6.949043017812073e-05, -0.07328156381845474)\n",
      "L2T2_lora_out: (0.1271565705537796, -0.00013898086035624146, -0.14656312763690948)\n",
      "\n",
      "g_out: (0.09403052926063538, -0.00011753591388696805, -0.11315908282995224)\n",
      "L1T2_out: (0.13647997379302979, -0.00018260306387674063, -0.1662350594997406)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006627301685512066, -1.4801697943767067e-05, -0.007036435883492231)\n",
      "L1T1_lora_out: (0.013254603371024132, -2.9603395887534134e-05, -0.014072871766984463)\n",
      "\n",
      "L2T1_lora_B_out: (0.005155521910637617, -5.043164037488168e-06, -0.005833432078361511)\n",
      "L2T1_lora_out: (0.010311043821275234, -1.0086328074976336e-05, -0.011666864156723022)\n",
      "\n",
      "L2T2_lora_B_out: (0.012027847580611706, -2.6551468181423843e-05, -0.01244471874088049)\n",
      "L2T2_lora_out: (0.02405569516122341, -5.3102936362847686e-05, -0.02488943748176098)\n",
      "\n",
      "g_out: (0.021067623049020767, -4.301660737837665e-05, -0.01744369976222515)\n",
      "L1T2_out: (0.03309380263090134, -7.26200159988366e-05, -0.029262922704219818)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.036823175847530365, 5.974384839646518e-05, -0.043721459805965424)\n",
      "L1T1_lora_out: (0.07364635169506073, 0.00011948769679293036, -0.08744291961193085)\n",
      "\n",
      "L2T1_lora_B_out: (0.03029537759721279, -2.8900516554131173e-05, -0.026233887299895287)\n",
      "L2T1_lora_out: (0.06059075519442558, -5.7801033108262345e-05, -0.05246777459979057)\n",
      "\n",
      "L2T2_lora_B_out: (0.08118545264005661, 8.196331327781081e-05, -0.08834873139858246)\n",
      "L2T2_lora_out: (0.16237090528011322, 0.00016392662655562162, -0.17669746279716492)\n",
      "\n",
      "g_out: (0.1366099715232849, 0.0002217276196461171, -0.13504734635353088)\n",
      "L1T2_out: (0.21025632321834564, 0.00034121525823138654, -0.20367184281349182)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023697949945926666, -5.494873767020181e-05, -0.026637179777026176)\n",
      "L1T1_lora_out: (0.04739589989185333, -0.00010989747534040362, -0.05327435955405235)\n",
      "\n",
      "L2T1_lora_B_out: (0.030393563210964203, 0.00011215352424187586, -0.025773588567972183)\n",
      "L2T1_lora_out: (0.060787126421928406, 0.00022430704848375171, -0.051547177135944366)\n",
      "\n",
      "L2T2_lora_B_out: (0.05512290075421333, 0.00023043862893246114, -0.0521794892847538)\n",
      "L2T2_lora_out: (0.11024580150842667, 0.0004608772578649223, -0.1043589785695076)\n",
      "\n",
      "g_out: (0.0961851179599762, 0.0002365702239330858, -0.09668712317943573)\n",
      "L1T2_out: (0.14358101785182953, 0.00012667271948885173, -0.14996148645877838)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023794256150722504, 0.00019868681556545198, -0.022085217759013176)\n",
      "L1T1_lora_out: (0.04758851230144501, 0.00039737363113090396, -0.04417043551802635)\n",
      "\n",
      "L2T1_lora_B_out: (0.015241743996739388, -3.469401417532936e-05, -0.012667899020016193)\n",
      "L2T1_lora_out: (0.030483487993478775, -6.938802835065871e-05, -0.025335798040032387)\n",
      "\n",
      "L2T2_lora_B_out: (0.054019615054130554, 0.00027271383441984653, -0.03769508749246597)\n",
      "L2T2_lora_out: (0.10803923010826111, 0.0005454276688396931, -0.07539017498493195)\n",
      "\n",
      "g_out: (0.09160871058702469, 0.0006148156826384366, -0.06451176106929779)\n",
      "L1T2_out: (0.1391972303390503, 0.0010121893137693405, -0.09945353865623474)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004827787633985281, -1.860546035459265e-05, -0.00453973188996315)\n",
      "L1T1_lora_out: (0.009655575267970562, -3.72109207091853e-05, -0.0090794637799263)\n",
      "\n",
      "L2T1_lora_B_out: (0.004527592100203037, 8.03580587671604e-06, -0.004245677962899208)\n",
      "L2T1_lora_out: (0.009055184200406075, 1.607161175343208e-05, -0.008491355925798416)\n",
      "\n",
      "L2T2_lora_B_out: (0.013313598930835724, -1.5942547179292887e-05, -0.009836089797317982)\n",
      "L2T2_lora_out: (0.026627197861671448, -3.1885094358585775e-05, -0.019672179594635963)\n",
      "\n",
      "g_out: (0.01904747448861599, -4.7956695198081434e-05, -0.017016325145959854)\n",
      "L1T2_out: (0.024945229291915894, -8.516761590726674e-05, -0.024619674310088158)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.058698661625385284, -0.00013119386858306825, -0.07117930054664612)\n",
      "L1T1_lora_out: (0.11739732325077057, -0.0002623877371661365, -0.14235860109329224)\n",
      "\n",
      "L2T1_lora_B_out: (0.12033616751432419, -0.0003597515169531107, -0.11163685470819473)\n",
      "L2T1_lora_out: (0.24067233502864838, -0.0007195030339062214, -0.22327370941638947)\n",
      "\n",
      "L2T2_lora_B_out: (0.2189563363790512, -0.0007182577974162996, -0.1958547830581665)\n",
      "L2T2_lora_out: (0.4379126727581024, -0.0014365155948325992, -0.391709566116333)\n",
      "\n",
      "g_out: (0.2343229353427887, -0.0007170125609263778, -0.2157396674156189)\n",
      "L1T2_out: (0.3321576714515686, -0.0009794003563001752, -0.3558075428009033)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.029612893238663673, -7.212905074993614e-06, -0.020962221547961235)\n",
      "L1T1_lora_out: (0.05922578647732735, -1.4425810149987228e-05, -0.04192444309592247)\n",
      "\n",
      "L2T1_lora_B_out: (0.01761523447930813, 9.179344488075003e-06, -0.018248025327920914)\n",
      "L2T1_lora_out: (0.03523046895861626, 1.8358688976150006e-05, -0.03649605065584183)\n",
      "\n",
      "L2T2_lora_B_out: (0.03859751671552658, -2.3064643755787984e-05, -0.042242322117090225)\n",
      "L2T2_lora_out: (0.07719503343105316, -4.612928751157597e-05, -0.08448464423418045)\n",
      "\n",
      "g_out: (0.06373036652803421, -6.448795465985313e-05, -0.06189872696995735)\n",
      "L1T2_out: (0.10099132359027863, -7.891377754276618e-05, -0.08860395848751068)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006709564011543989, -7.590336281282362e-06, -0.006695656105875969)\n",
      "L1T1_lora_out: (0.013419128023087978, -1.5180672562564723e-05, -0.013391312211751938)\n",
      "\n",
      "L2T1_lora_B_out: (0.005165605805814266, 1.3836378457199316e-05, -0.005498853512108326)\n",
      "L2T1_lora_out: (0.010331211611628532, 2.7672756914398633e-05, -0.010997707024216652)\n",
      "\n",
      "L2T2_lora_B_out: (0.011060774326324463, 1.813643757486716e-05, -0.012159997597336769)\n",
      "L2T2_lora_out: (0.022121548652648926, 3.627287514973432e-05, -0.024319995194673538)\n",
      "\n",
      "g_out: (0.02193363383412361, 8.6001255112933e-06, -0.022078657522797585)\n",
      "L1T2_out: (0.03337994962930679, -6.580547051271424e-06, -0.03546997159719467)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.015323835425078869, -6.89866556058405e-06, -0.016081886366009712)\n",
      "L1T1_lora_out: (0.030647670850157738, -1.37973311211681e-05, -0.032163772732019424)\n",
      "\n",
      "L2T1_lora_B_out: (0.021136393770575523, -3.920811286661774e-05, -0.020697781816124916)\n",
      "L2T1_lora_out: (0.04227278754115105, -7.841622573323548e-05, -0.04139556363224983)\n",
      "\n",
      "L2T2_lora_B_out: (0.039289169013500214, -4.181402618996799e-05, -0.044252194464206696)\n",
      "L2T2_lora_out: (0.07857833802700043, -8.362805237993598e-05, -0.08850438892841339)\n",
      "\n",
      "g_out: (0.07224124670028687, -5.211865754972678e-06, -0.05791333317756653)\n",
      "L1T2_out: (0.10288891941308975, -1.9009137758985162e-05, -0.07847952097654343)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.027082737535238266, -0.0001908688573166728, -0.023709965869784355)\n",
      "L1T1_lora_out: (0.05416547507047653, -0.0003817377146333456, -0.04741993173956871)\n",
      "\n",
      "L2T1_lora_B_out: (0.02164209820330143, 5.765436071669683e-05, -0.01989630050957203)\n",
      "L2T1_lora_out: (0.04328419640660286, 0.00011530872143339366, -0.03979260101914406)\n",
      "\n",
      "L2T2_lora_B_out: (0.04678482189774513, -0.00017885424313135445, -0.05981127917766571)\n",
      "L2T2_lora_out: (0.09356964379549026, -0.0003577084862627089, -0.11962255835533142)\n",
      "\n",
      "g_out: (0.07618146389722824, -0.0004730172804556787, -0.08699798583984375)\n",
      "L1T2_out: (0.11757709085941315, -0.0008547549950890243, -0.11755919456481934)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03029054030776024, 2.932424422397162e-06, -0.03001854568719864)\n",
      "L1T1_lora_out: (0.06058108061552048, 5.864848844794324e-06, -0.06003709137439728)\n",
      "\n",
      "L2T1_lora_B_out: (0.022010469809174538, 0.00010246581950923428, -0.01833670400083065)\n",
      "L2T1_lora_out: (0.044020939618349075, 0.00020493163901846856, -0.0366734080016613)\n",
      "\n",
      "L2T2_lora_B_out: (0.0565967820584774, 0.00012983035412617028, -0.059363991022109985)\n",
      "L2T2_lora_out: (0.1131935641169548, 0.00025966070825234056, -0.11872798204421997)\n",
      "\n",
      "g_out: (0.10062781721353531, 5.4729094699723646e-05, -0.10268419981002808)\n",
      "L1T2_out: (0.13508272171020508, 6.059383667889051e-05, -0.16272129118442535)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0034229017328470945, -1.5715164408902638e-05, -0.004184619523584843)\n",
      "L1T1_lora_out: (0.006845803465694189, -3.1430328817805275e-05, -0.008369239047169685)\n",
      "\n",
      "L2T1_lora_B_out: (0.0030616989824920893, 1.2253668501216453e-05, -0.002657744102180004)\n",
      "L2T1_lora_out: (0.0061233979649841785, 2.4507337002432905e-05, -0.005315488204360008)\n",
      "\n",
      "L2T2_lora_B_out: (0.007168828975409269, 2.038430011452874e-06, -0.00708962744101882)\n",
      "L2T2_lora_out: (0.014337657950818539, 4.076860022905748e-06, -0.01417925488203764)\n",
      "\n",
      "g_out: (0.01242949441075325, -2.0430479708011262e-05, -0.014863832853734493)\n",
      "L1T2_out: (0.01816520467400551, -5.1860806706827134e-05, -0.023233070969581604)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10039591044187546, -1.3545202818932012e-05, -0.10328415781259537)\n",
      "L1T1_lora_out: (0.20079182088375092, -2.7090405637864023e-05, -0.20656831562519073)\n",
      "\n",
      "L2T1_lora_B_out: (0.15217702090740204, -0.00032914557959884405, -0.15416692197322845)\n",
      "L2T1_lora_out: (0.3043540418148041, -0.0006582911591976881, -0.3083338439464569)\n",
      "\n",
      "L2T2_lora_B_out: (0.2567809820175171, -0.00047911322326399386, -0.33316734433174133)\n",
      "L2T2_lora_out: (0.5135619640350342, -0.0009582264465279877, -0.6663346886634827)\n",
      "\n",
      "g_out: (0.2814846634864807, -0.00029993540374562144, -0.3760998845100403)\n",
      "L1T2_out: (0.44016560912132263, -0.00032702565658837557, -0.5826681852340698)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.044343430548906326, 2.59845660366409e-06, -0.03873687982559204)\n",
      "L1T1_lora_out: (0.08868686109781265, 5.19691320732818e-06, -0.07747375965118408)\n",
      "\n",
      "L2T1_lora_B_out: (0.03712446615099907, 1.4079972061153967e-05, -0.03391332924365997)\n",
      "L2T1_lora_out: (0.07424893230199814, 2.8159944122307934e-05, -0.06782665848731995)\n",
      "\n",
      "L2T2_lora_B_out: (0.11409086734056473, 6.272158498177305e-05, -0.09888502955436707)\n",
      "L2T2_lora_out: (0.22818173468112946, 0.0001254431699635461, -0.19777005910873413)\n",
      "\n",
      "g_out: (0.17987260222434998, 9.728320583235472e-05, -0.16272617876529694)\n",
      "L1T2_out: (0.25879353284835815, 0.00010248013131786138, -0.22952775657176971)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006771959830075502, -2.5121913495240733e-05, -0.008737936615943909)\n",
      "L1T1_lora_out: (0.013543919660151005, -5.0243826990481466e-05, -0.017475873231887817)\n",
      "\n",
      "L2T1_lora_B_out: (0.008144261315464973, 3.7978105638103443e-07, -0.007647385820746422)\n",
      "L2T1_lora_out: (0.016288522630929947, 7.595621127620689e-07, -0.015294771641492844)\n",
      "\n",
      "L2T2_lora_B_out: (0.017457129433751106, -4.185978468740359e-05, -0.02000705525279045)\n",
      "L2T2_lora_out: (0.03491425886750221, -8.371956937480718e-05, -0.0400141105055809)\n",
      "\n",
      "g_out: (0.03162688761949539, -8.447912841802463e-05, -0.030979294329881668)\n",
      "L1T2_out: (0.043910153210163116, -0.0001347229554085061, -0.04307537525892258)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02934861369431019, 5.0613758503459394e-05, -0.027487896382808685)\n",
      "L1T1_lora_out: (0.05869722738862038, 0.00010122751700691879, -0.05497579276561737)\n",
      "\n",
      "L2T1_lora_B_out: (0.02616288512945175, 6.418269185815006e-05, -0.024154575541615486)\n",
      "L2T1_lora_out: (0.0523257702589035, 0.00012836538371630013, -0.04830915108323097)\n",
      "\n",
      "L2T2_lora_B_out: (0.05868562310934067, 0.0002465544966980815, -0.06704159080982208)\n",
      "L2T2_lora_out: (0.11737124621868134, 0.000493108993396163, -0.13408318161964417)\n",
      "\n",
      "g_out: (0.09955398738384247, 0.0003647435805760324, -0.10924021154642105)\n",
      "L1T2_out: (0.1572485715150833, 0.0004659711557906121, -0.16384132206439972)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028431881219148636, 4.334902769187465e-05, -0.033733222633600235)\n",
      "L1T1_lora_out: (0.05686376243829727, 8.66980553837493e-05, -0.06746644526720047)\n",
      "\n",
      "L2T1_lora_B_out: (0.02352346107363701, 7.264958549058065e-05, -0.024048687890172005)\n",
      "L2T1_lora_out: (0.04704692214727402, 0.0001452991709811613, -0.04809737578034401)\n",
      "\n",
      "L2T2_lora_B_out: (0.06750713288784027, 0.00012521076132543385, -0.06676904857158661)\n",
      "L2T2_lora_out: (0.13501426577568054, 0.0002504215226508677, -0.13353809714317322)\n",
      "\n",
      "g_out: (0.10464656352996826, 0.00010512240987736732, -0.10237278789281845)\n",
      "L1T2_out: (0.1549646258354187, 0.00019182059622835368, -0.15765000879764557)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0716988667845726, 0.00016080087516456842, -0.04350154846906662)\n",
      "L1T1_lora_out: (0.1433977335691452, 0.00032160175032913685, -0.08700309693813324)\n",
      "\n",
      "L2T1_lora_B_out: (0.025398023426532745, 0.00010423212370369583, -0.02874436229467392)\n",
      "L2T1_lora_out: (0.05079604685306549, 0.00020846424740739167, -0.05748872458934784)\n",
      "\n",
      "L2T2_lora_B_out: (0.0592493899166584, 0.00010057977488031611, -0.05502763390541077)\n",
      "L2T2_lora_out: (0.1184987798333168, 0.00020115954976063222, -0.11005526781082153)\n",
      "\n",
      "g_out: (0.1151779368519783, -7.304518476303201e-06, -0.09800438582897186)\n",
      "L1T2_out: (0.22974373400211334, 0.00031429738737642765, -0.15932801365852356)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004276407416909933, 1.264320144400699e-05, -0.003231619717553258)\n",
      "L1T1_lora_out: (0.008552814833819866, 2.528640288801398e-05, -0.006463239435106516)\n",
      "\n",
      "L2T1_lora_B_out: (0.0029620786663144827, -4.68858115709736e-06, -0.0027826661244034767)\n",
      "L2T1_lora_out: (0.005924157332628965, -9.37716231419472e-06, -0.005565332248806953)\n",
      "\n",
      "L2T2_lora_B_out: (0.005979196634143591, 8.390019502257928e-06, -0.0053425924852490425)\n",
      "L2T2_lora_out: (0.011958393268287182, 1.6780039004515857e-05, -0.010685184970498085)\n",
      "\n",
      "g_out: (0.010409326292574406, 2.615719859022647e-05, -0.00956747680902481)\n",
      "L1T2_out: (0.017201753333210945, 5.144361057318747e-05, -0.014107739552855492)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1033150926232338, -0.00017265921633224934, -0.09428894519805908)\n",
      "L1T1_lora_out: (0.2066301852464676, -0.0003453184326644987, -0.18857789039611816)\n",
      "\n",
      "L2T1_lora_B_out: (0.08736629039049149, -8.384661487070844e-05, -0.08722469955682755)\n",
      "L2T1_lora_out: (0.17473258078098297, -0.00016769322974141687, -0.1744493991136551)\n",
      "\n",
      "L2T2_lora_B_out: (0.22368839383125305, -0.00035393337020650506, -0.17946401238441467)\n",
      "L2T2_lora_out: (0.4473767876625061, -0.0007078667404130101, -0.35892802476882935)\n",
      "\n",
      "g_out: (0.3242076635360718, -0.000540173496119678, -0.31245437264442444)\n",
      "L1T2_out: (0.528129518032074, -0.0008854917832650244, -0.5010322332382202)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03395088389515877, -5.051722109783441e-05, -0.033487819135189056)\n",
      "L1T1_lora_out: (0.06790176779031754, -0.00010103444219566882, -0.06697563827037811)\n",
      "\n",
      "L2T1_lora_B_out: (0.038935769349336624, 2.0605848476407118e-05, -0.03553091734647751)\n",
      "L2T1_lora_out: (0.07787153869867325, 4.1211696952814236e-05, -0.07106183469295502)\n",
      "\n",
      "L2T2_lora_B_out: (0.07878677546977997, -1.724130015645642e-05, -0.055119872093200684)\n",
      "L2T2_lora_out: (0.15757355093955994, -3.448260031291284e-05, -0.11023974418640137)\n",
      "\n",
      "g_out: (0.09151963889598846, -7.569430454168469e-05, -0.09328752756118774)\n",
      "L1T2_out: (0.1594214141368866, -0.00017672876128926873, -0.14815816283226013)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006393742747604847, 1.2710434930340853e-05, -0.006981660611927509)\n",
      "L1T1_lora_out: (0.012787485495209694, 2.5420869860681705e-05, -0.013963321223855019)\n",
      "\n",
      "L2T1_lora_B_out: (0.005318149924278259, -9.558820238453336e-06, -0.0052668931894004345)\n",
      "L2T1_lora_out: (0.010636299848556519, -1.9117640476906672e-05, -0.010533786378800869)\n",
      "\n",
      "L2T2_lora_B_out: (0.01510621141642332, 8.444224476988893e-06, -0.016516173258423805)\n",
      "L2T2_lora_out: (0.03021242283284664, 1.6888448953977786e-05, -0.03303234651684761)\n",
      "\n",
      "g_out: (0.02862354926764965, 3.600609124987386e-05, -0.026481235399842262)\n",
      "L1T2_out: (0.041411034762859344, 6.142695201560855e-05, -0.03836430236697197)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028422033414244652, 7.229445327538997e-05, -0.025388384237885475)\n",
      "L1T1_lora_out: (0.056844066828489304, 0.00014458890655077994, -0.05077676847577095)\n",
      "\n",
      "L2T1_lora_B_out: (0.03351999446749687, -6.735615170327947e-05, -0.03111431933939457)\n",
      "L2T1_lora_out: (0.06703998893499374, -0.00013471230340655893, -0.06222863867878914)\n",
      "\n",
      "L2T2_lora_B_out: (0.07204140722751617, -5.432644684333354e-05, -0.07184667885303497)\n",
      "L2T2_lora_out: (0.14408281445503235, -0.00010865289368666708, -0.14369335770606995)\n",
      "\n",
      "g_out: (0.09953822195529938, 2.6059409719891846e-05, -0.10513822734355927)\n",
      "L1T2_out: (0.13973408937454224, 0.000170648330822587, -0.1448083221912384)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02304985001683235, 1.0898851542151533e-05, -0.027012929320335388)\n",
      "L1T1_lora_out: (0.0460997000336647, 2.1797703084303066e-05, -0.054025858640670776)\n",
      "\n",
      "L2T1_lora_B_out: (0.05003189668059349, 2.3969430912984535e-05, -0.04281285032629967)\n",
      "L2T1_lora_out: (0.10006379336118698, 4.793886182596907e-05, -0.08562570065259933)\n",
      "\n",
      "L2T2_lora_B_out: (0.0701252818107605, -0.00023722785408608615, -0.060855377465486526)\n",
      "L2T2_lora_out: (0.140250563621521, -0.0004744557081721723, -0.12171075493097305)\n",
      "\n",
      "g_out: (0.08617471903562546, -0.0005223946645855904, -0.0902266725897789)\n",
      "L1T2_out: (0.1170477494597435, -0.0005005968851037323, -0.14425253868103027)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.016294194385409355, 7.981724593264516e-06, -0.01713554561138153)\n",
      "L1T1_lora_out: (0.03258838877081871, 1.5963449186529033e-05, -0.03427109122276306)\n",
      "\n",
      "L2T1_lora_B_out: (0.033274728804826736, 1.4230502529244404e-05, -0.037412334233522415)\n",
      "L2T1_lora_out: (0.06654945760965347, 2.846100505848881e-05, -0.07482466846704483)\n",
      "\n",
      "L2T2_lora_B_out: (0.05651818588376045, 0.00014270016981754452, -0.05634741112589836)\n",
      "L2T2_lora_out: (0.1130363717675209, 0.00028540033963508904, -0.11269482225179672)\n",
      "\n",
      "g_out: (0.0744565799832344, 0.0002569393254816532, -0.07855275273323059)\n",
      "L1T2_out: (0.09537044167518616, 0.00027290283469483256, -0.09358157217502594)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0038487573619931936, -3.5939551707997452e-06, -0.004111290443688631)\n",
      "L1T1_lora_out: (0.007697514723986387, -7.1879103415994905e-06, -0.008222580887377262)\n",
      "\n",
      "L2T1_lora_B_out: (0.0022160429507493973, -1.39290159495431e-05, -0.0018946564523503184)\n",
      "L2T1_lora_out: (0.0044320859014987946, -2.78580318990862e-05, -0.003789312904700637)\n",
      "\n",
      "L2T2_lora_B_out: (0.006958838552236557, -2.8902779376949184e-05, -0.007191071752458811)\n",
      "L2T2_lora_out: (0.013917677104473114, -5.780555875389837e-05, -0.014382143504917622)\n",
      "\n",
      "g_out: (0.013111324980854988, -2.9947526854812168e-05, -0.011024782434105873)\n",
      "L1T2_out: (0.017824944108724594, -3.7135436286916956e-05, -0.01880772039294243)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07885190099477768, -5.7319386542076245e-05, -0.07647375017404556)\n",
      "L1T1_lora_out: (0.15770380198955536, -0.00011463877308415249, -0.15294750034809113)\n",
      "\n",
      "L2T1_lora_B_out: (0.0743071660399437, -0.00022807846835348755, -0.07914111763238907)\n",
      "L2T1_lora_out: (0.1486143320798874, -0.0004561569367069751, -0.15828223526477814)\n",
      "\n",
      "L2T2_lora_B_out: (0.2051869034767151, -0.0004805471980944276, -0.22904714941978455)\n",
      "L2T2_lora_out: (0.4103738069534302, -0.0009610943961888552, -0.4580942988395691)\n",
      "\n",
      "g_out: (0.30763310194015503, -0.0005049374303780496, -0.35214054584503174)\n",
      "L1T2_out: (0.44340699911117554, -0.0006195761961862445, -0.48897549510002136)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03576261177659035, -2.53134985541692e-05, -0.03681819513440132)\n",
      "L1T1_lora_out: (0.0715252235531807, -5.06269971083384e-05, -0.07363639026880264)\n",
      "\n",
      "L2T1_lora_B_out: (0.032113589346408844, -4.302120942156762e-06, -0.038185592740774155)\n",
      "L2T1_lora_out: (0.06422717869281769, -8.604241884313524e-06, -0.07637118548154831)\n",
      "\n",
      "L2T2_lora_B_out: (0.08307689428329468, 1.3962895536678843e-05, -0.06429644674062729)\n",
      "L2T2_lora_out: (0.16615378856658936, 2.7925791073357686e-05, -0.12859289348125458)\n",
      "\n",
      "g_out: (0.11900562793016434, 3.6530025681713596e-05, -0.10609876364469528)\n",
      "L1T2_out: (0.1713864803314209, -1.409695323673077e-05, -0.15286177396774292)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00525213498622179, -8.579998393543065e-06, -0.004579608794301748)\n",
      "L1T1_lora_out: (0.01050426997244358, -1.715999678708613e-05, -0.009159217588603497)\n",
      "\n",
      "L2T1_lora_B_out: (0.0036275414749979973, -8.20558034320129e-06, -0.004055369179695845)\n",
      "L2T1_lora_out: (0.007255082949995995, -1.641116068640258e-05, -0.00811073835939169)\n",
      "\n",
      "L2T2_lora_B_out: (0.009619051590561867, -1.363309274893254e-05, -0.009781197644770145)\n",
      "L2T2_lora_out: (0.019238103181123734, -2.726618549786508e-05, -0.01956239528954029)\n",
      "\n",
      "g_out: (0.01850937493145466, -1.0855036634893622e-05, -0.016133341938257217)\n",
      "L1T2_out: (0.027605708688497543, -2.8015030693495646e-05, -0.024087566882371902)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09646672755479813, 0.0001406486117048189, -0.0797206461429596)\n",
      "L1T1_lora_out: (0.19293345510959625, 0.0002812972234096378, -0.1594412922859192)\n",
      "\n",
      "L2T1_lora_B_out: (0.06340578198432922, -1.7162881704280153e-05, -0.05506608262658119)\n",
      "L2T1_lora_out: (0.12681156396865845, -3.4325763408560306e-05, -0.11013216525316238)\n",
      "\n",
      "L2T2_lora_B_out: (0.19729633629322052, 0.00011231651296839118, -0.12643103301525116)\n",
      "L2T2_lora_out: (0.39459267258644104, 0.00022463302593678236, -0.2528620660305023)\n",
      "\n",
      "g_out: (0.3262951374053955, 0.0002589588693808764, -0.23437675833702087)\n",
      "L1T2_out: (0.5192285776138306, 0.0005402559763751924, -0.39381805062294006)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04046238958835602, -0.00022414352861233056, -0.03854271024465561)\n",
      "L1T1_lora_out: (0.08092477917671204, -0.0004482870572246611, -0.07708542048931122)\n",
      "\n",
      "L2T1_lora_B_out: (0.03399435803294182, 6.54636460239999e-05, -0.029969602823257446)\n",
      "L2T1_lora_out: (0.06798871606588364, 0.0001309272920479998, -0.05993920564651489)\n",
      "\n",
      "L2T2_lora_B_out: (0.08954138308763504, -0.0003648732672445476, -0.07501338422298431)\n",
      "L2T2_lora_out: (0.17908276617527008, -0.0007297465344890952, -0.15002676844596863)\n",
      "\n",
      "g_out: (0.15508802235126495, -0.0008606740739196539, -0.1251946985721588)\n",
      "L1T2_out: (0.22820183634757996, -0.0013089608401060104, -0.19203650951385498)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03603668883442879, -9.66542138485238e-05, -0.040707290172576904)\n",
      "L1T1_lora_out: (0.07207337766885757, -0.0001933084276970476, -0.08141458034515381)\n",
      "\n",
      "L2T1_lora_B_out: (0.011457523331046104, 2.450255851726979e-05, -0.011786078102886677)\n",
      "L2T1_lora_out: (0.02291504666209221, 4.900511703453958e-05, -0.023572156205773354)\n",
      "\n",
      "L2T2_lora_B_out: (0.050517626106739044, -0.0002536186366342008, -0.05033627897500992)\n",
      "L2T2_lora_out: (0.10103525221347809, -0.0005072372732684016, -0.10067255795001984)\n",
      "\n",
      "g_out: (0.09614637494087219, -0.0005562423029914498, -0.09932620078325272)\n",
      "L1T2_out: (0.1559133380651474, -0.0007495508762076497, -0.18074077367782593)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0038200404960662127, -9.704041076474823e-06, -0.004483965691179037)\n",
      "L1T1_lora_out: (0.007640080992132425, -1.9408082152949646e-05, -0.008967931382358074)\n",
      "\n",
      "L2T1_lora_B_out: (0.0027703437954187393, 4.287782303435961e-06, -0.0025688321329653263)\n",
      "L2T1_lora_out: (0.005540687590837479, 8.575564606871922e-06, -0.005137664265930653)\n",
      "\n",
      "L2T2_lora_B_out: (0.007688026875257492, -1.2429984053596854e-05, -0.010091813281178474)\n",
      "L2T2_lora_out: (0.015376053750514984, -2.485996810719371e-05, -0.02018362656235695)\n",
      "\n",
      "g_out: (0.013189848512411118, -3.343553180457093e-05, -0.017357755452394485)\n",
      "L1T2_out: (0.020610591396689415, -5.2843613957520574e-05, -0.024372033774852753)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07119064033031464, -7.867512613302097e-05, -0.06332685053348541)\n",
      "L1T1_lora_out: (0.14238128066062927, -0.00015735025226604193, -0.12665370106697083)\n",
      "\n",
      "L2T1_lora_B_out: (0.07324513792991638, -7.050044951029122e-05, -0.06776707619428635)\n",
      "L2T1_lora_out: (0.14649027585983276, -0.00014100089902058244, -0.1355341523885727)\n",
      "\n",
      "L2T2_lora_B_out: (0.12957759201526642, -0.00014546187594532967, -0.10691981017589569)\n",
      "L2T2_lora_out: (0.25915518403053284, -0.00029092375189065933, -0.21383962035179138)\n",
      "\n",
      "g_out: (0.25214773416519165, -0.0001499229110777378, -0.18983455002307892)\n",
      "L1T2_out: (0.3840683698654175, -0.00030727317789569497, -0.3095603585243225)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07756835967302322, 1.7641799558987259e-06, -0.09267299622297287)\n",
      "L1T1_lora_out: (0.15513671934604645, 3.5283599117974518e-06, -0.18534599244594574)\n",
      "\n",
      "L2T1_lora_B_out: (0.03101213090121746, -3.7268669984769076e-05, -0.029602957889437675)\n",
      "L2T1_lora_out: (0.06202426180243492, -7.453733996953815e-05, -0.05920591577887535)\n",
      "\n",
      "L2T2_lora_B_out: (0.13088403642177582, -8.25176975922659e-05, -0.11355305463075638)\n",
      "L2T2_lora_out: (0.26176807284355164, -0.0001650353951845318, -0.22710610926151276)\n",
      "\n",
      "g_out: (0.23615288734436035, -9.049793152371421e-05, -0.20534776151180267)\n",
      "L1T2_out: (0.3912895917892456, -8.696971053723246e-05, -0.3906937539577484)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.010706311091780663, 1.4279246443038573e-06, -0.01061961892992258)\n",
      "L1T1_lora_out: (0.021412622183561325, 2.8558492886077147e-06, -0.02123923785984516)\n",
      "\n",
      "L2T1_lora_B_out: (0.006712069269269705, 4.959911166224629e-07, -0.0069139194674789906)\n",
      "L2T1_lora_out: (0.01342413853853941, 9.919822332449257e-07, -0.013827838934957981)\n",
      "\n",
      "L2T2_lora_B_out: (0.013416656292974949, 1.2302803952479735e-05, -0.013532174751162529)\n",
      "L2T2_lora_out: (0.026833312585949898, 2.460560790495947e-05, -0.027064349502325058)\n",
      "\n",
      "g_out: (0.0298983883112669, 2.3613625671714544e-05, -0.02832966484129429)\n",
      "L1T2_out: (0.04853472113609314, 2.6469466320122592e-05, -0.04896962642669678)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04514347016811371, -1.6244866856141016e-05, -0.058563198894262314)\n",
      "L1T1_lora_out: (0.09028694033622742, -3.248973371228203e-05, -0.11712639778852463)\n",
      "\n",
      "L2T1_lora_B_out: (0.04491906240582466, -0.00011078299576183781, -0.04457159340381622)\n",
      "L2T1_lora_out: (0.08983812481164932, -0.00022156599152367562, -0.08914318680763245)\n",
      "\n",
      "L2T2_lora_B_out: (0.12138388305902481, -0.0002726293168962002, -0.13427376747131348)\n",
      "L2T2_lora_out: (0.24276776611804962, -0.0005452586337924004, -0.26854753494262695)\n",
      "\n",
      "g_out: (0.18180032074451447, -0.0003236926859244704, -0.23809687793254852)\n",
      "L1T2_out: (0.2720872759819031, -0.00035618245601654053, -0.35522326827049255)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05271092429757118, 0.00022417509171646088, -0.03861602023243904)\n",
      "L1T1_lora_out: (0.10542184859514236, 0.00044835018343292177, -0.07723204046487808)\n",
      "\n",
      "L2T1_lora_B_out: (0.05358560010790825, 0.0002586949849501252, -0.04499181732535362)\n",
      "L2T1_lora_out: (0.1071712002158165, 0.0005173899699002504, -0.08998363465070724)\n",
      "\n",
      "L2T2_lora_B_out: (0.09876186400651932, 0.0008225933997891843, -0.08894315361976624)\n",
      "L2T2_lora_out: (0.19752372801303864, 0.0016451867995783687, -0.17788630723953247)\n",
      "\n",
      "g_out: (0.16199028491973877, 0.00112779694609344, -0.14608962833881378)\n",
      "L1T2_out: (0.26741212606430054, 0.0015761468093842268, -0.22332167625427246)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.037625476717948914, 0.00027968856738880277, -0.03069409169256687)\n",
      "L1T1_lora_out: (0.07525095343589783, 0.0005593771347776055, -0.06138818338513374)\n",
      "\n",
      "L2T1_lora_B_out: (0.015877410769462585, 4.711151632363908e-05, -0.016481619328260422)\n",
      "L2T1_lora_out: (0.03175482153892517, 9.422303264727816e-05, -0.032963238656520844)\n",
      "\n",
      "L2T2_lora_B_out: (0.054812222719192505, 0.0005817610071972013, -0.06468851119279861)\n",
      "L2T2_lora_out: (0.10962444543838501, 0.0011635220143944025, -0.12937702238559723)\n",
      "\n",
      "g_out: (0.09552155435085297, 0.0010692989453673363, -0.13153420388698578)\n",
      "L1T2_out: (0.1707725077867508, 0.0016286758473142982, -0.18858464062213898)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003784259781241417, 1.5557592405457399e-06, -0.003980747889727354)\n",
      "L1T1_lora_out: (0.007568519562482834, 3.1115184810914798e-06, -0.007961495779454708)\n",
      "\n",
      "L2T1_lora_B_out: (0.0036211924161762, -2.5809372345975135e-06, -0.0031473676208406687)\n",
      "L2T1_lora_out: (0.0072423848323524, -5.161874469195027e-06, -0.006294735241681337)\n",
      "\n",
      "L2T2_lora_B_out: (0.008766108192503452, 1.5635941963410005e-05, -0.009219629690051079)\n",
      "L2T2_lora_out: (0.017532216385006905, 3.127188392682001e-05, -0.018439259380102158)\n",
      "\n",
      "g_out: (0.012528419494628906, 3.643374293460511e-05, -0.015497528947889805)\n",
      "L1T2_out: (0.01781105063855648, 3.9545273466501385e-05, -0.022764159366488457)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.17555981874465942, 4.1683462768560275e-05, -0.08660012483596802)\n",
      "L1T1_lora_out: (0.35111963748931885, 8.336692553712055e-05, -0.17320024967193604)\n",
      "\n",
      "L2T1_lora_B_out: (0.07297007739543915, -0.00022187213471625, -0.07855634391307831)\n",
      "L2T1_lora_out: (0.1459401547908783, -0.0004437442694325, -0.15711268782615662)\n",
      "\n",
      "L2T2_lora_B_out: (0.3553284704685211, -0.00024110026424750686, -0.17814475297927856)\n",
      "L2T2_lora_out: (0.7106569409370422, -0.0004822005284950137, -0.35628950595855713)\n",
      "\n",
      "g_out: (0.6107256412506104, -3.845627361442894e-05, -0.2673141062259674)\n",
      "L1T2_out: (0.9618452787399292, 4.4910724682267755e-05, -0.42813628911972046)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06561059504747391, -2.2369389625964686e-05, -0.06136680394411087)\n",
      "L1T1_lora_out: (0.13122119009494781, -4.473877925192937e-05, -0.12273360788822174)\n",
      "\n",
      "L2T1_lora_B_out: (0.028712354600429535, 1.6891284758457914e-05, -0.03521035984158516)\n",
      "L2T1_lora_out: (0.05742470920085907, 3.378256951691583e-05, -0.07042071968317032)\n",
      "\n",
      "L2T2_lora_B_out: (0.14748884737491608, -2.572720950411167e-05, -0.11719914525747299)\n",
      "L2T2_lora_out: (0.29497769474983215, -5.145441900822334e-05, -0.23439829051494598)\n",
      "\n",
      "g_out: (0.2638189196586609, -8.523705037077889e-05, -0.18862785398960114)\n",
      "L1T2_out: (0.3950400948524475, -0.00012997587327845395, -0.2911272644996643)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.008054857142269611, -7.908416591817513e-06, -0.006882678251713514)\n",
      "L1T1_lora_out: (0.016109714284539223, -1.5816833183635026e-05, -0.013765356503427029)\n",
      "\n",
      "L2T1_lora_B_out: (0.005282615777105093, -8.941462510847487e-06, -0.006591354496777058)\n",
      "L2T1_lora_out: (0.010565231554210186, -1.7882925021694973e-05, -0.013182708993554115)\n",
      "\n",
      "L2T2_lora_B_out: (0.012104958295822144, -1.1415309927542694e-05, -0.012500270269811153)\n",
      "L2T2_lora_out: (0.024209916591644287, -2.2830619855085388e-05, -0.025000540539622307)\n",
      "\n",
      "g_out: (0.024524765089154243, -4.947694378643064e-06, -0.021099941805005074)\n",
      "L1T2_out: (0.038964614272117615, -2.076452801702544e-05, -0.03238365426659584)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07340298593044281, 0.00013746177137363702, -0.06661555916070938)\n",
      "L1T1_lora_out: (0.14680597186088562, 0.00027492354274727404, -0.13323111832141876)\n",
      "\n",
      "L2T1_lora_B_out: (0.020218083634972572, -4.3292799091432244e-05, -0.02121758833527565)\n",
      "L2T1_lora_out: (0.040436167269945145, -8.658559818286449e-05, -0.0424351766705513)\n",
      "\n",
      "L2T2_lora_B_out: (0.13037699460983276, 0.00010856195876840502, -0.11110986024141312)\n",
      "L2T2_lora_out: (0.2607539892196655, 0.00021712391753681004, -0.22221972048282623)\n",
      "\n",
      "g_out: (0.24685919284820557, 0.0003037096175830811, -0.2170647531747818)\n",
      "L1T2_out: (0.3936651647090912, 0.0005786331603303552, -0.35029587149620056)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10793514549732208, -0.00020717523875646293, -0.10407504439353943)\n",
      "L1T1_lora_out: (0.21587029099464417, -0.00041435047751292586, -0.20815008878707886)\n",
      "\n",
      "L2T1_lora_B_out: (0.011621348559856415, 6.535203283419833e-05, -0.013061707839369774)\n",
      "L2T1_lora_out: (0.02324269711971283, 0.00013070406566839665, -0.026123415678739548)\n",
      "\n",
      "L2T2_lora_B_out: (0.13316193222999573, 0.00010152578761335462, -0.12959039211273193)\n",
      "L2T2_lora_out: (0.26632386445999146, 0.00020305157522670925, -0.25918078422546387)\n",
      "\n",
      "g_out: (0.2540023624897003, 7.234762597363442e-05, -0.2523180842399597)\n",
      "L1T2_out: (0.44962167739868164, -0.0003420028369873762, -0.43130648136138916)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09679470956325531, 0.0003025488113053143, -0.08037266880273819)\n",
      "L1T1_lora_out: (0.19358941912651062, 0.0006050976226106286, -0.16074533760547638)\n",
      "\n",
      "L2T1_lora_B_out: (0.032688967883586884, 7.220963016152382e-05, -0.04422757402062416)\n",
      "L2T1_lora_out: (0.06537793576717377, 0.00014441926032304764, -0.08845514804124832)\n",
      "\n",
      "L2T2_lora_B_out: (0.19197231531143188, 0.0005745855160057545, -0.195174902677536)\n",
      "L2T2_lora_out: (0.38394463062286377, 0.001149171032011509, -0.390349805355072)\n",
      "\n",
      "g_out: (0.34675493836402893, 0.0010047516552731395, -0.3018946647644043)\n",
      "L1T2_out: (0.5403443574905396, 0.0016098495107144117, -0.4208661615848541)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0022695069201290607, -3.2961061151581816e-06, -0.0026127295568585396)\n",
      "L1T1_lora_out: (0.0045390138402581215, -6.592212230316363e-06, -0.005225459113717079)\n",
      "\n",
      "L2T1_lora_B_out: (0.002423873171210289, 1.0747597116278484e-05, -0.0024005232844501734)\n",
      "L2T1_lora_out: (0.004847746342420578, 2.149519423255697e-05, -0.004801046568900347)\n",
      "\n",
      "L2T2_lora_B_out: (0.005610727239400148, 1.1690687642840203e-05, -0.006168422754853964)\n",
      "L2T2_lora_out: (0.011221454478800297, 2.3381375285680406e-05, -0.012336845509707928)\n",
      "\n",
      "g_out: (0.007292920257896185, 1.886169570752827e-06, -0.008396663703024387)\n",
      "L1T2_out: (0.01123528927564621, -4.706041181634646e-06, -0.012121826410293579)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.13058917224407196, 3.0399229217437096e-05, -0.12491252273321152)\n",
      "L1T1_lora_out: (0.2611783444881439, 6.079845843487419e-05, -0.24982504546642303)\n",
      "\n",
      "L2T1_lora_B_out: (0.06129235774278641, -0.00018552815890870988, -0.065680593252182)\n",
      "L2T1_lora_out: (0.12258471548557281, -0.00037105631781741977, -0.131361186504364)\n",
      "\n",
      "L2T2_lora_B_out: (0.22514942288398743, -0.00022798284771852195, -0.27288755774497986)\n",
      "L2T2_lora_out: (0.45029884576797485, -0.0004559656954370439, -0.5457751154899597)\n",
      "\n",
      "g_out: (0.4258660078048706, -8.490952313877642e-05, -0.4688558280467987)\n",
      "L1T2_out: (0.6870443820953369, -2.4111055608955212e-05, -0.7186808586120605)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07281793653964996, -7.533062307629734e-05, -0.0982588529586792)\n",
      "L1T1_lora_out: (0.14563587307929993, -0.00015066124615259469, -0.1965177059173584)\n",
      "\n",
      "L2T1_lora_B_out: (0.04815634340047836, 3.223864041501656e-05, -0.039658322930336)\n",
      "L2T1_lora_out: (0.09631268680095673, 6.447728083003312e-05, -0.079316645860672)\n",
      "\n",
      "L2T2_lora_B_out: (0.15908430516719818, -9.157074964605272e-05, -0.19169794023036957)\n",
      "L2T2_lora_out: (0.31816861033439636, -0.00018314149929210544, -0.38339588046073914)\n",
      "\n",
      "g_out: (0.2741505205631256, -0.0002476187073625624, -0.33806005120277405)\n",
      "L1T2_out: (0.41978639364242554, -0.00039827998261898756, -0.5345777273178101)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006033368408679962, 8.458621778117958e-06, -0.006111111491918564)\n",
      "L1T1_lora_out: (0.012066736817359924, 1.6917243556235917e-05, -0.012222222983837128)\n",
      "\n",
      "L2T1_lora_B_out: (0.004750491119921207, -5.206218247622019e-06, -0.004354807548224926)\n",
      "L2T1_lora_out: (0.009500982239842415, -1.0412436495244037e-05, -0.008709615096449852)\n",
      "\n",
      "L2T2_lora_B_out: (0.0098164863884449, 9.704750482342206e-06, -0.010081958957016468)\n",
      "L2T2_lora_out: (0.0196329727768898, 1.9409500964684412e-05, -0.020163917914032936)\n",
      "\n",
      "g_out: (0.018125278875231743, 2.9821934731444344e-05, -0.017925914376974106)\n",
      "L1T2_out: (0.028779488056898117, 4.6739180106669664e-05, -0.02953309379518032)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04153486713767052, 7.05811835359782e-05, -0.028756963089108467)\n",
      "L1T1_lora_out: (0.08306973427534103, 0.0001411623670719564, -0.057513926178216934)\n",
      "\n",
      "L2T1_lora_B_out: (0.03578381612896919, -1.6722757209208794e-05, -0.02738792821764946)\n",
      "L2T1_lora_out: (0.07156763225793839, -3.344551441841759e-05, -0.05477585643529892)\n",
      "\n",
      "L2T2_lora_B_out: (0.06883006542921066, 0.00013957120245322585, -0.06985796242952347)\n",
      "L2T2_lora_out: (0.13766013085842133, 0.0002791424049064517, -0.13971592485904694)\n",
      "\n",
      "g_out: (0.1303366869688034, 0.0003125878283753991, -0.13113710284233093)\n",
      "L1T2_out: (0.19137388467788696, 0.0004537502536550164, -0.18603798747062683)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023853003978729248, 2.2146099581732415e-05, -0.02714403346180916)\n",
      "L1T1_lora_out: (0.047706007957458496, 4.429219916346483e-05, -0.05428806692361832)\n",
      "\n",
      "L2T1_lora_B_out: (0.018450163304805756, 4.417532909428701e-05, -0.018730618059635162)\n",
      "L2T1_lora_out: (0.03690032660961151, 8.835065818857402e-05, -0.037461236119270325)\n",
      "\n",
      "L2T2_lora_B_out: (0.04508701711893082, 0.0001561877434141934, -0.06431790441274643)\n",
      "L2T2_lora_out: (0.09017403423786163, 0.0003123754868283868, -0.12863580882549286)\n",
      "\n",
      "g_out: (0.07666436582803726, 0.0002240249013993889, -0.09361333400011063)\n",
      "L1T2_out: (0.11957015097141266, 0.00026831706054508686, -0.1392572522163391)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02743355557322502, -0.00018647310207597911, -0.02950183115899563)\n",
      "L1T1_lora_out: (0.05486711114645004, -0.00037294620415195823, -0.05900366231799126)\n",
      "\n",
      "L2T1_lora_B_out: (0.027584346011281013, -3.853044836432673e-05, -0.031190769746899605)\n",
      "L2T1_lora_out: (0.05516869202256203, -7.706089672865346e-05, -0.06238153949379921)\n",
      "\n",
      "L2T2_lora_B_out: (0.0793999508023262, -0.0005357047775760293, -0.07069573551416397)\n",
      "L2T2_lora_out: (0.1587999016046524, -0.0010714095551520586, -0.14139147102832794)\n",
      "\n",
      "g_out: (0.11873194575309753, -0.0009943486656993628, -0.12538960576057434)\n",
      "L1T2_out: (0.16389186680316925, -0.001367294928058982, -0.1843932718038559)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003656519576907158, -6.481766376964515e-06, -0.003336856374517083)\n",
      "L1T1_lora_out: (0.007313039153814316, -1.296353275392903e-05, -0.006673712749034166)\n",
      "\n",
      "L2T1_lora_B_out: (0.0027257222682237625, -8.822311428957619e-06, -0.002507896162569523)\n",
      "L2T1_lora_out: (0.005451444536447525, -1.7644622857915238e-05, -0.005015792325139046)\n",
      "\n",
      "L2T2_lora_B_out: (0.007358622271567583, -1.7225267583853565e-05, -0.007364440709352493)\n",
      "L2T2_lora_out: (0.014717244543135166, -3.445053516770713e-05, -0.014728881418704987)\n",
      "\n",
      "g_out: (0.012040037661790848, -1.6805912309791893e-05, -0.012075098231434822)\n",
      "L1T2_out: (0.018583819270133972, -2.9769445973215625e-05, -0.01850361004471779)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06498061865568161, -3.969310910179047e-06, -0.06972218304872513)\n",
      "L1T1_lora_out: (0.12996123731136322, -7.938621820358094e-06, -0.13944436609745026)\n",
      "\n",
      "L2T1_lora_B_out: (0.08357243239879608, 8.093420547083952e-06, -0.0765034630894661)\n",
      "L2T1_lora_out: (0.16714486479759216, 1.6186841094167903e-05, -0.1530069261789322)\n",
      "\n",
      "L2T2_lora_B_out: (0.13791458308696747, 3.44988948199898e-05, -0.12686146795749664)\n",
      "L2T2_lora_out: (0.27582916617393494, 6.89977896399796e-05, -0.2537229359149933)\n",
      "\n",
      "g_out: (0.18825840950012207, 5.281098128762096e-05, -0.18809834122657776)\n",
      "L1T2_out: (0.3171684145927429, 4.487234036787413e-05, -0.3051910996437073)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05597803741693497, -4.527699729806045e-06, -0.05096694827079773)\n",
      "L1T1_lora_out: (0.11195607483386993, -9.05539945961209e-06, -0.10193389654159546)\n",
      "\n",
      "L2T1_lora_B_out: (0.054534949362277985, -8.892498044588137e-06, -0.061740368604660034)\n",
      "L2T1_lora_out: (0.10906989872455597, -1.7784996089176275e-05, -0.12348073720932007)\n",
      "\n",
      "L2T2_lora_B_out: (0.11276604980230331, 3.912400188710308e-06, -0.13692790269851685)\n",
      "L2T2_lora_out: (0.22553209960460663, 7.824800377420615e-06, -0.2738558053970337)\n",
      "\n",
      "g_out: (0.16365090012550354, 2.5609762815292925e-05, -0.22792258858680725)\n",
      "L1T2_out: (0.25133252143859863, 1.6554347894270904e-05, -0.3286867141723633)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00975632481276989, 2.3171178327174857e-05, -0.008551941253244877)\n",
      "L1T1_lora_out: (0.01951264962553978, 4.6342356654349715e-05, -0.017103882506489754)\n",
      "\n",
      "L2T1_lora_B_out: (0.004956127144396305, 1.1952058684983058e-06, -0.004185253754258156)\n",
      "L2T1_lora_out: (0.00991225428879261, 2.3904117369966116e-06, -0.008370507508516312)\n",
      "\n",
      "L2T2_lora_B_out: (0.014605667442083359, 4.192654159851372e-05, -0.013235264457762241)\n",
      "L2T2_lora_out: (0.029211334884166718, 8.385308319702744e-05, -0.026470528915524483)\n",
      "\n",
      "g_out: (0.029440108686685562, 8.146268373820931e-05, -0.0262792706489563)\n",
      "L1T2_out: (0.04895275831222534, 0.0001278050331166014, -0.042598921805620193)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06468964368104935, 2.2678286768496037e-06, -0.07088635116815567)\n",
      "L1T1_lora_out: (0.1293792873620987, 4.535657353699207e-06, -0.14177270233631134)\n",
      "\n",
      "L2T1_lora_B_out: (0.03970795497298241, 2.0377539840410464e-05, -0.042648207396268845)\n",
      "L2T1_lora_out: (0.07941590994596481, 4.075507968082093e-05, -0.08529641479253769)\n",
      "\n",
      "L2T2_lora_B_out: (0.12174283713102341, 0.00010340951848775148, -0.12635928392410278)\n",
      "L2T2_lora_out: (0.24348567426204681, 0.00020681903697550297, -0.25271856784820557)\n",
      "\n",
      "g_out: (0.21393290162086487, 0.0001660640409681946, -0.24065254628658295)\n",
      "L1T2_out: (0.3372119069099426, 0.00017059952369891107, -0.3824252486228943)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0642046332359314, -0.0004697496769949794, -0.057058338075876236)\n",
      "L1T1_lora_out: (0.1284092664718628, -0.0009394993539899588, -0.11411667615175247)\n",
      "\n",
      "L2T1_lora_B_out: (0.07614144682884216, 0.00013674565707333386, -0.0823153555393219)\n",
      "L2T1_lora_out: (0.15228289365768433, 0.0002734913141466677, -0.1646307110786438)\n",
      "\n",
      "L2T2_lora_B_out: (0.17449386417865753, -0.0007659379625692964, -0.22047549486160278)\n",
      "L2T2_lora_out: (0.34898772835731506, -0.0015318759251385927, -0.44095098972320557)\n",
      "\n",
      "g_out: (0.27532488107681274, -0.0018053672974929214, -0.32946979999542236)\n",
      "L1T2_out: (0.37977334856987, -0.0027448665350675583, -0.44358646869659424)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08817573636770248, -0.0005522437277249992, -0.0868409276008606)\n",
      "L1T1_lora_out: (0.17635147273540497, -0.0011044874554499984, -0.1736818552017212)\n",
      "\n",
      "L2T1_lora_B_out: (0.03614717721939087, 0.00011011690367013216, -0.032902054488658905)\n",
      "L2T1_lora_out: (0.07229435443878174, 0.00022023380734026432, -0.06580410897731781)\n",
      "\n",
      "L2T2_lora_B_out: (0.152267187833786, -0.000540196371730417, -0.16087570786476135)\n",
      "L2T2_lora_out: (0.304534375667572, -0.001080392743460834, -0.3217514157295227)\n",
      "\n",
      "g_out: (0.26312652230262756, -0.001300626783631742, -0.277008056640625)\n",
      "L1T2_out: (0.43947798013687134, -0.0024051140062510967, -0.43890661001205444)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.017425281926989555, 2.8091635613236576e-06, -0.02085763029754162)\n",
      "L1T1_lora_out: (0.03485056385397911, 5.618327122647315e-06, -0.04171526059508324)\n",
      "\n",
      "L2T1_lora_B_out: (0.004495422821491957, 6.435251407310716e-07, -0.004603879526257515)\n",
      "L2T1_lora_out: (0.008990845642983913, 1.2870502814621432e-06, -0.00920775905251503)\n",
      "\n",
      "L2T2_lora_B_out: (0.02318848855793476, 2.190962504755589e-06, -0.02160515822470188)\n",
      "L2T2_lora_out: (0.04637697711586952, 4.381925009511178e-06, -0.04321031644940376)\n",
      "\n",
      "g_out: (0.04440204054117203, 3.094880867138272e-06, -0.04277552664279938)\n",
      "L1T2_out: (0.07376952469348907, 8.713206625543535e-06, -0.08449079096317291)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10485824197530746, -0.00011375082249287516, -0.11719952523708344)\n",
      "L1T1_lora_out: (0.20971648395061493, -0.00022750164498575032, -0.23439905047416687)\n",
      "\n",
      "L2T1_lora_B_out: (0.08818276226520538, -9.524791676085442e-05, -0.07367052882909775)\n",
      "L2T1_lora_out: (0.17636552453041077, -0.00019049583352170885, -0.1473410576581955)\n",
      "\n",
      "L2T2_lora_B_out: (0.325897216796875, -0.0005620823940262198, -0.25862041115760803)\n",
      "L2T2_lora_out: (0.65179443359375, -0.0011241647880524397, -0.5172408223152161)\n",
      "\n",
      "g_out: (0.4935806691646576, -0.0009336689254269004, -0.41384246945381165)\n",
      "L1T2_out: (0.6813768744468689, -0.001161170657724142, -0.6194674968719482)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07062763720750809, 8.671717660035938e-05, -0.07239200919866562)\n",
      "L1T1_lora_out: (0.14125527441501617, 0.00017343435320071876, -0.14478401839733124)\n",
      "\n",
      "L2T1_lora_B_out: (0.030183926224708557, 9.609309927327558e-06, -0.03411708027124405)\n",
      "L2T1_lora_out: (0.060367852449417114, 1.9218619854655117e-05, -0.0682341605424881)\n",
      "\n",
      "L2T2_lora_B_out: (0.11742895841598511, 4.5473116188077256e-05, -0.0957687646150589)\n",
      "L2T2_lora_out: (0.23485791683197021, 9.094623237615451e-05, -0.1915375292301178)\n",
      "\n",
      "g_out: (0.20850296318531036, 7.172765617724508e-05, -0.16703897714614868)\n",
      "L1T2_out: (0.3445643484592438, 0.00024516208213754, -0.3107334077358246)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.010687469504773617, -1.8860329873859882e-05, -0.009531295858323574)\n",
      "L1T1_lora_out: (0.021374939009547234, -3.7720659747719765e-05, -0.019062591716647148)\n",
      "\n",
      "L2T1_lora_B_out: (0.008288100361824036, -2.3060165403876454e-06, -0.00836121290922165)\n",
      "L2T1_lora_out: (0.01657620072364807, -4.612033080775291e-06, -0.0167224258184433)\n",
      "\n",
      "L2T2_lora_B_out: (0.02134552039206028, -1.8160571926273406e-05, -0.018849747255444527)\n",
      "L2T2_lora_out: (0.04269104078412056, -3.632114385254681e-05, -0.03769949451088905)\n",
      "\n",
      "g_out: (0.038766417652368546, -3.170908530591987e-05, -0.03101600520312786)\n",
      "L1T2_out: (0.06014135479927063, -6.942973413970321e-05, -0.04813843220472336)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03027789108455181, 1.8963792172144167e-05, -0.03136773034930229)\n",
      "L1T1_lora_out: (0.06055578216910362, 3.7927584344288334e-05, -0.06273546069860458)\n",
      "\n",
      "L2T1_lora_B_out: (0.023292331025004387, -1.919908754643984e-05, -0.025393875315785408)\n",
      "L2T1_lora_out: (0.046584662050008774, -3.839817509287968e-05, -0.050787750631570816)\n",
      "\n",
      "L2T2_lora_B_out: (0.06640127301216125, 8.220915333367884e-06, -0.06757806986570358)\n",
      "L2T2_lora_out: (0.1328025460243225, 1.644183066673577e-05, -0.13515613973140717)\n",
      "\n",
      "g_out: (0.10281822085380554, 5.484005669131875e-05, -0.1083991602063179)\n",
      "L1T2_out: (0.16202108561992645, 9.276767377741635e-05, -0.17113462090492249)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05051165819168091, 0.00022687119781039655, -0.043467290699481964)\n",
      "L1T1_lora_out: (0.10102331638336182, 0.0004537423956207931, -0.08693458139896393)\n",
      "\n",
      "L2T1_lora_B_out: (0.046359941363334656, -5.096386303193867e-06, -0.06423275917768478)\n",
      "L2T1_lora_out: (0.09271988272666931, -1.0192772606387734e-05, -0.12846551835536957)\n",
      "\n",
      "L2T2_lora_B_out: (0.1265273094177246, 0.00037283566780388355, -0.12545427680015564)\n",
      "L2T2_lora_out: (0.2530546188354492, 0.0007456713356077671, -0.2509085536003113)\n",
      "\n",
      "g_out: (0.18621280789375305, 0.000755863671656698, -0.18636246025562286)\n",
      "L1T2_out: (0.27911022305488586, 0.0012096064165234566, -0.25794240832328796)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08201673626899719, -0.0004631623160094023, -0.06831909716129303)\n",
      "L1T1_lora_out: (0.16403347253799438, -0.0009263246320188046, -0.13663819432258606)\n",
      "\n",
      "L2T1_lora_B_out: (0.03628282621502876, 2.722528006415814e-05, -0.0360654778778553)\n",
      "L2T1_lora_out: (0.07256565243005753, 5.445056012831628e-05, -0.0721309557557106)\n",
      "\n",
      "L2T2_lora_B_out: (0.18303385376930237, -0.0003654495521914214, -0.1706499457359314)\n",
      "L2T2_lora_out: (0.36606770753860474, -0.0007308991043828428, -0.3412998914718628)\n",
      "\n",
      "g_out: (0.3019665479660034, -0.0007853497518226504, -0.2691689431667328)\n",
      "L1T2_out: (0.44490647315979004, -0.0017116740345954895, -0.38715922832489014)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0036048470064997673, -1.2515246453403961e-05, -0.0033954717218875885)\n",
      "L1T1_lora_out: (0.007209694012999535, -2.5030492906807922e-05, -0.006790943443775177)\n",
      "\n",
      "L2T1_lora_B_out: (0.003831808688119054, -2.779186161205871e-06, -0.0035142225679010153)\n",
      "L2T1_lora_out: (0.007663617376238108, -5.558372322411742e-06, -0.0070284451358020306)\n",
      "\n",
      "L2T2_lora_B_out: (0.005438849795609713, -2.9221233489806764e-05, -0.005586110055446625)\n",
      "L2T2_lora_out: (0.010877699591219425, -5.844246697961353e-05, -0.01117222011089325)\n",
      "\n",
      "g_out: (0.01003292016685009, -5.288409738568589e-05, -0.010291730985045433)\n",
      "L1T2_out: (0.017242614179849625, -7.791459211148322e-05, -0.015820346772670746)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09001218527555466, -0.00014374390593729913, -0.09328174591064453)\n",
      "L1T1_lora_out: (0.18002437055110931, -0.00028748781187459826, -0.18656349182128906)\n",
      "\n",
      "L2T1_lora_B_out: (0.058040790259838104, -0.00012310466263443232, -0.06125260144472122)\n",
      "L2T1_lora_out: (0.11608158051967621, -0.00024620932526886463, -0.12250520288944244)\n",
      "\n",
      "L2T2_lora_B_out: (0.22818976640701294, -0.00048590541700832546, -0.18087202310562134)\n",
      "L2T2_lora_out: (0.4563795328140259, -0.0009718108340166509, -0.3617440462112427)\n",
      "\n",
      "g_out: (0.341435968875885, -0.0007256015087477863, -0.31486719846725464)\n",
      "L1T2_out: (0.5051101446151733, -0.0010130894370377064, -0.5014306902885437)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09159915894269943, 7.425796502502635e-05, -0.08380959182977676)\n",
      "L1T1_lora_out: (0.18319831788539886, 0.0001485159300500527, -0.16761918365955353)\n",
      "\n",
      "L2T1_lora_B_out: (0.03657735511660576, -3.122276029898785e-05, -0.04035994037985802)\n",
      "L2T1_lora_out: (0.07315471023321152, -6.24455205979757e-05, -0.08071988075971603)\n",
      "\n",
      "L2T2_lora_B_out: (0.12651287019252777, 0.00011480433749966323, -0.1657465696334839)\n",
      "L2T2_lora_out: (0.25302574038505554, 0.00022960867499932647, -0.3314931392669678)\n",
      "\n",
      "g_out: (0.23922277987003326, 0.0002920542028732598, -0.2700888514518738)\n",
      "L1T2_out: (0.42242109775543213, 0.00044057000195607543, -0.4349867105484009)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.011454731225967407, -1.4599767155232257e-06, -0.011166405864059925)\n",
      "L1T1_lora_out: (0.022909462451934814, -2.9199534310464514e-06, -0.02233281172811985)\n",
      "\n",
      "L2T1_lora_B_out: (0.00547646451741457, -1.9710523702087812e-06, -0.005604395642876625)\n",
      "L2T1_lora_out: (0.01095292903482914, -3.9421047404175624e-06, -0.01120879128575325)\n",
      "\n",
      "L2T2_lora_B_out: (0.018530523404479027, -1.4193453807820333e-06, -0.018508436158299446)\n",
      "L2T2_lora_out: (0.037061046808958054, -2.8386907615640666e-06, -0.03701687231659889)\n",
      "\n",
      "g_out: (0.03332876041531563, 1.103413524106145e-06, -0.03389449045062065)\n",
      "L1T2_out: (0.05456840246915817, -1.8165641222367412e-06, -0.053426653146743774)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.033259131014347076, -5.5029049690347165e-05, -0.03048727847635746)\n",
      "L1T1_lora_out: (0.06651826202869415, -0.00011005809938069433, -0.06097455695271492)\n",
      "\n",
      "L2T1_lora_B_out: (0.029348954558372498, 2.476573718013242e-05, -0.03503144159913063)\n",
      "L2T1_lora_out: (0.058697909116744995, 4.953147436026484e-05, -0.07006288319826126)\n",
      "\n",
      "L2T2_lora_B_out: (0.08019837737083435, -4.743860336020589e-05, -0.09236566722393036)\n",
      "L2T2_lora_out: (0.1603967547416687, -9.487720672041178e-05, -0.18473133444786072)\n",
      "\n",
      "g_out: (0.1310146301984787, -0.00014440869563259184, -0.12437909841537476)\n",
      "L1T2_out: (0.19609998166561127, -0.0002544667513575405, -0.1837901473045349)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02528468891978264, 3.701470996020362e-05, -0.025032654404640198)\n",
      "L1T1_lora_out: (0.05056937783956528, 7.402941992040724e-05, -0.050065308809280396)\n",
      "\n",
      "L2T1_lora_B_out: (0.028013925999403, 5.896668153582141e-05, -0.028069984167814255)\n",
      "L2T1_lora_out: (0.056027851998806, 0.00011793336307164282, -0.05613996833562851)\n",
      "\n",
      "L2T2_lora_B_out: (0.075016550719738, 0.00015076022827997804, -0.05397637188434601)\n",
      "L2T2_lora_out: (0.150033101439476, 0.0003015204565599561, -0.10795274376869202)\n",
      "\n",
      "g_out: (0.12195991724729538, 0.00018358694796916097, -0.10563955456018448)\n",
      "L1T2_out: (0.1608732044696808, 0.00025761648430489004, -0.15570485591888428)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08427302539348602, 0.00015409824845846742, -0.08644300699234009)\n",
      "L1T1_lora_out: (0.16854605078697205, 0.00030819649691693485, -0.17288601398468018)\n",
      "\n",
      "L2T1_lora_B_out: (0.07361052930355072, 0.00041797192534431815, -0.05335743725299835)\n",
      "L2T1_lora_out: (0.14722105860710144, 0.0008359438506886363, -0.1067148745059967)\n",
      "\n",
      "L2T2_lora_B_out: (0.12456729263067245, 0.0007700034766457975, -0.1189405620098114)\n",
      "L2T2_lora_out: (0.2491345852613449, 0.001540006953291595, -0.2378811240196228)\n",
      "\n",
      "g_out: (0.17909926176071167, 0.0007040632772259414, -0.18682309985160828)\n",
      "L1T2_out: (0.29235878586769104, 0.001012259628623724, -0.35970911383628845)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0037806956097483635, 7.67163669479487e-07, -0.0033934256061911583)\n",
      "L1T1_lora_out: (0.007561391219496727, 1.534327338958974e-06, -0.006786851212382317)\n",
      "\n",
      "L2T1_lora_B_out: (0.0029000602662563324, 8.88674367161002e-06, -0.0029042051173746586)\n",
      "L2T1_lora_out: (0.005800120532512665, 1.777348734322004e-05, -0.005808410234749317)\n",
      "\n",
      "L2T2_lora_B_out: (0.007947945967316628, 1.9771698134718463e-05, -0.008615369908511639)\n",
      "L2T2_lora_out: (0.015895891934633255, 3.9543396269436926e-05, -0.017230739817023277)\n",
      "\n",
      "g_out: (0.013820599764585495, 2.176991074520629e-05, -0.01368723250925541)\n",
      "L1T2_out: (0.021224144846200943, 2.3304242859012447e-05, -0.019293058663606644)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.12483658641576767, -0.00019578250066842884, -0.1404263824224472)\n",
      "L1T1_lora_out: (0.24967317283153534, -0.0003915650013368577, -0.2808527648448944)\n",
      "\n",
      "L2T1_lora_B_out: (0.06981644034385681, -0.00014372853911481798, -0.05728747323155403)\n",
      "L2T1_lora_out: (0.13963288068771362, -0.00028745707822963595, -0.11457494646310806)\n",
      "\n",
      "L2T2_lora_B_out: (0.2789832651615143, -0.0006571258418262005, -0.2689532935619354)\n",
      "L2T2_lora_out: (0.5579665303230286, -0.001314251683652401, -0.5379065871238708)\n",
      "\n",
      "g_out: (0.4425712823867798, -0.001026794547215104, -0.4616132378578186)\n",
      "L1T2_out: (0.6266930103302002, -0.0014183595776557922, -0.7232012748718262)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07870565354824066, -3.41939230565913e-05, -0.1484946459531784)\n",
      "L1T1_lora_out: (0.15741130709648132, -6.83878461131826e-05, -0.2969892919063568)\n",
      "\n",
      "L2T1_lora_B_out: (0.02341843768954277, 4.242342765792273e-05, -0.030302315950393677)\n",
      "L2T1_lora_out: (0.04683687537908554, 8.484685531584546e-05, -0.060604631900787354)\n",
      "\n",
      "L2T2_lora_B_out: (0.12094278633594513, -3.587207538657822e-05, -0.17580482363700867)\n",
      "L2T2_lora_out: (0.24188557267189026, -7.174415077315643e-05, -0.35160964727401733)\n",
      "\n",
      "g_out: (0.21482296288013458, -0.0001565910060890019, -0.3397166132926941)\n",
      "L1T2_out: (0.347160667181015, -0.00022497893951367587, -0.6147969365119934)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.010229283943772316, -3.236506699977326e-06, -0.011199400760233402)\n",
      "L1T1_lora_out: (0.020458567887544632, -6.473013399954652e-06, -0.022398801520466805)\n",
      "\n",
      "L2T1_lora_B_out: (0.004879381041973829, 5.886516646569362e-06, -0.005433225072920322)\n",
      "L2T1_lora_out: (0.009758762083947659, 1.1773033293138724e-05, -0.010866450145840645)\n",
      "\n",
      "L2T2_lora_B_out: (0.018585672602057457, -1.0055812708742451e-05, -0.020160533487796783)\n",
      "L2T2_lora_out: (0.037171345204114914, -2.0111625417484902e-05, -0.04032106697559357)\n",
      "\n",
      "g_out: (0.03572148084640503, -3.188466143910773e-05, -0.03579386696219444)\n",
      "L1T2_out: (0.05618004873394966, -3.8357680750777945e-05, -0.05591394752264023)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03768538683652878, 9.523288463242352e-05, -0.0436437614262104)\n",
      "L1T1_lora_out: (0.07537077367305756, 0.00019046576926484704, -0.0872875228524208)\n",
      "\n",
      "L2T1_lora_B_out: (0.039615776389837265, 3.621459472924471e-05, -0.03891871124505997)\n",
      "L2T1_lora_out: (0.07923155277967453, 7.242918945848942e-05, -0.07783742249011993)\n",
      "\n",
      "L2T2_lora_B_out: (0.11078324913978577, 0.00025046186055988073, -0.10588299483060837)\n",
      "L2T2_lora_out: (0.22156649827957153, 0.0005009237211197615, -0.21176598966121674)\n",
      "\n",
      "g_out: (0.17962506413459778, 0.00042849453166127205, -0.18781039118766785)\n",
      "L1T2_out: (0.2473156601190567, 0.0006189602427184582, -0.27509790658950806)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.030896084383130074, 2.247688826173544e-05, -0.029558300971984863)\n",
      "L1T1_lora_out: (0.06179216876626015, 4.495377652347088e-05, -0.05911660194396973)\n",
      "\n",
      "L2T1_lora_B_out: (0.04806862398982048, -6.443690654123202e-05, -0.047885119915008545)\n",
      "L2T1_lora_out: (0.09613724797964096, -0.00012887381308246404, -0.09577023983001709)\n",
      "\n",
      "L2T2_lora_B_out: (0.11493939906358719, 0.00010357729479437694, -0.09759686887264252)\n",
      "L2T2_lora_out: (0.22987879812717438, 0.00020715458958875388, -0.19519373774528503)\n",
      "\n",
      "g_out: (0.14721110463142395, 0.00033602825715206563, -0.11691319197416306)\n",
      "L1T2_out: (0.1900513768196106, 0.0003809821209870279, -0.16536970436573029)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06794445216655731, -0.0005014405469410121, -0.06761207431554794)\n",
      "L1T1_lora_out: (0.13588890433311462, -0.0010028810938820243, -0.1352241486310959)\n",
      "\n",
      "L2T1_lora_B_out: (0.07374843209981918, -0.00022190138406585902, -0.07512981444597244)\n",
      "L2T1_lora_out: (0.14749686419963837, -0.00044380276813171804, -0.15025962889194489)\n",
      "\n",
      "L2T2_lora_B_out: (0.16253253817558289, -0.0011310501722618937, -0.14626333117485046)\n",
      "L2T2_lora_out: (0.32506507635116577, -0.0022621003445237875, -0.2925266623497009)\n",
      "\n",
      "g_out: (0.2516328990459442, -0.001818296848796308, -0.24904440343379974)\n",
      "L1T2_out: (0.3826136887073517, -0.002821178175508976, -0.37973570823669434)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01001881156116724, -1.3064443919574842e-05, -0.011221760883927345)\n",
      "L1T1_lora_out: (0.02003762312233448, -2.6128887839149684e-05, -0.02244352176785469)\n",
      "\n",
      "L2T1_lora_B_out: (0.004897663835436106, -2.5374694814672694e-05, -0.005426544230431318)\n",
      "L2T1_lora_out: (0.009795327670872211, -5.074938962934539e-05, -0.010853088460862637)\n",
      "\n",
      "L2T2_lora_B_out: (0.01953892596065998, -4.881580389337614e-05, -0.021535946056246758)\n",
      "L2T2_lora_out: (0.03907785192131996, -9.763160778675228e-05, -0.043071892112493515)\n",
      "\n",
      "g_out: (0.033676981925964355, -4.688223998527974e-05, -0.03865985572338104)\n",
      "L1T2_out: (0.053714603185653687, -7.301112054847181e-05, -0.06110337749123573)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1409814953804016, -0.00031663620029576123, -0.15453645586967468)\n",
      "L1T1_lora_out: (0.2819629907608032, -0.0006332724005915225, -0.30907291173934937)\n",
      "\n",
      "L2T1_lora_B_out: (0.05250309035181999, -6.345924339257181e-05, -0.05610484629869461)\n",
      "L2T1_lora_out: (0.10500618070363998, -0.00012691848678514361, -0.11220969259738922)\n",
      "\n",
      "L2T2_lora_B_out: (0.25169897079467773, -0.0005542922299355268, -0.2656504511833191)\n",
      "L2T2_lora_out: (0.5033979415893555, -0.0011085844598710537, -0.5313009023666382)\n",
      "\n",
      "g_out: (0.4337323307991028, -0.0009816659148782492, -0.46388718485832214)\n",
      "L1T2_out: (0.715695321559906, -0.0016149383736774325, -0.7729600667953491)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09664478152990341, 7.098529749782756e-05, -0.17570145428180695)\n",
      "L1T1_lora_out: (0.19328956305980682, 0.00014197059499565512, -0.3514029085636139)\n",
      "\n",
      "L2T1_lora_B_out: (0.04092907905578613, -0.0001228025503223762, -0.057162877172231674)\n",
      "L2T1_lora_out: (0.08185815811157227, -0.0002456051006447524, -0.11432575434446335)\n",
      "\n",
      "L2T2_lora_B_out: (0.13839419186115265, 4.232280116411857e-05, -0.22106140851974487)\n",
      "L2T2_lora_out: (0.2767883837223053, 8.464560232823715e-05, -0.44212281703948975)\n",
      "\n",
      "g_out: (0.2454695850610733, 0.0003302506811451167, -0.435525119304657)\n",
      "L1T2_out: (0.43070122599601746, 0.0004722212615888566, -0.7869280576705933)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0067986431531608105, 9.213969747179362e-07, -0.008078278973698616)\n",
      "L1T1_lora_out: (0.013597286306321621, 1.8427939494358725e-06, -0.016156557947397232)\n",
      "\n",
      "L2T1_lora_B_out: (0.008383149281144142, -3.425227532716235e-06, -0.006200351286679506)\n",
      "L2T1_lora_out: (0.016766298562288284, -6.85045506543247e-06, -0.012400702573359013)\n",
      "\n",
      "L2T2_lora_B_out: (0.015351117588579655, 5.239247457211604e-06, -0.017157595604658127)\n",
      "L2T2_lora_out: (0.03070223517715931, 1.0478494914423209e-05, -0.034315191209316254)\n",
      "\n",
      "g_out: (0.026087600737810135, 1.7328973626717925e-05, -0.026913676410913467)\n",
      "L1T2_out: (0.034740254282951355, 1.9171766325598583e-05, -0.0401330441236496)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028499871492385864, 3.3164942578878254e-05, -0.026002399623394012)\n",
      "L1T1_lora_out: (0.05699974298477173, 6.632988515775651e-05, -0.052004799246788025)\n",
      "\n",
      "L2T1_lora_B_out: (0.031018851324915886, 2.6769874239107594e-05, -0.03549019619822502)\n",
      "L2T1_lora_out: (0.06203770264983177, 5.353974847821519e-05, -0.07098039239645004)\n",
      "\n",
      "L2T2_lora_B_out: (0.060169462114572525, 0.00013580688391812146, -0.06661265343427658)\n",
      "L2T2_lora_out: (0.12033892422914505, 0.0002716137678362429, -0.13322530686855316)\n",
      "\n",
      "g_out: (0.0930856466293335, 0.00021807389566674829, -0.11775581538677216)\n",
      "L1T2_out: (0.1469581425189972, 0.00028440379537642, -0.16976061463356018)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04033506661653519, 2.8519029001472518e-05, -0.05265655741095543)\n",
      "L1T1_lora_out: (0.08067013323307037, 5.7038058002945036e-05, -0.10531311482191086)\n",
      "\n",
      "L2T1_lora_B_out: (0.059868793934583664, 0.0002397053176537156, -0.056289736181497574)\n",
      "L2T1_lora_out: (0.11973758786916733, 0.0004794106353074312, -0.11257947236299515)\n",
      "\n",
      "L2T2_lora_B_out: (0.08556471019983292, 0.0003818164113909006, -0.10363121330738068)\n",
      "L2T2_lora_out: (0.17112942039966583, 0.0007636328227818012, -0.20726242661476135)\n",
      "\n",
      "g_out: (0.1596582680940628, 0.00028422230388969183, -0.16429443657398224)\n",
      "L1T2_out: (0.24032840132713318, 0.0003412602818571031, -0.2696075439453125)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.026741873472929, 3.14961580443196e-05, -0.025969183072447777)\n",
      "L1T1_lora_out: (0.053483746945858, 6.29923160886392e-05, -0.051938366144895554)\n",
      "\n",
      "L2T1_lora_B_out: (0.033685509115457535, -1.3224750546214636e-05, -0.029596058651804924)\n",
      "L2T1_lora_out: (0.06737101823091507, -2.6449501092429273e-05, -0.05919211730360985)\n",
      "\n",
      "L2T2_lora_B_out: (0.04587564617395401, -3.4904544008895755e-05, -0.05719970539212227)\n",
      "L2T2_lora_out: (0.09175129234790802, -6.980908801779151e-05, -0.11439941078424454)\n",
      "\n",
      "g_out: (0.0901910811662674, -4.335956327849999e-05, -0.09326440840959549)\n",
      "L1T2_out: (0.13766784965991974, 1.963272734428756e-05, -0.1323266327381134)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003633286338299513, 4.497515874390956e-06, -0.0035305358469486237)\n",
      "L1T1_lora_out: (0.007266572676599026, 8.995031748781912e-06, -0.007061071693897247)\n",
      "\n",
      "L2T1_lora_B_out: (0.003424674039706588, -3.235179974581115e-06, -0.00355806783773005)\n",
      "L2T1_lora_out: (0.006849348079413176, -6.47035994916223e-06, -0.0071161356754601)\n",
      "\n",
      "L2T2_lora_B_out: (0.008447956293821335, -1.2028174751321785e-05, -0.008769060485064983)\n",
      "L2T2_lora_out: (0.01689591258764267, -2.405634950264357e-05, -0.017538120970129967)\n",
      "\n",
      "g_out: (0.014685619622468948, -1.7585993191460148e-05, -0.013276617042720318)\n",
      "L1T2_out: (0.02143888548016548, -8.590965990151744e-06, -0.01970849558711052)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.23523728549480438, -0.000381259509595111, -0.24205432832241058)\n",
      "L1T1_lora_out: (0.47047457098960876, -0.000762519019190222, -0.48410865664482117)\n",
      "\n",
      "L2T1_lora_B_out: (0.10159569978713989, -0.00018513014947529882, -0.11327553540468216)\n",
      "L2T1_lora_out: (0.20319139957427979, -0.00037026029895059764, -0.22655107080936432)\n",
      "\n",
      "L2T2_lora_B_out: (0.39137598872184753, -0.0010516139445826411, -0.5999796986579895)\n",
      "L2T2_lora_out: (0.7827519774436951, -0.0021032278891652822, -1.199959397315979)\n",
      "\n",
      "g_out: (0.7517392635345459, -0.0017329677939414978, -0.9734083414077759)\n",
      "L1T2_out: (1.222213864326477, -0.002495486754924059, -1.4461170434951782)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.16174587607383728, -4.1731545934453607e-05, -0.10984817892313004)\n",
      "L1T1_lora_out: (0.32349175214767456, -8.346309186890721e-05, -0.21969635784626007)\n",
      "\n",
      "L2T1_lora_B_out: (0.07406981289386749, -2.316463906026911e-05, -0.05820329114794731)\n",
      "L2T1_lora_out: (0.14813962578773499, -4.632927812053822e-05, -0.11640658229589462)\n",
      "\n",
      "L2T2_lora_B_out: (0.29480981826782227, -0.00015716717462055385, -0.2074705958366394)\n",
      "L2T2_lora_out: (0.5896196365356445, -0.0003143343492411077, -0.4149411916732788)\n",
      "\n",
      "g_out: (0.491886705160141, -0.00026800509658642113, -0.348162978887558)\n",
      "L1T2_out: (0.8153784275054932, -0.0003514680138323456, -0.5506376624107361)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.010438626632094383, -1.67551297636237e-06, -0.010497302748262882)\n",
      "L1T1_lora_out: (0.020877253264188766, -3.35102595272474e-06, -0.020994605496525764)\n",
      "\n",
      "L2T1_lora_B_out: (0.009147025644779205, 4.0085124055622146e-05, -0.009412054903805256)\n",
      "L2T1_lora_out: (0.01829405128955841, 8.017024811124429e-05, -0.018824109807610512)\n",
      "\n",
      "L2T2_lora_B_out: (0.018053000792860985, 4.6349534386536106e-05, -0.018644671887159348)\n",
      "L2T2_lora_out: (0.03610600158572197, 9.269906877307221e-05, -0.037289343774318695)\n",
      "\n",
      "g_out: (0.0329955630004406, 1.2528826118796133e-05, -0.03078790009021759)\n",
      "L1T2_out: (0.044966328889131546, 9.177796528092586e-06, -0.0447249710559845)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023881569504737854, -7.833294148440473e-06, -0.02207605354487896)\n",
      "L1T1_lora_out: (0.04776313900947571, -1.5666588296880946e-05, -0.04415210708975792)\n",
      "\n",
      "L2T1_lora_B_out: (0.025717750191688538, -1.6510045952600194e-06, -0.029239432886242867)\n",
      "L2T1_lora_out: (0.051435500383377075, -3.302009190520039e-06, -0.05847886577248573)\n",
      "\n",
      "L2T2_lora_B_out: (0.04796472191810608, -6.878877320559695e-05, -0.05002853274345398)\n",
      "L2T2_lora_out: (0.09592944383621216, -0.0001375775464111939, -0.10005706548690796)\n",
      "\n",
      "g_out: (0.08322485536336899, -0.0001342755276709795, -0.0767759308218956)\n",
      "L1T2_out: (0.12508106231689453, -0.0001499420905020088, -0.10861783474683762)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09103777259588242, 0.0002751402498688549, -0.110208660364151)\n",
      "L1T1_lora_out: (0.18207554519176483, 0.0005502804997377098, -0.220417320728302)\n",
      "\n",
      "L2T1_lora_B_out: (0.07091879099607468, 0.00020088368910364807, -0.06403657793998718)\n",
      "L2T1_lora_out: (0.14183758199214935, 0.00040176737820729613, -0.12807315587997437)\n",
      "\n",
      "L2T2_lora_B_out: (0.20902501046657562, 0.0004257943946868181, -0.21620380878448486)\n",
      "L2T2_lora_out: (0.41805002093315125, 0.0008515887893736362, -0.4324076175689697)\n",
      "\n",
      "g_out: (0.3278520703315735, 0.00044982158578932285, -0.3654111623764038)\n",
      "L1T2_out: (0.5040938258171082, 0.0010001021437346935, -0.5816992521286011)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1498686522245407, 0.00047168912715278566, -0.16424809396266937)\n",
      "L1T1_lora_out: (0.2997373044490814, 0.0009433782543055713, -0.32849618792533875)\n",
      "\n",
      "L2T1_lora_B_out: (0.04071260616183281, 0.0001238177064806223, -0.0509428009390831)\n",
      "L2T1_lora_out: (0.08142521232366562, 0.0002476354129612446, -0.1018856018781662)\n",
      "\n",
      "L2T2_lora_B_out: (0.30146312713623047, 0.0011028886074200273, -0.3683583438396454)\n",
      "L2T2_lora_out: (0.6029262542724609, 0.0022057772148400545, -0.7367166876792908)\n",
      "\n",
      "g_out: (0.5242684483528137, 0.00195814180187881, -0.6348310708999634)\n",
      "L1T2_out: (0.8113082647323608, 0.002901520114392042, -0.9633272886276245)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.008058005012571812, 9.868407687463332e-06, -0.010848497971892357)\n",
      "L1T1_lora_out: (0.016116010025143623, 1.9736815374926664e-05, -0.021696995943784714)\n",
      "\n",
      "L2T1_lora_B_out: (0.009842794388532639, -5.252379196463153e-06, -0.0085897296667099)\n",
      "L2T1_lora_out: (0.019685588777065277, -1.0504758392926306e-05, -0.0171794593334198)\n",
      "\n",
      "L2T2_lora_B_out: (0.019450712949037552, -2.0664103431045078e-05, -0.019909443333745003)\n",
      "L2T2_lora_out: (0.038901425898075104, -4.1328206862090155e-05, -0.039818886667490005)\n",
      "\n",
      "g_out: (0.03129107505083084, -3.082344846916385e-05, -0.030308686196804047)\n",
      "L1T2_out: (0.04619347304105759, -1.1086653103120625e-05, -0.05200568214058876)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.4516075551509857, -0.0007948795682750642, -0.4627555310726166)\n",
      "L1T1_lora_out: (0.9032151103019714, -0.0015897591365501285, -0.9255110621452332)\n",
      "\n",
      "L2T1_lora_B_out: (0.23574645817279816, -0.0005562331061810255, -0.18102216720581055)\n",
      "L2T1_lora_out: (0.4714929163455963, -0.001112466212362051, -0.3620443344116211)\n",
      "\n",
      "L2T2_lora_B_out: (0.520983874797821, -0.0016016956651583314, -0.4479151964187622)\n",
      "L2T2_lora_out: (1.041967749595642, -0.003203391330316663, -0.8958303928375244)\n",
      "\n",
      "g_out: (0.9218420386314392, -0.0020909251179546118, -0.90320885181427)\n",
      "L1T2_out: (1.8250571489334106, -0.0036806848365813494, -1.8287198543548584)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.3673975169658661, 2.5962355721276253e-05, -0.20802587270736694)\n",
      "L1T1_lora_out: (0.7347950339317322, 5.192471144255251e-05, -0.4160517454147339)\n",
      "\n",
      "L2T1_lora_B_out: (0.28934210538864136, -9.133746971201617e-06, -0.3860231637954712)\n",
      "L2T1_lora_out: (0.5786842107772827, -1.8267493942403235e-05, -0.7720463275909424)\n",
      "\n",
      "L2T2_lora_B_out: (0.6156455278396606, 8.655353303765878e-05, -0.6085920333862305)\n",
      "L2T2_lora_out: (1.2312910556793213, 0.00017310706607531756, -1.217184066772461)\n",
      "\n",
      "g_out: (0.9675967693328857, 0.00019137455092277378, -0.5915831327438354)\n",
      "L1T2_out: (1.7023918628692627, 0.00024329920415766537, -0.9954812526702881)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03301582112908363, 3.285423372290097e-05, -0.035462845116853714)\n",
      "L1T1_lora_out: (0.06603164225816727, 6.570846744580194e-05, -0.07092569023370743)\n",
      "\n",
      "L2T1_lora_B_out: (0.02529805526137352, 1.5636243915650994e-05, -0.024601032957434654)\n",
      "L2T1_lora_out: (0.05059611052274704, 3.127248783130199e-05, -0.04920206591486931)\n",
      "\n",
      "L2T2_lora_B_out: (0.07411368191242218, 7.47231169953011e-05, -0.07755819708108902)\n",
      "L2T2_lora_out: (0.14822736382484436, 0.0001494462339906022, -0.15511639416217804)\n",
      "\n",
      "g_out: (0.133274644613266, 0.00011817377526313066, -0.1279502958059311)\n",
      "L1T2_out: (0.19930627942085266, 0.0001838821917772293, -0.18544195592403412)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.034300025552511215, -5.554730523726903e-05, -0.03530000150203705)\n",
      "L1T1_lora_out: (0.06860005110502243, -0.00011109461047453806, -0.0706000030040741)\n",
      "\n",
      "L2T1_lora_B_out: (0.10309743881225586, 8.892474579624832e-05, -0.07557198405265808)\n",
      "L2T1_lora_out: (0.20619487762451172, 0.00017784949159249663, -0.15114396810531616)\n",
      "\n",
      "L2T2_lora_B_out: (0.10588312894105911, 0.00013320565631147474, -0.13176387548446655)\n",
      "L2T2_lora_out: (0.21176625788211823, 0.0002664113126229495, -0.2635277509689331)\n",
      "\n",
      "g_out: (0.15146568417549133, 8.856181375449523e-05, -0.1538691520690918)\n",
      "L1T2_out: (0.1864946484565735, -2.253284583275672e-05, -0.18529777228832245)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02780422382056713, -0.00015572202391922474, -0.026844168081879616)\n",
      "L1T1_lora_out: (0.05560844764113426, -0.0003114440478384495, -0.05368833616375923)\n",
      "\n",
      "L2T1_lora_B_out: (0.07574481517076492, -9.005791798699647e-05, -0.07519157975912094)\n",
      "L2T1_lora_out: (0.15148963034152985, -0.00018011583597399294, -0.15038315951824188)\n",
      "\n",
      "L2T2_lora_B_out: (0.08065921068191528, -0.0005726026138290763, -0.10870389640331268)\n",
      "L2T2_lora_out: (0.16131842136383057, -0.0011452052276581526, -0.21740779280662537)\n",
      "\n",
      "g_out: (0.13816064596176147, -0.000965089478995651, -0.13952022790908813)\n",
      "L1T2_out: (0.1782991588115692, -0.0012765337014570832, -0.18497686088085175)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.16770756244659424, 0.0008832886232994497, -0.12234998494386673)\n",
      "L1T1_lora_out: (0.3354151248931885, 0.0017665772465988994, -0.24469996988773346)\n",
      "\n",
      "L2T1_lora_B_out: (0.10019290447235107, -0.0006166720995679498, -0.07330769300460815)\n",
      "L2T1_lora_out: (0.20038580894470215, -0.0012333441991358995, -0.1466153860092163)\n",
      "\n",
      "L2T2_lora_B_out: (0.3800790011882782, 0.0005658396985381842, -0.246580109000206)\n",
      "L2T2_lora_out: (0.7601580023765564, 0.0011316793970763683, -0.493160218000412)\n",
      "\n",
      "g_out: (0.6285964250564575, 0.0023650233633816242, -0.4424455761909485)\n",
      "L1T2_out: (0.964011549949646, 0.004131601192057133, -0.6653792262077332)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.027868110686540604, 2.1834392100572586e-05, -0.019659042358398438)\n",
      "L1T1_lora_out: (0.05573622137308121, 4.366878420114517e-05, -0.039318084716796875)\n",
      "\n",
      "L2T1_lora_B_out: (0.004216096363961697, -1.5279983927030116e-05, -0.00415829848498106)\n",
      "L2T1_lora_out: (0.008432192727923393, -3.055996785406023e-05, -0.00831659696996212)\n",
      "\n",
      "L2T2_lora_B_out: (0.030212437734007835, 3.5327950172359124e-05, -0.022767091169953346)\n",
      "L2T2_lora_out: (0.06042487546801567, 7.065590034471825e-05, -0.04553418233990669)\n",
      "\n",
      "g_out: (0.06103866547346115, 0.00010121586819877848, -0.04685629531741142)\n",
      "L1T2_out: (0.11677488684654236, 0.00014488465967588127, -0.08605341613292694)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.4670940339565277, -0.0005599664873443544, -0.5612740516662598)\n",
      "L1T1_lora_out: (0.9341880679130554, -0.0011199329746887088, -1.1225481033325195)\n",
      "\n",
      "L2T1_lora_B_out: (0.22289139032363892, -0.0005477942759171128, -0.17845632135868073)\n",
      "L2T1_lora_out: (0.44578278064727783, -0.0010955885518342257, -0.35691264271736145)\n",
      "\n",
      "L2T2_lora_B_out: (0.663770854473114, -0.0011690257815644145, -0.5444685220718384)\n",
      "L2T2_lora_out: (1.327541708946228, -0.002338051563128829, -1.0889370441436768)\n",
      "\n",
      "g_out: (1.0059949159622192, -0.0012424628948792815, -1.1181470155715942)\n",
      "L1T2_out: (1.9401829242706299, -0.002362396102398634, -2.240694999694824)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0984496995806694, 7.051118882372975e-05, -0.09094657003879547)\n",
      "L1T1_lora_out: (0.1968993991613388, 0.0001410223776474595, -0.18189314007759094)\n",
      "\n",
      "L2T1_lora_B_out: (0.3952438533306122, -4.7147674195002764e-05, -0.27869001030921936)\n",
      "L2T1_lora_out: (0.7904877066612244, -9.429534839000553e-05, -0.5573800206184387)\n",
      "\n",
      "L2T2_lora_B_out: (0.4099442660808563, 0.00028142036171630025, -0.45122259855270386)\n",
      "L2T2_lora_out: (0.8198885321617126, 0.0005628407234326005, -0.9024451971054077)\n",
      "\n",
      "g_out: (0.35476115345954895, 0.0006571360863745213, -0.4652951657772064)\n",
      "L1T2_out: (0.47291892766952515, 0.0007981583476066589, -0.6272891163825989)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04122250899672508, 3.851453948300332e-05, -0.059755589812994)\n",
      "L1T1_lora_out: (0.08244501799345016, 7.702907896600664e-05, -0.119511179625988)\n",
      "\n",
      "L2T1_lora_B_out: (0.1373347043991089, 0.0004278027336113155, -0.13579249382019043)\n",
      "L2T1_lora_out: (0.2746694087982178, 0.000855605467222631, -0.27158498764038086)\n",
      "\n",
      "L2T2_lora_B_out: (0.2497224658727646, 0.0006856569088995457, -0.26416513323783875)\n",
      "L2T2_lora_out: (0.4994449317455292, 0.0013713138177990913, -0.5283302664756775)\n",
      "\n",
      "g_out: (0.28719598054885864, 0.0005157084669917822, -0.2650465965270996)\n",
      "L1T2_out: (0.3221946656703949, 0.0005927375750616193, -0.30080825090408325)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05352732166647911, 2.0103620045119897e-05, -0.06818697601556778)\n",
      "L1T1_lora_out: (0.10705464333295822, 4.020724009023979e-05, -0.13637395203113556)\n",
      "\n",
      "L2T1_lora_B_out: (0.06453755497932434, 4.987022839486599e-05, -0.06423357129096985)\n",
      "L2T1_lora_out: (0.12907510995864868, 9.974045678973198e-05, -0.1284671425819397)\n",
      "\n",
      "L2T2_lora_B_out: (0.05490652844309807, 0.0001411230769008398, -0.054262325167655945)\n",
      "L2T2_lora_out: (0.10981305688619614, 0.0002822461538016796, -0.10852465033531189)\n",
      "\n",
      "g_out: (0.1302793323993683, 0.00018250563880428672, -0.154848113656044)\n",
      "L1T2_out: (0.23132148385047913, 0.00022271295893006027, -0.29122206568717957)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06759140640497208, -6.915591075085104e-05, -0.07047645002603531)\n",
      "L1T1_lora_out: (0.13518281280994415, -0.00013831182150170207, -0.14095290005207062)\n",
      "\n",
      "L2T1_lora_B_out: (0.03981813043355942, -0.00014270018436945975, -0.03496503084897995)\n",
      "L2T1_lora_out: (0.07963626086711884, -0.0002854003687389195, -0.0699300616979599)\n",
      "\n",
      "L2T2_lora_B_out: (0.10900963842868805, -0.0003706380375660956, -0.11491645872592926)\n",
      "L2T2_lora_out: (0.2180192768573761, -0.0007412760751321912, -0.22983291745185852)\n",
      "\n",
      "g_out: (0.18038150668144226, -0.0004558753571473062, -0.1933860182762146)\n",
      "L1T2_out: (0.315461665391922, -0.0005941871786490083, -0.32880955934524536)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04610760882496834, 5.6316050176974386e-05, -0.045538097620010376)\n",
      "L1T1_lora_out: (0.09221521764993668, 0.00011263210035394877, -0.09107619524002075)\n",
      "\n",
      "L2T1_lora_B_out: (0.07702811062335968, 0.0002992792287841439, -0.05651387944817543)\n",
      "L2T1_lora_out: (0.15405622124671936, 0.0005985584575682878, -0.11302775889635086)\n",
      "\n",
      "L2T2_lora_B_out: (0.17607347667217255, 0.0004897852777503431, -0.10983864217996597)\n",
      "L2T2_lora_out: (0.3521469533443451, 0.0009795705555006862, -0.21967728435993195)\n",
      "\n",
      "g_out: (0.19809073209762573, 0.0003810120397247374, -0.16439072787761688)\n",
      "L1T2_out: (0.2903059422969818, 0.0004936440964229405, -0.25546693801879883)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.041282352060079575, 3.2635871320962906e-05, -0.023236583918333054)\n",
      "L1T1_lora_out: (0.08256470412015915, 6.527174264192581e-05, -0.04647316783666611)\n",
      "\n",
      "L2T1_lora_B_out: (0.07071972638368607, 0.00010818359442055225, -0.046652182936668396)\n",
      "L2T1_lora_out: (0.14143945276737213, 0.0002163671888411045, -0.09330436587333679)\n",
      "\n",
      "L2T2_lora_B_out: (0.11119769513607025, 0.00017186181503348053, -0.09744907170534134)\n",
      "L2T2_lora_out: (0.2223953902721405, 0.00034372363006696105, -0.19489814341068268)\n",
      "\n",
      "g_out: (0.14124593138694763, 0.00012735633936244994, -0.1049150824546814)\n",
      "L1T2_out: (0.22381064295768738, 0.00019262809655629098, -0.1479405015707016)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.4564513862133026, -6.275027408264577e-05, -0.5404516458511353)\n",
      "L1T1_lora_out: (0.9129027724266052, -0.00012550054816529155, -1.0809032917022705)\n",
      "\n",
      "L2T1_lora_B_out: (0.7030864357948303, -0.0004064779495820403, -0.2552756667137146)\n",
      "L2T1_lora_out: (1.4061728715896606, -0.0008129558991640806, -0.5105513334274292)\n",
      "\n",
      "L2T2_lora_B_out: (2.241924524307251, -0.0007235046359710395, -1.0695587396621704)\n",
      "L2T2_lora_out: (4.483849048614502, -0.001447009271942079, -2.139117479324341)\n",
      "\n",
      "g_out: (3.077676296234131, -0.0006340531399473548, -1.8421745300292969)\n",
      "L1T2_out: (3.813581943511963, -0.0007595539791509509, -2.7526662349700928)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.22498826682567596, 1.343503754469566e-05, -0.21202944219112396)\n",
      "L1T1_lora_out: (0.44997653365135193, 2.687007508939132e-05, -0.4240588843822479)\n",
      "\n",
      "L2T1_lora_B_out: (0.2551562488079071, -9.468446660321206e-05, -0.1911800354719162)\n",
      "L2T1_lora_out: (0.5103124976158142, -0.00018936893320642412, -0.3823600709438324)\n",
      "\n",
      "L2T2_lora_B_out: (0.4419698715209961, -0.0001815831201383844, -0.3491738438606262)\n",
      "L2T2_lora_out: (0.8839397430419922, -0.0003631662402767688, -0.6983476877212524)\n",
      "\n",
      "g_out: (0.7078957557678223, -0.00017379748169332743, -0.674232542514801)\n",
      "L1T2_out: (1.1578723192214966, -0.0001469272538088262, -1.0982913970947266)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.down_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (3.7169833183288574, -0.0014324057847261429, -6.250096797943115)\n",
      "L1T1_lora_out: (7.433966636657715, -0.0028648115694522858, -12.50019359588623)\n",
      "\n",
      "L2T1_lora_B_out: (9.647444725036621, 0.0009571055416017771, -8.588224411010742)\n",
      "L2T1_lora_out: (19.294889450073242, 0.0019142110832035542, -17.176448822021484)\n",
      "\n",
      "L2T2_lora_B_out: (10.490523338317871, -0.0008029050077311695, -12.78404712677002)\n",
      "L2T2_lora_out: (20.981046676635742, -0.001605810015462339, -25.56809425354004)\n",
      "\n",
      "g_out: (7.945842266082764, -0.003520019818097353, -13.506647109985352)\n",
      "L1T2_out: (15.37980842590332, -0.006384831853210926, -26.006839752197266)\n",
      "\n",
      "Loss: tensor(3.5048, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "================================================================\n",
      "model.layers.31.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05352732166647911, 2.0103620045119897e-05, -0.06818697601556778)\n",
      "L1T1_lora_out: (0.10705464333295822, 4.020724009023979e-05, -0.13637395203113556)\n",
      "\n",
      "L2T1_lora_B_out: (0.06453755497932434, 4.987022839486599e-05, -0.06423357129096985)\n",
      "L2T1_lora_out: (0.12907510995864868, 9.974045678973198e-05, -0.1284671425819397)\n",
      "\n",
      "L2T2_lora_B_out: (0.05490652844309807, 0.0001411230769008398, -0.054262325167655945)\n",
      "L2T2_lora_out: (0.10981305688619614, 0.0002822461538016796, -0.10852465033531189)\n",
      "\n",
      "g_out: (0.1302793323993683, 0.00018250563880428672, -0.154848113656044)\n",
      "L1T2_out: (0.23132148385047913, 0.00022271295893006027, -0.29122206568717957)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06759140640497208, -6.915591075085104e-05, -0.07047645002603531)\n",
      "L1T1_lora_out: (0.13518281280994415, -0.00013831182150170207, -0.14095290005207062)\n",
      "\n",
      "L2T1_lora_B_out: (0.03981813043355942, -0.00014270018436945975, -0.03496503084897995)\n",
      "L2T1_lora_out: (0.07963626086711884, -0.0002854003687389195, -0.0699300616979599)\n",
      "\n",
      "L2T2_lora_B_out: (0.10900963842868805, -0.0003706380375660956, -0.11491645872592926)\n",
      "L2T2_lora_out: (0.2180192768573761, -0.0007412760751321912, -0.22983291745185852)\n",
      "\n",
      "g_out: (0.18038150668144226, -0.0004558753571473062, -0.1933860182762146)\n",
      "L1T2_out: (0.315461665391922, -0.0005941871786490083, -0.32880955934524536)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04610760882496834, 5.6316050176974386e-05, -0.045538097620010376)\n",
      "L1T1_lora_out: (0.09221521764993668, 0.00011263210035394877, -0.09107619524002075)\n",
      "\n",
      "L2T1_lora_B_out: (0.07702811062335968, 0.0002992792287841439, -0.05651387944817543)\n",
      "L2T1_lora_out: (0.15405622124671936, 0.0005985584575682878, -0.11302775889635086)\n",
      "\n",
      "L2T2_lora_B_out: (0.17607347667217255, 0.0004897852777503431, -0.10983864217996597)\n",
      "L2T2_lora_out: (0.3521469533443451, 0.0009795705555006862, -0.21967728435993195)\n",
      "\n",
      "g_out: (0.19809073209762573, 0.0003810120397247374, -0.16439072787761688)\n",
      "L1T2_out: (0.2903059422969818, 0.0004936440964229405, -0.25546693801879883)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.041282352060079575, 3.2635871320962906e-05, -0.023236583918333054)\n",
      "L1T1_lora_out: (0.08256470412015915, 6.527174264192581e-05, -0.04647316783666611)\n",
      "\n",
      "L2T1_lora_B_out: (0.07071972638368607, 0.00010818359442055225, -0.046652182936668396)\n",
      "L2T1_lora_out: (0.14143945276737213, 0.0002163671888411045, -0.09330436587333679)\n",
      "\n",
      "L2T2_lora_B_out: (0.11119769513607025, 0.00017186181503348053, -0.09744907170534134)\n",
      "L2T2_lora_out: (0.2223953902721405, 0.00034372363006696105, -0.19489814341068268)\n",
      "\n",
      "g_out: (0.14124593138694763, 0.00012735633936244994, -0.1049150824546814)\n",
      "L1T2_out: (0.22381064295768738, 0.00019262809655629098, -0.1479405015707016)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.4564513862133026, -6.275027408264577e-05, -0.5404516458511353)\n",
      "L1T1_lora_out: (0.9129027724266052, -0.00012550054816529155, -1.0809032917022705)\n",
      "\n",
      "L2T1_lora_B_out: (0.7030864357948303, -0.0004064779495820403, -0.2552756667137146)\n",
      "L2T1_lora_out: (1.4061728715896606, -0.0008129558991640806, -0.5105513334274292)\n",
      "\n",
      "L2T2_lora_B_out: (2.241924524307251, -0.0007235046359710395, -1.0695587396621704)\n",
      "L2T2_lora_out: (4.483849048614502, -0.001447009271942079, -2.139117479324341)\n",
      "\n",
      "g_out: (3.077676296234131, -0.0006340531399473548, -1.8421745300292969)\n",
      "L1T2_out: (3.813581943511963, -0.0007595539791509509, -2.7526662349700928)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.up_proj\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1T1_lora_B_out: (0.22498826682567596, 1.343503754469566e-05, -0.21202944219112396)\n",
      "L1T1_lora_out: (0.44997653365135193, 2.687007508939132e-05, -0.4240588843822479)\n",
      "\n",
      "L2T1_lora_B_out: (0.2551562488079071, -9.468446660321206e-05, -0.1911800354719162)\n",
      "L2T1_lora_out: (0.5103124976158142, -0.00018936893320642412, -0.3823600709438324)\n",
      "\n",
      "L2T2_lora_B_out: (0.4419698715209961, -0.0001815831201383844, -0.3491738438606262)\n",
      "L2T2_lora_out: (0.8839397430419922, -0.0003631662402767688, -0.6983476877212524)\n",
      "\n",
      "g_out: (0.7078957557678223, -0.00017379748169332743, -0.674232542514801)\n",
      "L1T2_out: (1.1578723192214966, -0.0001469272538088262, -1.0982913970947266)\n",
      "\n",
      "================================================================\n",
      "model.layers.31.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.30.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.034300025552511215, -5.554730523726903e-05, -0.03530000150203705)\n",
      "L1T1_lora_out: (0.06860005110502243, -0.00011109461047453806, -0.0706000030040741)\n",
      "\n",
      "L2T1_lora_B_out: (0.10309743881225586, 8.892474579624832e-05, -0.07557198405265808)\n",
      "L2T1_lora_out: (0.20619487762451172, 0.00017784949159249663, -0.15114396810531616)\n",
      "\n",
      "L2T2_lora_B_out: (0.10588312894105911, 0.00013320565631147474, -0.13176387548446655)\n",
      "L2T2_lora_out: (0.21176625788211823, 0.0002664113126229495, -0.2635277509689331)\n",
      "\n",
      "g_out: (0.15146568417549133, 8.856181375449523e-05, -0.1538691520690918)\n",
      "L1T2_out: (0.1864946484565735, -2.253284583275672e-05, -0.18529777228832245)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02780422382056713, -0.00015572202391922474, -0.026844168081879616)\n",
      "L1T1_lora_out: (0.05560844764113426, -0.0003114440478384495, -0.05368833616375923)\n",
      "\n",
      "L2T1_lora_B_out: (0.07574481517076492, -9.005791798699647e-05, -0.07519157975912094)\n",
      "L2T1_lora_out: (0.15148963034152985, -0.00018011583597399294, -0.15038315951824188)\n",
      "\n",
      "L2T2_lora_B_out: (0.08065921068191528, -0.0005726026138290763, -0.10870389640331268)\n",
      "L2T2_lora_out: (0.16131842136383057, -0.0011452052276581526, -0.21740779280662537)\n",
      "\n",
      "g_out: (0.13816064596176147, -0.000965089478995651, -0.13952022790908813)\n",
      "L1T2_out: (0.1782991588115692, -0.0012765337014570832, -0.18497686088085175)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.16770756244659424, 0.0008832886232994497, -0.12234998494386673)\n",
      "L1T1_lora_out: (0.3354151248931885, 0.0017665772465988994, -0.24469996988773346)\n",
      "\n",
      "L2T1_lora_B_out: (0.10019290447235107, -0.0006166720995679498, -0.07330769300460815)\n",
      "L2T1_lora_out: (0.20038580894470215, -0.0012333441991358995, -0.1466153860092163)\n",
      "\n",
      "L2T2_lora_B_out: (0.3800790011882782, 0.0005658396985381842, -0.246580109000206)\n",
      "L2T2_lora_out: (0.7601580023765564, 0.0011316793970763683, -0.493160218000412)\n",
      "\n",
      "g_out: (0.6285964250564575, 0.0023650233633816242, -0.4424455761909485)\n",
      "L1T2_out: (0.964011549949646, 0.004131601192057133, -0.6653792262077332)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.027868110686540604, 2.1834392100572586e-05, -0.019659042358398438)\n",
      "L1T1_lora_out: (0.05573622137308121, 4.366878420114517e-05, -0.039318084716796875)\n",
      "\n",
      "L2T1_lora_B_out: (0.004216096363961697, -1.5279983927030116e-05, -0.00415829848498106)\n",
      "L2T1_lora_out: (0.008432192727923393, -3.055996785406023e-05, -0.00831659696996212)\n",
      "\n",
      "L2T2_lora_B_out: (0.030212437734007835, 3.5327950172359124e-05, -0.022767091169953346)\n",
      "L2T2_lora_out: (0.06042487546801567, 7.065590034471825e-05, -0.04553418233990669)\n",
      "\n",
      "g_out: (0.06103866547346115, 0.00010121586819877848, -0.04685629531741142)\n",
      "L1T2_out: (0.11677488684654236, 0.00014488465967588127, -0.08605341613292694)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.4670940339565277, -0.0005599664873443544, -0.5612740516662598)\n",
      "L1T1_lora_out: (0.9341880679130554, -0.0011199329746887088, -1.1225481033325195)\n",
      "\n",
      "L2T1_lora_B_out: (0.22289139032363892, -0.0005477942759171128, -0.17845632135868073)\n",
      "L2T1_lora_out: (0.44578278064727783, -0.0010955885518342257, -0.35691264271736145)\n",
      "\n",
      "L2T2_lora_B_out: (0.663770854473114, -0.0011690257815644145, -0.5444685220718384)\n",
      "L2T2_lora_out: (1.327541708946228, -0.002338051563128829, -1.0889370441436768)\n",
      "\n",
      "g_out: (1.0059949159622192, -0.0012424628948792815, -1.1181470155715942)\n",
      "L1T2_out: (1.9401829242706299, -0.002362396102398634, -2.240694999694824)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0984496995806694, 7.051118882372975e-05, -0.09094657003879547)\n",
      "L1T1_lora_out: (0.1968993991613388, 0.0001410223776474595, -0.18189314007759094)\n",
      "\n",
      "L2T1_lora_B_out: (0.3952438533306122, -4.7147674195002764e-05, -0.27869001030921936)\n",
      "L2T1_lora_out: (0.7904877066612244, -9.429534839000553e-05, -0.5573800206184387)\n",
      "\n",
      "L2T2_lora_B_out: (0.4099442660808563, 0.00028142036171630025, -0.45122259855270386)\n",
      "L2T2_lora_out: (0.8198885321617126, 0.0005628407234326005, -0.9024451971054077)\n",
      "\n",
      "g_out: (0.35476115345954895, 0.0006571360863745213, -0.4652951657772064)\n",
      "L1T2_out: (0.47291892766952515, 0.0007981583476066589, -0.6272891163825989)\n",
      "\n",
      "================================================================\n",
      "model.layers.30.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.29.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023881569504737854, -7.833294148440473e-06, -0.02207605354487896)\n",
      "L1T1_lora_out: (0.04776313900947571, -1.5666588296880946e-05, -0.04415210708975792)\n",
      "\n",
      "L2T1_lora_B_out: (0.025717750191688538, -1.6510045952600194e-06, -0.029239432886242867)\n",
      "L2T1_lora_out: (0.051435500383377075, -3.302009190520039e-06, -0.05847886577248573)\n",
      "\n",
      "L2T2_lora_B_out: (0.04796472191810608, -6.878877320559695e-05, -0.05002853274345398)\n",
      "L2T2_lora_out: (0.09592944383621216, -0.0001375775464111939, -0.10005706548690796)\n",
      "\n",
      "g_out: (0.08322485536336899, -0.0001342755276709795, -0.0767759308218956)\n",
      "L1T2_out: (0.12508106231689453, -0.0001499420905020088, -0.10861783474683762)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09103777259588242, 0.0002751402498688549, -0.110208660364151)\n",
      "L1T1_lora_out: (0.18207554519176483, 0.0005502804997377098, -0.220417320728302)\n",
      "\n",
      "L2T1_lora_B_out: (0.07091879099607468, 0.00020088368910364807, -0.06403657793998718)\n",
      "L2T1_lora_out: (0.14183758199214935, 0.00040176737820729613, -0.12807315587997437)\n",
      "\n",
      "L2T2_lora_B_out: (0.20902501046657562, 0.0004257943946868181, -0.21620380878448486)\n",
      "L2T2_lora_out: (0.41805002093315125, 0.0008515887893736362, -0.4324076175689697)\n",
      "\n",
      "g_out: (0.3278520703315735, 0.00044982158578932285, -0.3654111623764038)\n",
      "L1T2_out: (0.5040938258171082, 0.0010001021437346935, -0.5816992521286011)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1498686522245407, 0.00047168912715278566, -0.16424809396266937)\n",
      "L1T1_lora_out: (0.2997373044490814, 0.0009433782543055713, -0.32849618792533875)\n",
      "\n",
      "L2T1_lora_B_out: (0.04071260616183281, 0.0001238177064806223, -0.0509428009390831)\n",
      "L2T1_lora_out: (0.08142521232366562, 0.0002476354129612446, -0.1018856018781662)\n",
      "\n",
      "L2T2_lora_B_out: (0.30146312713623047, 0.0011028886074200273, -0.3683583438396454)\n",
      "L2T2_lora_out: (0.6029262542724609, 0.0022057772148400545, -0.7367166876792908)\n",
      "\n",
      "g_out: (0.5242684483528137, 0.00195814180187881, -0.6348310708999634)\n",
      "L1T2_out: (0.8113082647323608, 0.002901520114392042, -0.9633272886276245)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.008058005012571812, 9.868407687463332e-06, -0.010848497971892357)\n",
      "L1T1_lora_out: (0.016116010025143623, 1.9736815374926664e-05, -0.021696995943784714)\n",
      "\n",
      "L2T1_lora_B_out: (0.009842794388532639, -5.252379196463153e-06, -0.0085897296667099)\n",
      "L2T1_lora_out: (0.019685588777065277, -1.0504758392926306e-05, -0.0171794593334198)\n",
      "\n",
      "L2T2_lora_B_out: (0.019450712949037552, -2.0664103431045078e-05, -0.019909443333745003)\n",
      "L2T2_lora_out: (0.038901425898075104, -4.1328206862090155e-05, -0.039818886667490005)\n",
      "\n",
      "g_out: (0.03129107505083084, -3.082344846916385e-05, -0.030308686196804047)\n",
      "L1T2_out: (0.04619347304105759, -1.1086653103120625e-05, -0.05200568214058876)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.4516075551509857, -0.0007948795682750642, -0.4627555310726166)\n",
      "L1T1_lora_out: (0.9032151103019714, -0.0015897591365501285, -0.9255110621452332)\n",
      "\n",
      "L2T1_lora_B_out: (0.23574645817279816, -0.0005562331061810255, -0.18102216720581055)\n",
      "L2T1_lora_out: (0.4714929163455963, -0.001112466212362051, -0.3620443344116211)\n",
      "\n",
      "L2T2_lora_B_out: (0.520983874797821, -0.0016016956651583314, -0.4479151964187622)\n",
      "L2T2_lora_out: (1.041967749595642, -0.003203391330316663, -0.8958303928375244)\n",
      "\n",
      "g_out: (0.9218420386314392, -0.0020909251179546118, -0.90320885181427)\n",
      "L1T2_out: (1.8250571489334106, -0.0036806848365813494, -1.8287198543548584)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.3673975169658661, 2.5962355721276253e-05, -0.20802587270736694)\n",
      "L1T1_lora_out: (0.7347950339317322, 5.192471144255251e-05, -0.4160517454147339)\n",
      "\n",
      "L2T1_lora_B_out: (0.28934210538864136, -9.133746971201617e-06, -0.3860231637954712)\n",
      "L2T1_lora_out: (0.5786842107772827, -1.8267493942403235e-05, -0.7720463275909424)\n",
      "\n",
      "L2T2_lora_B_out: (0.6156455278396606, 8.655353303765878e-05, -0.6085920333862305)\n",
      "L2T2_lora_out: (1.2312910556793213, 0.00017310706607531756, -1.217184066772461)\n",
      "\n",
      "g_out: (0.9675967693328857, 0.00019137455092277378, -0.5915831327438354)\n",
      "L1T2_out: (1.7023918628692627, 0.00024329920415766537, -0.9954812526702881)\n",
      "\n",
      "================================================================\n",
      "model.layers.29.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.28.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028499871492385864, 3.3164942578878254e-05, -0.026002399623394012)\n",
      "L1T1_lora_out: (0.05699974298477173, 6.632988515775651e-05, -0.052004799246788025)\n",
      "\n",
      "L2T1_lora_B_out: (0.031018851324915886, 2.6769874239107594e-05, -0.03549019619822502)\n",
      "L2T1_lora_out: (0.06203770264983177, 5.353974847821519e-05, -0.07098039239645004)\n",
      "\n",
      "L2T2_lora_B_out: (0.060169462114572525, 0.00013580688391812146, -0.06661265343427658)\n",
      "L2T2_lora_out: (0.12033892422914505, 0.0002716137678362429, -0.13322530686855316)\n",
      "\n",
      "g_out: (0.0930856466293335, 0.00021807389566674829, -0.11775581538677216)\n",
      "L1T2_out: (0.1469581425189972, 0.00028440379537642, -0.16976061463356018)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04033506661653519, 2.8519029001472518e-05, -0.05265655741095543)\n",
      "L1T1_lora_out: (0.08067013323307037, 5.7038058002945036e-05, -0.10531311482191086)\n",
      "\n",
      "L2T1_lora_B_out: (0.059868793934583664, 0.0002397053176537156, -0.056289736181497574)\n",
      "L2T1_lora_out: (0.11973758786916733, 0.0004794106353074312, -0.11257947236299515)\n",
      "\n",
      "L2T2_lora_B_out: (0.08556471019983292, 0.0003818164113909006, -0.10363121330738068)\n",
      "L2T2_lora_out: (0.17112942039966583, 0.0007636328227818012, -0.20726242661476135)\n",
      "\n",
      "g_out: (0.1596582680940628, 0.00028422230388969183, -0.16429443657398224)\n",
      "L1T2_out: (0.24032840132713318, 0.0003412602818571031, -0.2696075439453125)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.026741873472929, 3.14961580443196e-05, -0.025969183072447777)\n",
      "L1T1_lora_out: (0.053483746945858, 6.29923160886392e-05, -0.051938366144895554)\n",
      "\n",
      "L2T1_lora_B_out: (0.033685509115457535, -1.3224750546214636e-05, -0.029596058651804924)\n",
      "L2T1_lora_out: (0.06737101823091507, -2.6449501092429273e-05, -0.05919211730360985)\n",
      "\n",
      "L2T2_lora_B_out: (0.04587564617395401, -3.4904544008895755e-05, -0.05719970539212227)\n",
      "L2T2_lora_out: (0.09175129234790802, -6.980908801779151e-05, -0.11439941078424454)\n",
      "\n",
      "g_out: (0.0901910811662674, -4.335956327849999e-05, -0.09326440840959549)\n",
      "L1T2_out: (0.13766784965991974, 1.963272734428756e-05, -0.1323266327381134)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003633286338299513, 4.497515874390956e-06, -0.0035305358469486237)\n",
      "L1T1_lora_out: (0.007266572676599026, 8.995031748781912e-06, -0.007061071693897247)\n",
      "\n",
      "L2T1_lora_B_out: (0.003424674039706588, -3.235179974581115e-06, -0.00355806783773005)\n",
      "L2T1_lora_out: (0.006849348079413176, -6.47035994916223e-06, -0.0071161356754601)\n",
      "\n",
      "L2T2_lora_B_out: (0.008447956293821335, -1.2028174751321785e-05, -0.008769060485064983)\n",
      "L2T2_lora_out: (0.01689591258764267, -2.405634950264357e-05, -0.017538120970129967)\n",
      "\n",
      "g_out: (0.014685619622468948, -1.7585993191460148e-05, -0.013276617042720318)\n",
      "L1T2_out: (0.02143888548016548, -8.590965990151744e-06, -0.01970849558711052)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.23523728549480438, -0.000381259509595111, -0.24205432832241058)\n",
      "L1T1_lora_out: (0.47047457098960876, -0.000762519019190222, -0.48410865664482117)\n",
      "\n",
      "L2T1_lora_B_out: (0.10159569978713989, -0.00018513014947529882, -0.11327553540468216)\n",
      "L2T1_lora_out: (0.20319139957427979, -0.00037026029895059764, -0.22655107080936432)\n",
      "\n",
      "L2T2_lora_B_out: (0.39137598872184753, -0.0010516139445826411, -0.5999796986579895)\n",
      "L2T2_lora_out: (0.7827519774436951, -0.0021032278891652822, -1.199959397315979)\n",
      "\n",
      "g_out: (0.7517392635345459, -0.0017329677939414978, -0.9734083414077759)\n",
      "L1T2_out: (1.222213864326477, -0.002495486754924059, -1.4461170434951782)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.16174587607383728, -4.1731545934453607e-05, -0.10984817892313004)\n",
      "L1T1_lora_out: (0.32349175214767456, -8.346309186890721e-05, -0.21969635784626007)\n",
      "\n",
      "L2T1_lora_B_out: (0.07406981289386749, -2.316463906026911e-05, -0.05820329114794731)\n",
      "L2T1_lora_out: (0.14813962578773499, -4.632927812053822e-05, -0.11640658229589462)\n",
      "\n",
      "L2T2_lora_B_out: (0.29480981826782227, -0.00015716717462055385, -0.2074705958366394)\n",
      "L2T2_lora_out: (0.5896196365356445, -0.0003143343492411077, -0.4149411916732788)\n",
      "\n",
      "g_out: (0.491886705160141, -0.00026800509658642113, -0.348162978887558)\n",
      "L1T2_out: (0.8153784275054932, -0.0003514680138323456, -0.5506376624107361)\n",
      "\n",
      "================================================================\n",
      "model.layers.28.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.27.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03768538683652878, 9.523288463242352e-05, -0.0436437614262104)\n",
      "L1T1_lora_out: (0.07537077367305756, 0.00019046576926484704, -0.0872875228524208)\n",
      "\n",
      "L2T1_lora_B_out: (0.039615776389837265, 3.621459472924471e-05, -0.03891871124505997)\n",
      "L2T1_lora_out: (0.07923155277967453, 7.242918945848942e-05, -0.07783742249011993)\n",
      "\n",
      "L2T2_lora_B_out: (0.11078324913978577, 0.00025046186055988073, -0.10588299483060837)\n",
      "L2T2_lora_out: (0.22156649827957153, 0.0005009237211197615, -0.21176598966121674)\n",
      "\n",
      "g_out: (0.17962506413459778, 0.00042849453166127205, -0.18781039118766785)\n",
      "L1T2_out: (0.2473156601190567, 0.0006189602427184582, -0.27509790658950806)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.030896084383130074, 2.247688826173544e-05, -0.029558300971984863)\n",
      "L1T1_lora_out: (0.06179216876626015, 4.495377652347088e-05, -0.05911660194396973)\n",
      "\n",
      "L2T1_lora_B_out: (0.04806862398982048, -6.443690654123202e-05, -0.047885119915008545)\n",
      "L2T1_lora_out: (0.09613724797964096, -0.00012887381308246404, -0.09577023983001709)\n",
      "\n",
      "L2T2_lora_B_out: (0.11493939906358719, 0.00010357729479437694, -0.09759686887264252)\n",
      "L2T2_lora_out: (0.22987879812717438, 0.00020715458958875388, -0.19519373774528503)\n",
      "\n",
      "g_out: (0.14721110463142395, 0.00033602825715206563, -0.11691319197416306)\n",
      "L1T2_out: (0.1900513768196106, 0.0003809821209870279, -0.16536970436573029)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06794445216655731, -0.0005014405469410121, -0.06761207431554794)\n",
      "L1T1_lora_out: (0.13588890433311462, -0.0010028810938820243, -0.1352241486310959)\n",
      "\n",
      "L2T1_lora_B_out: (0.07374843209981918, -0.00022190138406585902, -0.07512981444597244)\n",
      "L2T1_lora_out: (0.14749686419963837, -0.00044380276813171804, -0.15025962889194489)\n",
      "\n",
      "L2T2_lora_B_out: (0.16253253817558289, -0.0011310501722618937, -0.14626333117485046)\n",
      "L2T2_lora_out: (0.32506507635116577, -0.0022621003445237875, -0.2925266623497009)\n",
      "\n",
      "g_out: (0.2516328990459442, -0.001818296848796308, -0.24904440343379974)\n",
      "L1T2_out: (0.3826136887073517, -0.002821178175508976, -0.37973570823669434)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01001881156116724, -1.3064443919574842e-05, -0.011221760883927345)\n",
      "L1T1_lora_out: (0.02003762312233448, -2.6128887839149684e-05, -0.02244352176785469)\n",
      "\n",
      "L2T1_lora_B_out: (0.004897663835436106, -2.5374694814672694e-05, -0.005426544230431318)\n",
      "L2T1_lora_out: (0.009795327670872211, -5.074938962934539e-05, -0.010853088460862637)\n",
      "\n",
      "L2T2_lora_B_out: (0.01953892596065998, -4.881580389337614e-05, -0.021535946056246758)\n",
      "L2T2_lora_out: (0.03907785192131996, -9.763160778675228e-05, -0.043071892112493515)\n",
      "\n",
      "g_out: (0.033676981925964355, -4.688223998527974e-05, -0.03865985572338104)\n",
      "L1T2_out: (0.053714603185653687, -7.301112054847181e-05, -0.06110337749123573)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1409814953804016, -0.00031663620029576123, -0.15453645586967468)\n",
      "L1T1_lora_out: (0.2819629907608032, -0.0006332724005915225, -0.30907291173934937)\n",
      "\n",
      "L2T1_lora_B_out: (0.05250309035181999, -6.345924339257181e-05, -0.05610484629869461)\n",
      "L2T1_lora_out: (0.10500618070363998, -0.00012691848678514361, -0.11220969259738922)\n",
      "\n",
      "L2T2_lora_B_out: (0.25169897079467773, -0.0005542922299355268, -0.2656504511833191)\n",
      "L2T2_lora_out: (0.5033979415893555, -0.0011085844598710537, -0.5313009023666382)\n",
      "\n",
      "g_out: (0.4337323307991028, -0.0009816659148782492, -0.46388718485832214)\n",
      "L1T2_out: (0.715695321559906, -0.0016149383736774325, -0.7729600667953491)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09664478152990341, 7.098529749782756e-05, -0.17570145428180695)\n",
      "L1T1_lora_out: (0.19328956305980682, 0.00014197059499565512, -0.3514029085636139)\n",
      "\n",
      "L2T1_lora_B_out: (0.04092907905578613, -0.0001228025503223762, -0.057162877172231674)\n",
      "L2T1_lora_out: (0.08185815811157227, -0.0002456051006447524, -0.11432575434446335)\n",
      "\n",
      "L2T2_lora_B_out: (0.13839419186115265, 4.232280116411857e-05, -0.22106140851974487)\n",
      "L2T2_lora_out: (0.2767883837223053, 8.464560232823715e-05, -0.44212281703948975)\n",
      "\n",
      "g_out: (0.2454695850610733, 0.0003302506811451167, -0.435525119304657)\n",
      "L1T2_out: (0.43070122599601746, 0.0004722212615888566, -0.7869280576705933)\n",
      "\n",
      "================================================================\n",
      "model.layers.27.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.26.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.033259131014347076, -5.5029049690347165e-05, -0.03048727847635746)\n",
      "L1T1_lora_out: (0.06651826202869415, -0.00011005809938069433, -0.06097455695271492)\n",
      "\n",
      "L2T1_lora_B_out: (0.029348954558372498, 2.476573718013242e-05, -0.03503144159913063)\n",
      "L2T1_lora_out: (0.058697909116744995, 4.953147436026484e-05, -0.07006288319826126)\n",
      "\n",
      "L2T2_lora_B_out: (0.08019837737083435, -4.743860336020589e-05, -0.09236566722393036)\n",
      "L2T2_lora_out: (0.1603967547416687, -9.487720672041178e-05, -0.18473133444786072)\n",
      "\n",
      "g_out: (0.1310146301984787, -0.00014440869563259184, -0.12437909841537476)\n",
      "L1T2_out: (0.19609998166561127, -0.0002544667513575405, -0.1837901473045349)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02528468891978264, 3.701470996020362e-05, -0.025032654404640198)\n",
      "L1T1_lora_out: (0.05056937783956528, 7.402941992040724e-05, -0.050065308809280396)\n",
      "\n",
      "L2T1_lora_B_out: (0.028013925999403, 5.896668153582141e-05, -0.028069984167814255)\n",
      "L2T1_lora_out: (0.056027851998806, 0.00011793336307164282, -0.05613996833562851)\n",
      "\n",
      "L2T2_lora_B_out: (0.075016550719738, 0.00015076022827997804, -0.05397637188434601)\n",
      "L2T2_lora_out: (0.150033101439476, 0.0003015204565599561, -0.10795274376869202)\n",
      "\n",
      "g_out: (0.12195991724729538, 0.00018358694796916097, -0.10563955456018448)\n",
      "L1T2_out: (0.1608732044696808, 0.00025761648430489004, -0.15570485591888428)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08427302539348602, 0.00015409824845846742, -0.08644300699234009)\n",
      "L1T1_lora_out: (0.16854605078697205, 0.00030819649691693485, -0.17288601398468018)\n",
      "\n",
      "L2T1_lora_B_out: (0.07361052930355072, 0.00041797192534431815, -0.05335743725299835)\n",
      "L2T1_lora_out: (0.14722105860710144, 0.0008359438506886363, -0.1067148745059967)\n",
      "\n",
      "L2T2_lora_B_out: (0.12456729263067245, 0.0007700034766457975, -0.1189405620098114)\n",
      "L2T2_lora_out: (0.2491345852613449, 0.001540006953291595, -0.2378811240196228)\n",
      "\n",
      "g_out: (0.17909926176071167, 0.0007040632772259414, -0.18682309985160828)\n",
      "L1T2_out: (0.29235878586769104, 0.001012259628623724, -0.35970911383628845)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0037806956097483635, 7.67163669479487e-07, -0.0033934256061911583)\n",
      "L1T1_lora_out: (0.007561391219496727, 1.534327338958974e-06, -0.006786851212382317)\n",
      "\n",
      "L2T1_lora_B_out: (0.0029000602662563324, 8.88674367161002e-06, -0.0029042051173746586)\n",
      "L2T1_lora_out: (0.005800120532512665, 1.777348734322004e-05, -0.005808410234749317)\n",
      "\n",
      "L2T2_lora_B_out: (0.007947945967316628, 1.9771698134718463e-05, -0.008615369908511639)\n",
      "L2T2_lora_out: (0.015895891934633255, 3.9543396269436926e-05, -0.017230739817023277)\n",
      "\n",
      "g_out: (0.013820599764585495, 2.176991074520629e-05, -0.01368723250925541)\n",
      "L1T2_out: (0.021224144846200943, 2.3304242859012447e-05, -0.019293058663606644)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.12483658641576767, -0.00019578250066842884, -0.1404263824224472)\n",
      "L1T1_lora_out: (0.24967317283153534, -0.0003915650013368577, -0.2808527648448944)\n",
      "\n",
      "L2T1_lora_B_out: (0.06981644034385681, -0.00014372853911481798, -0.05728747323155403)\n",
      "L2T1_lora_out: (0.13963288068771362, -0.00028745707822963595, -0.11457494646310806)\n",
      "\n",
      "L2T2_lora_B_out: (0.2789832651615143, -0.0006571258418262005, -0.2689532935619354)\n",
      "L2T2_lora_out: (0.5579665303230286, -0.001314251683652401, -0.5379065871238708)\n",
      "\n",
      "g_out: (0.4425712823867798, -0.001026794547215104, -0.4616132378578186)\n",
      "L1T2_out: (0.6266930103302002, -0.0014183595776557922, -0.7232012748718262)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07870565354824066, -3.41939230565913e-05, -0.1484946459531784)\n",
      "L1T1_lora_out: (0.15741130709648132, -6.83878461131826e-05, -0.2969892919063568)\n",
      "\n",
      "L2T1_lora_B_out: (0.02341843768954277, 4.242342765792273e-05, -0.030302315950393677)\n",
      "L2T1_lora_out: (0.04683687537908554, 8.484685531584546e-05, -0.060604631900787354)\n",
      "\n",
      "L2T2_lora_B_out: (0.12094278633594513, -3.587207538657822e-05, -0.17580482363700867)\n",
      "L2T2_lora_out: (0.24188557267189026, -7.174415077315643e-05, -0.35160964727401733)\n",
      "\n",
      "g_out: (0.21482296288013458, -0.0001565910060890019, -0.3397166132926941)\n",
      "L1T2_out: (0.347160667181015, -0.00022497893951367587, -0.6147969365119934)\n",
      "\n",
      "================================================================\n",
      "model.layers.26.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.25.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03027789108455181, 1.8963792172144167e-05, -0.03136773034930229)\n",
      "L1T1_lora_out: (0.06055578216910362, 3.7927584344288334e-05, -0.06273546069860458)\n",
      "\n",
      "L2T1_lora_B_out: (0.023292331025004387, -1.919908754643984e-05, -0.025393875315785408)\n",
      "L2T1_lora_out: (0.046584662050008774, -3.839817509287968e-05, -0.050787750631570816)\n",
      "\n",
      "L2T2_lora_B_out: (0.06640127301216125, 8.220915333367884e-06, -0.06757806986570358)\n",
      "L2T2_lora_out: (0.1328025460243225, 1.644183066673577e-05, -0.13515613973140717)\n",
      "\n",
      "g_out: (0.10281822085380554, 5.484005669131875e-05, -0.1083991602063179)\n",
      "L1T2_out: (0.16202108561992645, 9.276767377741635e-05, -0.17113462090492249)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05051165819168091, 0.00022687119781039655, -0.043467290699481964)\n",
      "L1T1_lora_out: (0.10102331638336182, 0.0004537423956207931, -0.08693458139896393)\n",
      "\n",
      "L2T1_lora_B_out: (0.046359941363334656, -5.096386303193867e-06, -0.06423275917768478)\n",
      "L2T1_lora_out: (0.09271988272666931, -1.0192772606387734e-05, -0.12846551835536957)\n",
      "\n",
      "L2T2_lora_B_out: (0.1265273094177246, 0.00037283566780388355, -0.12545427680015564)\n",
      "L2T2_lora_out: (0.2530546188354492, 0.0007456713356077671, -0.2509085536003113)\n",
      "\n",
      "g_out: (0.18621280789375305, 0.000755863671656698, -0.18636246025562286)\n",
      "L1T2_out: (0.27911022305488586, 0.0012096064165234566, -0.25794240832328796)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08201673626899719, -0.0004631623160094023, -0.06831909716129303)\n",
      "L1T1_lora_out: (0.16403347253799438, -0.0009263246320188046, -0.13663819432258606)\n",
      "\n",
      "L2T1_lora_B_out: (0.03628282621502876, 2.722528006415814e-05, -0.0360654778778553)\n",
      "L2T1_lora_out: (0.07256565243005753, 5.445056012831628e-05, -0.0721309557557106)\n",
      "\n",
      "L2T2_lora_B_out: (0.18303385376930237, -0.0003654495521914214, -0.1706499457359314)\n",
      "L2T2_lora_out: (0.36606770753860474, -0.0007308991043828428, -0.3412998914718628)\n",
      "\n",
      "g_out: (0.3019665479660034, -0.0007853497518226504, -0.2691689431667328)\n",
      "L1T2_out: (0.44490647315979004, -0.0017116740345954895, -0.38715922832489014)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0036048470064997673, -1.2515246453403961e-05, -0.0033954717218875885)\n",
      "L1T1_lora_out: (0.007209694012999535, -2.5030492906807922e-05, -0.006790943443775177)\n",
      "\n",
      "L2T1_lora_B_out: (0.003831808688119054, -2.779186161205871e-06, -0.0035142225679010153)\n",
      "L2T1_lora_out: (0.007663617376238108, -5.558372322411742e-06, -0.0070284451358020306)\n",
      "\n",
      "L2T2_lora_B_out: (0.005438849795609713, -2.9221233489806764e-05, -0.005586110055446625)\n",
      "L2T2_lora_out: (0.010877699591219425, -5.844246697961353e-05, -0.01117222011089325)\n",
      "\n",
      "g_out: (0.01003292016685009, -5.288409738568589e-05, -0.010291730985045433)\n",
      "L1T2_out: (0.017242614179849625, -7.791459211148322e-05, -0.015820346772670746)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09001218527555466, -0.00014374390593729913, -0.09328174591064453)\n",
      "L1T1_lora_out: (0.18002437055110931, -0.00028748781187459826, -0.18656349182128906)\n",
      "\n",
      "L2T1_lora_B_out: (0.058040790259838104, -0.00012310466263443232, -0.06125260144472122)\n",
      "L2T1_lora_out: (0.11608158051967621, -0.00024620932526886463, -0.12250520288944244)\n",
      "\n",
      "L2T2_lora_B_out: (0.22818976640701294, -0.00048590541700832546, -0.18087202310562134)\n",
      "L2T2_lora_out: (0.4563795328140259, -0.0009718108340166509, -0.3617440462112427)\n",
      "\n",
      "g_out: (0.341435968875885, -0.0007256015087477863, -0.31486719846725464)\n",
      "L1T2_out: (0.5051101446151733, -0.0010130894370377064, -0.5014306902885437)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09159915894269943, 7.425796502502635e-05, -0.08380959182977676)\n",
      "L1T1_lora_out: (0.18319831788539886, 0.0001485159300500527, -0.16761918365955353)\n",
      "\n",
      "L2T1_lora_B_out: (0.03657735511660576, -3.122276029898785e-05, -0.04035994037985802)\n",
      "L2T1_lora_out: (0.07315471023321152, -6.24455205979757e-05, -0.08071988075971603)\n",
      "\n",
      "L2T2_lora_B_out: (0.12651287019252777, 0.00011480433749966323, -0.1657465696334839)\n",
      "L2T2_lora_out: (0.25302574038505554, 0.00022960867499932647, -0.3314931392669678)\n",
      "\n",
      "g_out: (0.23922277987003326, 0.0002920542028732598, -0.2700888514518738)\n",
      "L1T2_out: (0.42242109775543213, 0.00044057000195607543, -0.4349867105484009)\n",
      "\n",
      "================================================================\n",
      "model.layers.25.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.24.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06468964368104935, 2.2678286768496037e-06, -0.07088635116815567)\n",
      "L1T1_lora_out: (0.1293792873620987, 4.535657353699207e-06, -0.14177270233631134)\n",
      "\n",
      "L2T1_lora_B_out: (0.03970795497298241, 2.0377539840410464e-05, -0.042648207396268845)\n",
      "L2T1_lora_out: (0.07941590994596481, 4.075507968082093e-05, -0.08529641479253769)\n",
      "\n",
      "L2T2_lora_B_out: (0.12174283713102341, 0.00010340951848775148, -0.12635928392410278)\n",
      "L2T2_lora_out: (0.24348567426204681, 0.00020681903697550297, -0.25271856784820557)\n",
      "\n",
      "g_out: (0.21393290162086487, 0.0001660640409681946, -0.24065254628658295)\n",
      "L1T2_out: (0.3372119069099426, 0.00017059952369891107, -0.3824252486228943)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0642046332359314, -0.0004697496769949794, -0.057058338075876236)\n",
      "L1T1_lora_out: (0.1284092664718628, -0.0009394993539899588, -0.11411667615175247)\n",
      "\n",
      "L2T1_lora_B_out: (0.07614144682884216, 0.00013674565707333386, -0.0823153555393219)\n",
      "L2T1_lora_out: (0.15228289365768433, 0.0002734913141466677, -0.1646307110786438)\n",
      "\n",
      "L2T2_lora_B_out: (0.17449386417865753, -0.0007659379625692964, -0.22047549486160278)\n",
      "L2T2_lora_out: (0.34898772835731506, -0.0015318759251385927, -0.44095098972320557)\n",
      "\n",
      "g_out: (0.27532488107681274, -0.0018053672974929214, -0.32946979999542236)\n",
      "L1T2_out: (0.37977334856987, -0.0027448665350675583, -0.44358646869659424)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.08817573636770248, -0.0005522437277249992, -0.0868409276008606)\n",
      "L1T1_lora_out: (0.17635147273540497, -0.0011044874554499984, -0.1736818552017212)\n",
      "\n",
      "L2T1_lora_B_out: (0.03614717721939087, 0.00011011690367013216, -0.032902054488658905)\n",
      "L2T1_lora_out: (0.07229435443878174, 0.00022023380734026432, -0.06580410897731781)\n",
      "\n",
      "L2T2_lora_B_out: (0.152267187833786, -0.000540196371730417, -0.16087570786476135)\n",
      "L2T2_lora_out: (0.304534375667572, -0.001080392743460834, -0.3217514157295227)\n",
      "\n",
      "g_out: (0.26312652230262756, -0.001300626783631742, -0.277008056640625)\n",
      "L1T2_out: (0.43947798013687134, -0.0024051140062510967, -0.43890661001205444)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.017425281926989555, 2.8091635613236576e-06, -0.02085763029754162)\n",
      "L1T1_lora_out: (0.03485056385397911, 5.618327122647315e-06, -0.04171526059508324)\n",
      "\n",
      "L2T1_lora_B_out: (0.004495422821491957, 6.435251407310716e-07, -0.004603879526257515)\n",
      "L2T1_lora_out: (0.008990845642983913, 1.2870502814621432e-06, -0.00920775905251503)\n",
      "\n",
      "L2T2_lora_B_out: (0.02318848855793476, 2.190962504755589e-06, -0.02160515822470188)\n",
      "L2T2_lora_out: (0.04637697711586952, 4.381925009511178e-06, -0.04321031644940376)\n",
      "\n",
      "g_out: (0.04440204054117203, 3.094880867138272e-06, -0.04277552664279938)\n",
      "L1T2_out: (0.07376952469348907, 8.713206625543535e-06, -0.08449079096317291)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10485824197530746, -0.00011375082249287516, -0.11719952523708344)\n",
      "L1T1_lora_out: (0.20971648395061493, -0.00022750164498575032, -0.23439905047416687)\n",
      "\n",
      "L2T1_lora_B_out: (0.08818276226520538, -9.524791676085442e-05, -0.07367052882909775)\n",
      "L2T1_lora_out: (0.17636552453041077, -0.00019049583352170885, -0.1473410576581955)\n",
      "\n",
      "L2T2_lora_B_out: (0.325897216796875, -0.0005620823940262198, -0.25862041115760803)\n",
      "L2T2_lora_out: (0.65179443359375, -0.0011241647880524397, -0.5172408223152161)\n",
      "\n",
      "g_out: (0.4935806691646576, -0.0009336689254269004, -0.41384246945381165)\n",
      "L1T2_out: (0.6813768744468689, -0.001161170657724142, -0.6194674968719482)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07062763720750809, 8.671717660035938e-05, -0.07239200919866562)\n",
      "L1T1_lora_out: (0.14125527441501617, 0.00017343435320071876, -0.14478401839733124)\n",
      "\n",
      "L2T1_lora_B_out: (0.030183926224708557, 9.609309927327558e-06, -0.03411708027124405)\n",
      "L2T1_lora_out: (0.060367852449417114, 1.9218619854655117e-05, -0.0682341605424881)\n",
      "\n",
      "L2T2_lora_B_out: (0.11742895841598511, 4.5473116188077256e-05, -0.0957687646150589)\n",
      "L2T2_lora_out: (0.23485791683197021, 9.094623237615451e-05, -0.1915375292301178)\n",
      "\n",
      "g_out: (0.20850296318531036, 7.172765617724508e-05, -0.16703897714614868)\n",
      "L1T2_out: (0.3445643484592438, 0.00024516208213754, -0.3107334077358246)\n",
      "\n",
      "================================================================\n",
      "model.layers.24.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.23.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04153486713767052, 7.05811835359782e-05, -0.028756963089108467)\n",
      "L1T1_lora_out: (0.08306973427534103, 0.0001411623670719564, -0.057513926178216934)\n",
      "\n",
      "L2T1_lora_B_out: (0.03578381612896919, -1.6722757209208794e-05, -0.02738792821764946)\n",
      "L2T1_lora_out: (0.07156763225793839, -3.344551441841759e-05, -0.05477585643529892)\n",
      "\n",
      "L2T2_lora_B_out: (0.06883006542921066, 0.00013957120245322585, -0.06985796242952347)\n",
      "L2T2_lora_out: (0.13766013085842133, 0.0002791424049064517, -0.13971592485904694)\n",
      "\n",
      "g_out: (0.1303366869688034, 0.0003125878283753991, -0.13113710284233093)\n",
      "L1T2_out: (0.19137388467788696, 0.0004537502536550164, -0.18603798747062683)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023853003978729248, 2.2146099581732415e-05, -0.02714403346180916)\n",
      "L1T1_lora_out: (0.047706007957458496, 4.429219916346483e-05, -0.05428806692361832)\n",
      "\n",
      "L2T1_lora_B_out: (0.018450163304805756, 4.417532909428701e-05, -0.018730618059635162)\n",
      "L2T1_lora_out: (0.03690032660961151, 8.835065818857402e-05, -0.037461236119270325)\n",
      "\n",
      "L2T2_lora_B_out: (0.04508701711893082, 0.0001561877434141934, -0.06431790441274643)\n",
      "L2T2_lora_out: (0.09017403423786163, 0.0003123754868283868, -0.12863580882549286)\n",
      "\n",
      "g_out: (0.07666436582803726, 0.0002240249013993889, -0.09361333400011063)\n",
      "L1T2_out: (0.11957015097141266, 0.00026831706054508686, -0.1392572522163391)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02743355557322502, -0.00018647310207597911, -0.02950183115899563)\n",
      "L1T1_lora_out: (0.05486711114645004, -0.00037294620415195823, -0.05900366231799126)\n",
      "\n",
      "L2T1_lora_B_out: (0.027584346011281013, -3.853044836432673e-05, -0.031190769746899605)\n",
      "L2T1_lora_out: (0.05516869202256203, -7.706089672865346e-05, -0.06238153949379921)\n",
      "\n",
      "L2T2_lora_B_out: (0.0793999508023262, -0.0005357047775760293, -0.07069573551416397)\n",
      "L2T2_lora_out: (0.1587999016046524, -0.0010714095551520586, -0.14139147102832794)\n",
      "\n",
      "g_out: (0.11873194575309753, -0.0009943486656993628, -0.12538960576057434)\n",
      "L1T2_out: (0.16389186680316925, -0.001367294928058982, -0.1843932718038559)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003656519576907158, -6.481766376964515e-06, -0.003336856374517083)\n",
      "L1T1_lora_out: (0.007313039153814316, -1.296353275392903e-05, -0.006673712749034166)\n",
      "\n",
      "L2T1_lora_B_out: (0.0027257222682237625, -8.822311428957619e-06, -0.002507896162569523)\n",
      "L2T1_lora_out: (0.005451444536447525, -1.7644622857915238e-05, -0.005015792325139046)\n",
      "\n",
      "L2T2_lora_B_out: (0.007358622271567583, -1.7225267583853565e-05, -0.007364440709352493)\n",
      "L2T2_lora_out: (0.014717244543135166, -3.445053516770713e-05, -0.014728881418704987)\n",
      "\n",
      "g_out: (0.012040037661790848, -1.6805912309791893e-05, -0.012075098231434822)\n",
      "L1T2_out: (0.018583819270133972, -2.9769445973215625e-05, -0.01850361004471779)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06498061865568161, -3.969310910179047e-06, -0.06972218304872513)\n",
      "L1T1_lora_out: (0.12996123731136322, -7.938621820358094e-06, -0.13944436609745026)\n",
      "\n",
      "L2T1_lora_B_out: (0.08357243239879608, 8.093420547083952e-06, -0.0765034630894661)\n",
      "L2T1_lora_out: (0.16714486479759216, 1.6186841094167903e-05, -0.1530069261789322)\n",
      "\n",
      "L2T2_lora_B_out: (0.13791458308696747, 3.44988948199898e-05, -0.12686146795749664)\n",
      "L2T2_lora_out: (0.27582916617393494, 6.89977896399796e-05, -0.2537229359149933)\n",
      "\n",
      "g_out: (0.18825840950012207, 5.281098128762096e-05, -0.18809834122657776)\n",
      "L1T2_out: (0.3171684145927429, 4.487234036787413e-05, -0.3051910996437073)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05597803741693497, -4.527699729806045e-06, -0.05096694827079773)\n",
      "L1T1_lora_out: (0.11195607483386993, -9.05539945961209e-06, -0.10193389654159546)\n",
      "\n",
      "L2T1_lora_B_out: (0.054534949362277985, -8.892498044588137e-06, -0.061740368604660034)\n",
      "L2T1_lora_out: (0.10906989872455597, -1.7784996089176275e-05, -0.12348073720932007)\n",
      "\n",
      "L2T2_lora_B_out: (0.11276604980230331, 3.912400188710308e-06, -0.13692790269851685)\n",
      "L2T2_lora_out: (0.22553209960460663, 7.824800377420615e-06, -0.2738558053970337)\n",
      "\n",
      "g_out: (0.16365090012550354, 2.5609762815292925e-05, -0.22792258858680725)\n",
      "L1T2_out: (0.25133252143859863, 1.6554347894270904e-05, -0.3286867141723633)\n",
      "\n",
      "================================================================\n",
      "model.layers.23.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.22.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07340298593044281, 0.00013746177137363702, -0.06661555916070938)\n",
      "L1T1_lora_out: (0.14680597186088562, 0.00027492354274727404, -0.13323111832141876)\n",
      "\n",
      "L2T1_lora_B_out: (0.020218083634972572, -4.3292799091432244e-05, -0.02121758833527565)\n",
      "L2T1_lora_out: (0.040436167269945145, -8.658559818286449e-05, -0.0424351766705513)\n",
      "\n",
      "L2T2_lora_B_out: (0.13037699460983276, 0.00010856195876840502, -0.11110986024141312)\n",
      "L2T2_lora_out: (0.2607539892196655, 0.00021712391753681004, -0.22221972048282623)\n",
      "\n",
      "g_out: (0.24685919284820557, 0.0003037096175830811, -0.2170647531747818)\n",
      "L1T2_out: (0.3936651647090912, 0.0005786331603303552, -0.35029587149620056)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10793514549732208, -0.00020717523875646293, -0.10407504439353943)\n",
      "L1T1_lora_out: (0.21587029099464417, -0.00041435047751292586, -0.20815008878707886)\n",
      "\n",
      "L2T1_lora_B_out: (0.011621348559856415, 6.535203283419833e-05, -0.013061707839369774)\n",
      "L2T1_lora_out: (0.02324269711971283, 0.00013070406566839665, -0.026123415678739548)\n",
      "\n",
      "L2T2_lora_B_out: (0.13316193222999573, 0.00010152578761335462, -0.12959039211273193)\n",
      "L2T2_lora_out: (0.26632386445999146, 0.00020305157522670925, -0.25918078422546387)\n",
      "\n",
      "g_out: (0.2540023624897003, 7.234762597363442e-05, -0.2523180842399597)\n",
      "L1T2_out: (0.44962167739868164, -0.0003420028369873762, -0.43130648136138916)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09679470956325531, 0.0003025488113053143, -0.08037266880273819)\n",
      "L1T1_lora_out: (0.19358941912651062, 0.0006050976226106286, -0.16074533760547638)\n",
      "\n",
      "L2T1_lora_B_out: (0.032688967883586884, 7.220963016152382e-05, -0.04422757402062416)\n",
      "L2T1_lora_out: (0.06537793576717377, 0.00014441926032304764, -0.08845514804124832)\n",
      "\n",
      "L2T2_lora_B_out: (0.19197231531143188, 0.0005745855160057545, -0.195174902677536)\n",
      "L2T2_lora_out: (0.38394463062286377, 0.001149171032011509, -0.390349805355072)\n",
      "\n",
      "g_out: (0.34675493836402893, 0.0010047516552731395, -0.3018946647644043)\n",
      "L1T2_out: (0.5403443574905396, 0.0016098495107144117, -0.4208661615848541)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0022695069201290607, -3.2961061151581816e-06, -0.0026127295568585396)\n",
      "L1T1_lora_out: (0.0045390138402581215, -6.592212230316363e-06, -0.005225459113717079)\n",
      "\n",
      "L2T1_lora_B_out: (0.002423873171210289, 1.0747597116278484e-05, -0.0024005232844501734)\n",
      "L2T1_lora_out: (0.004847746342420578, 2.149519423255697e-05, -0.004801046568900347)\n",
      "\n",
      "L2T2_lora_B_out: (0.005610727239400148, 1.1690687642840203e-05, -0.006168422754853964)\n",
      "L2T2_lora_out: (0.011221454478800297, 2.3381375285680406e-05, -0.012336845509707928)\n",
      "\n",
      "g_out: (0.007292920257896185, 1.886169570752827e-06, -0.008396663703024387)\n",
      "L1T2_out: (0.01123528927564621, -4.706041181634646e-06, -0.012121826410293579)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.13058917224407196, 3.0399229217437096e-05, -0.12491252273321152)\n",
      "L1T1_lora_out: (0.2611783444881439, 6.079845843487419e-05, -0.24982504546642303)\n",
      "\n",
      "L2T1_lora_B_out: (0.06129235774278641, -0.00018552815890870988, -0.065680593252182)\n",
      "L2T1_lora_out: (0.12258471548557281, -0.00037105631781741977, -0.131361186504364)\n",
      "\n",
      "L2T2_lora_B_out: (0.22514942288398743, -0.00022798284771852195, -0.27288755774497986)\n",
      "L2T2_lora_out: (0.45029884576797485, -0.0004559656954370439, -0.5457751154899597)\n",
      "\n",
      "g_out: (0.4258660078048706, -8.490952313877642e-05, -0.4688558280467987)\n",
      "L1T2_out: (0.6870443820953369, -2.4111055608955212e-05, -0.7186808586120605)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07281793653964996, -7.533062307629734e-05, -0.0982588529586792)\n",
      "L1T1_lora_out: (0.14563587307929993, -0.00015066124615259469, -0.1965177059173584)\n",
      "\n",
      "L2T1_lora_B_out: (0.04815634340047836, 3.223864041501656e-05, -0.039658322930336)\n",
      "L2T1_lora_out: (0.09631268680095673, 6.447728083003312e-05, -0.079316645860672)\n",
      "\n",
      "L2T2_lora_B_out: (0.15908430516719818, -9.157074964605272e-05, -0.19169794023036957)\n",
      "L2T2_lora_out: (0.31816861033439636, -0.00018314149929210544, -0.38339588046073914)\n",
      "\n",
      "g_out: (0.2741505205631256, -0.0002476187073625624, -0.33806005120277405)\n",
      "L1T2_out: (0.41978639364242554, -0.00039827998261898756, -0.5345777273178101)\n",
      "\n",
      "================================================================\n",
      "model.layers.22.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.21.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04514347016811371, -1.6244866856141016e-05, -0.058563198894262314)\n",
      "L1T1_lora_out: (0.09028694033622742, -3.248973371228203e-05, -0.11712639778852463)\n",
      "\n",
      "L2T1_lora_B_out: (0.04491906240582466, -0.00011078299576183781, -0.04457159340381622)\n",
      "L2T1_lora_out: (0.08983812481164932, -0.00022156599152367562, -0.08914318680763245)\n",
      "\n",
      "L2T2_lora_B_out: (0.12138388305902481, -0.0002726293168962002, -0.13427376747131348)\n",
      "L2T2_lora_out: (0.24276776611804962, -0.0005452586337924004, -0.26854753494262695)\n",
      "\n",
      "g_out: (0.18180032074451447, -0.0003236926859244704, -0.23809687793254852)\n",
      "L1T2_out: (0.2720872759819031, -0.00035618245601654053, -0.35522326827049255)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05271092429757118, 0.00022417509171646088, -0.03861602023243904)\n",
      "L1T1_lora_out: (0.10542184859514236, 0.00044835018343292177, -0.07723204046487808)\n",
      "\n",
      "L2T1_lora_B_out: (0.05358560010790825, 0.0002586949849501252, -0.04499181732535362)\n",
      "L2T1_lora_out: (0.1071712002158165, 0.0005173899699002504, -0.08998363465070724)\n",
      "\n",
      "L2T2_lora_B_out: (0.09876186400651932, 0.0008225933997891843, -0.08894315361976624)\n",
      "L2T2_lora_out: (0.19752372801303864, 0.0016451867995783687, -0.17788630723953247)\n",
      "\n",
      "g_out: (0.16199028491973877, 0.00112779694609344, -0.14608962833881378)\n",
      "L1T2_out: (0.26741212606430054, 0.0015761468093842268, -0.22332167625427246)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.037625476717948914, 0.00027968856738880277, -0.03069409169256687)\n",
      "L1T1_lora_out: (0.07525095343589783, 0.0005593771347776055, -0.06138818338513374)\n",
      "\n",
      "L2T1_lora_B_out: (0.015877410769462585, 4.711151632363908e-05, -0.016481619328260422)\n",
      "L2T1_lora_out: (0.03175482153892517, 9.422303264727816e-05, -0.032963238656520844)\n",
      "\n",
      "L2T2_lora_B_out: (0.054812222719192505, 0.0005817610071972013, -0.06468851119279861)\n",
      "L2T2_lora_out: (0.10962444543838501, 0.0011635220143944025, -0.12937702238559723)\n",
      "\n",
      "g_out: (0.09552155435085297, 0.0010692989453673363, -0.13153420388698578)\n",
      "L1T2_out: (0.1707725077867508, 0.0016286758473142982, -0.18858464062213898)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003784259781241417, 1.5557592405457399e-06, -0.003980747889727354)\n",
      "L1T1_lora_out: (0.007568519562482834, 3.1115184810914798e-06, -0.007961495779454708)\n",
      "\n",
      "L2T1_lora_B_out: (0.0036211924161762, -2.5809372345975135e-06, -0.0031473676208406687)\n",
      "L2T1_lora_out: (0.0072423848323524, -5.161874469195027e-06, -0.006294735241681337)\n",
      "\n",
      "L2T2_lora_B_out: (0.008766108192503452, 1.5635941963410005e-05, -0.009219629690051079)\n",
      "L2T2_lora_out: (0.017532216385006905, 3.127188392682001e-05, -0.018439259380102158)\n",
      "\n",
      "g_out: (0.012528419494628906, 3.643374293460511e-05, -0.015497528947889805)\n",
      "L1T2_out: (0.01781105063855648, 3.9545273466501385e-05, -0.022764159366488457)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.17555981874465942, 4.1683462768560275e-05, -0.08660012483596802)\n",
      "L1T1_lora_out: (0.35111963748931885, 8.336692553712055e-05, -0.17320024967193604)\n",
      "\n",
      "L2T1_lora_B_out: (0.07297007739543915, -0.00022187213471625, -0.07855634391307831)\n",
      "L2T1_lora_out: (0.1459401547908783, -0.0004437442694325, -0.15711268782615662)\n",
      "\n",
      "L2T2_lora_B_out: (0.3553284704685211, -0.00024110026424750686, -0.17814475297927856)\n",
      "L2T2_lora_out: (0.7106569409370422, -0.0004822005284950137, -0.35628950595855713)\n",
      "\n",
      "g_out: (0.6107256412506104, -3.845627361442894e-05, -0.2673141062259674)\n",
      "L1T2_out: (0.9618452787399292, 4.4910724682267755e-05, -0.42813628911972046)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06561059504747391, -2.2369389625964686e-05, -0.06136680394411087)\n",
      "L1T1_lora_out: (0.13122119009494781, -4.473877925192937e-05, -0.12273360788822174)\n",
      "\n",
      "L2T1_lora_B_out: (0.028712354600429535, 1.6891284758457914e-05, -0.03521035984158516)\n",
      "L2T1_lora_out: (0.05742470920085907, 3.378256951691583e-05, -0.07042071968317032)\n",
      "\n",
      "L2T2_lora_B_out: (0.14748884737491608, -2.572720950411167e-05, -0.11719914525747299)\n",
      "L2T2_lora_out: (0.29497769474983215, -5.145441900822334e-05, -0.23439829051494598)\n",
      "\n",
      "g_out: (0.2638189196586609, -8.523705037077889e-05, -0.18862785398960114)\n",
      "L1T2_out: (0.3950400948524475, -0.00012997587327845395, -0.2911272644996643)\n",
      "\n",
      "================================================================\n",
      "model.layers.21.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.20.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.09646672755479813, 0.0001406486117048189, -0.0797206461429596)\n",
      "L1T1_lora_out: (0.19293345510959625, 0.0002812972234096378, -0.1594412922859192)\n",
      "\n",
      "L2T1_lora_B_out: (0.06340578198432922, -1.7162881704280153e-05, -0.05506608262658119)\n",
      "L2T1_lora_out: (0.12681156396865845, -3.4325763408560306e-05, -0.11013216525316238)\n",
      "\n",
      "L2T2_lora_B_out: (0.19729633629322052, 0.00011231651296839118, -0.12643103301525116)\n",
      "L2T2_lora_out: (0.39459267258644104, 0.00022463302593678236, -0.2528620660305023)\n",
      "\n",
      "g_out: (0.3262951374053955, 0.0002589588693808764, -0.23437675833702087)\n",
      "L1T2_out: (0.5192285776138306, 0.0005402559763751924, -0.39381805062294006)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04046238958835602, -0.00022414352861233056, -0.03854271024465561)\n",
      "L1T1_lora_out: (0.08092477917671204, -0.0004482870572246611, -0.07708542048931122)\n",
      "\n",
      "L2T1_lora_B_out: (0.03399435803294182, 6.54636460239999e-05, -0.029969602823257446)\n",
      "L2T1_lora_out: (0.06798871606588364, 0.0001309272920479998, -0.05993920564651489)\n",
      "\n",
      "L2T2_lora_B_out: (0.08954138308763504, -0.0003648732672445476, -0.07501338422298431)\n",
      "L2T2_lora_out: (0.17908276617527008, -0.0007297465344890952, -0.15002676844596863)\n",
      "\n",
      "g_out: (0.15508802235126495, -0.0008606740739196539, -0.1251946985721588)\n",
      "L1T2_out: (0.22820183634757996, -0.0013089608401060104, -0.19203650951385498)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03603668883442879, -9.66542138485238e-05, -0.040707290172576904)\n",
      "L1T1_lora_out: (0.07207337766885757, -0.0001933084276970476, -0.08141458034515381)\n",
      "\n",
      "L2T1_lora_B_out: (0.011457523331046104, 2.450255851726979e-05, -0.011786078102886677)\n",
      "L2T1_lora_out: (0.02291504666209221, 4.900511703453958e-05, -0.023572156205773354)\n",
      "\n",
      "L2T2_lora_B_out: (0.050517626106739044, -0.0002536186366342008, -0.05033627897500992)\n",
      "L2T2_lora_out: (0.10103525221347809, -0.0005072372732684016, -0.10067255795001984)\n",
      "\n",
      "g_out: (0.09614637494087219, -0.0005562423029914498, -0.09932620078325272)\n",
      "L1T2_out: (0.1559133380651474, -0.0007495508762076497, -0.18074077367782593)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0038200404960662127, -9.704041076474823e-06, -0.004483965691179037)\n",
      "L1T1_lora_out: (0.007640080992132425, -1.9408082152949646e-05, -0.008967931382358074)\n",
      "\n",
      "L2T1_lora_B_out: (0.0027703437954187393, 4.287782303435961e-06, -0.0025688321329653263)\n",
      "L2T1_lora_out: (0.005540687590837479, 8.575564606871922e-06, -0.005137664265930653)\n",
      "\n",
      "L2T2_lora_B_out: (0.007688026875257492, -1.2429984053596854e-05, -0.010091813281178474)\n",
      "L2T2_lora_out: (0.015376053750514984, -2.485996810719371e-05, -0.02018362656235695)\n",
      "\n",
      "g_out: (0.013189848512411118, -3.343553180457093e-05, -0.017357755452394485)\n",
      "L1T2_out: (0.020610591396689415, -5.2843613957520574e-05, -0.024372033774852753)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07119064033031464, -7.867512613302097e-05, -0.06332685053348541)\n",
      "L1T1_lora_out: (0.14238128066062927, -0.00015735025226604193, -0.12665370106697083)\n",
      "\n",
      "L2T1_lora_B_out: (0.07324513792991638, -7.050044951029122e-05, -0.06776707619428635)\n",
      "L2T1_lora_out: (0.14649027585983276, -0.00014100089902058244, -0.1355341523885727)\n",
      "\n",
      "L2T2_lora_B_out: (0.12957759201526642, -0.00014546187594532967, -0.10691981017589569)\n",
      "L2T2_lora_out: (0.25915518403053284, -0.00029092375189065933, -0.21383962035179138)\n",
      "\n",
      "g_out: (0.25214773416519165, -0.0001499229110777378, -0.18983455002307892)\n",
      "L1T2_out: (0.3840683698654175, -0.00030727317789569497, -0.3095603585243225)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07756835967302322, 1.7641799558987259e-06, -0.09267299622297287)\n",
      "L1T1_lora_out: (0.15513671934604645, 3.5283599117974518e-06, -0.18534599244594574)\n",
      "\n",
      "L2T1_lora_B_out: (0.03101213090121746, -3.7268669984769076e-05, -0.029602957889437675)\n",
      "L2T1_lora_out: (0.06202426180243492, -7.453733996953815e-05, -0.05920591577887535)\n",
      "\n",
      "L2T2_lora_B_out: (0.13088403642177582, -8.25176975922659e-05, -0.11355305463075638)\n",
      "L2T2_lora_out: (0.26176807284355164, -0.0001650353951845318, -0.22710610926151276)\n",
      "\n",
      "g_out: (0.23615288734436035, -9.049793152371421e-05, -0.20534776151180267)\n",
      "L1T2_out: (0.3912895917892456, -8.696971053723246e-05, -0.3906937539577484)\n",
      "\n",
      "================================================================\n",
      "model.layers.20.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.19.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028422033414244652, 7.229445327538997e-05, -0.025388384237885475)\n",
      "L1T1_lora_out: (0.056844066828489304, 0.00014458890655077994, -0.05077676847577095)\n",
      "\n",
      "L2T1_lora_B_out: (0.03351999446749687, -6.735615170327947e-05, -0.03111431933939457)\n",
      "L2T1_lora_out: (0.06703998893499374, -0.00013471230340655893, -0.06222863867878914)\n",
      "\n",
      "L2T2_lora_B_out: (0.07204140722751617, -5.432644684333354e-05, -0.07184667885303497)\n",
      "L2T2_lora_out: (0.14408281445503235, -0.00010865289368666708, -0.14369335770606995)\n",
      "\n",
      "g_out: (0.09953822195529938, 2.6059409719891846e-05, -0.10513822734355927)\n",
      "L1T2_out: (0.13973408937454224, 0.000170648330822587, -0.1448083221912384)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02304985001683235, 1.0898851542151533e-05, -0.027012929320335388)\n",
      "L1T1_lora_out: (0.0460997000336647, 2.1797703084303066e-05, -0.054025858640670776)\n",
      "\n",
      "L2T1_lora_B_out: (0.05003189668059349, 2.3969430912984535e-05, -0.04281285032629967)\n",
      "L2T1_lora_out: (0.10006379336118698, 4.793886182596907e-05, -0.08562570065259933)\n",
      "\n",
      "L2T2_lora_B_out: (0.0701252818107605, -0.00023722785408608615, -0.060855377465486526)\n",
      "L2T2_lora_out: (0.140250563621521, -0.0004744557081721723, -0.12171075493097305)\n",
      "\n",
      "g_out: (0.08617471903562546, -0.0005223946645855904, -0.0902266725897789)\n",
      "L1T2_out: (0.1170477494597435, -0.0005005968851037323, -0.14425253868103027)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.016294194385409355, 7.981724593264516e-06, -0.01713554561138153)\n",
      "L1T1_lora_out: (0.03258838877081871, 1.5963449186529033e-05, -0.03427109122276306)\n",
      "\n",
      "L2T1_lora_B_out: (0.033274728804826736, 1.4230502529244404e-05, -0.037412334233522415)\n",
      "L2T1_lora_out: (0.06654945760965347, 2.846100505848881e-05, -0.07482466846704483)\n",
      "\n",
      "L2T2_lora_B_out: (0.05651818588376045, 0.00014270016981754452, -0.05634741112589836)\n",
      "L2T2_lora_out: (0.1130363717675209, 0.00028540033963508904, -0.11269482225179672)\n",
      "\n",
      "g_out: (0.0744565799832344, 0.0002569393254816532, -0.07855275273323059)\n",
      "L1T2_out: (0.09537044167518616, 0.00027290283469483256, -0.09358157217502594)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0038487573619931936, -3.5939551707997452e-06, -0.004111290443688631)\n",
      "L1T1_lora_out: (0.007697514723986387, -7.1879103415994905e-06, -0.008222580887377262)\n",
      "\n",
      "L2T1_lora_B_out: (0.0022160429507493973, -1.39290159495431e-05, -0.0018946564523503184)\n",
      "L2T1_lora_out: (0.0044320859014987946, -2.78580318990862e-05, -0.003789312904700637)\n",
      "\n",
      "L2T2_lora_B_out: (0.006958838552236557, -2.8902779376949184e-05, -0.007191071752458811)\n",
      "L2T2_lora_out: (0.013917677104473114, -5.780555875389837e-05, -0.014382143504917622)\n",
      "\n",
      "g_out: (0.013111324980854988, -2.9947526854812168e-05, -0.011024782434105873)\n",
      "L1T2_out: (0.017824944108724594, -3.7135436286916956e-05, -0.01880772039294243)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.07885190099477768, -5.7319386542076245e-05, -0.07647375017404556)\n",
      "L1T1_lora_out: (0.15770380198955536, -0.00011463877308415249, -0.15294750034809113)\n",
      "\n",
      "L2T1_lora_B_out: (0.0743071660399437, -0.00022807846835348755, -0.07914111763238907)\n",
      "L2T1_lora_out: (0.1486143320798874, -0.0004561569367069751, -0.15828223526477814)\n",
      "\n",
      "L2T2_lora_B_out: (0.2051869034767151, -0.0004805471980944276, -0.22904714941978455)\n",
      "L2T2_lora_out: (0.4103738069534302, -0.0009610943961888552, -0.4580942988395691)\n",
      "\n",
      "g_out: (0.30763310194015503, -0.0005049374303780496, -0.35214054584503174)\n",
      "L1T2_out: (0.44340699911117554, -0.0006195761961862445, -0.48897549510002136)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03576261177659035, -2.53134985541692e-05, -0.03681819513440132)\n",
      "L1T1_lora_out: (0.0715252235531807, -5.06269971083384e-05, -0.07363639026880264)\n",
      "\n",
      "L2T1_lora_B_out: (0.032113589346408844, -4.302120942156762e-06, -0.038185592740774155)\n",
      "L2T1_lora_out: (0.06422717869281769, -8.604241884313524e-06, -0.07637118548154831)\n",
      "\n",
      "L2T2_lora_B_out: (0.08307689428329468, 1.3962895536678843e-05, -0.06429644674062729)\n",
      "L2T2_lora_out: (0.16615378856658936, 2.7925791073357686e-05, -0.12859289348125458)\n",
      "\n",
      "g_out: (0.11900562793016434, 3.6530025681713596e-05, -0.10609876364469528)\n",
      "L1T2_out: (0.1713864803314209, -1.409695323673077e-05, -0.15286177396774292)\n",
      "\n",
      "================================================================\n",
      "model.layers.19.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.18.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02934861369431019, 5.0613758503459394e-05, -0.027487896382808685)\n",
      "L1T1_lora_out: (0.05869722738862038, 0.00010122751700691879, -0.05497579276561737)\n",
      "\n",
      "L2T1_lora_B_out: (0.02616288512945175, 6.418269185815006e-05, -0.024154575541615486)\n",
      "L2T1_lora_out: (0.0523257702589035, 0.00012836538371630013, -0.04830915108323097)\n",
      "\n",
      "L2T2_lora_B_out: (0.05868562310934067, 0.0002465544966980815, -0.06704159080982208)\n",
      "L2T2_lora_out: (0.11737124621868134, 0.000493108993396163, -0.13408318161964417)\n",
      "\n",
      "g_out: (0.09955398738384247, 0.0003647435805760324, -0.10924021154642105)\n",
      "L1T2_out: (0.1572485715150833, 0.0004659711557906121, -0.16384132206439972)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028431881219148636, 4.334902769187465e-05, -0.033733222633600235)\n",
      "L1T1_lora_out: (0.05686376243829727, 8.66980553837493e-05, -0.06746644526720047)\n",
      "\n",
      "L2T1_lora_B_out: (0.02352346107363701, 7.264958549058065e-05, -0.024048687890172005)\n",
      "L2T1_lora_out: (0.04704692214727402, 0.0001452991709811613, -0.04809737578034401)\n",
      "\n",
      "L2T2_lora_B_out: (0.06750713288784027, 0.00012521076132543385, -0.06676904857158661)\n",
      "L2T2_lora_out: (0.13501426577568054, 0.0002504215226508677, -0.13353809714317322)\n",
      "\n",
      "g_out: (0.10464656352996826, 0.00010512240987736732, -0.10237278789281845)\n",
      "L1T2_out: (0.1549646258354187, 0.00019182059622835368, -0.15765000879764557)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0716988667845726, 0.00016080087516456842, -0.04350154846906662)\n",
      "L1T1_lora_out: (0.1433977335691452, 0.00032160175032913685, -0.08700309693813324)\n",
      "\n",
      "L2T1_lora_B_out: (0.025398023426532745, 0.00010423212370369583, -0.02874436229467392)\n",
      "L2T1_lora_out: (0.05079604685306549, 0.00020846424740739167, -0.05748872458934784)\n",
      "\n",
      "L2T2_lora_B_out: (0.0592493899166584, 0.00010057977488031611, -0.05502763390541077)\n",
      "L2T2_lora_out: (0.1184987798333168, 0.00020115954976063222, -0.11005526781082153)\n",
      "\n",
      "g_out: (0.1151779368519783, -7.304518476303201e-06, -0.09800438582897186)\n",
      "L1T2_out: (0.22974373400211334, 0.00031429738737642765, -0.15932801365852356)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004276407416909933, 1.264320144400699e-05, -0.003231619717553258)\n",
      "L1T1_lora_out: (0.008552814833819866, 2.528640288801398e-05, -0.006463239435106516)\n",
      "\n",
      "L2T1_lora_B_out: (0.0029620786663144827, -4.68858115709736e-06, -0.0027826661244034767)\n",
      "L2T1_lora_out: (0.005924157332628965, -9.37716231419472e-06, -0.005565332248806953)\n",
      "\n",
      "L2T2_lora_B_out: (0.005979196634143591, 8.390019502257928e-06, -0.0053425924852490425)\n",
      "L2T2_lora_out: (0.011958393268287182, 1.6780039004515857e-05, -0.010685184970498085)\n",
      "\n",
      "g_out: (0.010409326292574406, 2.615719859022647e-05, -0.00956747680902481)\n",
      "L1T2_out: (0.017201753333210945, 5.144361057318747e-05, -0.014107739552855492)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.1033150926232338, -0.00017265921633224934, -0.09428894519805908)\n",
      "L1T1_lora_out: (0.2066301852464676, -0.0003453184326644987, -0.18857789039611816)\n",
      "\n",
      "L2T1_lora_B_out: (0.08736629039049149, -8.384661487070844e-05, -0.08722469955682755)\n",
      "L2T1_lora_out: (0.17473258078098297, -0.00016769322974141687, -0.1744493991136551)\n",
      "\n",
      "L2T2_lora_B_out: (0.22368839383125305, -0.00035393337020650506, -0.17946401238441467)\n",
      "L2T2_lora_out: (0.4473767876625061, -0.0007078667404130101, -0.35892802476882935)\n",
      "\n",
      "g_out: (0.3242076635360718, -0.000540173496119678, -0.31245437264442444)\n",
      "L1T2_out: (0.528129518032074, -0.0008854917832650244, -0.5010322332382202)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03395088389515877, -5.051722109783441e-05, -0.033487819135189056)\n",
      "L1T1_lora_out: (0.06790176779031754, -0.00010103444219566882, -0.06697563827037811)\n",
      "\n",
      "L2T1_lora_B_out: (0.038935769349336624, 2.0605848476407118e-05, -0.03553091734647751)\n",
      "L2T1_lora_out: (0.07787153869867325, 4.1211696952814236e-05, -0.07106183469295502)\n",
      "\n",
      "L2T2_lora_B_out: (0.07878677546977997, -1.724130015645642e-05, -0.055119872093200684)\n",
      "L2T2_lora_out: (0.15757355093955994, -3.448260031291284e-05, -0.11023974418640137)\n",
      "\n",
      "g_out: (0.09151963889598846, -7.569430454168469e-05, -0.09328752756118774)\n",
      "L1T2_out: (0.1594214141368866, -0.00017672876128926873, -0.14815816283226013)\n",
      "\n",
      "================================================================\n",
      "model.layers.18.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.17.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.015323835425078869, -6.89866556058405e-06, -0.016081886366009712)\n",
      "L1T1_lora_out: (0.030647670850157738, -1.37973311211681e-05, -0.032163772732019424)\n",
      "\n",
      "L2T1_lora_B_out: (0.021136393770575523, -3.920811286661774e-05, -0.020697781816124916)\n",
      "L2T1_lora_out: (0.04227278754115105, -7.841622573323548e-05, -0.04139556363224983)\n",
      "\n",
      "L2T2_lora_B_out: (0.039289169013500214, -4.181402618996799e-05, -0.044252194464206696)\n",
      "L2T2_lora_out: (0.07857833802700043, -8.362805237993598e-05, -0.08850438892841339)\n",
      "\n",
      "g_out: (0.07224124670028687, -5.211865754972678e-06, -0.05791333317756653)\n",
      "L1T2_out: (0.10288891941308975, -1.9009137758985162e-05, -0.07847952097654343)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.027082737535238266, -0.0001908688573166728, -0.023709965869784355)\n",
      "L1T1_lora_out: (0.05416547507047653, -0.0003817377146333456, -0.04741993173956871)\n",
      "\n",
      "L2T1_lora_B_out: (0.02164209820330143, 5.765436071669683e-05, -0.01989630050957203)\n",
      "L2T1_lora_out: (0.04328419640660286, 0.00011530872143339366, -0.03979260101914406)\n",
      "\n",
      "L2T2_lora_B_out: (0.04678482189774513, -0.00017885424313135445, -0.05981127917766571)\n",
      "L2T2_lora_out: (0.09356964379549026, -0.0003577084862627089, -0.11962255835533142)\n",
      "\n",
      "g_out: (0.07618146389722824, -0.0004730172804556787, -0.08699798583984375)\n",
      "L1T2_out: (0.11757709085941315, -0.0008547549950890243, -0.11755919456481934)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03029054030776024, 2.932424422397162e-06, -0.03001854568719864)\n",
      "L1T1_lora_out: (0.06058108061552048, 5.864848844794324e-06, -0.06003709137439728)\n",
      "\n",
      "L2T1_lora_B_out: (0.022010469809174538, 0.00010246581950923428, -0.01833670400083065)\n",
      "L2T1_lora_out: (0.044020939618349075, 0.00020493163901846856, -0.0366734080016613)\n",
      "\n",
      "L2T2_lora_B_out: (0.0565967820584774, 0.00012983035412617028, -0.059363991022109985)\n",
      "L2T2_lora_out: (0.1131935641169548, 0.00025966070825234056, -0.11872798204421997)\n",
      "\n",
      "g_out: (0.10062781721353531, 5.4729094699723646e-05, -0.10268419981002808)\n",
      "L1T2_out: (0.13508272171020508, 6.059383667889051e-05, -0.16272129118442535)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0034229017328470945, -1.5715164408902638e-05, -0.004184619523584843)\n",
      "L1T1_lora_out: (0.006845803465694189, -3.1430328817805275e-05, -0.008369239047169685)\n",
      "\n",
      "L2T1_lora_B_out: (0.0030616989824920893, 1.2253668501216453e-05, -0.002657744102180004)\n",
      "L2T1_lora_out: (0.0061233979649841785, 2.4507337002432905e-05, -0.005315488204360008)\n",
      "\n",
      "L2T2_lora_B_out: (0.007168828975409269, 2.038430011452874e-06, -0.00708962744101882)\n",
      "L2T2_lora_out: (0.014337657950818539, 4.076860022905748e-06, -0.01417925488203764)\n",
      "\n",
      "g_out: (0.01242949441075325, -2.0430479708011262e-05, -0.014863832853734493)\n",
      "L1T2_out: (0.01816520467400551, -5.1860806706827134e-05, -0.023233070969581604)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10039591044187546, -1.3545202818932012e-05, -0.10328415781259537)\n",
      "L1T1_lora_out: (0.20079182088375092, -2.7090405637864023e-05, -0.20656831562519073)\n",
      "\n",
      "L2T1_lora_B_out: (0.15217702090740204, -0.00032914557959884405, -0.15416692197322845)\n",
      "L2T1_lora_out: (0.3043540418148041, -0.0006582911591976881, -0.3083338439464569)\n",
      "\n",
      "L2T2_lora_B_out: (0.2567809820175171, -0.00047911322326399386, -0.33316734433174133)\n",
      "L2T2_lora_out: (0.5135619640350342, -0.0009582264465279877, -0.6663346886634827)\n",
      "\n",
      "g_out: (0.2814846634864807, -0.00029993540374562144, -0.3760998845100403)\n",
      "L1T2_out: (0.44016560912132263, -0.00032702565658837557, -0.5826681852340698)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.044343430548906326, 2.59845660366409e-06, -0.03873687982559204)\n",
      "L1T1_lora_out: (0.08868686109781265, 5.19691320732818e-06, -0.07747375965118408)\n",
      "\n",
      "L2T1_lora_B_out: (0.03712446615099907, 1.4079972061153967e-05, -0.03391332924365997)\n",
      "L2T1_lora_out: (0.07424893230199814, 2.8159944122307934e-05, -0.06782665848731995)\n",
      "\n",
      "L2T2_lora_B_out: (0.11409086734056473, 6.272158498177305e-05, -0.09888502955436707)\n",
      "L2T2_lora_out: (0.22818173468112946, 0.0001254431699635461, -0.19777005910873413)\n",
      "\n",
      "g_out: (0.17987260222434998, 9.728320583235472e-05, -0.16272617876529694)\n",
      "L1T2_out: (0.25879353284835815, 0.00010248013131786138, -0.22952775657176971)\n",
      "\n",
      "================================================================\n",
      "model.layers.17.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.16.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.036823175847530365, 5.974384839646518e-05, -0.043721459805965424)\n",
      "L1T1_lora_out: (0.07364635169506073, 0.00011948769679293036, -0.08744291961193085)\n",
      "\n",
      "L2T1_lora_B_out: (0.03029537759721279, -2.8900516554131173e-05, -0.026233887299895287)\n",
      "L2T1_lora_out: (0.06059075519442558, -5.7801033108262345e-05, -0.05246777459979057)\n",
      "\n",
      "L2T2_lora_B_out: (0.08118545264005661, 8.196331327781081e-05, -0.08834873139858246)\n",
      "L2T2_lora_out: (0.16237090528011322, 0.00016392662655562162, -0.17669746279716492)\n",
      "\n",
      "g_out: (0.1366099715232849, 0.0002217276196461171, -0.13504734635353088)\n",
      "L1T2_out: (0.21025632321834564, 0.00034121525823138654, -0.20367184281349182)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023697949945926666, -5.494873767020181e-05, -0.026637179777026176)\n",
      "L1T1_lora_out: (0.04739589989185333, -0.00010989747534040362, -0.05327435955405235)\n",
      "\n",
      "L2T1_lora_B_out: (0.030393563210964203, 0.00011215352424187586, -0.025773588567972183)\n",
      "L2T1_lora_out: (0.060787126421928406, 0.00022430704848375171, -0.051547177135944366)\n",
      "\n",
      "L2T2_lora_B_out: (0.05512290075421333, 0.00023043862893246114, -0.0521794892847538)\n",
      "L2T2_lora_out: (0.11024580150842667, 0.0004608772578649223, -0.1043589785695076)\n",
      "\n",
      "g_out: (0.0961851179599762, 0.0002365702239330858, -0.09668712317943573)\n",
      "L1T2_out: (0.14358101785182953, 0.00012667271948885173, -0.14996148645877838)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023794256150722504, 0.00019868681556545198, -0.022085217759013176)\n",
      "L1T1_lora_out: (0.04758851230144501, 0.00039737363113090396, -0.04417043551802635)\n",
      "\n",
      "L2T1_lora_B_out: (0.015241743996739388, -3.469401417532936e-05, -0.012667899020016193)\n",
      "L2T1_lora_out: (0.030483487993478775, -6.938802835065871e-05, -0.025335798040032387)\n",
      "\n",
      "L2T2_lora_B_out: (0.054019615054130554, 0.00027271383441984653, -0.03769508749246597)\n",
      "L2T2_lora_out: (0.10803923010826111, 0.0005454276688396931, -0.07539017498493195)\n",
      "\n",
      "g_out: (0.09160871058702469, 0.0006148156826384366, -0.06451176106929779)\n",
      "L1T2_out: (0.1391972303390503, 0.0010121893137693405, -0.09945353865623474)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004827787633985281, -1.860546035459265e-05, -0.00453973188996315)\n",
      "L1T1_lora_out: (0.009655575267970562, -3.72109207091853e-05, -0.0090794637799263)\n",
      "\n",
      "L2T1_lora_B_out: (0.004527592100203037, 8.03580587671604e-06, -0.004245677962899208)\n",
      "L2T1_lora_out: (0.009055184200406075, 1.607161175343208e-05, -0.008491355925798416)\n",
      "\n",
      "L2T2_lora_B_out: (0.013313598930835724, -1.5942547179292887e-05, -0.009836089797317982)\n",
      "L2T2_lora_out: (0.026627197861671448, -3.1885094358585775e-05, -0.019672179594635963)\n",
      "\n",
      "g_out: (0.01904747448861599, -4.7956695198081434e-05, -0.017016325145959854)\n",
      "L1T2_out: (0.024945229291915894, -8.516761590726674e-05, -0.024619674310088158)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.058698661625385284, -0.00013119386858306825, -0.07117930054664612)\n",
      "L1T1_lora_out: (0.11739732325077057, -0.0002623877371661365, -0.14235860109329224)\n",
      "\n",
      "L2T1_lora_B_out: (0.12033616751432419, -0.0003597515169531107, -0.11163685470819473)\n",
      "L2T1_lora_out: (0.24067233502864838, -0.0007195030339062214, -0.22327370941638947)\n",
      "\n",
      "L2T2_lora_B_out: (0.2189563363790512, -0.0007182577974162996, -0.1958547830581665)\n",
      "L2T2_lora_out: (0.4379126727581024, -0.0014365155948325992, -0.391709566116333)\n",
      "\n",
      "g_out: (0.2343229353427887, -0.0007170125609263778, -0.2157396674156189)\n",
      "L1T2_out: (0.3321576714515686, -0.0009794003563001752, -0.3558075428009033)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.029612893238663673, -7.212905074993614e-06, -0.020962221547961235)\n",
      "L1T1_lora_out: (0.05922578647732735, -1.4425810149987228e-05, -0.04192444309592247)\n",
      "\n",
      "L2T1_lora_B_out: (0.01761523447930813, 9.179344488075003e-06, -0.018248025327920914)\n",
      "L2T1_lora_out: (0.03523046895861626, 1.8358688976150006e-05, -0.03649605065584183)\n",
      "\n",
      "L2T2_lora_B_out: (0.03859751671552658, -2.3064643755787984e-05, -0.042242322117090225)\n",
      "L2T2_lora_out: (0.07719503343105316, -4.612928751157597e-05, -0.08448464423418045)\n",
      "\n",
      "g_out: (0.06373036652803421, -6.448795465985313e-05, -0.06189872696995735)\n",
      "L1T2_out: (0.10099132359027863, -7.891377754276618e-05, -0.08860395848751068)\n",
      "\n",
      "================================================================\n",
      "model.layers.16.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.15.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028224319219589233, 2.3230079023051076e-05, -0.02737293764948845)\n",
      "L1T1_lora_out: (0.05644863843917847, 4.646015804610215e-05, -0.0547458752989769)\n",
      "\n",
      "L2T1_lora_B_out: (0.016180768609046936, -2.6304565835744143e-05, -0.01516177412122488)\n",
      "L2T1_lora_out: (0.03236153721809387, -5.2609131671488285e-05, -0.03032354824244976)\n",
      "\n",
      "L2T2_lora_B_out: (0.04469591751694679, 5.043969940743409e-05, -0.042886946350336075)\n",
      "L2T2_lora_out: (0.08939183503389359, 0.00010087939881486818, -0.08577389270067215)\n",
      "\n",
      "g_out: (0.07242434471845627, 0.00015348856686614454, -0.08686903119087219)\n",
      "L1T2_out: (0.12643592059612274, 0.00019994871399831027, -0.13449932634830475)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02991640567779541, -0.0001627498713787645, -0.026357827708125114)\n",
      "L1T1_lora_out: (0.05983281135559082, -0.000325499742757529, -0.05271565541625023)\n",
      "\n",
      "L2T1_lora_B_out: (0.03020365908741951, -0.00017826768453232944, -0.02825896069407463)\n",
      "L2T1_lora_out: (0.06040731817483902, -0.0003565353690646589, -0.05651792138814926)\n",
      "\n",
      "L2T2_lora_B_out: (0.0585847832262516, -0.0004820935137104243, -0.06295159459114075)\n",
      "L2T2_lora_out: (0.1171695664525032, -0.0009641870274208486, -0.1259031891822815)\n",
      "\n",
      "g_out: (0.07847465574741364, -0.0006076516583561897, -0.09910250455141068)\n",
      "L1T2_out: (0.11329236626625061, -0.0009331514011137187, -0.14917537569999695)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.024805014953017235, 5.409903678810224e-05, -0.020333856344223022)\n",
      "L1T1_lora_out: (0.04961002990603447, 0.00010819807357620448, -0.040667712688446045)\n",
      "\n",
      "L2T1_lora_B_out: (0.037695255130529404, 0.0001623154676053673, -0.02767951600253582)\n",
      "L2T1_lora_out: (0.07539051026105881, 0.0003246309352107346, -0.05535903200507164)\n",
      "\n",
      "L2T2_lora_B_out: (0.06610716879367828, 0.00034591200528666377, -0.07209379971027374)\n",
      "L2T2_lora_out: (0.13221433758735657, 0.0006918240105733275, -0.14418759942054749)\n",
      "\n",
      "g_out: (0.10319259762763977, 0.00036719298805110157, -0.10361413657665253)\n",
      "L1T2_out: (0.14831379055976868, 0.0004753910470753908, -0.1386372447013855)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0023290591780096292, 3.6716553495352855e-06, -0.002143519464880228)\n",
      "L1T1_lora_out: (0.0046581183560192585, 7.343310699070571e-06, -0.004287038929760456)\n",
      "\n",
      "L2T1_lora_B_out: (0.003970754332840443, 6.178578587423544e-06, -0.003993647173047066)\n",
      "L2T1_lora_out: (0.007941508665680885, 1.2357157174847089e-05, -0.007987294346094131)\n",
      "\n",
      "L2T2_lora_B_out: (0.005271630361676216, 6.695641332044033e-06, -0.0049387249164283276)\n",
      "L2T2_lora_out: (0.010543260723352432, 1.3391282664088067e-05, -0.009877449832856655)\n",
      "\n",
      "g_out: (0.008935105986893177, 1.0341282177250832e-06, -0.008889144286513329)\n",
      "L1T2_out: (0.012513081543147564, 8.377435733564198e-06, -0.013176183216273785)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.15685224533081055, -0.0001881476491689682, -0.1279098242521286)\n",
      "L1T1_lora_out: (0.3137044906616211, -0.0003762952983379364, -0.2558196485042572)\n",
      "\n",
      "L2T1_lora_B_out: (0.12574724853038788, -0.00029786425875499845, -0.18147505819797516)\n",
      "L2T1_lora_out: (0.25149449706077576, -0.0005957285175099969, -0.3629501163959503)\n",
      "\n",
      "L2T2_lora_B_out: (0.32710936665534973, -0.0008974673110060394, -0.4140234887599945)\n",
      "L2T2_lora_out: (0.6542187333106995, -0.0017949346220120788, -0.828046977519989)\n",
      "\n",
      "g_out: (0.5907580256462097, -0.0011992058716714382, -0.6138989329338074)\n",
      "L1T2_out: (0.9044625163078308, -0.0015755014028400183, -0.8697185516357422)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02622680738568306, -3.253356408094987e-05, -0.03024320676922798)\n",
      "L1T1_lora_out: (0.05245361477136612, -6.506712816189975e-05, -0.06048641353845596)\n",
      "\n",
      "L2T1_lora_B_out: (0.024433013051748276, -1.072245868272148e-05, -0.028058938682079315)\n",
      "L2T1_lora_out: (0.04886602610349655, -2.144491736544296e-05, -0.05611787736415863)\n",
      "\n",
      "L2T2_lora_B_out: (0.0635782852768898, -6.949043017812073e-05, -0.07328156381845474)\n",
      "L2T2_lora_out: (0.1271565705537796, -0.00013898086035624146, -0.14656312763690948)\n",
      "\n",
      "g_out: (0.09403052926063538, -0.00011753591388696805, -0.11315908282995224)\n",
      "L1T2_out: (0.13647997379302979, -0.00018260306387674063, -0.1662350594997406)\n",
      "\n",
      "================================================================\n",
      "model.layers.15.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.14.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04035539552569389, 7.202558481367305e-05, -0.04135074466466904)\n",
      "L1T1_lora_out: (0.08071079105138779, 0.0001440511696273461, -0.08270148932933807)\n",
      "\n",
      "L2T1_lora_B_out: (0.033369410783052444, 3.1857562134973705e-05, -0.043776702135801315)\n",
      "L2T1_lora_out: (0.06673882156610489, 6.371512426994741e-05, -0.08755340427160263)\n",
      "\n",
      "L2T2_lora_B_out: (0.08115722984075546, 0.0001651679922360927, -0.08682887256145477)\n",
      "L2T2_lora_out: (0.16231445968151093, 0.0003303359844721854, -0.17365774512290955)\n",
      "\n",
      "g_out: (0.1534425914287567, 0.0002666208893060684, -0.14132218062877655)\n",
      "L1T2_out: (0.23087544739246368, 0.00041067213169299066, -0.22402366995811462)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.029755230993032455, 0.0001452046853955835, -0.025427132844924927)\n",
      "L1T1_lora_out: (0.05951046198606491, 0.000290409370791167, -0.050854265689849854)\n",
      "\n",
      "L2T1_lora_B_out: (0.03980967774987221, 0.00025325146270915866, -0.03506661579012871)\n",
      "L2T1_lora_out: (0.07961935549974442, 0.0005065029254183173, -0.07013323158025742)\n",
      "\n",
      "L2T2_lora_B_out: (0.07825477421283722, 0.0005804244428873062, -0.0755426213145256)\n",
      "L2T2_lora_out: (0.15650954842567444, 0.0011608488857746124, -0.1510852426290512)\n",
      "\n",
      "g_out: (0.13043653964996338, 0.000654346018563956, -0.10256950557231903)\n",
      "L1T2_out: (0.1899470090866089, 0.000944755389355123, -0.13883385062217712)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.018603302538394928, 0.00013434691936708987, -0.023950349539518356)\n",
      "L1T1_lora_out: (0.037206605076789856, 0.00026869383873417974, -0.04790069907903671)\n",
      "\n",
      "L2T1_lora_B_out: (0.01788289286196232, 9.919345029629767e-05, -0.018035827204585075)\n",
      "L2T1_lora_out: (0.03576578572392464, 0.00019838690059259534, -0.03607165440917015)\n",
      "\n",
      "L2T2_lora_B_out: (0.03770357370376587, 0.0006042271852493286, -0.03892505541443825)\n",
      "L2T2_lora_out: (0.07540714740753174, 0.0012084543704986572, -0.0778501108288765)\n",
      "\n",
      "g_out: (0.06979963183403015, 0.0010100675281137228, -0.06803916394710541)\n",
      "L1T2_out: (0.10070683807134628, 0.0012787613086402416, -0.11227446049451828)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0029585356824100018, 1.8690715251068468e-06, -0.0026994324289262295)\n",
      "L1T1_lora_out: (0.0059170713648200035, 3.7381430502136936e-06, -0.005398864857852459)\n",
      "\n",
      "L2T1_lora_B_out: (0.005591301713138819, 3.0476101528620347e-05, -0.004774678032845259)\n",
      "L2T1_lora_out: (0.011182603426277637, 6.0952203057240695e-05, -0.009549356065690517)\n",
      "\n",
      "L2T2_lora_B_out: (0.011190386489033699, 3.423820453463122e-05, -0.010872717946767807)\n",
      "L2T2_lora_out: (0.022380772978067398, 6.847640906926244e-05, -0.021745435893535614)\n",
      "\n",
      "g_out: (0.01607329398393631, 7.524203738284996e-06, -0.017035074532032013)\n",
      "L1T2_out: (0.021235082298517227, 1.1262338375672698e-05, -0.021853268146514893)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06372813880443573, -0.00011427833669586107, -0.0698331668972969)\n",
      "L1T1_lora_out: (0.12745627760887146, -0.00022855667339172214, -0.1396663337945938)\n",
      "\n",
      "L2T1_lora_B_out: (0.10880617797374725, -0.0002100669516948983, -0.09485074132680893)\n",
      "L2T1_lora_out: (0.2176123559474945, -0.0004201339033897966, -0.18970148265361786)\n",
      "\n",
      "L2T2_lora_B_out: (0.17264094948768616, -0.0005818135105073452, -0.25411051511764526)\n",
      "L2T2_lora_out: (0.3452818989753723, -0.0011636270210146904, -0.5082210302352905)\n",
      "\n",
      "g_out: (0.2627767026424408, -0.0007434930885210633, -0.34090614318847656)\n",
      "L1T2_out: (0.36256521940231323, -0.0009720497764647007, -0.46258968114852905)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03647482022643089, 3.199754064553417e-05, -0.03418475762009621)\n",
      "L1T1_lora_out: (0.07294964045286179, 6.399508129106835e-05, -0.06836951524019241)\n",
      "\n",
      "L2T1_lora_B_out: (0.03173932060599327, -3.834614471998066e-05, -0.034813765436410904)\n",
      "L2T1_lora_out: (0.06347864121198654, -7.669228943996131e-05, -0.06962753087282181)\n",
      "\n",
      "L2T2_lora_B_out: (0.08019398897886276, -8.838816938805394e-06, -0.08792389929294586)\n",
      "L2T2_lora_out: (0.16038797795772552, -1.7677633877610788e-05, -0.17584779858589172)\n",
      "\n",
      "g_out: (0.11940179765224457, 5.9014608268626034e-05, -0.14326734840869904)\n",
      "L1T2_out: (0.18709702789783478, 0.00012300966773182154, -0.21163687109947205)\n",
      "\n",
      "================================================================\n",
      "model.layers.14.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.13.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023990025743842125, 2.087960092467256e-05, -0.021207090467214584)\n",
      "L1T1_lora_out: (0.04798005148768425, 4.175920184934512e-05, -0.04241418093442917)\n",
      "\n",
      "L2T1_lora_B_out: (0.027339432388544083, -5.1027977860940155e-06, -0.030542714521288872)\n",
      "L2T1_lora_out: (0.054678864777088165, -1.0205595572188031e-05, -0.061085429042577744)\n",
      "\n",
      "L2T2_lora_B_out: (0.0617302730679512, 8.244984201155603e-05, -0.08240100741386414)\n",
      "L2T2_lora_out: (0.1234605461359024, 0.00016489968402311206, -0.16480201482772827)\n",
      "\n",
      "g_out: (0.09137623012065887, 0.00017510526231490076, -0.11273084580898285)\n",
      "L1T2_out: (0.12757226824760437, 0.00021686448599211872, -0.1511508971452713)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03377281874418259, -7.314200774999335e-05, -0.0312538743019104)\n",
      "L1T1_lora_out: (0.06754563748836517, -0.0001462840154999867, -0.0625077486038208)\n",
      "\n",
      "L2T1_lora_B_out: (0.02008483000099659, -1.6348258213838562e-05, -0.024838948622345924)\n",
      "L2T1_lora_out: (0.04016966000199318, -3.2696516427677125e-05, -0.04967789724469185)\n",
      "\n",
      "L2T2_lora_B_out: (0.07504966855049133, -2.897757076425478e-05, -0.08877547830343246)\n",
      "L2T2_lora_out: (0.15009933710098267, -5.795514152850956e-05, -0.17755095660686493)\n",
      "\n",
      "g_out: (0.11938221752643585, -2.5258637833758257e-05, -0.13991716504096985)\n",
      "L1T2_out: (0.16645345091819763, -0.00017154263332486153, -0.19805744290351868)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.011867628432810307, -3.570041008060798e-05, -0.012852207757532597)\n",
      "L1T1_lora_out: (0.023735256865620613, -7.140082016121596e-05, -0.025704415515065193)\n",
      "\n",
      "L2T1_lora_B_out: (0.019248563796281815, 1.900206189020537e-05, -0.020986497402191162)\n",
      "L2T1_lora_out: (0.03849712759256363, 3.800412378041074e-05, -0.041972994804382324)\n",
      "\n",
      "L2T2_lora_B_out: (0.03365760296583176, -0.00014092130004428327, -0.038617923855781555)\n",
      "L2T2_lora_out: (0.06731520593166351, -0.00028184260008856654, -0.07723584771156311)\n",
      "\n",
      "g_out: (0.0596119724214077, -0.0003198467311449349, -0.05616065859794617)\n",
      "L1T2_out: (0.08334723114967346, -0.0003912475367542356, -0.07725072652101517)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.003716842271387577, 4.2149230239374447e-07, -0.0032678351271897554)\n",
      "L1T1_lora_out: (0.007433684542775154, 8.429846047874889e-07, -0.006535670254379511)\n",
      "\n",
      "L2T1_lora_B_out: (0.003683330724015832, 1.0844301868928596e-05, -0.0035669126082211733)\n",
      "L2T1_lora_out: (0.007366661448031664, 2.1688603737857193e-05, -0.007133825216442347)\n",
      "\n",
      "L2T2_lora_B_out: (0.007053771987557411, 2.31372396228835e-05, -0.007747170049697161)\n",
      "L2T2_lora_out: (0.014107543975114822, 4.6274479245767e-05, -0.015494340099394321)\n",
      "\n",
      "g_out: (0.01269006822258234, 2.458589369780384e-05, -0.012339900247752666)\n",
      "L1T2_out: (0.02012375369668007, 2.5428871595067903e-05, -0.01795920729637146)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04257144406437874, -0.00018115843704435974, -0.042375124990940094)\n",
      "L1T1_lora_out: (0.08514288812875748, -0.0003623168740887195, -0.08475024998188019)\n",
      "\n",
      "L2T1_lora_B_out: (0.06339582800865173, -0.0001807322696549818, -0.06366384029388428)\n",
      "L2T1_lora_out: (0.12679165601730347, -0.0003614645393099636, -0.12732768058776855)\n",
      "\n",
      "L2T2_lora_B_out: (0.13687680661678314, -0.0006315671489574015, -0.15470266342163086)\n",
      "L2T2_lora_out: (0.2737536132335663, -0.001263134297914803, -0.3094053268432617)\n",
      "\n",
      "g_out: (0.1811046153306961, -0.0009016696130856872, -0.2112172245979309)\n",
      "L1T2_out: (0.265471488237381, -0.0012639864580705762, -0.2947414517402649)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.024811701849102974, 4.149907545070164e-05, -0.02213355526328087)\n",
      "L1T1_lora_out: (0.04962340369820595, 8.299815090140328e-05, -0.04426711052656174)\n",
      "\n",
      "L2T1_lora_B_out: (0.028300268575549126, -1.0609288437990472e-05, -0.027347123250365257)\n",
      "L2T1_lora_out: (0.05660053715109825, -2.1218576875980943e-05, -0.054694246500730515)\n",
      "\n",
      "L2T2_lora_B_out: (0.06234443187713623, 5.943742871750146e-05, -0.066594697535038)\n",
      "L2T2_lora_out: (0.12468886375427246, 0.00011887485743500292, -0.133189395070076)\n",
      "\n",
      "g_out: (0.10202103853225708, 0.0001400933979311958, -0.10036680102348328)\n",
      "L1T2_out: (0.1457674205303192, 0.00022309154155664146, -0.13657836616039276)\n",
      "\n",
      "================================================================\n",
      "model.layers.13.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.12.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020791208371520042, -1.0680355444492307e-05, -0.02117444947361946)\n",
      "L1T1_lora_out: (0.041582416743040085, -2.1360710888984613e-05, -0.04234889894723892)\n",
      "\n",
      "L2T1_lora_B_out: (0.021412819623947144, -2.8470869438024238e-05, -0.021858325228095055)\n",
      "L2T1_lora_out: (0.04282563924789429, -5.6941738876048476e-05, -0.04371665045619011)\n",
      "\n",
      "L2T2_lora_B_out: (0.06151837110519409, -4.743785393657163e-05, -0.07181353867053986)\n",
      "L2T2_lora_out: (0.12303674221038818, -9.487570787314326e-05, -0.1436270773410797)\n",
      "\n",
      "g_out: (0.09967830032110214, -3.793404539464973e-05, -0.10886897146701813)\n",
      "L1T2_out: (0.14126071333885193, -5.9294710808899254e-05, -0.14564675092697144)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03461277484893799, -0.0001716718397801742, -0.028442440554499626)\n",
      "L1T1_lora_out: (0.06922554969787598, -0.0003433436795603484, -0.05688488110899925)\n",
      "\n",
      "L2T1_lora_B_out: (0.030016571283340454, 0.00012911255180370063, -0.03410471975803375)\n",
      "L2T1_lora_out: (0.06003314256668091, 0.00025822510360740125, -0.0682094395160675)\n",
      "\n",
      "L2T2_lora_B_out: (0.08424486219882965, -2.8642089091590606e-05, -0.07649838924407959)\n",
      "L2T2_lora_out: (0.1684897243976593, -5.728417818318121e-05, -0.15299677848815918)\n",
      "\n",
      "g_out: (0.14520154893398285, -0.0003155091544613242, -0.12822261452674866)\n",
      "L1T2_out: (0.21442709863185883, -0.0006588529795408249, -0.18420900404453278)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.023933209478855133, -4.851695848628879e-06, -0.02916562370955944)\n",
      "L1T1_lora_out: (0.047866418957710266, -9.703391697257757e-06, -0.05833124741911888)\n",
      "\n",
      "L2T1_lora_B_out: (0.016857784241437912, -0.0002782716474030167, -0.019633354619145393)\n",
      "L2T1_lora_out: (0.033715568482875824, -0.0005565432948060334, -0.03926670923829079)\n",
      "\n",
      "L2T2_lora_B_out: (0.06058108061552048, -0.0006033668178133667, -0.06185612827539444)\n",
      "L2T2_lora_out: (0.12116216123104095, -0.0012067336356267333, -0.12371225655078888)\n",
      "\n",
      "g_out: (0.10068327188491821, -0.0006501903990283608, -0.10561323165893555)\n",
      "L1T2_out: (0.1464899182319641, -0.0006598938489332795, -0.15857131779193878)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0024272622540593147, -5.989978490106296e-06, -0.002671561436727643)\n",
      "L1T1_lora_out: (0.0048545245081186295, -1.1979956980212592e-05, -0.005343122873455286)\n",
      "\n",
      "L2T1_lora_B_out: (0.0034932829439640045, -2.9219022508186754e-06, -0.003484112210571766)\n",
      "L2T1_lora_out: (0.006986565887928009, -5.843804501637351e-06, -0.006968224421143532)\n",
      "\n",
      "L2T2_lora_B_out: (0.007757642772048712, -7.853479473851621e-06, -0.006602066569030285)\n",
      "L2T2_lora_out: (0.015515285544097424, -1.5706958947703242e-05, -0.01320413313806057)\n",
      "\n",
      "g_out: (0.011424597352743149, -9.863152627076488e-06, -0.011714459396898746)\n",
      "L1T2_out: (0.01426306925714016, -2.1843108697794378e-05, -0.014980231411755085)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03617461770772934, -0.00011410502338549122, -0.06493371725082397)\n",
      "L1T1_lora_out: (0.07234923541545868, -0.00022821004677098244, -0.12986743450164795)\n",
      "\n",
      "L2T1_lora_B_out: (0.08800429105758667, -9.271282760892063e-05, -0.11483175307512283)\n",
      "L2T1_lora_out: (0.17600858211517334, -0.00018542565521784127, -0.22966350615024567)\n",
      "\n",
      "L2T2_lora_B_out: (0.13344112038612366, -0.000334591546561569, -0.2724856734275818)\n",
      "L2T2_lora_out: (0.2668822407722473, -0.000669183093123138, -0.5449713468551636)\n",
      "\n",
      "g_out: (0.1803865134716034, -0.0004837574379052967, -0.32477712631225586)\n",
      "L1T2_out: (0.24750252068042755, -0.0007119674701243639, -0.4546445608139038)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.029497181996703148, 3.568138345144689e-05, -0.041176021099090576)\n",
      "L1T1_lora_out: (0.058994363993406296, 7.136276690289378e-05, -0.08235204219818115)\n",
      "\n",
      "L2T1_lora_B_out: (0.03843333572149277, 2.205022974521853e-05, -0.03748521953821182)\n",
      "L2T1_lora_out: (0.07686667144298553, 4.410045949043706e-05, -0.07497043907642365)\n",
      "\n",
      "L2T2_lora_B_out: (0.0998278558254242, 0.00011794755118899047, -0.13707111775875092)\n",
      "L2T2_lora_out: (0.1996557116508484, 0.00023589510237798095, -0.27414223551750183)\n",
      "\n",
      "g_out: (0.15507802367210388, 0.00019179466471541673, -0.21718764305114746)\n",
      "L1T2_out: (0.21407239139080048, 0.00026315744617022574, -0.2995396852493286)\n",
      "\n",
      "================================================================\n",
      "model.layers.12.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.11.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03093036450445652, -5.922585387452273e-06, -0.029367905110120773)\n",
      "L1T1_lora_out: (0.06186072900891304, -1.1845170774904545e-05, -0.05873581022024155)\n",
      "\n",
      "L2T1_lora_B_out: (0.03450283780694008, -6.390717317117378e-05, -0.03244208171963692)\n",
      "L2T1_lora_out: (0.06900567561388016, -0.00012781434634234756, -0.06488416343927383)\n",
      "\n",
      "L2T2_lora_B_out: (0.0830705463886261, -0.0001303351018577814, -0.07373279333114624)\n",
      "L2T2_lora_out: (0.1661410927772522, -0.0002606702037155628, -0.14746558666229248)\n",
      "\n",
      "g_out: (0.1251811683177948, -0.00013285588647704571, -0.12796054780483246)\n",
      "L1T2_out: (0.182949960231781, -0.0001447010727133602, -0.1823921501636505)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.057128340005874634, -3.662232074930216e-06, -0.05963309109210968)\n",
      "L1T1_lora_out: (0.11425668001174927, -7.324464149860432e-06, -0.11926618218421936)\n",
      "\n",
      "L2T1_lora_B_out: (0.013844734989106655, 7.617396477144212e-05, -0.014793166890740395)\n",
      "L2T1_lora_out: (0.02768946997821331, 0.00015234792954288423, -0.02958633378148079)\n",
      "\n",
      "L2T2_lora_B_out: (0.07964757084846497, -0.0001513849274488166, -0.07343120872974396)\n",
      "L2T2_lora_out: (0.15929514169692993, -0.0003027698548976332, -0.14686241745948792)\n",
      "\n",
      "g_out: (0.14805038273334503, -0.0004551177844405174, -0.13742893934249878)\n",
      "L1T2_out: (0.26046884059906006, -0.00046244222903624177, -0.22233819961547852)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.012504653073847294, 7.293694216059521e-05, -0.012124063447117805)\n",
      "L1T1_lora_out: (0.025009306147694588, 0.00014587388432119042, -0.02424812689423561)\n",
      "\n",
      "L2T1_lora_B_out: (0.020540855824947357, 3.868612111546099e-05, -0.01868928037583828)\n",
      "L2T1_lora_out: (0.041081711649894714, 7.737224223092198e-05, -0.03737856075167656)\n",
      "\n",
      "L2T2_lora_B_out: (0.05463895574212074, 0.00032556382939219475, -0.04809146374464035)\n",
      "L2T2_lora_out: (0.10927791148424149, 0.0006511276587843895, -0.0961829274892807)\n",
      "\n",
      "g_out: (0.06819619983434677, 0.0005737554165534675, -0.060806773602962494)\n",
      "L1T2_out: (0.08486331254243851, 0.0007196292281150818, -0.0784897655248642)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0036353913601487875, 2.4694978492334485e-05, -0.003507426707074046)\n",
      "L1T1_lora_out: (0.007270782720297575, 4.938995698466897e-05, -0.007014853414148092)\n",
      "\n",
      "L2T1_lora_B_out: (0.005097453016787767, 1.0434458090458065e-05, -0.005607915576547384)\n",
      "L2T1_lora_out: (0.010194906033575535, 2.086891618091613e-05, -0.011215831153094769)\n",
      "\n",
      "L2T2_lora_B_out: (0.013093695975840092, 9.039798169396818e-05, -0.012178976088762283)\n",
      "L2T2_lora_out: (0.026187391951680183, 0.00018079596338793635, -0.024357952177524567)\n",
      "\n",
      "g_out: (0.020615609362721443, 0.00015992706175893545, -0.01839011162519455)\n",
      "L1T2_out: (0.027886392548680305, 0.0002093170041916892, -0.02540496550500393)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06347537040710449, -0.00012524712656158954, -0.0784638300538063)\n",
      "L1T1_lora_out: (0.12695074081420898, -0.0002504942531231791, -0.1569276601076126)\n",
      "\n",
      "L2T1_lora_B_out: (0.06365468353033066, -0.0001894957385957241, -0.07158888131380081)\n",
      "L2T1_lora_out: (0.12730936706066132, -0.0003789914771914482, -0.14317776262760162)\n",
      "\n",
      "L2T2_lora_B_out: (0.14526280760765076, -0.0005422625108622015, -0.2307235598564148)\n",
      "L2T2_lora_out: (0.2905256152153015, -0.001084525021724403, -0.4614471197128296)\n",
      "\n",
      "g_out: (0.2214880883693695, -0.0007055335445329547, -0.3212238550186157)\n",
      "L1T2_out: (0.3484388291835785, -0.0009560277685523033, -0.47815150022506714)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02586470916867256, -4.639297185349278e-05, -0.025352735072374344)\n",
      "L1T1_lora_out: (0.05172941833734512, -9.278594370698556e-05, -0.05070547014474869)\n",
      "\n",
      "L2T1_lora_B_out: (0.02551063895225525, -6.682551611447707e-05, -0.023629700765013695)\n",
      "L2T1_lora_out: (0.0510212779045105, -0.00013365103222895414, -0.04725940153002739)\n",
      "\n",
      "L2T2_lora_B_out: (0.05912283807992935, -0.00020121908164583147, -0.06273894011974335)\n",
      "L2T2_lora_out: (0.1182456761598587, -0.00040243816329166293, -0.1254778802394867)\n",
      "\n",
      "g_out: (0.09693396091461182, -0.0002687871747184545, -0.0967366024851799)\n",
      "L1T2_out: (0.14267930388450623, -0.00036157312570139766, -0.14095795154571533)\n",
      "\n",
      "================================================================\n",
      "model.layers.11.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.10.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.016561167314648628, 1.107924799725879e-05, -0.015234628692269325)\n",
      "L1T1_lora_out: (0.033122334629297256, 2.215849599451758e-05, -0.03046925738453865)\n",
      "\n",
      "L2T1_lora_B_out: (0.02623562701046467, -3.4184951800853014e-05, -0.0393565371632576)\n",
      "L2T1_lora_out: (0.05247125402092934, -6.836990360170603e-05, -0.0787130743265152)\n",
      "\n",
      "L2T2_lora_B_out: (0.04690863564610481, -5.208434595260769e-05, -0.059015579521656036)\n",
      "L2T2_lora_out: (0.09381727129220963, -0.00010416869190521538, -0.11803115904331207)\n",
      "\n",
      "g_out: (0.07588301599025726, -3.579879557946697e-05, -0.08647963404655457)\n",
      "L1T2_out: (0.1048317700624466, -1.3640276847581845e-05, -0.1129584088921547)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02690211683511734, -8.716238880879246e-06, -0.030064422637224197)\n",
      "L1T1_lora_out: (0.05380423367023468, -1.743247776175849e-05, -0.060128845274448395)\n",
      "\n",
      "L2T1_lora_B_out: (0.08647563308477402, 0.0005825328407809138, -0.05227820575237274)\n",
      "L2T1_lora_out: (0.17295126616954803, 0.0011650656815618277, -0.10455641150474548)\n",
      "\n",
      "L2T2_lora_B_out: (0.12469011545181274, 0.00077378487912938, -0.10622447729110718)\n",
      "L2T2_lora_out: (0.2493802309036255, 0.00154756975825876, -0.21244895458221436)\n",
      "\n",
      "g_out: (0.14627793431282043, 0.00038250413490459323, -0.17231020331382751)\n",
      "L1T2_out: (0.20008216798305511, 0.0003650716971606016, -0.21049685776233673)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.019523847848176956, -0.00013002645573578775, -0.021377597004175186)\n",
      "L1T1_lora_out: (0.03904769569635391, -0.0002600529114715755, -0.04275519400835037)\n",
      "\n",
      "L2T1_lora_B_out: (0.022419866174459457, 7.867180102039129e-05, -0.027859536930918694)\n",
      "L2T1_lora_out: (0.044839732348918915, 0.00015734360204078257, -0.05571907386183739)\n",
      "\n",
      "L2T2_lora_B_out: (0.04567818343639374, -7.50981635064818e-05, -0.050256822258234024)\n",
      "L2T2_lora_out: (0.09135636687278748, -0.0001501963270129636, -0.10051364451646805)\n",
      "\n",
      "g_out: (0.06211321800947189, -0.0003075399436056614, -0.08177965879440308)\n",
      "L1T2_out: (0.09067963063716888, -0.0005675928550772369, -0.11998447775840759)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0035851234570145607, 7.421165719279088e-06, -0.0033416396472603083)\n",
      "L1T1_lora_out: (0.007170246914029121, 1.4842331438558176e-05, -0.0066832792945206165)\n",
      "\n",
      "L2T1_lora_B_out: (0.0046272603794932365, 1.3622715187011636e-06, -0.004133146721869707)\n",
      "L2T1_lora_out: (0.009254520758986473, 2.724543037402327e-06, -0.008266293443739414)\n",
      "\n",
      "L2T2_lora_B_out: (0.010169467888772488, 2.972412039525807e-05, -0.009811654686927795)\n",
      "L2T2_lora_out: (0.020338935777544975, 5.944824079051614e-05, -0.01962330937385559)\n",
      "\n",
      "g_out: (0.015434227883815765, 5.6723707530181855e-05, -0.014720095321536064)\n",
      "L1T2_out: (0.022604474797844887, 7.156604988267645e-05, -0.019601963460445404)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.049832701683044434, -0.00014234328409656882, -0.06114817410707474)\n",
      "L1T1_lora_out: (0.09966540336608887, -0.00028468656819313765, -0.12229634821414948)\n",
      "\n",
      "L2T1_lora_B_out: (0.05000320449471474, -0.00022845265630166978, -0.054030489176511765)\n",
      "L2T1_lora_out: (0.10000640898942947, -0.00045690531260333955, -0.10806097835302353)\n",
      "\n",
      "L2T2_lora_B_out: (0.10624305158853531, -0.00047517396160401404, -0.1369851976633072)\n",
      "L2T2_lora_out: (0.21248610317707062, -0.0009503479232080281, -0.2739703953266144)\n",
      "\n",
      "g_out: (0.16749390959739685, -0.0004934426979161799, -0.19159486889839172)\n",
      "L1T2_out: (0.25180360674858093, -0.0007781292661093175, -0.3138912320137024)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021069742739200592, 3.3592002637305995e-06, -0.02021888457238674)\n",
      "L1T1_lora_out: (0.042139485478401184, 6.718400527461199e-06, -0.04043776914477348)\n",
      "\n",
      "L2T1_lora_B_out: (0.019739538431167603, 2.084954576275777e-05, -0.020671920850872993)\n",
      "L2T1_lora_out: (0.039479076862335205, 4.169909152551554e-05, -0.04134384170174599)\n",
      "\n",
      "L2T2_lora_B_out: (0.04655660316348076, 1.7702810509945266e-05, -0.05046837031841278)\n",
      "L2T2_lora_out: (0.09311320632696152, 3.540562101989053e-05, -0.10093674063682556)\n",
      "\n",
      "g_out: (0.07134661823511124, -6.293512342381291e-06, -0.06668928265571594)\n",
      "L1T2_out: (0.11034876108169556, 4.248826712682785e-07, -0.09492108970880508)\n",
      "\n",
      "================================================================\n",
      "model.layers.10.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.9.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.015288535505533218, -3.783523425227031e-05, -0.015327692963182926)\n",
      "L1T1_lora_out: (0.030577071011066437, -7.567046850454062e-05, -0.030655385926365852)\n",
      "\n",
      "L2T1_lora_B_out: (0.020865701138973236, -4.275653191143647e-05, -0.023097701370716095)\n",
      "L2T1_lora_out: (0.04173140227794647, -8.551306382287294e-05, -0.04619540274143219)\n",
      "\n",
      "L2T2_lora_B_out: (0.04315808042883873, -4.364066626294516e-05, -0.03765052556991577)\n",
      "L2T2_lora_out: (0.08631616085767746, -8.728133252589032e-05, -0.07530105113983154)\n",
      "\n",
      "g_out: (0.06612236052751541, -1.7682566522125853e-06, -0.07076368480920792)\n",
      "L1T2_out: (0.09037706255912781, -7.7438737207558e-05, -0.09028077870607376)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04838236793875694, -0.0006970111280679703, -0.053310785442590714)\n",
      "L1T1_lora_out: (0.09676473587751389, -0.0013940222561359406, -0.10662157088518143)\n",
      "\n",
      "L2T1_lora_B_out: (0.026236185804009438, -0.0002858931547962129, -0.023663604632019997)\n",
      "L2T1_lora_out: (0.052472371608018875, -0.0005717863095924258, -0.04732720926403999)\n",
      "\n",
      "L2T2_lora_B_out: (0.09619621932506561, -0.00099681515712291, -0.12832187116146088)\n",
      "L2T2_lora_out: (0.19239243865013123, -0.00199363031424582, -0.25664374232292175)\n",
      "\n",
      "g_out: (0.16996337473392487, -0.0014218440046533942, -0.21129097044467926)\n",
      "L1T2_out: (0.2566431164741516, -0.0028158663772046566, -0.3179125487804413)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01227526180446148, -2.5231653125956655e-05, -0.013408727943897247)\n",
      "L1T1_lora_out: (0.02455052360892296, -5.046330625191331e-05, -0.026817455887794495)\n",
      "\n",
      "L2T1_lora_B_out: (0.014874606393277645, -4.893324145882616e-08, -0.014293772168457508)\n",
      "L2T1_lora_out: (0.02974921278655529, -9.786648291765232e-08, -0.028587544336915016)\n",
      "\n",
      "L2T2_lora_B_out: (0.025431575253605843, -7.634692883584648e-05, -0.030929818749427795)\n",
      "L2T2_lora_out: (0.050863150507211685, -0.00015269385767169297, -0.06185963749885559)\n",
      "\n",
      "g_out: (0.041870906949043274, -0.00015259595238603652, -0.04411475360393524)\n",
      "L1T2_out: (0.06511414051055908, -0.00020305921498220414, -0.06054571270942688)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.002025730675086379, 1.2675167454290204e-05, -0.0021119662560522556)\n",
      "L1T1_lora_out: (0.004051461350172758, 2.5350334908580408e-05, -0.004223932512104511)\n",
      "\n",
      "L2T1_lora_B_out: (0.004503376316279173, 5.413126928033307e-06, -0.0054757678881287575)\n",
      "L2T1_lora_out: (0.009006752632558346, 1.0826253856066614e-05, -0.010951535776257515)\n",
      "\n",
      "L2T2_lora_B_out: (0.011695301160216331, 3.959950845455751e-05, -0.010141356848180294)\n",
      "L2T2_lora_out: (0.023390602320432663, 7.919901690911502e-05, -0.020282713696360588)\n",
      "\n",
      "g_out: (0.01558559201657772, 6.837277032900602e-05, -0.016436750069260597)\n",
      "L1T2_out: (0.019618907943367958, 9.372308704769239e-05, -0.020519331097602844)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03525417670607567, -4.5607150241266936e-05, -0.04879323020577431)\n",
      "L1T1_lora_out: (0.07050835341215134, -9.121430048253387e-05, -0.09758646041154861)\n",
      "\n",
      "L2T1_lora_B_out: (0.07536505162715912, -0.00013484254304785281, -0.045762885361909866)\n",
      "L2T1_lora_out: (0.15073010325431824, -0.00026968508609570563, -0.09152577072381973)\n",
      "\n",
      "L2T2_lora_B_out: (0.13835439085960388, -0.0002974019153043628, -0.17603257298469543)\n",
      "L2T2_lora_out: (0.27670878171920776, -0.0005948038306087255, -0.35206514596939087)\n",
      "\n",
      "g_out: (0.16362139582633972, -0.00032511871540918946, -0.27461516857147217)\n",
      "L1T2_out: (0.23300272226333618, -0.00041633303044363856, -0.3722016215324402)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.026163509115576744, -8.95412995305378e-06, -0.02180728130042553)\n",
      "L1T1_lora_out: (0.05232701823115349, -1.790825990610756e-05, -0.04361456260085106)\n",
      "\n",
      "L2T1_lora_B_out: (0.02589794620871544, -5.971842711005593e-06, -0.02538946643471718)\n",
      "L2T1_lora_out: (0.05179589241743088, -1.1943685422011185e-05, -0.05077893286943436)\n",
      "\n",
      "L2T2_lora_B_out: (0.08549249172210693, -1.9558048734324984e-05, -0.06316487491130829)\n",
      "L2T2_lora_out: (0.17098498344421387, -3.911609746864997e-05, -0.12632974982261658)\n",
      "\n",
      "g_out: (0.13513903319835663, -2.717240022320766e-05, -0.09578201919794083)\n",
      "L1T2_out: (0.18746605515480042, -4.5080647396389395e-05, -0.13301873207092285)\n",
      "\n",
      "================================================================\n",
      "model.layers.9.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.8.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.022610343992710114, 1.5030156646389514e-05, -0.023972099646925926)\n",
      "L1T1_lora_out: (0.04522068798542023, 3.006031329277903e-05, -0.04794419929385185)\n",
      "\n",
      "L2T1_lora_B_out: (0.032993707805871964, 8.94444456207566e-05, -0.028278296813368797)\n",
      "L2T1_lora_out: (0.06598741561174393, 0.0001788888912415132, -0.056556593626737595)\n",
      "\n",
      "L2T2_lora_B_out: (0.05971141159534454, 0.00012987531954422593, -0.07273777574300766)\n",
      "L2T2_lora_out: (0.11942282319068909, 0.00025975063908845186, -0.14547555148601532)\n",
      "\n",
      "g_out: (0.08825314044952393, 8.086172601906583e-05, -0.10635009407997131)\n",
      "L1T2_out: (0.12839281558990479, 0.00011092205386376008, -0.1472521275281906)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06314335763454437, 0.0003563318168744445, -0.06672868877649307)\n",
      "L1T1_lora_out: (0.12628671526908875, 0.000712663633748889, -0.13345737755298615)\n",
      "\n",
      "L2T1_lora_B_out: (0.03312239050865173, 0.00017842224042396992, -0.026389632374048233)\n",
      "L2T1_lora_out: (0.06624478101730347, 0.00035684448084793985, -0.052779264748096466)\n",
      "\n",
      "L2T2_lora_B_out: (0.11909586936235428, 0.0007314867107197642, -0.12365313619375229)\n",
      "L2T2_lora_out: (0.23819173872470856, 0.0014629734214395285, -0.24730627238750458)\n",
      "\n",
      "g_out: (0.20555379986763, 0.0011061287950724363, -0.22125592827796936)\n",
      "L1T2_out: (0.31587114930152893, 0.0018187923124060035, -0.3464755415916443)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02124049700796604, -3.625527460826561e-05, -0.02030937932431698)\n",
      "L1T1_lora_out: (0.04248099401593208, -7.251054921653122e-05, -0.04061875864863396)\n",
      "\n",
      "L2T1_lora_B_out: (0.020245369523763657, -3.688801371026784e-05, -0.02283569611608982)\n",
      "L2T1_lora_out: (0.04049073904752731, -7.377602742053568e-05, -0.04567139223217964)\n",
      "\n",
      "L2T2_lora_B_out: (0.06384515762329102, -0.00016983864770736545, -0.06222236901521683)\n",
      "L2T2_lora_out: (0.12769031524658203, -0.0003396772954147309, -0.12444473803043365)\n",
      "\n",
      "g_out: (0.09261748194694519, -0.00026590132620185614, -0.0919230505824089)\n",
      "L1T2_out: (0.1342070996761322, -0.0003384117444511503, -0.1302589774131775)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0037736587692052126, -4.430934041010914e-06, -0.004202552139759064)\n",
      "L1T1_lora_out: (0.007547317538410425, -8.861868082021829e-06, -0.008405104279518127)\n",
      "\n",
      "L2T1_lora_B_out: (0.003899791045114398, -7.00558302924037e-06, -0.003552924608811736)\n",
      "L2T1_lora_out: (0.007799582090228796, -1.401116605848074e-05, -0.007105849217623472)\n",
      "\n",
      "L2T2_lora_B_out: (0.011042078025639057, -1.800142490537837e-05, -0.01082761213183403)\n",
      "L2T2_lora_out: (0.022084156051278114, -3.600284981075674e-05, -0.02165522426366806)\n",
      "\n",
      "g_out: (0.016537770628929138, -2.19916819332866e-05, -0.01865304633975029)\n",
      "L1T2_out: (0.022406375035643578, -3.085354910581373e-05, -0.027058150619268417)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.072112537920475, -4.321204687585123e-05, -0.11280635744333267)\n",
      "L1T1_lora_out: (0.14422507584095, -8.642409375170246e-05, -0.22561271488666534)\n",
      "\n",
      "L2T1_lora_B_out: (0.058249324560165405, -7.565740816062316e-05, -0.04121560603380203)\n",
      "L2T1_lora_out: (0.11649864912033081, -0.00015131481632124633, -0.08243121206760406)\n",
      "\n",
      "L2T2_lora_B_out: (0.06505896151065826, -0.00011494844511616975, -0.08263552933931351)\n",
      "L2T2_lora_out: (0.13011792302131653, -0.0002298968902323395, -0.16527105867862701)\n",
      "\n",
      "g_out: (0.12729568779468536, -7.858205935917795e-05, -0.15901508927345276)\n",
      "L1T2_out: (0.2366636097431183, -0.0001650061458349228, -0.33791816234588623)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.026499561965465546, -1.307739603362279e-05, -0.0215771421790123)\n",
      "L1T1_lora_out: (0.05299912393093109, -2.615479206724558e-05, -0.0431542843580246)\n",
      "\n",
      "L2T1_lora_B_out: (0.01797020249068737, 1.7473385582889023e-07, -0.01587679237127304)\n",
      "L2T1_lora_out: (0.03594040498137474, 3.4946771165778046e-07, -0.03175358474254608)\n",
      "\n",
      "L2T2_lora_B_out: (0.06519696861505508, -1.1330650522722863e-05, -0.05451926961541176)\n",
      "L2T2_lora_out: (0.13039393723011017, -2.2661301045445725e-05, -0.10903853923082352)\n",
      "\n",
      "g_out: (0.10716519504785538, -2.3010763470665552e-05, -0.08444319665431976)\n",
      "L1T2_out: (0.15966933965682983, -4.9165551899932325e-05, -0.11965049058198929)\n",
      "\n",
      "================================================================\n",
      "model.layers.8.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.7.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.022975610569119453, -9.015308023663238e-06, -0.019208529964089394)\n",
      "L1T1_lora_out: (0.04595122113823891, -1.8030616047326475e-05, -0.03841705992817879)\n",
      "\n",
      "L2T1_lora_B_out: (0.027119241654872894, 2.517922803235706e-05, -0.028315631672739983)\n",
      "L2T1_lora_out: (0.05423848330974579, 5.035845606471412e-05, -0.056631263345479965)\n",
      "\n",
      "L2T2_lora_B_out: (0.0601327009499073, 7.5679590736399405e-06, -0.05181514844298363)\n",
      "L2T2_lora_out: (0.1202654018998146, 1.5135918147279881e-05, -0.10363029688596725)\n",
      "\n",
      "g_out: (0.08263859897851944, -3.522253973642364e-05, -0.07832987606525421)\n",
      "L1T2_out: (0.11695699393749237, -5.325316305970773e-05, -0.1051110178232193)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04184029623866081, 5.003419573768042e-05, -0.03621739521622658)\n",
      "L1T1_lora_out: (0.08368059247732162, 0.00010006839147536084, -0.07243479043245316)\n",
      "\n",
      "L2T1_lora_B_out: (0.02082991786301136, -0.00013020091864746064, -0.022369444370269775)\n",
      "L2T1_lora_out: (0.04165983572602272, -0.0002604018372949213, -0.04473888874053955)\n",
      "\n",
      "L2T2_lora_B_out: (0.06888516247272491, -9.044991747941822e-05, -0.06881631910800934)\n",
      "L2T2_lora_out: (0.13777032494544983, -0.00018089983495883644, -0.13763263821601868)\n",
      "\n",
      "g_out: (0.12275366485118866, 7.950200233608484e-05, -0.12283545732498169)\n",
      "L1T2_out: (0.1876908242702484, 0.00017957028467208147, -0.19127222895622253)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.016869831830263138, 6.389690679498017e-05, -0.01715654879808426)\n",
      "L1T1_lora_out: (0.033739663660526276, 0.00012779381358996034, -0.03431309759616852)\n",
      "\n",
      "L2T1_lora_B_out: (0.019441675394773483, -4.929544047627132e-06, -0.023387819528579712)\n",
      "L2T1_lora_out: (0.03888335078954697, -9.859088095254265e-06, -0.046775639057159424)\n",
      "\n",
      "L2T2_lora_B_out: (0.04139738157391548, 4.954973701387644e-05, -0.04165356233716011)\n",
      "L2T2_lora_out: (0.08279476314783096, 9.909947402775288e-05, -0.08330712467432022)\n",
      "\n",
      "g_out: (0.06376584619283676, 0.00010895851301029325, -0.06765665113925934)\n",
      "L1T2_out: (0.08843272924423218, 0.00023675229749642313, -0.09372128546237946)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.004789356142282486, -9.413308362127282e-06, -0.0047788252122700214)\n",
      "L1T1_lora_out: (0.009578712284564972, -1.8826616724254563e-05, -0.009557650424540043)\n",
      "\n",
      "L2T1_lora_B_out: (0.0035274545662105083, 1.550190518173622e-06, -0.003458684775978327)\n",
      "L2T1_lora_out: (0.007054909132421017, 3.100381036347244e-06, -0.006917369551956654)\n",
      "\n",
      "L2T2_lora_B_out: (0.010922874324023724, -1.2779115422745235e-05, -0.011522367596626282)\n",
      "L2T2_lora_out: (0.021845748648047447, -2.555823084549047e-05, -0.023044735193252563)\n",
      "\n",
      "g_out: (0.019026830792427063, -2.865860005840659e-05, -0.02085314877331257)\n",
      "L1T2_out: (0.027443744242191315, -4.748521314468235e-05, -0.030410800129175186)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05756295472383499, -0.00011104383156634867, -0.04676653817296028)\n",
      "L1T1_lora_out: (0.11512590944766998, -0.00022208766313269734, -0.09353307634592056)\n",
      "\n",
      "L2T1_lora_B_out: (0.09632705897092819, -0.00014060278772376478, -0.052985820919275284)\n",
      "L2T1_lora_out: (0.19265411794185638, -0.00028120557544752955, -0.10597164183855057)\n",
      "\n",
      "L2T2_lora_B_out: (0.207667276263237, -0.000388919870601967, -0.17059510946273804)\n",
      "L2T2_lora_out: (0.415334552526474, -0.000777839741203934, -0.3411902189254761)\n",
      "\n",
      "g_out: (0.2857774794101715, -0.0004966342821717262, -0.24979084730148315)\n",
      "L1T2_out: (0.3863447308540344, -0.0007187218870967627, -0.3433239161968231)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020757243037223816, 3.5526041756384075e-05, -0.02117803879082203)\n",
      "L1T1_lora_out: (0.04151448607444763, 7.105208351276815e-05, -0.04235607758164406)\n",
      "\n",
      "L2T1_lora_B_out: (0.02375325933098793, -7.617419669259107e-06, -0.020066246390342712)\n",
      "L2T1_lora_out: (0.04750651866197586, -1.5234839338518213e-05, -0.040132492780685425)\n",
      "\n",
      "L2T2_lora_B_out: (0.0783977210521698, 7.961039955262095e-05, -0.051182474941015244)\n",
      "L2T2_lora_out: (0.1567954421043396, 0.0001592207991052419, -0.10236494988203049)\n",
      "\n",
      "g_out: (0.129734069108963, 0.00017445566481910646, -0.08321435749530792)\n",
      "L1T2_out: (0.16959039866924286, 0.00024550771922804415, -0.12557043135166168)\n",
      "\n",
      "================================================================\n",
      "model.layers.7.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.6.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021487940102815628, 7.046038808766752e-05, -0.023524880409240723)\n",
      "L1T1_lora_out: (0.042975880205631256, 0.00014092077617533505, -0.047049760818481445)\n",
      "\n",
      "L2T1_lora_B_out: (0.016945606097579002, 1.2507322026067413e-05, -0.020322700962424278)\n",
      "L2T1_lora_out: (0.033891212195158005, 2.5014644052134827e-05, -0.040645401924848557)\n",
      "\n",
      "L2T2_lora_B_out: (0.05990399047732353, 0.00014718387683387846, -0.05383993312716484)\n",
      "L2T2_lora_out: (0.11980798095464706, 0.0002943677536677569, -0.10767986625432968)\n",
      "\n",
      "g_out: (0.09202241897583008, 0.0002693531569093466, -0.09537988156080246)\n",
      "L1T2_out: (0.12850825488567352, 0.0004102739039808512, -0.1327882558107376)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03514368459582329, -4.813041232409887e-05, -0.03646579012274742)\n",
      "L1T1_lora_out: (0.07028736919164658, -9.626082464819774e-05, -0.07293158024549484)\n",
      "\n",
      "L2T1_lora_B_out: (0.02824123576283455, -4.0515409637009725e-05, -0.02495616488158703)\n",
      "L2T1_lora_out: (0.0564824715256691, -8.103081927401945e-05, -0.04991232976317406)\n",
      "\n",
      "L2T2_lora_B_out: (0.0752091333270073, 0.00013587175635620952, -0.09142110496759415)\n",
      "L2T2_lora_out: (0.1504182666540146, 0.00027174351271241903, -0.1828422099351883)\n",
      "\n",
      "g_out: (0.12443232536315918, 0.00035277436836622655, -0.14089232683181763)\n",
      "L1T2_out: (0.19104081392288208, 0.0002565135946497321, -0.2079651951789856)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.017406154423952103, 7.827449007891119e-05, -0.017715582624077797)\n",
      "L1T1_lora_out: (0.034812308847904205, 0.00015654898015782237, -0.035431165248155594)\n",
      "\n",
      "L2T1_lora_B_out: (0.016335254535079002, 5.956059249001555e-05, -0.016493557021021843)\n",
      "L2T1_lora_out: (0.032670509070158005, 0.0001191211849800311, -0.032987114042043686)\n",
      "\n",
      "L2T2_lora_B_out: (0.04968991503119469, 0.00019134057220071554, -0.048609256744384766)\n",
      "L2T2_lora_out: (0.09937983006238937, 0.0003826811444014311, -0.09721851348876953)\n",
      "\n",
      "g_out: (0.0835580825805664, 0.00026355989393778145, -0.07000561058521271)\n",
      "L1T2_out: (0.11837039142847061, 0.00042010884499177337, -0.09911125153303146)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005269446410238743, 4.382796760182828e-06, -0.0054239328019320965)\n",
      "L1T1_lora_out: (0.010538892820477486, 8.765593520365655e-06, -0.010847865603864193)\n",
      "\n",
      "L2T1_lora_B_out: (0.0037312933709472418, 2.9072293727949727e-06, -0.003230128902941942)\n",
      "L2T1_lora_out: (0.0074625867418944836, 5.8144587455899455e-06, -0.006460257805883884)\n",
      "\n",
      "L2T2_lora_B_out: (0.008025354705750942, 9.630623026168905e-06, -0.009447813965380192)\n",
      "L2T2_lora_out: (0.016050709411501884, 1.926124605233781e-05, -0.018895627930760384)\n",
      "\n",
      "g_out: (0.014879010617733002, 1.3446780940284953e-05, -0.01752949133515358)\n",
      "L1T2_out: (0.025417903438210487, 2.2212356270756572e-05, -0.028377357870340347)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.06158486381173134, -0.00011604427709244192, -0.048949841409921646)\n",
      "L1T1_lora_out: (0.12316972762346268, -0.00023208855418488383, -0.09789968281984329)\n",
      "\n",
      "L2T1_lora_B_out: (0.051977742463350296, -0.0002478279930073768, -0.05962011590600014)\n",
      "L2T1_lora_out: (0.10395548492670059, -0.0004956559860147536, -0.11924023181200027)\n",
      "\n",
      "L2T2_lora_B_out: (0.12220480293035507, -0.0005486978334374726, -0.14256393909454346)\n",
      "L2T2_lora_out: (0.24440960586071014, -0.0010973956668749452, -0.2851278781890869)\n",
      "\n",
      "g_out: (0.2129639983177185, -0.0006017396808601916, -0.17792877554893494)\n",
      "L1T2_out: (0.32527831196784973, -0.0008338281768374145, -0.27582845091819763)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.043952472507953644, 1.6042369679780677e-05, -0.04285642132163048)\n",
      "L1T1_lora_out: (0.08790494501590729, 3.2084739359561354e-05, -0.08571284264326096)\n",
      "\n",
      "L2T1_lora_B_out: (0.023188024759292603, 2.3021271772449836e-05, -0.023781098425388336)\n",
      "L2T1_lora_out: (0.046376049518585205, 4.604254354489967e-05, -0.04756219685077667)\n",
      "\n",
      "L2T2_lora_B_out: (0.0994473323225975, 7.759754225844517e-05, -0.12331004440784454)\n",
      "L2T2_lora_out: (0.198894664645195, 0.00015519508451689035, -0.2466200888156891)\n",
      "\n",
      "g_out: (0.1560177356004715, 0.00010915252642007545, -0.20074757933616638)\n",
      "L1T2_out: (0.24392268061637878, 0.00014123728033155203, -0.28646042943000793)\n",
      "\n",
      "================================================================\n",
      "model.layers.6.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.5.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02007506974041462, 3.876711161865387e-06, -0.021719755604863167)\n",
      "L1T1_lora_out: (0.04015013948082924, 7.753422323730774e-06, -0.043439511209726334)\n",
      "\n",
      "L2T1_lora_B_out: (0.02768080122768879, 7.873280992498621e-06, -0.029006710276007652)\n",
      "L2T1_lora_out: (0.05536160245537758, 1.5746561984997243e-05, -0.058013420552015305)\n",
      "\n",
      "L2T2_lora_B_out: (0.07693173736333847, 3.970659599872306e-05, -0.06978750973939896)\n",
      "L2T2_lora_out: (0.15386347472667694, 7.941319199744612e-05, -0.1395750194787979)\n",
      "\n",
      "g_out: (0.09850187599658966, 6.366665184032172e-05, -0.10388664901256561)\n",
      "L1T2_out: (0.12440860271453857, 7.142003596527502e-05, -0.14691928029060364)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.028877202421426773, -0.0001578818482812494, -0.025657638907432556)\n",
      "L1T1_lora_out: (0.057754404842853546, -0.0003157636965624988, -0.05131527781486511)\n",
      "\n",
      "L2T1_lora_B_out: (0.01568630523979664, 6.465087062679231e-05, -0.014151358045637608)\n",
      "L2T1_lora_out: (0.03137261047959328, 0.00012930174125358462, -0.028302716091275215)\n",
      "\n",
      "L2T2_lora_B_out: (0.05581699311733246, -0.00016549455176573247, -0.06805513054132462)\n",
      "L2T2_lora_out: (0.11163398623466492, -0.00033098910353146493, -0.13611026108264923)\n",
      "\n",
      "g_out: (0.09364920109510422, -0.0004602906992658973, -0.11645521223545074)\n",
      "L1T2_out: (0.1508631408214569, -0.000776054454036057, -0.16452667117118835)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03441103547811508, -1.515837629995076e-05, -0.044394806027412415)\n",
      "L1T1_lora_out: (0.06882207095623016, -3.031675259990152e-05, -0.08878961205482483)\n",
      "\n",
      "L2T1_lora_B_out: (0.02018480934202671, 0.0001250837231054902, -0.01585506461560726)\n",
      "L2T1_lora_out: (0.04036961868405342, 0.0002501674462109804, -0.03171012923121452)\n",
      "\n",
      "L2T2_lora_B_out: (0.09433558583259583, 7.442622882081196e-05, -0.10014394670724869)\n",
      "L2T2_lora_out: (0.18867117166519165, 0.00014885245764162391, -0.20028789341449738)\n",
      "\n",
      "g_out: (0.15964485704898834, -0.00010131499584531412, -0.18322937190532684)\n",
      "L1T2_out: (0.21918053925037384, -0.00013163175026420504, -0.2720189690589905)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0028081473428756, -1.812136133594322e-06, -0.0033893780782818794)\n",
      "L1T1_lora_out: (0.0056162946857512, -3.624272267188644e-06, -0.006778756156563759)\n",
      "\n",
      "L2T1_lora_B_out: (0.0019339114660397172, -3.6101396290177945e-06, -0.002007710747420788)\n",
      "L2T1_lora_out: (0.0038678229320794344, -7.220279258035589e-06, -0.004015421494841576)\n",
      "\n",
      "L2T2_lora_B_out: (0.006196720991283655, -2.7496960683492944e-05, -0.006106822285801172)\n",
      "L2T2_lora_out: (0.01239344198256731, -5.499392136698589e-05, -0.012213644571602345)\n",
      "\n",
      "g_out: (0.009785226546227932, -4.777363938046619e-05, -0.010129420086741447)\n",
      "L1T2_out: (0.014231942594051361, -5.139790300745517e-05, -0.01684238202869892)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.027411475777626038, -2.795740329020191e-05, -0.030927903950214386)\n",
      "L1T1_lora_out: (0.054822951555252075, -5.591480658040382e-05, -0.06185580790042877)\n",
      "\n",
      "L2T1_lora_B_out: (0.08126167207956314, -0.00010832468251464888, -0.07404813915491104)\n",
      "L2T1_lora_out: (0.16252334415912628, -0.00021664936502929777, -0.14809627830982208)\n",
      "\n",
      "L2T2_lora_B_out: (0.14674915373325348, -0.0001440385531168431, -0.16435375809669495)\n",
      "L2T2_lora_out: (0.29349830746650696, -0.0002880771062336862, -0.3287075161933899)\n",
      "\n",
      "g_out: (0.19367527961730957, -7.142772665247321e-05, -0.19551555812358856)\n",
      "L1T2_out: (0.23576851189136505, -0.00012734254414681345, -0.25737136602401733)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.029140563681721687, 5.507139940164052e-05, -0.028123419731855392)\n",
      "L1T1_lora_out: (0.058281127363443375, 0.00011014279880328104, -0.056246839463710785)\n",
      "\n",
      "L2T1_lora_B_out: (0.04355112835764885, 2.53322741627926e-05, -0.03952259570360184)\n",
      "L2T1_lora_out: (0.0871022567152977, 5.06645483255852e-05, -0.07904519140720367)\n",
      "\n",
      "L2T2_lora_B_out: (0.0989447683095932, 0.00010486340761417523, -0.09427950531244278)\n",
      "L2T2_lora_out: (0.1978895366191864, 0.00020972681522835046, -0.18855901062488556)\n",
      "\n",
      "g_out: (0.1521938443183899, 0.00015906229964457452, -0.1236967146396637)\n",
      "L1T2_out: (0.21047496795654297, 0.0002692050766199827, -0.17524650692939758)\n",
      "\n",
      "================================================================\n",
      "model.layers.5.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.4.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01904020644724369, -1.752052230585832e-05, -0.017266832292079926)\n",
      "L1T1_lora_out: (0.03808041289448738, -3.504104461171664e-05, -0.03453366458415985)\n",
      "\n",
      "L2T1_lora_B_out: (0.019354719668626785, 4.322190216043964e-05, -0.02072816900908947)\n",
      "L2T1_lora_out: (0.03870943933725357, 8.644380432087928e-05, -0.04145633801817894)\n",
      "\n",
      "L2T2_lora_B_out: (0.03382312133908272, 2.8929818654432893e-05, -0.03325323760509491)\n",
      "L2T2_lora_out: (0.06764624267816544, 5.7859637308865786e-05, -0.06650647521018982)\n",
      "\n",
      "g_out: (0.06882838159799576, -2.858412881323602e-05, -0.06655798852443695)\n",
      "L1T2_out: (0.0989021435379982, -6.362518615787849e-05, -0.09543865919113159)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.015456191264092922, -3.418159030843526e-05, -0.01729300059378147)\n",
      "L1T1_lora_out: (0.030912382528185844, -6.836318061687052e-05, -0.03458600118756294)\n",
      "\n",
      "L2T1_lora_B_out: (0.018226258456707, -3.087782533839345e-05, -0.01624462567269802)\n",
      "L2T1_lora_out: (0.036452516913414, -6.17556506767869e-05, -0.03248925134539604)\n",
      "\n",
      "L2T2_lora_B_out: (0.042305637151002884, -9.03788604773581e-05, -0.046899594366550446)\n",
      "L2T2_lora_out: (0.08461127430200577, -0.0001807577209547162, -0.09379918873310089)\n",
      "\n",
      "g_out: (0.06551852822303772, -0.00011900205572601408, -0.07422663271427155)\n",
      "L1T2_out: (0.08360889554023743, -0.00018736526544671506, -0.10881263017654419)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.025556376203894615, 3.9819595258450136e-05, -0.023765835911035538)\n",
      "L1T1_lora_out: (0.05111275240778923, 7.963919051690027e-05, -0.047531671822071075)\n",
      "\n",
      "L2T1_lora_B_out: (0.017548346891999245, 1.297506969422102e-05, -0.01843593642115593)\n",
      "L2T1_lora_out: (0.03509669378399849, 2.595013938844204e-05, -0.03687187284231186)\n",
      "\n",
      "L2T2_lora_B_out: (0.044175632297992706, 9.358444367535412e-05, -0.042061299085617065)\n",
      "L2T2_lora_out: (0.08835126459598541, 0.00018716888735070825, -0.08412259817123413)\n",
      "\n",
      "g_out: (0.0774170309305191, 0.00016121873341035098, -0.07541808485984802)\n",
      "L1T2_out: (0.11310562491416931, 0.00024085798941086978, -0.11515568196773529)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0018003421137109399, 8.700764738023281e-06, -0.0018203537911176682)\n",
      "L1T1_lora_out: (0.0036006842274218798, 1.7401529476046562e-05, -0.0036407075822353363)\n",
      "\n",
      "L2T1_lora_B_out: (0.0012437471887096763, 1.0446278793097008e-06, -0.0011433976469561458)\n",
      "L2T1_lora_out: (0.0024874943774193525, 2.0892557586194016e-06, -0.0022867952939122915)\n",
      "\n",
      "L2T2_lora_B_out: (0.0038781180046498775, 1.0736730473581702e-05, -0.003668013494461775)\n",
      "L2T2_lora_out: (0.007756236009299755, 2.1473460947163403e-05, -0.00733602698892355)\n",
      "\n",
      "g_out: (0.006729216314852238, 1.938420973601751e-05, -0.00631004199385643)\n",
      "L1T2_out: (0.009651292115449905, 3.678574285004288e-05, -0.009448076598346233)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.10928814113140106, -5.062659693066962e-05, -0.08148662745952606)\n",
      "L1T1_lora_out: (0.21857628226280212, -0.00010125319386133924, -0.16297325491905212)\n",
      "\n",
      "L2T1_lora_B_out: (0.06608428806066513, 6.662862142547965e-05, -0.04092627763748169)\n",
      "L2T1_lora_out: (0.13216857612133026, 0.0001332572428509593, -0.08185255527496338)\n",
      "\n",
      "L2T2_lora_B_out: (0.20256824791431427, 4.381508188089356e-05, -0.13410405814647675)\n",
      "L2T2_lora_out: (0.40513649582862854, 8.763016376178712e-05, -0.2682081162929535)\n",
      "\n",
      "g_out: (0.33693844079971313, -4.562714457279071e-05, -0.24251624941825867)\n",
      "L1T2_out: (0.5307025909423828, -0.0001468803093302995, -0.4054895043373108)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.014255732297897339, -3.857309366139816e-06, -0.01964458078145981)\n",
      "L1T1_lora_out: (0.028511464595794678, -7.714618732279632e-06, -0.03928916156291962)\n",
      "\n",
      "L2T1_lora_B_out: (0.01772545836865902, 1.0165076673729345e-05, -0.01200275681912899)\n",
      "L2T1_lora_out: (0.03545091673731804, 2.033015334745869e-05, -0.02400551363825798)\n",
      "\n",
      "L2T2_lora_B_out: (0.026072748005390167, 2.1643634681822732e-05, -0.025887517258524895)\n",
      "L2T2_lora_out: (0.052145496010780334, 4.3287269363645464e-05, -0.05177503451704979)\n",
      "\n",
      "g_out: (0.044998899102211, 2.2957110559218563e-05, -0.04360920190811157)\n",
      "L1T2_out: (0.06821854412555695, 1.5242510926327668e-05, -0.08032163977622986)\n",
      "\n",
      "================================================================\n",
      "model.layers.4.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.3.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.02863040380179882, 9.99440235318616e-05, -0.03127958998084068)\n",
      "L1T1_lora_out: (0.05726080760359764, 0.0001998880470637232, -0.06255917996168137)\n",
      "\n",
      "L2T1_lora_B_out: (0.017718691378831863, -4.344513854448451e-06, -0.01956755854189396)\n",
      "L2T1_lora_out: (0.03543738275766373, -8.689027708896901e-06, -0.03913511708378792)\n",
      "\n",
      "L2T2_lora_B_out: (0.048162732273340225, 0.0002242646150989458, -0.07196137309074402)\n",
      "L2T2_lora_out: (0.09632546454668045, 0.0004485292301978916, -0.14392274618148804)\n",
      "\n",
      "g_out: (0.10089735686779022, 0.0004572183242999017, -0.11971588432788849)\n",
      "L1T2_out: (0.15815816819667816, 0.0006571062840521336, -0.18227505683898926)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.015015986748039722, 4.876801904174499e-05, -0.014609637670218945)\n",
      "L1T1_lora_out: (0.030031973496079445, 9.753603808348998e-05, -0.02921927534043789)\n",
      "\n",
      "L2T1_lora_B_out: (0.01336844451725483, -4.688306580646895e-05, -0.011754806153476238)\n",
      "L2T1_lora_out: (0.02673688903450966, -9.37661316129379e-05, -0.023509612306952477)\n",
      "\n",
      "L2T2_lora_B_out: (0.03473106771707535, 4.4098727812524885e-05, -0.033581774681806564)\n",
      "L2T2_lora_out: (0.0694621354341507, 8.819745562504977e-05, -0.06716354936361313)\n",
      "\n",
      "g_out: (0.053926195949316025, 0.0001819635508581996, -0.06157875061035156)\n",
      "L1T2_out: (0.07686467468738556, 0.00027949962532147765, -0.08376110345125198)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021597763523459435, 1.2078561667294707e-05, -0.02021394856274128)\n",
      "L1T1_lora_out: (0.04319552704691887, 2.4157123334589414e-05, -0.04042789712548256)\n",
      "\n",
      "L2T1_lora_B_out: (0.012218157760798931, -5.124395465827547e-05, -0.010425787419080734)\n",
      "L2T1_lora_out: (0.024436315521597862, -0.00010248790931655094, -0.02085157483816147)\n",
      "\n",
      "L2T2_lora_B_out: (0.04497203230857849, -8.040078682824969e-05, -0.052581001073122025)\n",
      "L2T2_lora_out: (0.08994406461715698, -0.00016080157365649939, -0.10516200214624405)\n",
      "\n",
      "g_out: (0.08040471374988556, -5.831365706399083e-05, -0.08781759440898895)\n",
      "L1T2_out: (0.11758685111999512, -3.4156568290200084e-05, -0.1282454878091812)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.00208045425824821, 4.21721506427275e-06, -0.0021323764231055975)\n",
      "L1T1_lora_out: (0.00416090851649642, 8.4344301285455e-06, -0.004264752846211195)\n",
      "\n",
      "L2T1_lora_B_out: (0.0013101111399009824, 3.9453325371141545e-06, -0.0011398288188502192)\n",
      "L2T1_lora_out: (0.0026202222798019648, 7.890665074228309e-06, -0.0022796576377004385)\n",
      "\n",
      "L2T2_lora_B_out: (0.003955925814807415, 1.2254953617230058e-05, -0.0042608496733009815)\n",
      "L2T2_lora_out: (0.00791185162961483, 2.4509907234460115e-05, -0.008521699346601963)\n",
      "\n",
      "g_out: (0.006820155307650566, 1.661924397922121e-05, -0.007467494811862707)\n",
      "L1T2_out: (0.01003161258995533, 2.505367410776671e-05, -0.01105921994894743)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.020563311874866486, 4.955757958668983e-06, -0.020826023072004318)\n",
      "L1T1_lora_out: (0.04112662374973297, 9.911515917337965e-06, -0.041652046144008636)\n",
      "\n",
      "L2T1_lora_B_out: (0.023566633462905884, -6.9664883994846605e-06, -0.024705586954951286)\n",
      "L2T1_lora_out: (0.04713326692581177, -1.3932976798969321e-05, -0.04941117390990257)\n",
      "\n",
      "L2T2_lora_B_out: (0.05083231255412102, 3.5555083286453737e-06, -0.05165070667862892)\n",
      "L2T2_lora_out: (0.10166462510824203, 7.111016657290747e-06, -0.10330141335725784)\n",
      "\n",
      "g_out: (0.08302921801805496, 2.1043977540102787e-05, -0.07538190484046936)\n",
      "L1T2_out: (0.12301802635192871, 3.095552165177651e-05, -0.10683439671993256)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01887110248208046, 1.0357184692111332e-05, -0.017004288733005524)\n",
      "L1T1_lora_out: (0.03774220496416092, 2.0714369384222664e-05, -0.03400857746601105)\n",
      "\n",
      "L2T1_lora_B_out: (0.024650059640407562, 1.1528137292771135e-05, -0.019406411796808243)\n",
      "L2T1_lora_out: (0.049300119280815125, 2.305627458554227e-05, -0.038812823593616486)\n",
      "\n",
      "L2T2_lora_B_out: (0.05951937660574913, 4.1330582462251186e-05, -0.06178600341081619)\n",
      "L2T2_lora_out: (0.11903875321149826, 8.266116492450237e-05, -0.12357200682163239)\n",
      "\n",
      "g_out: (0.09329961985349655, 5.960491034784354e-05, -0.09491395950317383)\n",
      "L1T2_out: (0.13104182481765747, 8.0319274275098e-05, -0.12722499668598175)\n",
      "\n",
      "================================================================\n",
      "model.layers.3.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.2.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01806771755218506, 6.105452484916896e-06, -0.016924256458878517)\n",
      "L1T1_lora_out: (0.03613543510437012, 1.2210904969833791e-05, -0.033848512917757034)\n",
      "\n",
      "L2T1_lora_B_out: (0.013497618027031422, -4.037204689666396e-06, -0.012387802824378014)\n",
      "L2T1_lora_out: (0.026995236054062843, -8.074409379332792e-06, -0.024775605648756027)\n",
      "\n",
      "L2T2_lora_B_out: (0.025289364159107208, -1.268797495868057e-05, -0.026121260598301888)\n",
      "L2T2_lora_out: (0.050578728318214417, -2.537594991736114e-05, -0.052242521196603775)\n",
      "\n",
      "g_out: (0.0464409776031971, -1.7301557818427682e-05, -0.04809010401368141)\n",
      "L1T2_out: (0.07576794922351837, -5.090638296678662e-06, -0.08078794181346893)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.03644375875592232, 0.0001263038720935583, -0.034529998898506165)\n",
      "L1T1_lora_out: (0.07288751751184464, 0.0002526077441871166, -0.06905999779701233)\n",
      "\n",
      "L2T1_lora_B_out: (0.010133032687008381, -3.0497991247102618e-05, -0.009359166026115417)\n",
      "L2T1_lora_out: (0.020266065374016762, -6.0995982494205236e-05, -0.018718332052230835)\n",
      "\n",
      "L2T2_lora_B_out: (0.08124188333749771, 0.00011055330833187327, -0.07003989815711975)\n",
      "L2T2_lora_out: (0.16248376667499542, 0.00022110661666374654, -0.1400797963142395)\n",
      "\n",
      "g_out: (0.1466459035873413, 0.000282102613709867, -0.12790939211845398)\n",
      "L1T2_out: (0.1990625411272049, 0.0005347104743123055, -0.1969693899154663)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.04073552414774895, 2.2345197066897526e-05, -0.04172387719154358)\n",
      "L1T1_lora_out: (0.0814710482954979, 4.469039413379505e-05, -0.08344775438308716)\n",
      "\n",
      "L2T1_lora_B_out: (0.023088505491614342, -2.258350832562428e-05, -0.0208408385515213)\n",
      "L2T1_lora_out: (0.046177010983228683, -4.516701665124856e-05, -0.0416816771030426)\n",
      "\n",
      "L2T2_lora_B_out: (0.09176009148359299, 2.695800139917992e-05, -0.09335771948099136)\n",
      "L2T2_lora_out: (0.18352018296718597, 5.391600279835984e-05, -0.18671543896198273)\n",
      "\n",
      "g_out: (0.16072587668895721, 9.908297215588391e-05, -0.1640452742576599)\n",
      "L1T2_out: (0.2421969175338745, 0.00014377341722138226, -0.24749302864074707)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.006093454081565142, -8.929801879276056e-06, -0.006077616475522518)\n",
      "L1T1_lora_out: (0.012186908163130283, -1.785960375855211e-05, -0.012155232951045036)\n",
      "\n",
      "L2T1_lora_B_out: (0.0026763221248984337, -8.962708193394064e-07, -0.002888704650104046)\n",
      "L2T1_lora_out: (0.005352644249796867, -1.7925416386788129e-06, -0.005777409300208092)\n",
      "\n",
      "L2T2_lora_B_out: (0.011920123361051083, -1.4651155652245507e-05, -0.012733908370137215)\n",
      "L2T2_lora_out: (0.023840246722102165, -2.9302311304491013e-05, -0.02546781674027443)\n",
      "\n",
      "g_out: (0.021672947332262993, -2.7509780920809135e-05, -0.02146003395318985)\n",
      "L1T2_out: (0.03385985642671585, -4.536937558441423e-05, -0.03049318492412567)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0707327350974083, -2.6035533664980903e-05, -0.06296531856060028)\n",
      "L1T1_lora_out: (0.1414654701948166, -5.2071067329961807e-05, -0.12593063712120056)\n",
      "\n",
      "L2T1_lora_B_out: (0.029919100925326347, -8.529834190085239e-07, -0.030939800664782524)\n",
      "L2T1_lora_out: (0.059838201850652695, -1.7059668380170478e-06, -0.06187960132956505)\n",
      "\n",
      "L2T2_lora_B_out: (0.11058509349822998, 5.111282916914206e-06, -0.12839700281620026)\n",
      "L2T2_lora_out: (0.22117018699645996, 1.0222565833828412e-05, -0.2567940056324005)\n",
      "\n",
      "g_out: (0.1976257562637329, 1.1928545973205473e-05, -0.21731352806091309)\n",
      "L1T2_out: (0.3390912413597107, -4.014256046502851e-05, -0.34324416518211365)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.019578345119953156, 1.3118064089212567e-05, -0.01749570667743683)\n",
      "L1T1_lora_out: (0.03915669023990631, 2.6236128178425133e-05, -0.03499141335487366)\n",
      "\n",
      "L2T1_lora_B_out: (0.02344536781311035, 1.4581961295334622e-05, -0.028140263631939888)\n",
      "L2T1_lora_out: (0.0468907356262207, 2.9163922590669245e-05, -0.056280527263879776)\n",
      "\n",
      "L2T2_lora_B_out: (0.05784020200371742, 5.624890036415309e-05, -0.05774389207363129)\n",
      "L2T2_lora_out: (0.11568040400743484, 0.00011249780072830617, -0.11548778414726257)\n",
      "\n",
      "g_out: (0.08794348686933517, 8.333387086167932e-05, -0.07207569479942322)\n",
      "L1T2_out: (0.1271001696586609, 0.00010957001359201968, -0.09797632694244385)\n",
      "\n",
      "================================================================\n",
      "model.layers.2.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.1.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.008210139349102974, 4.4754051486961544e-07, -0.0072332522831857204)\n",
      "L1T1_lora_out: (0.016420278698205948, 8.950810297392309e-07, -0.014466504566371441)\n",
      "\n",
      "L2T1_lora_B_out: (0.006062277127057314, -5.045543275628006e-06, -0.006374927703291178)\n",
      "L2T1_lora_out: (0.012124554254114628, -1.0091086551256012e-05, -0.012749855406582355)\n",
      "\n",
      "L2T2_lora_B_out: (0.02030194364488125, -1.0420544640510343e-05, -0.017776360735297203)\n",
      "L2T2_lora_out: (0.0406038872897625, -2.0841089281020686e-05, -0.035552721470594406)\n",
      "\n",
      "g_out: (0.03099501132965088, -1.0750008186732884e-05, -0.027999067679047585)\n",
      "L1T2_out: (0.04417916387319565, -9.85493079497246e-06, -0.03880295902490616)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.009729277342557907, 3.522485201301606e-07, -0.007644969504326582)\n",
      "L1T1_lora_out: (0.019458554685115814, 7.044970402603212e-07, -0.015289939008653164)\n",
      "\n",
      "L2T1_lora_B_out: (0.008091649040579796, -5.788200269307708e-06, -0.010573714971542358)\n",
      "L2T1_lora_out: (0.01618329808115959, -1.1576400538615417e-05, -0.021147429943084717)\n",
      "\n",
      "L2T2_lora_B_out: (0.02107861638069153, -1.3599985322798602e-05, -0.027618005871772766)\n",
      "L2T2_lora_out: (0.04215723276138306, -2.7199970645597205e-05, -0.05523601174354553)\n",
      "\n",
      "g_out: (0.034880176186561584, -1.5623538274667226e-05, -0.038182325661182404)\n",
      "L1T2_out: (0.0543387308716774, -1.4919055502105039e-05, -0.04742109775543213)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005091794766485691, 4.409344455780229e-06, -0.004819472786039114)\n",
      "L1T1_lora_out: (0.010183589532971382, 8.818688911560457e-06, -0.009638945572078228)\n",
      "\n",
      "L2T1_lora_B_out: (0.007443771231919527, -2.3585769667988643e-05, -0.0074156406335532665)\n",
      "L2T1_lora_out: (0.014887542463839054, -4.7171539335977286e-05, -0.014831281267106533)\n",
      "\n",
      "L2T2_lora_B_out: (0.019663071259856224, 1.1209095646336209e-05, -0.019948109984397888)\n",
      "L2T2_lora_out: (0.03932614251971245, 2.2418191292672418e-05, -0.039896219968795776)\n",
      "\n",
      "g_out: (0.030109530314803123, 6.958974699955434e-05, -0.030292121693491936)\n",
      "L1T2_out: (0.039681289345026016, 7.840841863071546e-05, -0.03947548568248749)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0029402347281575203, -8.959324986790307e-06, -0.003149444470182061)\n",
      "L1T1_lora_out: (0.005880469456315041, -1.7918649973580614e-05, -0.006298888940364122)\n",
      "\n",
      "L2T1_lora_B_out: (0.0018293034518137574, 2.3023594621918164e-06, -0.001757293939590454)\n",
      "L2T1_lora_out: (0.003658606903627515, 4.604718924383633e-06, -0.003514587879180908)\n",
      "\n",
      "L2T2_lora_B_out: (0.005407507065683603, -1.610648541827686e-05, -0.0059700608253479)\n",
      "L2T2_lora_out: (0.010815014131367207, -3.221297083655372e-05, -0.0119401216506958)\n",
      "\n",
      "g_out: (0.008914642035961151, -3.6817684303969145e-05, -0.009968291968107224)\n",
      "L1T2_out: (0.014522147364914417, -5.473634155350737e-05, -0.0159786157310009)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0354384146630764, -1.2761555808538105e-05, -0.0525871142745018)\n",
      "L1T1_lora_out: (0.0708768293261528, -2.552311161707621e-05, -0.1051742285490036)\n",
      "\n",
      "L2T1_lora_B_out: (0.1721680462360382, 1.5229039490805008e-05, -0.06731771677732468)\n",
      "L2T1_lora_out: (0.3443360924720764, 3.0458078981610015e-05, -0.13463543355464935)\n",
      "\n",
      "L2T2_lora_B_out: (0.3374742567539215, 3.2765616197139025e-05, -0.10700773447751999)\n",
      "L2T2_lora_out: (0.674948513507843, 6.553123239427805e-05, -0.21401546895503998)\n",
      "\n",
      "g_out: (0.3306124210357666, 3.507316068862565e-05, -0.13375644385814667)\n",
      "L1T2_out: (0.4014892578125, 9.550059075991157e-06, -0.23872233927249908)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0798613578081131, -1.3876509910915047e-05, -0.0880509465932846)\n",
      "L1T1_lora_out: (0.1597227156162262, -2.7753019821830094e-05, -0.1761018931865692)\n",
      "\n",
      "L2T1_lora_B_out: (0.08858136832714081, 3.05989378830418e-05, -0.221498504281044)\n",
      "L2T1_lora_out: (0.17716273665428162, 6.11978757660836e-05, -0.442997008562088)\n",
      "\n",
      "L2T2_lora_B_out: (0.24899567663669586, 3.017476274180808e-06, -0.5718624591827393)\n",
      "L2T2_lora_out: (0.4979913532733917, 6.034952548361616e-06, -1.1437249183654785)\n",
      "\n",
      "g_out: (0.44782620668411255, -5.516289093066007e-05, -0.710545539855957)\n",
      "L1T2_out: (0.6075489521026611, -8.291594713227823e-05, -0.8866474628448486)\n",
      "\n",
      "================================================================\n",
      "model.layers.1.mlp.down_proj\n",
      "================================================================\n",
      "================================================================\n",
      "model.layers.0.self_attn.q_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.05573910102248192, 1.1024943887605332e-05, -0.04592706635594368)\n",
      "L1T1_lora_out: (0.11147820204496384, 2.2049887775210664e-05, -0.09185413271188736)\n",
      "\n",
      "L2T1_lora_B_out: (0.04789327457547188, 0.00019716875976882875, -0.05974280461668968)\n",
      "L2T1_lora_out: (0.09578654915094376, 0.0003943375195376575, -0.11948560923337936)\n",
      "\n",
      "L2T2_lora_B_out: (0.1532118022441864, 0.000313695112708956, -0.16253496706485748)\n",
      "L2T2_lora_out: (0.3064236044883728, 0.000627390225417912, -0.32506993412971497)\n",
      "\n",
      "g_out: (0.24000284075737, 0.00023305288050323725, -0.21710637211799622)\n",
      "L1T2_out: (0.35148105025291443, 0.0002551026991568506, -0.308960497379303)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.k_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.021090907976031303, -3.678964276332408e-05, -0.023282183334231377)\n",
      "L1T1_lora_out: (0.04218181595206261, -7.357928552664816e-05, -0.04656436666846275)\n",
      "\n",
      "L2T1_lora_B_out: (0.03269142284989357, -0.0001890525163616985, -0.032031938433647156)\n",
      "L2T1_lora_out: (0.06538284569978714, -0.000378105032723397, -0.06406387686729431)\n",
      "\n",
      "L2T2_lora_B_out: (0.04024292156100273, -0.00014135301171336323, -0.0653601810336113)\n",
      "L2T2_lora_out: (0.08048584312200546, -0.00028270602342672646, -0.1307203620672226)\n",
      "\n",
      "g_out: (0.0706072524189949, 9.539897291688249e-05, -0.11090990900993347)\n",
      "L1T2_out: (0.11278906464576721, 2.181966556236148e-05, -0.15747427940368652)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.v_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.005514121148735285, -2.7487656552693807e-05, -0.006385340355336666)\n",
      "L1T1_lora_out: (0.01102824229747057, -5.497531310538761e-05, -0.012770680710673332)\n",
      "\n",
      "L2T1_lora_B_out: (0.004455916583538055, 6.06398452873691e-06, -0.00437548104673624)\n",
      "L2T1_lora_out: (0.00891183316707611, 1.212796905747382e-05, -0.00875096209347248)\n",
      "\n",
      "L2T2_lora_B_out: (0.014418411999940872, -2.960453821287956e-05, -0.015186331234872341)\n",
      "L2T2_lora_out: (0.028836823999881744, -5.920907642575912e-05, -0.030372662469744682)\n",
      "\n",
      "g_out: (0.024743549525737762, -7.133703911677003e-05, -0.02322794497013092)\n",
      "L1T2_out: (0.03493303060531616, -0.00012631237041205168, -0.03531135991215706)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.self_attn.o_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.0006652554147876799, -1.3039052646490745e-06, -0.000710756634362042)\n",
      "L1T1_lora_out: (0.0013305108295753598, -2.607810529298149e-06, -0.001421513268724084)\n",
      "\n",
      "L2T1_lora_B_out: (0.0011847891146317124, 1.5541626225967775e-06, -0.0009297866490669549)\n",
      "L2T1_lora_out: (0.002369578229263425, 3.108325245193555e-06, -0.0018595732981339097)\n",
      "\n",
      "L2T2_lora_B_out: (0.0027238570619374514, 6.774147323085344e-07, -0.002159038558602333)\n",
      "L2T2_lora_out: (0.005447714123874903, 1.3548294646170689e-06, -0.004318077117204666)\n",
      "\n",
      "g_out: (0.003936042543500662, -1.7534943026475958e-06, -0.0035113664343953133)\n",
      "L1T2_out: (0.005159071646630764, -4.3613049456325825e-06, -0.004788490012288094)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.gate_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01719825342297554, 2.4826416847645305e-05, -0.016772279515862465)\n",
      "L1T1_lora_out: (0.03439650684595108, 4.965283369529061e-05, -0.03354455903172493)\n",
      "\n",
      "L2T1_lora_B_out: (0.02204914763569832, 1.4443315194512252e-05, -0.014421598054468632)\n",
      "L2T1_lora_out: (0.04409829527139664, 2.8886630389024504e-05, -0.028843196108937263)\n",
      "\n",
      "L2T2_lora_B_out: (0.0719948336482048, 6.336949445540085e-05, -0.05591467395424843)\n",
      "L2T2_lora_out: (0.1439896672964096, 0.0001267389889108017, -0.11182934790849686)\n",
      "\n",
      "g_out: (0.09989137202501297, 9.785238216863945e-05, -0.08298615366220474)\n",
      "L1T2_out: (0.13428787887096405, 0.00014750522677786648, -0.11523351073265076)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.up_proj\n",
      "================================================================\n",
      "L1T1_lora_B_out: (0.01294557936489582, 1.4893067600496579e-05, -0.013282570987939835)\n",
      "L1T1_lora_out: (0.02589115872979164, 2.9786135200993158e-05, -0.02656514197587967)\n",
      "\n",
      "L2T1_lora_B_out: (0.02251545898616314, -1.1957946298934985e-05, -0.017174670472741127)\n",
      "L2T1_lora_out: (0.04503091797232628, -2.391589259786997e-05, -0.034349340945482254)\n",
      "\n",
      "L2T2_lora_B_out: (0.05428126081824303, 4.130592969886493e-06, -0.047953568398952484)\n",
      "L2T2_lora_out: (0.10856252163648605, 8.261185939772986e-06, -0.09590713679790497)\n",
      "\n",
      "g_out: (0.06719720363616943, 3.217707489966415e-05, -0.07475320994853973)\n",
      "L1T2_out: (0.08845661580562592, 6.196321919560432e-05, -0.1013183519244194)\n",
      "\n",
      "================================================================\n",
      "model.layers.0.mlp.down_proj\n",
      "================================================================\n",
      "Gradient norm: 38.74132596192605\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"[INFO] Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "# model.set_return_hidden_outputs(False)\n",
    "check_loss_and_grad_norm(model, tokenizer)\n",
    "# model.set_return_hidden_outputs(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(hf_data_id, data_dir=hf_data_dir)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total batches: 1250\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"[INFO] Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask):\n",
      "(torch.Size([4, 512]), torch.Size([4, 512]))\n",
      "\n",
      "First batch text:\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "Mrs. Snyder used to  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "# check_loss_and_grad_norm(model, tokenizer, prompt=first_batch_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1766/2584788064.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  L2T2_scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0d082808fd4fcc82263a063641491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112777455557282, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250824_115926-51ozj2rw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/51ozj2rw' target=\"_blank\">smooth-smoke-239</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/51ozj2rw' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/51ozj2rw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# If `device` is not specified or set to 'auto', use the model's device\n",
    "# if device is None or device == 'auto':\n",
    "device = next(iter(model.parameters())).device\n",
    "\n",
    "# Set up optimizer and gradient scaler\n",
    "# for L2T2 LoRA\n",
    "model.train_L2T2_lora_and_g()\n",
    "L2T2_lora_params = [p for n, p in model.named_parameters() if p.requires_grad]\n",
    "L2T2_optimizer = torch.optim.Adam(L2T2_lora_params, lr=lr)\n",
    "L2T2_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# for nonlinear functions\n",
    "# model.train_nonlinear_fn()\n",
    "# nonlinear_fn_params = [p for n, p in model.named_parameters() if p.requires_grad]\n",
    "# nonlinear_fn_optimizer = torch.optim.Adam(nonlinear_fn_params, lr=lr)\n",
    "# nonlinear_fn_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Set up LR scheduler\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "if warmup_ratio > 0:\n",
    "    # If `warmup_ratio` > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup # type: ignore\n",
    "    max_optimizer_steps = (max_global_steps // grad_accumulation_steps) // 2 # divide by 2 because we train in 2 modes\n",
    "    num_warmup_steps = int(warmup_ratio * max_optimizer_steps)\n",
    "    L2T2_scheduler = get_cosine_schedule_with_warmup(\n",
    "        L2T2_optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "    # nonlinear_fn_scheduler = get_cosine_schedule_with_warmup(\n",
    "    #     nonlinear_fn_optimizer,\n",
    "    #     num_warmup_steps=num_warmup_steps,\n",
    "    #     num_training_steps=max_optimizer_steps,\n",
    "\n",
    "    #     # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "    #     num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    # )\n",
    "else:\n",
    "    # If `warmup_ratio` is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "    L2T2_scheduler = LambdaLR(L2T2_optimizer, lr_lambda=lambda step: 1.0)\n",
    "    # nonlinear_fn_scheduler = LambdaLR(nonlinear_fn_optimizer, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True, # End previous run and start a new one\n",
    "    config=dict(\n",
    "        # Project configuration\n",
    "        seed = seed,\n",
    "        device = device,\n",
    "\n",
    "        # Data configuration\n",
    "        hf_data_id = hf_data_id,\n",
    "        hf_data_dir = hf_data_dir,\n",
    "\n",
    "        # Training configuration\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "        lr = lr,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        checkpoint_steps = checkpoint_steps,\n",
    "        resume_step = resume_step,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "global_step = resume_step\n",
    "start_epoch = 0\n",
    "\n",
    "def load_trainer_params(target, model, optimizer, scheduler, scaler, checkpoint_dir, device):\n",
    "    # Load Nero parameters\n",
    "    nero_path = os.path.join(checkpoint_dir, f'{target}.safetensors')\n",
    "    # model.load_nero_params(mode, nero_path)\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_dir, f'{target}_optimizer.pt')\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "    \n",
    "    # Move optimizer state to the correct device\n",
    "    for param in optimizer.state:\n",
    "        param_device = param.device\n",
    "        param_dtype = param.dtype\n",
    "        for key, value in optimizer.state[param].items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                optimizer.state[param][key] = value.to(device=param_device, dtype=param_dtype)\n",
    "\n",
    "    # Load scheduler state\n",
    "    scheduler_path = os.path.join(checkpoint_dir, f'{target}_scheduler.pt')\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
    "\n",
    "    # Load scaler state\n",
    "    scaler_path = os.path.join(checkpoint_dir, f'{target}_scaler.pt')\n",
    "    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "\n",
    "if resume_step > 0:\n",
    "    checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{resume_step}')\n",
    "    print(f\"[INFO] Resuming training from checkpoint directory:\", checkpoint_dir)\n",
    "\n",
    "    # Load trainer parameters\n",
    "    load_trainer_params('L2T2_lora', model, L2T2_optimizer, L2T2_scheduler, L2T2_scaler, checkpoint_dir, device)\n",
    "    # load_trainer_params('nonlinear_fn', model, nonlinear_fn_optimizer, nonlinear_fn_scheduler, nonlinear_fn_scaler, checkpoint_dir, device)\n",
    "\n",
    "    # Load trainer state\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, 'trainer_state.json')\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        start_epoch = log_history[-1]['epoch'] if log_history else 0\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch} and step {resume_step}.\")\n",
    "\n",
    "    # Load RNG state for reproducibility\n",
    "    rng_path = os.path.join(checkpoint_dir, 'rng_state.pth')\n",
    "    if os.path.exists(rng_path):\n",
    "        rng_state = torch.load(rng_path)\n",
    "        random.setstate(rng_state['python'])\n",
    "        np.random.set_state(rng_state['numpy'])\n",
    "        torch.set_rng_state(rng_state['cpu'])\n",
    "        if torch.cuda.is_available() and rng_state['cuda']:\n",
    "            torch.cuda.set_rng_state_all(rng_state['cuda'])\n",
    "    \n",
    "    if resume_step % grad_accumulation_steps != 0:\n",
    "        print(\"[WARN] Resuming mid-gradient accumulation cycle. Make sure this is intended.\")\n",
    "else:\n",
    "    if push_to_hf:\n",
    "        # If it's new training, create Hugging Face repository\n",
    "        print(f\"[INFO] Creating Hugging Face repository:\", hf_nero_id) # print the link instead\n",
    "        create_repo(repo_id=hf_nero_id, repo_type='model', exist_ok=True)\n",
    "        print(f\"[INFO] Hugging Face repository created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_beta_schedule(num_warmup_steps, start=0.0, schedule='linear'):\n",
    "    def lora_beta_scheduler(global_step):\n",
    "        if schedule == 'constant':\n",
    "            return start  # fixed at start value\n",
    "\n",
    "        if global_step >= num_warmup_steps:\n",
    "            return 1.0  # fully on after warmup\n",
    "\n",
    "        progress = global_step / num_warmup_steps\n",
    "        if schedule == 'linear':\n",
    "            # interpolate from start -> 1.0\n",
    "            return start + (1.0 - start) * progress\n",
    "        elif schedule == 'cosine':\n",
    "            # cosine from start -> 1.0\n",
    "            return start + (1.0 - start) * (0.5 * (1 - math.cos(math.pi * progress)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {schedule}\")\n",
    "    return lora_beta_scheduler\n",
    "\n",
    "lora_beta_scheduler = get_lora_beta_schedule(num_warmup_steps=500, start=0.05, schedule='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# generated = generate_text(model, tokenizer, L1T2_sample_prompt, device=device)\n",
    "# print(\"================================\")\n",
    "# print(\"CHECK GENERATED TEXT (L1T2)\")\n",
    "# print(\"================================\")\n",
    "# print(f\"{'Generated':<9}:\", generated)\n",
    "# print()\n",
    "\n",
    "# model.eval_L2T2_lora()\n",
    "# generated = generate_text(model, tokenizer, L2T2_sample_prompt, device=device)\n",
    "# print(\"================================\")\n",
    "# print(\"CHECK GENERATED TEXT (L2T2)\")\n",
    "# print(\"================================\")\n",
    "# print(f\"{'Generated':<9}:\", generated)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1766/1560263458.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: L2T2_lora, epoch: 0, step: 1, loss: 6.8698, lr: 6.4516e-06, lora_beta: 0.0500, grad_norm: 0.6779, grad_norm_clipped: 0.6779\n",
      "target: L2T2_lora, epoch: 0, step: 2, loss: 8.1905, lr: 6.4516e-06, lora_beta: 0.0500\n",
      "target: L2T2_lora, epoch: 0, step: 3, loss: 7.9981, lr: 1.2903e-05, lora_beta: 0.0501, grad_norm: 1.5343, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 4, loss: 7.8096, lr: 1.2903e-05, lora_beta: 0.0502\n",
      "target: L2T2_lora, epoch: 0, step: 5, loss: 8.1152, lr: 1.9355e-05, lora_beta: 0.0502, grad_norm: 1.4493, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 6, loss: 8.1369, lr: 1.9355e-05, lora_beta: 0.0503\n",
      "target: L2T2_lora, epoch: 0, step: 7, loss: 6.9513, lr: 2.5806e-05, lora_beta: 0.0505, grad_norm: 1.3610, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 8, loss: 6.8713, lr: 2.5806e-05, lora_beta: 0.0506\n",
      "target: L2T2_lora, epoch: 0, step: 9, loss: 7.3652, lr: 3.2258e-05, lora_beta: 0.0508, grad_norm: 1.2395, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 10, loss: 7.7609, lr: 3.2258e-05, lora_beta: 0.0509\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1T2)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question:\n",
      "Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns are there total?\n",
      "\n",
      "### Answer:\n",
      "$\\left(\\right. 3 \\times 25 \\left.\\right) + 8$\n",
      "$75 + 8$\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83$\n",
      "\n",
      "### Answer:\n",
      "$83\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2T2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問:\n",
      "ダンは3本のバラの灌木を植えました。各バラの灌木には25本のバラがあります。各バラには8本のトゲがあります。合計で何本のトゲがあるでしょうか？\n",
      "\n",
      "### 答え:\n",
      "$25 \\times 8 = 200$\n",
      "\n",
      "target: L2T2_lora, epoch: 0, step: 11, loss: 7.7986, lr: 3.8710e-05, lora_beta: 0.0511, grad_norm: 1.3004, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 12, loss: 7.8594, lr: 3.8710e-05, lora_beta: 0.0513\n",
      "target: L2T2_lora, epoch: 0, step: 13, loss: 7.3647, lr: 4.5161e-05, lora_beta: 0.0516, grad_norm: 1.4382, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 14, loss: 8.7405, lr: 4.5161e-05, lora_beta: 0.0518\n",
      "target: L2T2_lora, epoch: 0, step: 15, loss: 7.5738, lr: 5.1613e-05, lora_beta: 0.0521, grad_norm: 1.3339, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 16, loss: 7.1666, lr: 5.1613e-05, lora_beta: 0.0524\n",
      "target: L2T2_lora, epoch: 0, step: 17, loss: 7.6654, lr: 5.8065e-05, lora_beta: 0.0527, grad_norm: 1.2657, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 18, loss: 7.3365, lr: 5.8065e-05, lora_beta: 0.0530\n",
      "target: L2T2_lora, epoch: 0, step: 19, loss: 7.8621, lr: 6.4516e-05, lora_beta: 0.0534, grad_norm: 1.3048, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 20, loss: 7.6916, lr: 6.4516e-05, lora_beta: 0.0537\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1T2)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question:\n",
      "Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns are there total?\n",
      "\n",
      "### Answer:\n",
      "We can solve this problem in the following way:\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Answer\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2T2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問:\n",
      "ダンは3本のバラの灌木を植えました。各バラの灌木には25本のバラがあります。各バラには8本のトゲがあります。合計で何本のトゲがあるでしょうか？\n",
      "\n",
      "### 答え:\n",
      "3 x 25 x 8 = 600\n",
      "\n",
      "\n",
      "target: L2T2_lora, epoch: 0, step: 21, loss: 6.4861, lr: 7.0968e-05, lora_beta: 0.0541, grad_norm: 1.1899, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 22, loss: 7.6361, lr: 7.0968e-05, lora_beta: 0.0545\n",
      "target: L2T2_lora, epoch: 0, step: 23, loss: 7.5576, lr: 7.7419e-05, lora_beta: 0.0550, grad_norm: 1.2444, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 24, loss: 6.8856, lr: 7.7419e-05, lora_beta: 0.0554\n",
      "target: L2T2_lora, epoch: 0, step: 25, loss: 7.1546, lr: 8.3871e-05, lora_beta: 0.0558, grad_norm: 1.2028, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 26, loss: 7.3025, lr: 8.3871e-05, lora_beta: 0.0563\n",
      "target: L2T2_lora, epoch: 0, step: 27, loss: 6.5722, lr: 9.0323e-05, lora_beta: 0.0568, grad_norm: 1.1261, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 28, loss: 6.7020, lr: 9.0323e-05, lora_beta: 0.0573\n",
      "target: L2T2_lora, epoch: 0, step: 29, loss: 6.8959, lr: 9.6774e-05, lora_beta: 0.0579, grad_norm: 1.0444, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 30, loss: 6.7030, lr: 9.6774e-05, lora_beta: 0.0584\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1T2)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question:\n",
      "Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns are there total?\n",
      "\n",
      "### Answer:\n",
      "The answer is 600 thorns. Here is the step-by-step solution.\n",
      "\n",
      "### Step 1:\n",
      "Multiply 25 roses by 8 thorns.\n",
      "$25 \\times 8 = 200$ thorns.\n",
      "\n",
      "### Step 2:\n",
      "Multiply 200 thorns by 3 rose bushes.\n",
      "$200 \\times 3 = 600$ thorns.\n",
      "\n",
      "### Step 3:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "### Step 4:\n",
      "The answer is 600 thorns.\n",
      "\n",
      "###\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2T2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問:\n",
      "ダンは3本のバラの灌木を植えました。各バラの灌木には25本のバラがあります。各バラには8本のトゲがあります。合計で何本のトゲがあるでしょうか？\n",
      "\n",
      "### 答え:\n",
      "合計で300本のトゲがあるでしょう。\n",
      "\n",
      "target: L2T2_lora, epoch: 0, step: 31, loss: 5.7382, lr: 0.0001, lora_beta: 0.0590, grad_norm: 0.8975, grad_norm_clipped: 0.8975\n",
      "target: L2T2_lora, epoch: 0, step: 32, loss: 6.5447, lr: 0.0001, lora_beta: 0.0596\n",
      "target: L2T2_lora, epoch: 0, step: 33, loss: 6.9811, lr: 0.0001, lora_beta: 0.0602, grad_norm: 1.0077, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 34, loss: 6.5832, lr: 0.0001, lora_beta: 0.0608\n",
      "target: L2T2_lora, epoch: 0, step: 35, loss: 7.0919, lr: 0.0001, lora_beta: 0.0614, grad_norm: 1.1299, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 36, loss: 5.9371, lr: 0.0001, lora_beta: 0.0621\n",
      "target: L2T2_lora, epoch: 0, step: 37, loss: 5.9244, lr: 0.0001, lora_beta: 0.0628, grad_norm: 0.9057, grad_norm_clipped: 0.9057\n",
      "target: L2T2_lora, epoch: 0, step: 38, loss: 5.5945, lr: 0.0001, lora_beta: 0.0635\n",
      "target: L2T2_lora, epoch: 0, step: 39, loss: 5.9684, lr: 0.0001, lora_beta: 0.0642, grad_norm: 0.9673, grad_norm_clipped: 0.9673\n",
      "target: L2T2_lora, epoch: 0, step: 40, loss: 5.4025, lr: 0.0001, lora_beta: 0.0649\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1T2)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question:\n",
      "Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns are there total?\n",
      "\n",
      "### Answer:\n",
      "Step 1: 25 x 3 = 75\n",
      "Step 2: 75 x 8 = 600\n",
      "\n",
      "### Answer:\n",
      "Step 1: 25 x 3 = 75\n",
      "Step 2: 75 x 8 = 600\n",
      "\n",
      "### Answer:\n",
      "Step 1: 25 x 3 = 75\n",
      "Step 2: 75 x 8 = 600\n",
      "\n",
      "### Answer:\n",
      "Step 1: 25 x 3 = 75\n",
      "\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2T2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問:\n",
      "ダンは3本のバラの灌木を植えました。各バラの灌木には25本のバラがあります。各バラには8本のトゲがあります。合計で何本のトゲがあるでしょうか？\n",
      "\n",
      "### 答え:\n",
      "```python\n",
      "answer = 25 * 8 * 3\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "\n",
      "target: L2T2_lora, epoch: 0, step: 41, loss: 5.7806, lr: 0.0001, lora_beta: 0.0657, grad_norm: 0.8844, grad_norm_clipped: 0.8844\n",
      "target: L2T2_lora, epoch: 0, step: 42, loss: 5.9446, lr: 0.0001, lora_beta: 0.0664\n",
      "target: L2T2_lora, epoch: 0, step: 43, loss: 5.9130, lr: 0.0001, lora_beta: 0.0672, grad_norm: 1.1559, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 44, loss: 5.3265, lr: 0.0001, lora_beta: 0.0680\n",
      "target: L2T2_lora, epoch: 0, step: 45, loss: 6.0349, lr: 0.0001, lora_beta: 0.0689, grad_norm: 1.0485, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 46, loss: 6.0899, lr: 0.0001, lora_beta: 0.0697\n",
      "target: L2T2_lora, epoch: 0, step: 47, loss: 5.7491, lr: 0.0002, lora_beta: 0.0706, grad_norm: 1.1876, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 48, loss: 6.1748, lr: 0.0002, lora_beta: 0.0714\n",
      "target: L2T2_lora, epoch: 0, step: 49, loss: 5.3143, lr: 0.0002, lora_beta: 0.0723, grad_norm: 1.0996, grad_norm_clipped: 1.0000\n",
      "target: L2T2_lora, epoch: 0, step: 50, loss: 4.9995, lr: 0.0002, lora_beta: 0.0732\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1T2)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question:\n",
      "Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns are there total?\n",
      "\n",
      "### Answer:\n",
      "Step 1: The first step is to multiply the number of rose bushes by the number of roses on each bush. 3 x 25 = 75\n",
      "\n",
      "Step 2: The next step is to multiply the number of roses by the number of thorns on each rose. 75 x 8 = 600\n",
      "\n",
      "Step 3: The final step is to add the number of thorns to the total. 600 + 0 = 600\n",
      "\n",
      "Step 4: The final\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2T2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問:\n",
      "ダンは3本のバラの灌木を植えました。各バラの灌木には25本のバラがあります。各バラには8本のトゲがあります。合計で何本のトゲがあるでしょうか？\n",
      "\n",
      "### 答え:\n",
      "25 x 8 = 200\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m grad_accumulation_steps\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# log['lm_loss'] = lm_loss.item()\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# log['hidden_loss'] = hidden_loss.item()\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[43mL2T2_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Update parameters only at the end of gradient accumulation cycle\u001b[39;00m\n\u001b[1;32m     93\u001b[0m grad_norm_log \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target = 'L2T2_lora' # or 'nonlinear_fn'\n",
    "log_history = []\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of gradient accumulation cycle\n",
    "# This ensures there are no leftover gradients when resuming mid-cycle or after a previous cycle was interrupted\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    L2T2_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Skip previously completed steps\n",
    "        if global_step <= resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        # Stop training if `max_global_steps` reached\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Set model to training mode\n",
    "            model.train_L2T2_lora_and_g()\n",
    "            # model.set_return_layer_loss(False)\n",
    "            \n",
    "            # Update LoRA beta\n",
    "            lora_beta = lora_beta_scheduler(global_step)\n",
    "            model.set_lora_beta(lora_beta)\n",
    "            \n",
    "            # Forward pass            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids,\n",
    "                use_cache=False, # Disable cache to avoid conflict with gradient checkpointing\n",
    "            )            \n",
    "            model.set_return_layer_loss(False)\n",
    "            \n",
    "            # total_loss = None\n",
    "            # lm_loss = None\n",
    "            # layer_loss = None\n",
    "            # if isinstance(outputs, tuple) and len(outputs) == 2:\n",
    "            #     lm_loss = outputs[0]\n",
    "            #     layer_loss = outputs[1]\n",
    "            #     total_loss = lm_loss + layer_loss\n",
    "            # else:\n",
    "            #     lm_loss = outputs.loss\n",
    "            #     total_loss = lm_loss\n",
    "            \n",
    "            lm_loss = outputs.loss\n",
    "            # L2T2_lora_outs = outputs[1]\n",
    "            # L2T2_tgts = outputs[2]\n",
    "\n",
    "            # loss_device = next(iter(L2T2_tgts.values())).device\n",
    "\n",
    "            # hidden_loss = torch.tensor(0.0, device=loss_device)\n",
    "            # for layer_name in L2T2_tgts.keys():\n",
    "            #     # print(layer_name)\n",
    "            #     L2T2_lora_out = L2T2_lora_outs[layer_name]\n",
    "            #     L2T2_tgt = L2T2_tgts[layer_name].detach()\n",
    "                \n",
    "            #     # print(L2T2_lora_out.shape, L2T2_tgt.shape)\n",
    "            #     # print(L2T2_lora_out.grad_fn, L2T2_tgt.grad_fn)\n",
    "            #     hidden_loss_i = F.mse_loss(L2T2_lora_out, L2T2_tgt, reduction='mean')\n",
    "            #     hidden_loss += hidden_loss_i.to(loss_device)\n",
    "                \n",
    "            #     # print()\n",
    "            # # print(hidden_loss)\n",
    "            # hidden_loss = outputs[1]\n",
    "            \n",
    "            loss = lm_loss / grad_accumulation_steps\n",
    "\n",
    "        log = {\n",
    "            'target': target,\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "        }\n",
    "\n",
    "        # Backward pass\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        log['loss'] = loss.item() * grad_accumulation_steps\n",
    "        # log['lm_loss'] = lm_loss.item()\n",
    "        # log['hidden_loss'] = hidden_loss.item()\n",
    "        L2T2_scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update parameters only at the end of gradient accumulation cycle\n",
    "        grad_norm_log = {}\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            if target == 'L2T2_lora':\n",
    "                # Unscale gradients before computing gradient norm and applying clipping\n",
    "                L2T2_scaler.unscale_(L2T2_optimizer)\n",
    "                \n",
    "                # DEBUG: Grad norm\n",
    "                # named_params = {n: p for n, p in model.named_parameters() if 'lora.L2T2.' in n or 'g.' in n}\n",
    "                # compute_named_grad_norm(named_params)\n",
    "\n",
    "                # Compute gradient norm\n",
    "                grad_norm = compute_grad_norm(L2T2_lora_params)\n",
    "                grad_norm_log['grad_norm'] = grad_norm\n",
    "\n",
    "                # Clip gradients\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(L2T2_lora_params, clip_grad_norm)\n",
    "                \n",
    "                # Compute clipped gradient norm\n",
    "                grad_norm_clipped = compute_grad_norm(L2T2_lora_params)\n",
    "                grad_norm_log['grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "                # Update parameters\n",
    "                L2T2_scaler.step(L2T2_optimizer)\n",
    "                L2T2_scaler.update()\n",
    "                L2T2_scheduler.step()\n",
    "\n",
    "                # Zero gradients for the next gradient accumulation cycle\n",
    "                L2T2_optimizer.zero_grad(set_to_none=True)\n",
    "            # else:\n",
    "            #     print(\"TEST 3\")\n",
    "            #     # Unscale gradients before computing gradient norm and applying clipping\n",
    "            #     nonlinear_fn_scaler.unscale_(nonlinear_fn_optimizer)\n",
    "                \n",
    "            #     # Compute gradient norm\n",
    "            #     grad_norm = compute_grad_norm(nonlinear_fn_params)\n",
    "            #     grad_norm_log['nonlinear_fn/grad_norm'] = grad_norm\n",
    "\n",
    "            #     # Clip gradients\n",
    "            #     if clip_grad_norm is not None:\n",
    "            #         torch.nn.utils.clip_grad_norm_(nonlinear_fn_params, clip_grad_norm)\n",
    "                \n",
    "            #     # Compute clipped gradient norm\n",
    "            #     grad_norm_clipped = compute_grad_norm(nonlinear_fn_params)\n",
    "            #     grad_norm_log['nonlinear_fn/grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "            #     # Update parameters\n",
    "            #     nonlinear_fn_scaler.step(nonlinear_fn_optimizer)\n",
    "            #     nonlinear_fn_scaler.update()\n",
    "            #     nonlinear_fn_scheduler.step()\n",
    "\n",
    "            #     # Zero gradients for the next gradient accumulation cycle\n",
    "            #     nonlinear_fn_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # # After updating parameters, toggle training target\n",
    "            # target = 'nonlinear_fn' if target == 'L2T2_lora' else 'L2T2_lora'\n",
    "\n",
    "        # Logging\n",
    "        scheduler_log = {\n",
    "            'lr': L2T2_scheduler.get_last_lr()[0],\n",
    "            'lora_beta': lora_beta,\n",
    "        }\n",
    "        log = {\n",
    "            **log, \n",
    "            **scheduler_log, \n",
    "            **grad_norm_log,\n",
    "        }\n",
    "        log_history.append(log)\n",
    "        wandb.log(log)\n",
    "        print(\", \".join(\n",
    "            f\"{k}: {format_float(v)}\" if isinstance(v, float) else f\"{k}: {v}\"\n",
    "            for k, v in log.items()\n",
    "        ))\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step > 0 and global_step % checkpoint_steps == 0:\n",
    "            # Create checkpoint directory\n",
    "            checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save Nero parameters, along with optimizer, scheduler, and scaler states\n",
    "            # L1T2_nero_state_dict = {n: p.detach().cpu() for n, p in model.named_parameters() if 'L1T2' in n}\n",
    "            # save_file(L1T2_nero_state_dict, os.path.join(checkpoint_dir, 'L1T2_nero.safetensors'))\n",
    "            # torch.save(L1T2_optimizer.state_dict(), os.path.join(checkpoint_dir, 'L1T2_optimizer.pt'))\n",
    "            # torch.save(L1T2_scheduler.state_dict(), os.path.join(checkpoint_dir, 'L1T2_scheduler.pt'))\n",
    "            # torch.save(L1T2_scaler.state_dict(), os.path.join(checkpoint_dir, 'L1T2_scaler.pt'))\n",
    "\n",
    "            L2T2_nero_state_dict = {n: p.detach().cpu() for n, p in model.named_parameters() if 'L2T2' in n}\n",
    "            save_file(L2T2_nero_state_dict, os.path.join(checkpoint_dir, 'L2T2_nero.safetensors'))\n",
    "            torch.save(L2T2_optimizer.state_dict(), os.path.join(checkpoint_dir, 'L2T2_optimizer.pt'))\n",
    "            torch.save(L2T2_scheduler.state_dict(), os.path.join(checkpoint_dir, 'L2T2_scheduler.pt'))\n",
    "            torch.save(L2T2_scaler.state_dict(), os.path.join(checkpoint_dir, 'L2T2_scaler.pt'))\n",
    "\n",
    "            # Save trainer state for resuming training\n",
    "            trainer_state = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'log_history': log_history,\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'w') as f:\n",
    "                json.dump(trainer_state, f, indent=2)\n",
    "\n",
    "            # Save RNG state for reproducibility\n",
    "            rng_state = {\n",
    "                'python': random.getstate(),\n",
    "                'numpy': np.random.get_state(),\n",
    "                'cpu': torch.get_rng_state(),\n",
    "                'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else [],\n",
    "            }\n",
    "            torch.save(rng_state, os.path.join(checkpoint_dir, 'rng_state.pth'))\n",
    "\n",
    "            # Upload checkpoint directory to Hugging Face repository\n",
    "            if push_to_hf:\n",
    "                upload_folder(\n",
    "                    folder_path=checkpoint_dir,\n",
    "                    repo_id=hf_nero_id,\n",
    "                    path_in_repo=f\"checkpoint-{global_step}\",\n",
    "                    commit_message=f\"Add checkpoint at step {global_step}\",\n",
    "                    repo_type='model',\n",
    "                )\n",
    "        \n",
    "        # Check generated text every `generate_steps`\n",
    "        if global_step > 0 and global_step % generate_steps == 0:\n",
    "            print()\n",
    "            \n",
    "            model.eval()\n",
    "            generated = generate_text(model, tokenizer, L1T2_sample_prompt, device=device)\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT (L1T2)\")\n",
    "            print(\"================================\")\n",
    "            print(generated)\n",
    "            print()\n",
    "\n",
    "            model.eval_L2T2_lora()\n",
    "            generated = generate_text(model, tokenizer, L2T2_sample_prompt, device=device)\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT (L2T2)\")\n",
    "            print(\"================================\")\n",
    "            print(generated)\n",
    "            print()\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
