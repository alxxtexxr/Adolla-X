{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if 'COLAB_' not in ''.join(os.environ.keys()):\n",
    "    %pip install unsloth==2025.3.4\n",
    "else:\n",
    "    # Do this only in Colab notebooks and Kaggle notebooks!\n",
    "    %pip install transformers==4.48.3\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "    %pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    %pip install --no-deps unsloth==2025.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel, UnslothTrainer, UnslothTrainingArguments, is_bf16_supported\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e357e973ad0742499e800408f0b87be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 176 files:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF LoRA ID: alxxtexxr/L3.1-8B-wikipedia-id-LoRA-v20250403044132\n"
     ]
    }
   ],
   "source": [
    "# Project configs\n",
    "seed = 69\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lang = 'id' # 'en' | 'id'\n",
    "task = 'wikipedia' # 'wikipedia' | 'gsm8k'\n",
    "\n",
    "# Data Configs\n",
    "max_data_length = 2500\n",
    "max_seq_length = 1024\n",
    "test_size = 0.2 # 2500 * 0.2 = 500 test data\n",
    "hf_data_id = 'wikimedia/wikipedia' # 'wikimedia/wikipedia' | 'openai/gsm8k'\n",
    "hf_data_dir = '20231101.id' # 'wikipedia': '20231101.en' | '20231101.id' || 'gsm8k': 'main'\n",
    "hf_data_split = f'train[:{max_data_length}]'\n",
    "\n",
    "# Model configs\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# LoRA config\n",
    "hf_lora_id = 'alxxtexxr/L3.1-8B-wikipedia-id-LoRA-v20250403044132'\n",
    "lora_dir = hf_lora_id.split('/')[-1]\n",
    "\n",
    "# Download the trained LoRA adapter to the local directory\n",
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(\n",
    "    repo_id=hf_lora_id, \n",
    "    local_dir=lora_dir, \n",
    "    # ignore_patterns='checkpoint-*/*',\n",
    ")\n",
    "\n",
    "print(\"HF LoRA ID:\", hf_lora_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0288cd20ac2c4b2fad957892afbeb219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444e59fa73654849a4f1bbbab8ae6c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2663fcf2ef3e47dfbe05285e17de6f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29361833d0d4fd69f5f325f57aaaf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a022466b1cea497ab47ed889308c3e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3242b0bd8454a7f83fb9f8284b6f153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (2-31): 30 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the LoRA-adapted model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(lora_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained('unsloth/Meta-Llama-3.1-8B')\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20d25ed06b74b878977053aa82906d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(hf_data_id, data_dir=hf_data_dir, split=hf_data_split)\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "# def format_gsm8k_prompts(examples):\n",
    "#     gsm8k_prompt = \"\"\"### Instruction:\n",
    "# Solve the following math problem step by step.\n",
    "\n",
    "# ### Question: \n",
    "# {question}\n",
    "\n",
    "# ### Answer: \n",
    "# {answer}\"\"\" + eos_token\n",
    "    \n",
    "#     return {'text': [gsm8k_prompt.format(question=question, answer=answer) for question, answer in zip(examples['question'], examples['answer'])]}\n",
    "\n",
    "# def format_prompts(examples):\n",
    "#     return {'text': [example + eos_token for example in examples['text']]}\n",
    "\n",
    "# if task == 'gsm8k':\n",
    "#     dataset = dataset.map(format_gsm8k_prompts, batched=True)\n",
    "# else:\n",
    "#     dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=max_seq_length,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_split = dataset.train_test_split(test_size=test_size)\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Proton adalah partikel subatomik, simbol  atau , dengan muatan listrik positif +1e muatan elementer dan massa sedikit lebih kecil dari neutron. Proton dan neutron, masing-masing dengan massa sekitar satu satuan massa atom, secara kolektif disebut sebagai \"nukleon\".\n",
      "\n",
      "Suatu atom biasanya terdiri dari sejumlah proton dan neutron yang berada di bagian inti (tengah) atom, dan sejumlah elektron yang mengelilingi inti tersebut. Dalam atom bermuatan netral, banyaknya proton akan sama dengan jumlah elektronnya. Banyaknya proton di bagian inti biasanya akan menentukan sifat kimia suatu atom. Inti atom sering dikenal juga dengan istilah nukleus atau nukleon (nucleon), dan reaksi yang terjadi atau berkaitan dengan inti atom ini disebut reaksi nuklir.\n",
      "\n",
      "Kata proton adalah bahasa Yunani untuk \"pertama\", dan nama ini diberikan kepada inti hidrogen oleh Ernest Rutherford pada tahun 1920. Pada tahun-tahun sebelumnya, Rutherford telah menemukan bahwa inti hidrogen (dikenal sebagai inti paling ringan) dapat diekstraksi dari inti nitrogen dengan tumbukan atom. Oleh karena itu, proton adalah kandidat untuk menjadi partikel dasar, dan merupakan blok pembangun nitrogen dan semua inti atom yang lebih berat lainnya.\n",
      "\n",
      "Referensi\n",
      "\n",
      "Lihat pula \n",
      " Atom\n",
      " Neutron\n",
      " Elektron\n",
      "\n",
      "Pranala luar \n",
      "\n",
      " Particle Data Group\n",
      " Large Hadron Collider\n",
      "\n",
      "Partikel subatom\n",
      "Fisika partikel\n",
      "Fisika atom\n",
      "Atom\n",
      "================================================================\n",
      "Kabupaten Murung Raya adalah sebuah wilayah kabupaten yang terletak di provinsi Kalimantan Tengah, Indonesia. Ibu kotanya adalah Puruk Cahu. Kabupaten ini merupakan pemekaran dari Kabupaten Barito Utara pada tahun 2002 dengan luas wilayah 23.700 km² dan berpenduduk sebanyak 115.239 jiwa, pada pertengahan tahun 2023. Semboyan kabupaten ini adalah \"Tira Tangka Balang\".\n",
      "\n",
      "Sejarah \n",
      "Menurut Staatsblad van Nederlandisch Indië tahun 1849, wilayah Dusun Hulu termasuk daerah ini merupakan bagian dari zuid-ooster-afdeeling berdasarkan Bêsluit van den Minister van Staat, Gouverneur-Generaal van Nederlandsch-Indie, pada 27 Agustus 1849, No. 8.\n",
      "\n",
      "Geografis \n",
      "Secara geografis, Kabupaten Murung Raya terletak di 0°47\" Lintang Utara – 0°51\" Lintang Selatan dan 113°12\"–115°08\" Bujur Timur. Kabupaten Murung Raya merupakan kabupaten paling utara di Provinsi Kalimantan Tengah yang berbatasan langsung dengan Provinsi Kalimantan Timur dan Provinsi Kalimantan Barat. Ibu kota kabupaten ini yakni Puruk Cahu berjarak sekira 403 Kilometer dari Kota Palangkaraya melalui Kabupaten Barito Utara.\n",
      "\n",
      "Batas Wilayah \n",
      "Batas-batas wilayahnya adalah sebagai berikut:\n",
      "\n",
      "Wilayah administratif \n",
      "Secara administratif Kabupaten Murung Raya memiliki luas wilayah sebesar 23.700 km². Wilayah Kabupaten Murung Raya terdiri dari 10 kecamatan dengan kecamatan terbesar adalah Kecamatan Uut Murung dan kecamatan terkecil adalah Kecamatan Tanah Siang Selatan. Berikut merupakan luas wilayah tiap kecamatan di Kabupaten Murung Raya.\n",
      " Kecamatan Permata Intan dengan luas wilayah sebesar 804 km²,\n",
      " Kecamatan Babuat dengan luas wilayah sebesar 423 km², \n",
      " Kecamatan Murung dengan luas wilayah sebesar 730 km², \n",
      " Kecamatan Laung Tuhup dengan luas wilayah sebesar 1611 km², \n",
      " Kecamatan Barito Tuhup Raya dengan luas wilayah sebesar 1500 km², \n",
      " Kecamatan Tanah Siang dengan luas wilayah sebesar 1239 km², \n",
      " Kecamatan Tanah Siang Selatan dengan luas wilayah sebesar 310 km², \n",
      " Kecamatan Sumber Barito dengan luas wilayah sebesar 2797 km², \n",
      " Kecamatan Seribu Riam dengan luas wilayah sebesar 7023 km²,\n",
      " Kecamatan Uut Murung dengan luas wilayah sebesar 7263 km².\n",
      "\n",
      "Topografi \n",
      "Kabupaten Murung Raya memiliki topografi berupa dataran rendah, perbukitan hingga bergunung-gunung dengan ketinggian wilayah antara 15-1.910 meter di atas permukaan air laut (mdpl). Sebagian besar merupakan pegunungan terutama bagian utara yang merupakan lajur Pegunungan Muller sehingga memiliki banyak puncak gunung seperti Gunung Sapathawung (1.906 m), Gunung Liyang (1.720 m), Gunung Batikap (1,580 m) dan lainnya.\n",
      "\n",
      "Hidrologi \n",
      "Potensi hidrologi Kabupaten Murung Raya cukup besar, terutama adanya aliran beberapa sungai antara lain Sungai Barito, Sungai Murung, Sungai Busang, Sungai Laung, Sungai Tuhup, dan beberapa sungai kecil lainnya. Sungai terbesar yang berada di Kabupaten Murung Raya adalah Sungai Barito yang sejalur dengan Kabupaten Barito Utara dan Kabupaten Barito Selatan dengan panjang sungai lebih kurang 900 km dan lebar rata-rata 650 m dengan kedalaman rata-rata 8 m yang bermuara di Laut Jawa.\n",
      "\n",
      "Kabupaten ini juga merupakan wilayah hulu daerah aliran sungai (DAS) Barito yaitu Sungai Julai dan Sungai Murung. Sebagai tempat terdapatnya sumber air hulu sungai Barito, kabupaten ini terletak di dalam Daerah Aliran Sungai (DAS) Barito. Air dari Sungai Barito sebagai sungai utama maupun anak-anak sungainya dimanfaatkan penduduk untuk MCK (mandi, cuci, kakus, sumber air minum dan prasarana perangkutan air serta sumber pengairan untuk persawahan yang memiliki luas 2,1 % dari keseluruhan wilayah. Kedalaman air tanah di wilayah ini mencapai sekitar satu meter sampai tujuh meter yang terdapat di sistem lahan dataran. Air tanah digunakan di semua wilayah berbukit di Kabupaten Murung raya. Kedalaman air tanah yang relatif cukup dangkal ini dipengaruhi pula dengan besarnya curah hujan, faktor geologi, serta sistem lahan yang ada.\n",
      "\n",
      "Iklim \n",
      "Oleh karena wilayahnya yang dilalui garis khatulistiwa, wilayah Kabupaten Murung Raya beriklim hutan hujan tropis (Af) dengan pengaruh ekuatorial yang disertai curah hujan yang cenderung tinggi sepanjang tahun, kecuali pada periode Juni hingga September yang cenderung mengalami penurunan intensitas curah hujan, meskipun masih terbilang tinggi. Suhu udara di wilayah ini cenderung konstan antara 23°–34° C di wilayah dataran rendah dengan tingkat kelembapan relatif yang juga tinggi antara 70%–90%.\n",
      "\n",
      "Pemerintahan\n",
      "\n",
      "Bupati \n",
      "\n",
      "Bupati Murung Raya untuk periode 2018-2023 dijabat oleh Perdie M. Yoseph, didampingi wakil bupati, Rejikinnor. Perdie dan Rejikinnor adalah pemenang pada pemilihan umum bupati Murung Raya 2018. Mereka menjabat sejak 24 September 2018 hingga 24 September 2023. Setelah masa jabatan mereka selasi, Hermon dilantik menjadi penjabat bupati Murung Raya, pada 25 September 2023.\n",
      "\n",
      "Dewan Perwakilan\n",
      "\n",
      "Kecamatan\n",
      "\n",
      "Ekonomi \n",
      "Potensi terbesar wilayah ini ada pada sektor kehutanan dan pertambangan. Sektor kehutanan sudah cukup lama turut menyumbang pemasukan bagi negara sedangkan sektor pertambangan seperti tambang emas juga memberi andil yang cukup besar. Tambang batu bara mulai diproduksi yang nantinya diharapkan akan dapat memberikan pemasukan yang cukup besar bagi negara dan daerah.\n",
      "\n",
      "Kependudukan \n",
      "Berdasarkan sensus penduduk pada tahun 2020, jumlah penduduk Kabupaten Murung Raya adalah 111.527 jiwa dengan jumlah penduduk terbanyak berada di Kecamatan Murung dengan jumlah penduduk sekitar ±39.600 jiwa dan jumlah penduduk paling sedikit berada di Kecamatan Uut Murung dengan jumlah hanya ±2.000 jiwa. Dibandingkan dengan wilayahnya yang luas, kepadatan penduduk di Kabupaten Murung Raya terbilang kecil di wilayah Kalimantan Tengah yaitu hanya 5 orang per km². Di Murung Raya, jumlah penduduk berjenis kelamin laki-laki sebesar 57.948 jiwa serta penduduk berjenis kelamin perempuan sebesar 53.579 jiwa dan rasio jenis kelamin di wilayah ini berada pada angka 108 yang dapat dimaknai bahwa pada setiap 100 orang perempuan maka terdapat 108 orang laki-laki.\n",
      "\n",
      "Laju Pertumbuhan Penduduk \n",
      "Pertumbuhan penduduk di Kabupaten Murung Raya meningkat setiap tahunnya, dengan rata-rata laju pertumbuhan berada di antara angka 1% hingga 2% per tahunnya. Berikut merupakan data pertumbuhan jumlah penduduk di Kabupaten Murung Raya per lima tahun.\n",
      "\n",
      "Agama \n",
      "\n",
      "Penduduk Kabupaten Murung Raya sebagian besarnya memeluk agama Islam dengan persentase 66,21% dari jumlah penduduk. Kemudian di peringkat kedua disusul oleh agama Kekristenan dengan persentase 22,74% dari keseluruhan jumlah penduduk, dengan rincian Protestan sebanyak 17,20% dan katolik sebanyak 5,54%. Pada peringkat ketiga adalah agama Hindu dengan persentase 11,04% dari total penduduk Murung Raya, dan 0,01% lainnya menganut agama Buddha, Konghucu, serta aliran kepercayaan lainnya. Sementara untuk sarana rumah ibadah, terdapat 87 masjid, 105 mushola, 104 gereja Protestan, 32 gereja Katolik, 51 basarah, dan 1 pura.\n",
      "\n",
      "Referensi\n",
      "\n",
      "Pranala luar \n",
      " BPS Kabupaten Murung Raya\n",
      "\n",
      " \n",
      "Murung Raya\n",
      "Murung Raya\n",
      "================================================================\n",
      "Untuk artikel sebuah daerah di Afrika Selatan dengan nama yang sama, lihat Utrecht, KwaZulu-Natal.\n",
      "\n",
      "Kota dan munisipalitas Utrecht (, ) adalah ibu kota provinsi Utrecht di Belanda dan kota yang paling padat penduduknya di provinsi ini. Utrecht terletak di ujung timur Randstad dan merupakan kota terbesar keempat di Belanda. Pada tahun 2022, munisipalitas Utrecht akan memiliki sekitar 362.000 penduduk.\n",
      "\n",
      "Pusat kota kuno Utrecht memiliki banyak bangunan dan struktur dari awal Abad Pertengahan. Utrecht berperan sebagai pusat agama Belanda sejak abad ke-8. Saat ini kota adalah Tahta Suci Uskup Agung Utrecht, pemimpin Katolik Roma yang terpenting. Utrecht juga merupakan Tahta Suci Uskup Agung Gereja Katolik Tua, kepala tituler Uni Utrecht, dan lokasi kantor utama Gereja protestan Belanda. Sampai Zaman Keemasan, Utrecht merupakan kota yang terpenting di Belanda Utara (kini dikenal sebagai Belanda, mengecualikan Belgia dan Luksemburg), sampai Amsterdam menjadi kota budaya dan kota yang paling padat penduduknya Belanda.\n",
      "\n",
      "Utrecht peran sebagai tuan rumah Universitas Utrecht, Universitas Belanda yang terbesar, serta beberapa lembaga pendidikan tinggi lain. Karena posisi sentral di negeri, Utrecht adalah pusat transportasi penting (kereta api dan transportasi darat) di Belanda. Utrecht memiliki jumlah tertinggi kedua peristiwa budaya di Belanda, setelah Amsterdam.\n",
      "\n",
      "Sejarah\n",
      "\n",
      "Sebelum 650 M \n",
      "Meskipun ada beberapa bukti pendudukan sebelumnya di wilayah Utrecht, berasal dari Zaman Batu (sekitar 2200 SM) dan menetap di Zaman Perunggu (sekitar 1800–800 SM), pendirian kota biasanya terkait dengan pembangunan benteng Romawi (castellum), dibangun sekitar tahun 50 M. Serangkaian benteng semacam itu dibangun setelah kaisar Romawi Claudius memutuskan bahwa kekaisaran tidak boleh berkembang lebih jauh ke utara. Untuk mengkonsolidasikan perbatasan, garis pertahanan Limes Germanicus dibangun di sepanjang cabang utama sungai Rhine.\n",
      "\n",
      "Di zaman Romawi, nama benteng Utrecht adalah Traiectum. Traiectum menjadi Trecht Belanda; dengan \"U\" dari Bahasa Belanda Kuno \"uut\" (di hilir) ditambahkan untuk membedakan U-trecht dari Maas-tricht, di sungai Meuse. Dalam dokumen resmi abad ke-11, itu dilatinkan sebagai Ultra Traiectum.\n",
      "\n",
      "Pusat Kekristenan di Belanda (650-1579) \n",
      "Pada pertengahan abad ke-7, misionaris Inggris dan Irlandia berangkat untuk mempertobatkan Frisia. Paus Sergius I mengangkat Santo Wilibrordus sebagai uskup Frisia. Masa jabatan Willibrordus umumnya dianggap sebagai awal dari Keuskupan Utrecht. Pada tahun 723, pemimpin Frank, Karl Martell mendirikan benteng di Utrecht dan tanah sekitarnya sebagai pangkalan para uskup. Sejak saat itu Utrecht menjadi salah satu kursi kekuasaan paling berpengaruh bagi Gereja Katolik di Belanda. Setelah jatuhnya Dorestad sekitar tahun 850, Utrecht menjadi salah satu kota terpenting di Belanda.\n",
      "\n",
      "Republik Belanda (1579–1806) \n",
      "\n",
      "Pada tahun 1579, tujuh provinsi utara menandatangani Uni Utrecht, di mana mereka memutuskan untuk bergabung melawan kekuasaan Spanyol. Uni Utrecht dipandang sebagai awal berdirinya Republik Belanda. Pada tahun 1580, negara baru yang didominasi Protestan menghapuskan keuskupan, termasuk keuskupan agung Utrecht. Para stadtholder tidak menyetujui jalan independen borjuis Utrecht dan membawa kota itu di bawah kendali republik, menggeser kekuasaan ke provinsi dominannya, Belanda. Utrecht tetap menjadi kota atipikal di republik baru yang sekitar 40% Katolik pada pertengahan abad ke-17, dan terlebih lagi di antara kelompok elit, yang mencakup banyak bangsawan pedesaan dan bangsawan dengan adanya rumah kota di sana.\n",
      "\n",
      "Sejarah modern (1815–sekarang) \n",
      "Pertumbuhan kota meningkat  pada tahun 1843 ketika jalur kereta api yang menghubungkan Utrecht ke Amsterdam dibuka. Setelah itu, Utrecht secara bertahap menjadi hub utama jaringan kereta api Belanda. Dengan Revolusi Industri yang mulai berpengaruh di Belanda, Utrecht mulai tumbuh jauh melampaui zaman abad pertengahan.\n",
      "\n",
      "Selama Perang Dunia II, Utrecht diduduki oleh pasukan Jerman hingga Jerman menyerah atas Belanda pada 5 Mei 1945. Pasukan Britania Raya dan Kanada yang mengepung kota itu memasukinya setelah penyerahan itu, pada 7 Mei 1945. Setelah berakhirnya Perang Dunia II, kota ini berkembang pesat ketika lingkungan baru seperti Overvecht, Kanaleneiland, Hoograven, dan Lunetten dibangun. Sekitar tahun 2000, kawasan perumahan Leidsche Rijn dikembangkan sebagai perpanjangan kota ke barat.\n",
      "\n",
      "Geografi \n",
      "\n",
      "Utrecht terletak di tengah-tengah Belanda dan di provinsi Utrecht. Kota ini berasal dari sebuah tikungan sungai Rhine, ketika lengan utama sungai yang mengikuti jalur Kromme Rijn dan Oude Rijn saat ini. Ada kastil Romawi di situs Domplein hari ini. Kromme Rijn mengalir ke Utrecht di timur meninggalkan kanal-kanal kota seperti Vecht (utara) dan Leidse Rijn (barat). Kanal Amsterdam-Rhine yang lebar membentang ke barat kota, ke selatan mengaliri Vaartsche Rijn, kanal yang jauh lebih tua.\n",
      "\n",
      "Di sebelah barat kota, \"di atas Kanal Amsterdam-Rhine\" (over het Amsterdam-Rijnkanaal), terletak proyek perluasan pinggiran kota Leidsche Rijn, lokasi Vinex terbesar dan proyek konstruksi baru di Belanda. Ketika selesai, serangkaian perumahan baru akan menampung sekitar 90.000 jiwa. Di utara, selatan dan timur kota terdapat beberapa kota pinggiran, komuter dan satelit. Aglomerasi metropolitan bertepatan dengan BRU (Dewan Wilayah Utrecht).\n",
      "\n",
      "Iklim \n",
      "Utrecht adalah wilayah dengan iklim laut sedang (Köppen: Cfb) yang mirip dengan keseluruhan Belanda.\n",
      "\n",
      "Transportasi\n",
      "\n",
      "Transportasi publik \n",
      "\n",
      "Karena lokasinya yang sentral, Utrecht terhubung dengan baik ke seluruh Belanda dan memiliki jaringan transportasi umum yang berkembang dengan sangat baik.\n",
      "\n",
      "Utrecht Centraal adalah stasiun kereta api utama Utrecht dan merupakan yang terbesar di Belanda. Ada layanan antarkota reguler ke semua kota besar di Belanda; layanan langsung ke Bandara Schiphol. Utrecht Centraal adalah stasiun dengan layanan malam hari, menyediakan layanan sepanjang malam selama 7 hari seminggu ke (antara lain) Bandara Schiphol, Amsterdam dan Rotterdam. Layanan InterCityExpress (ICE) International ke Jerman (dan selanjutnya) melalui Arnhem di Utrecht Centraal.\n",
      "\n",
      "Utrecht adalah lokasi kantor pusat Nederlandse Spoorwegen (Kereta Api Belanda), operator kereta api terbesar di Belanda, dan ProRail, perusahaan milik negara yang bertanggung jawab atas pembangunan dan pemeliharaan infrastruktur kereta api negara.\n",
      "\n",
      "Bus dan trem \n",
      "\n",
      "Terminal bus lokal dan regional utama Utrecht terletak berdekatan dengan stasiun kereta api Utrecht Centraal, di pintu masuk Timur dan Barat. Karena renovasi besar-besaran dan pekerjaan konstruksi di stasiun kereta api, halte bus stasiun sering berubah. Sebagai aturan umum, bus tujuan barat berangkat dari terminal bus di pintu masuk barat, bus lain dari stasiun sisi timur. Bus lokal di Utrecht dioperasikan oleh Qbuzz; layanannya mencakup layanan frekuensi tinggi ke distrik universitas Uithof. Armada bus lokal adalah salah satu yang terbersih di Eropa, hanya menggunakan bus yang memenuhi standar Euro-VI serta bus listrik untuk transportasi dalam kota. Bus regional dari kota dioperasikan oleh Arriva dan Connexxion.\n",
      "\n",
      "Sneltram Utrecht adalah skema LRT yang berjalan ke selatan dari Utrecht Centraal ke pinggiran IJsselstein, Kanaleneiland, Lombok, dan Nieuwegein. Sneltram mulai beroperasi pada tahun 1983 dan saat ini dioperasikan oleh perusahaan transportasi swasta Qbuzz. Pada 16 Desember 2019, jalur trem baru ke Uithof mulai beroperasi, menciptakan koneksi transit massal langsung dari stasiun pusat ke kampus utama universitas Utrecht.\n",
      "\n",
      "Bersepeda \n",
      "\n",
      "Seperti kebanyakan kota di Belanda, Utrecht memiliki jaringan jalur sepeda yang luas, sehingga bersepeda menjadi aman dan populer. 33% perjalanan di dalam kota dilakukan dengan sepeda, lebih banyak daripada moda transportasi lainnya. Sepeda digunakan oleh orang muda dan tua, dan oleh individu dan keluarga. Kebanyakan adalah sepeda tradisional yang tegak, berbingkai baja, dengan sedikit roda gigi. Ada juga sepeda gerobak, untuk membawa belanjaan atau anak kecil.\n",
      "\n",
      "Pada tahun 2014, Dewan Kota memutuskan untuk membangun stasiun parkir sepeda terbesar di dunia, di dekat Stasiun Kereta Api Utama. Konstruksi tiga lantai ini akan menelan biaya sekitar €48 juta dan akan menampung 12.500 sepeda. Stasiun parkir sepeda akhirnya dibuka pada 19 Agustus 2019.\n",
      "\n",
      "Pendidikan \n",
      "\n",
      "Utrecht adalah rumah dari beberapa institusi pendidikan tinggi besar. Yang paling menonjol adalah Universitas Utrecht (perkiraan sejak 1636), universitas terbesar di Belanda dengan 30.449 mahasiswa (per 2012). Universitas ini sebagian berbasis di pusat kota serta di area kampus Uithof, di sebelah timur kota. Universitas Utrecht adalah universitas terbaik ke-57 di dunia. Utrecht juga menampung Universitas Studi Humanistik yang jauh lebih kecil, yang menampung sekitar 400 mahasiswa.\n",
      "\n",
      "Utrecht adalah rumah dari salah satu lokasi TIAS School for Business and Society, yang berfokus pada pendidikan manajemen pasca-pengalaman dan sekolah manajemen terbesar dari jenisnya di Belanda. Pada tahun 2008, program MBA eksekutifnya dinilai sebagai program terbaik ke-24 di dunia oleh Financial Times.\n",
      "\n",
      "Utrecht juga merupakan rumah bagi dua institusi besar pendidikan tinggi lainnya: universitas kejuruan Hogeschool Utrecht (37.000 siswa), dengan lokasi di kota dan kampus Uithof; dan Sekolah Seni HKU Utrecht (3.000 siswa).\n",
      "\n",
      "Tempat menarik\n",
      "\n",
      "Gereja \n",
      "\n",
      " Domtoren: Domtoren adalah menara gereja tertinggi di Belanda dengan ketinggian 112,5 meter. Menara bergaya Gotik ini juga menjadi simbol kota Utrecht. Menara ini merupakan bagian dari Katedral Santo Martinus (juga dikenal dengan sebutan Gereja Katedral). Menara ini sendiri dibangun dari tahun 1321 hingga 1382 berdasarkan rancangan Johan dari Hainaut. Katedral ini tidak pernah dituntaskan pembangunannya akibat kekurangan biaya. Panti umat yang masih belum selesai pembangunannya runtuh pada tahun 1674, alhasil menara ini pun berdiri sendiri.\n",
      " Katedral Santo Martinus: Katedral Santo Martinus, Utrecht, atau Gereja Dom (Belanda: Domkerk), adalah sebuah gereja Gotik yang didedikasikan untuk Santo Martinus dari Tours, yang merupakan katedral Keuskupan Utrecht selama Abad Pertengahan. Ini adalah satu-satunya katedral pra-Reformasi di Belanda, tetapi telah menjadi gereja Protestan sejak 1580.\n",
      " Gereja Santo Petrus: Gereja Santo Petrus (Belanda: Pieterskerk) adalah sebuah gereja Reformasi dan bekas gereja Katolik Roma yang didedikasikan untuk Rasul Petrus. Ini adalah salah satu gereja tertua di Utrecht. Gereja ini diresmikan pada 1 Mei 1048 oleh Bernold, Uskup Utrecht. Gereja ini adalah ujung timur \"Kerkenkruis\" Utrecht, di mana Domkerk menjadi pusatnya. Ciri khas gaya Romanesque di mana ia dibangun adalah pilar-pilar nave gereja yang besar, masing-masing dipahat dari sepotong batu pasir merah, dan ruang bawah tanah di bawah paduan suara. Bangunan itu sekarang digunakan oleh Gereja Walloon.\n",
      "\n",
      "Lainnya \n",
      "\n",
      " Centraal Museum: Museum Centraal adalah museum di Utrecht yang didirikan pada tahun 1838. Museum ini memiliki koleksi yang sangat luas. Koleksi lukisan oleh Northern Mannerist Joachim Wtewael sejauh ini adalah yang terbesar di dunia. Sorotan lainnya adalah banyak lukisan penting oleh Utrecht Caravaggisti, seperti Gerard van Honthorst dan Hendrick ter Brugghen. Keduanya melakukan perjalanan ke Roma pada awal abad ke-17 untuk mempelajari karya-karya master Italia, Caravaggio. Pada generasi sebelumnya termasuk Wtewael, Abraham Bloemaert dan pelukis potret Paulus Moreelse adalah pelukis Utrecht yang paling signifikan, dengan Jan van Scorel masih lebih awal.\n",
      " Duitse Huis: Duitse Huis adalah kompleks bangunan di kota Utrecht, Belanda, yang dilindungi sebagai monumen nasional. Bagian-bagian yang lebih tua berasal dari sebuah biara Bailiwick of Utrecht dari Ksatria Teutonik yang didirikan pada tahun 1348. Awalnya Katolik, ordo tersebut menjadi Protestan selama Reformasi. Sebuah rumah sakit militer ditambahkan pada tahun 1823 setelah para ksatria menjual properti itu.\n",
      " Kastil de Haar: Kastil de Haar (Kasteel de Haar) adalah sebuah kastel yang terletak di dekat Haarzuilens di Utrecht, Belanda. Kastil ini dibangun pada tahun 1391, hingga hancur dan kemudian direstorasi dari tahun 1892 hingga 1907 oleh arsitek Belanda Pierre Cuypers. Restorasi ini merupakan bagian dari proyek restorasi neo-Gotik yang didanai oleh keluarga Rothschild.\n",
      " Museum Kereta Api: Museum Kereta Api (Het Spoorwegmuseum) di Utrecht adalah museum kereta api nasional Belanda. Didirikan pada tahun 1927 dan sejak 1954 telah ditempatkan di bekas stasiun Maliebaan. Museum saat ini memiliki koleksi kereta api yang besar dan beragam. Koleksi yang dipamerkan antara lain lokomotif uap, lokomotif listrik, lokomotif diesel, gerbong kereta, gerbong barang, dan beberapa trem.\n",
      "\n",
      "Kanal \n",
      "\n",
      " Oudegracht: Oudegracht (Kanal Tua) adaah kanal sepanjang kira-kira dua kilometer yang menghubungkan antara Kromme Rijn dan Vecht, dan memotong seluruh pusat kota dari selatan ke utara. Selama berabad-abad kanal itu telah menjadi arteri utama kota. Sistem pekarangan dan gudang bawah tanah di Utrecht Oudegracht dan Nieuwegracht adalah salah satu yang terunik di dunia dalam skala ini. Pada tahun 2008, sistem kanal, jembatan dan dermaga dinominasikan untuk Daftar Warisan Dunia UNESCO, tetapi nominasi tersebut belum dilakukan.\n",
      "\n",
      "Galeri\n",
      "\n",
      "Lihat pula \n",
      " Daftar munisipalitas Belanda\n",
      " Museum Maluku\n",
      "\n",
      "Pranala luar \n",
      "  Situs resmi\n",
      "\n",
      "Referensi \n",
      "\n",
      "Kota di Utrecht\n",
      " \n",
      "Kota di Belanda\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "for row in dataset_split['test'][:3][\"text\"]:\n",
    "    print(\"================================================================\")\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://huggingface.co/docs/transformers/en/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=dataset_split['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 55:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250405_125129-hsizwwoa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/huggingface/runs/hsizwwoa' target=\"_blank\">tmp_trainer</a></strong> to <a href='https://wandb.ai/alimtegar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/huggingface' target=\"_blank\">https://wandb.ai/alimtegar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/huggingface/runs/hsizwwoa' target=\"_blank\">https://wandb.ai/alimtegar/huggingface/runs/hsizwwoa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-272cc76e3ad8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Perplexity:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eval_loss'"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_model_preparation_time': 0.0079,\n",
       " 'eval_runtime': 3389.5764,\n",
       " 'eval_samples_per_second': 0.148,\n",
       " 'eval_steps_per_second': 0.019}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results\n",
    "# loss = eval_results['eval_loss']\n",
    "# print(\"Loss:\", loss)\n",
    "# print(\"Perplexity:\", math.exp(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
