{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "!fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from typing import Optional, Literal, Union, List\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download, create_repo, upload_folder\n",
    "from safetensors.torch import load_file, save_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import cuml\n",
    "from cuml.decomposition import PCA as cuPCA\n",
    "from cuml.manifold import UMAP as cuUMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(\n",
    "        repo_id: str, \n",
    "        checkpoint: Optional[int], \n",
    "        max_checkpoints: str = 10_000, \n",
    "        checkpoint_steps: str = 25,\n",
    "    ):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_steps) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dir = os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir, checkpoint_dir\n",
    "\n",
    "def check_loss_and_grad_norm(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt=\"Paris is the capital of\",\n",
    "):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "    print(\"Loss:\", outputs.loss)\n",
    "\n",
    "    # Backward pass\n",
    "    if outputs.loss.grad_fn is None:\n",
    "        print(\"Gradient norm:\", None)\n",
    "        return\n",
    "\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "\n",
    "def check_parameter(n, p):\n",
    "    print(f\"- {'name':<8}:\", n)\n",
    "    print(f\"- {'device':<8}:\", p.device)\n",
    "    print(f\"- {'dtype':<8}:\", p.dtype)\n",
    "    print(f\"- {'mean':<8}:\", p.mean().item())\n",
    "    print(f\"- {'min':<8}:\", p.min().item())\n",
    "    print(f\"- {'max':<8}:\", p.max().item())\n",
    "\n",
    "def check_lora_parameters(model, prefix=None):\n",
    "    prefix = 'lora.' + prefix if prefix != None else 'lora'\n",
    "    for n, p in model.named_parameters():\n",
    "        if prefix in n:\n",
    "            check_parameter(n, p)\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=32, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def get_task_and_lang_from_repo_id(repo_id: str):\n",
    "    task, lang, _ = repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "    return task, lang\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id: str,\n",
    "    train_size: int = 5000,\n",
    "    test_size: int = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang = get_task_and_lang_from_repo_id(lora_repo_id)\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    \n",
    "    # Load dataset using streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_grad_norm(params):\n",
    "    grad_norm = 0.0\n",
    "    for p in params:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def compute_named_grad_norm(named_params):\n",
    "    grad_norm = 0.0\n",
    "    for n, p in named_params.items():\n",
    "        if p.grad is not None:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            print(\"{n} p_grad_norm:\", p_grad_norm)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "        else:\n",
    "            print(f\"[WARN] No gradient for {n}\")\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def format_float(v):\n",
    "    if abs(v) < 0.0001 or abs(v) >= 10000:\n",
    "        return f\"{v:.4e}\"\n",
    "    else:\n",
    "        return f\"{v:.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9960991020d7490f8268a44e86986eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439d275fe3614c689a751ba7f1d757ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "- L1T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650/adapter_model.safetensors\n",
      "- L2T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650/adapter_model.safetensors\n",
      "\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "device = 'auto'\n",
    "\n",
    "# Data configuration\n",
    "hf_data_id = 'alxxtexxr/Nero-XLT-Dataset'\n",
    "hf_data_dir = 'gsm8k_en_5K_1K_1K_512'\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 1.0\n",
    "lr = 2e-4\n",
    "warmup_ratio = 0.1\n",
    "# num_warmup_steps = 100\n",
    "checkpoint_steps = 25\n",
    "push_to_hf = False\n",
    "generate_steps = 25\n",
    "sample_prompt = '102452 + 102453 ='\n",
    "distance_fn = 'euclidean'\n",
    "\n",
    "# Model configurations\n",
    "model_configs = {\n",
    "    # L1T1 (Source Language - Source Task)\n",
    "    'L1T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L2T1 (Target Language - Source Task)\n",
    "    'L2T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L1T2 (Source Language - Target Task)\n",
    "    # 'L1T2': {\n",
    "    #     'hf_lora_id': 'alxxtexxr/L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457',\n",
    "    #     'checkpoint': 1875,\n",
    "    # },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    _, lora_dir = download_hf_model(config['hf_lora_id'], config['checkpoint'])\n",
    "    model_configs[key]['lora_dir'] = lora_dir\n",
    "    model_configs[key]['lora_path'] = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "    model_configs[key]['lora_config'] = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "for key, config in model_configs.items():\n",
    "    print(f\"- {key}:\")\n",
    "    for config_name, config_value in config.items():\n",
    "        if config_name == 'lora_config':\n",
    "            continue\n",
    "        print(f\"{'-':>3} {config_name:<10}: {config_value}\")\n",
    "print()\n",
    "\n",
    "assert (\n",
    "    model_configs['L1T1']['lora_config'].base_model_name_or_path == \n",
    "    model_configs['L2T1']['lora_config'].base_model_name_or_path\n",
    "), \"Base models must be the same\"\n",
    "base_model_name = model_configs['L1T1']['lora_config'].base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating Nero directory: L3.1-8B-gsm8k-en-5K-1K-1K-512-Nero-euclidean-v20250821173023\n",
      "[INFO] Nero directory created!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face configuration\n",
    "hf_nero_id = None\n",
    "resume_step = 0\n",
    "\n",
    "if hf_nero_id is not None and resume_step > 0:\n",
    "    print(f\"[INFO] Downloading Nero checkpoint at step {resume_step} from Hugging Face repository:\", hf_nero_id)\n",
    "    nero_dir, _ = download_hf_model(hf_nero_id, resume_step)\n",
    "    print(f\"[INFO] Nero checkpoint downloaded successfully!\")\n",
    "else:\n",
    "    hf_username = 'alxxtexxr'\n",
    "    nero_dir = f'L3.1-8B-{hf_data_dir.replace(\"_\", \"-\")}-Nero-{distance_fn}-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    print(f\"[INFO] Creating Nero directory:\", nero_dir)\n",
    "    hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "    os.makedirs(nero_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Nero directory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearFunction(nn.Module):\n",
    "    def __init__(self, dim, rank=8):\n",
    "        super().__init__()\n",
    "        self.A = nn.Linear(dim, rank, bias=False)\n",
    "        self.B = nn.Linear(rank, dim, bias=False)\n",
    "        self.act = nn.Tanh() # or GELU\n",
    "    \n",
    "    def forward(self, x, name=None):\n",
    "        act_out = self.act(self.B(self.A(x)))\n",
    "        # print()\n",
    "        # print(f\"{name}.act_out ->\", act_out.mean().item(), act_out.max().item(), act_out.min().item())\n",
    "        # print()\n",
    "        return x + act_out\n",
    "\n",
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer,\n",
    "                 \n",
    "                 # LoRA parameters\n",
    "                 L1T1_lora_params, \n",
    "                 L2T1_lora_params,\n",
    "                 L2T2_lora_params,\n",
    "                 eval_L2T2_lora = False,\n",
    "                 \n",
    "                 # Debugging parameters\n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self._eval_L2T2_lora = eval_L2T2_lora\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "\n",
    "        # Initialize LoRA layers\n",
    "        self.lora = nn.ModuleDict({\n",
    "            'L1T1': self._init_lora_layer('L1T1', in_features, out_features, **L1T1_lora_params, device=self.device), # frozen\n",
    "            'L2T1': self._init_lora_layer('L2T1', in_features, out_features, **L2T1_lora_params, device=self.device), # frozen\n",
    "            'L2T2': self._init_lora_layer('L2T2', in_features, out_features, **L2T2_lora_params, device=self.device), # trained alternately with nonlinear function\n",
    "        })\n",
    "        \n",
    "        # Initialize nonlinear function layer\n",
    "        self.nonlinear_fn = NonlinearFunction(out_features).to(self.device) # trained alternately with L2T2 LoRA layer\n",
    "        # self.nonlinear_fn = nn.Identity()\n",
    "        \n",
    "    def _init_lora_layer(self, name, in_features, out_features, rank, alpha, dropout=0.0, bias=True, use_rslora=False, device=None):\n",
    "        scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        scaling *= 0.1 # temporary to prevent overflow\n",
    "        dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        layer = nn.ModuleDict({\n",
    "            'A': nn.Linear(in_features, rank, bias=bias, device=device),\n",
    "            'B': nn.Linear(rank, out_features, bias=bias, device=device),\n",
    "            'dropout': dropout_layer\n",
    "        })\n",
    "        layer.scaling = scaling\n",
    "        layer.bias_flag = bias\n",
    "        nn.init.normal_(layer['A'].weight, 0.0, 1 / math.sqrt(rank))\n",
    "        if name == 'L2T2':\n",
    "            nn.init.normal_(layer['B'].weight, 0.0, 1e-3)\n",
    "        else:\n",
    "            nn.init.zeros_(layer['B'].weight)\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ================================================================\n",
    "        # Base Layer\n",
    "        # ================================================================\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora.L1T1.A.weight.dtype)\n",
    "            \n",
    "        # ================================================================\n",
    "        # L2T2 LoRA Layer (Trainable)\n",
    "        # ================================================================\n",
    "        # L2T2_lora_out = self.lora.L2T2.B(self.lora.L2T2.A(self.lora.L2T2.dropout(x))) #* self.lora.L2T2.scaling\n",
    "        L2T2_lora_dropout_out = self.lora.L2T2.dropout(x)\n",
    "        L2T2_lora_A_out = self.lora.L2T2.A(L2T2_lora_dropout_out)\n",
    "        L2T2_lora_B_out = self.lora.L2T2.B(L2T2_lora_A_out)\n",
    "        L2T2_lora_out = L2T2_lora_B_out * self.lora.L2T2.scaling\n",
    "        \n",
    "        if self._eval_L2T2_lora:\n",
    "            if requires_conversion:\n",
    "                L2T2_lora_out = L2T2_lora_out.to(base_out.dtype)\n",
    "            return base_out + L2T2_lora_out\n",
    "\n",
    "        # ================================================================\n",
    "        # L1T1 LoRA Layer (Frozen)\n",
    "        # ================================================================\n",
    "        # L1T1_lora_out = self.lora.L1T1.B(self.lora.L1T1.A(self.lora.L1T1.dropout(x))) #* self.lora.L1T1.scaling\n",
    "        L1T1_lora_dropout_out = self.lora.L1T1.dropout(x)\n",
    "        L1T1_lora_A_out = self.lora.L1T1.A(L1T1_lora_dropout_out)\n",
    "        L1T1_lora_B_out = self.lora.L1T1.B(L1T1_lora_A_out)\n",
    "        L1T1_lora_out = L1T1_lora_B_out * self.lora.L1T1.scaling\n",
    "        \n",
    "        # ================================================================\n",
    "        # L2T1 LoRA Layer (Frozen)\n",
    "        # ================================================================\n",
    "        # L2T1_lora_out = self.lora.L2T1.B(self.lora.L2T1.A(self.lora.L2T1.dropout(x))) #* self.lora.L2T1.scaling\n",
    "        L2T1_lora_dropout_out = self.lora.L2T1.dropout(x)\n",
    "        L2T1_lora_A_out = self.lora.L2T1.A(L2T1_lora_dropout_out)\n",
    "        L2T1_lora_B_out = self.lora.L2T1.B(L2T1_lora_A_out)\n",
    "        L2T1_lora_out = L2T1_lora_B_out * self.lora.L2T1.scaling\n",
    "\n",
    "        # ================================================================\n",
    "        # Output\n",
    "        # ================================================================\n",
    "        # Linear:\n",
    "        # queen = king - man + king\n",
    "        # L2T2 = L1T2 - L1T1 + L2T1\n",
    "        \n",
    "        # Nonlinear:\n",
    "        # f(L2T2) = f(L1T2) - f(L1T1) + f(L2T1)\n",
    "        # f(L1T2) = f(L2T2) - f(L2T1) + f(L1T1)\n",
    "        # L1T2 = f^-1(f(L2T2) - f(L2T1) + f(L1T1))\n",
    "        \n",
    "        # with torch.no_grad():  # avoid affecting computation graph\n",
    "        #     print()\n",
    "        #     print(\"x:\", x.mean().item(), x.max().item(), x.min().item())\n",
    "        #     print()\n",
    "        #     print(\"L2T2_lora_B_out:\", L2T2_lora_B_out.mean().item(), L2T2_lora_B_out.max().item(), L2T2_lora_B_out.min().item())\n",
    "        #     print(\"L2T2_lora_out:\", L2T2_lora_out.mean().item(), L2T2_lora_out.max().item(), L2T2_lora_out.min().item())\n",
    "        #     print(\"L1T1_lora_B_out:\", L1T1_lora_B_out.mean().item(), L1T1_lora_B_out.max().item(), L1T1_lora_B_out.min().item())\n",
    "        #     print(\"L1T1_lora_out:\", L1T1_lora_out.mean().item(), L1T1_lora_out.max().item(), L1T1_lora_out.min().item())\n",
    "        #     print(\"L2T1_lora_B_out:\", L2T1_lora_B_out.mean().item(), L2T1_lora_B_out.max().item(), L2T1_lora_B_out.min().item())\n",
    "        #     print(\"L2T1_lora_out:\", L2T1_lora_out.mean().item(), L2T1_lora_out.max().item(), L2T1_lora_out.min().item())\n",
    "        #     print()\n",
    "        \n",
    "        f_L2T2_out = self.nonlinear_fn(L2T2_lora_out, name='L2T2_lora_out')\n",
    "        f_diff = self.nonlinear_fn(L1T1_lora_out, name='L1T1_lora_out') - self.nonlinear_fn(L2T1_lora_out, name='L2T1_lora_out')\n",
    "        L1T2_out = f_L2T2_out - f_diff # approximate f^-1 as subtraction\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     print(\"f_L2T2_out:\", f_L2T2_out.mean().item(), f_L2T2_out.max().item(), f_L2T2_out.min().item())\n",
    "        #     print(\"f_diff:\", f_diff.mean().item(), f_diff.max().item(), f_diff.min().item())\n",
    "        #     print(\"L1T2_out:\", L1T2_out.mean().item(), L1T2_out.max().item(), L1T2_out.min().item())\n",
    "        #     print()\n",
    "        \n",
    "        if requires_conversion:\n",
    "            L1T2_out = L1T2_out.to(base_out.dtype)\n",
    "        return base_out + L1T2_out\n",
    "\n",
    "    def load_lora_params(self, mode: Literal['L1T1', 'L2T1', 'L2T2'], state_dict, prefix: str):\n",
    "        self.lora[mode].A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora[mode].B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora[mode].bias_flag:\n",
    "            self.lora[mode].A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora[mode].B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "            \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 base_model: nn.Module, \n",
    "                 L1T1_lora_config: LoraConfig, \n",
    "                 L2T1_lora_config: LoraConfig, \n",
    "                 debug: bool = False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self._eval_L2T2_lora = False\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self._wrap_target_layers(L1T1_lora_config, L2T1_lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, L1T1_lora_config, L2T1_lora_config):\n",
    "        assert L1T1_lora_config.target_modules == L2T1_lora_config.target_modules, \"[ERROR] L1T1 and L2T1 LoRA configurations must have the same target modules.\"\n",
    "\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in L1T1_lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    base_layer=module,\n",
    "                    eval_L2T2_lora=self._eval_L2T2_lora,\n",
    "\n",
    "                    # L1T1 LoRA parameters\n",
    "                    L1T1_lora_params={\n",
    "                        'rank': L1T1_lora_config.r, \n",
    "                        'alpha': L1T1_lora_config.lora_alpha, \n",
    "                        'dropout': L1T1_lora_config.lora_dropout,\n",
    "                        'bias': L1T1_lora_config.lora_bias,\n",
    "                        'use_rslora': L1T1_lora_config.use_rslora,\n",
    "                    },\n",
    "                \n",
    "                    # L2T1 LoRA parameters\n",
    "                    L2T1_lora_params={\n",
    "                        'rank': L2T1_lora_config.r, \n",
    "                        'alpha': L2T1_lora_config.lora_alpha, \n",
    "                        'dropout': L2T1_lora_config.lora_dropout,\n",
    "                        'bias': L2T1_lora_config.lora_bias,\n",
    "                        'use_rslora': L2T1_lora_config.use_rslora,\n",
    "                    },\n",
    "\n",
    "                    # L2T2 parameters (for temporary, use L2T1 LoRA parameters)\n",
    "                    L2T2_lora_params={\n",
    "                        'rank': L2T1_lora_config.r, \n",
    "                        'alpha': L2T1_lora_config.lora_alpha, \n",
    "                        'dropout': L2T1_lora_config.lora_dropout,\n",
    "                        'bias': L2T1_lora_config.lora_bias,\n",
    "                        'use_rslora': L2T1_lora_config.use_rslora,\n",
    "                    },\n",
    "                    \n",
    "                    # Debugging parameters\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        \n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "            \n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def train_L2T2_lora(self, verbose: bool=False):\n",
    "        self.freeze_all_except_L2T2_lora()\n",
    "        \n",
    "        for layer in self.nero_layers.values():\n",
    "            layer._eval_L2T2_lora = False\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Training L2T2 LoRA!\")\n",
    "    \n",
    "    def train_nonlinear_fn(self, verbose: bool=False):\n",
    "        self.freeze_all_except_nonlinear_fn()\n",
    "        \n",
    "        for layer in self.nero_layers.values():\n",
    "            layer._eval_L2T2_lora = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Training nonlinear functions!\")\n",
    "            \n",
    "    def eval_L2T2_lora(self, verbose: bool=False):\n",
    "        self.freeze_all()\n",
    "        \n",
    "        for layer in self.nero_layers.values():\n",
    "            layer._eval_L2T2_lora = True\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Evaluating L2T2 LoRA!\")\n",
    "            \n",
    "    def freeze_all(self, verbose: bool=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen!\")\n",
    "\n",
    "    def freeze_all_except_L2T2_lora(self, verbose=False):\n",
    "        self.freeze_all(verbose=verbose)\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'lora.L2T2' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen except L2T2 LoRA layers!\")\n",
    "    \n",
    "    def freeze_all_except_nonlinear_fn(self, verbose=False):\n",
    "        self.freeze_all(verbose=verbose)\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nonlinear_fn' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen except nonlinear functions layers\")\n",
    "    \n",
    "    def load_lora_params(self, mode: Literal['L1T1', 'L2T1', 'L2T2'], lora_path: str):\n",
    "        if not os.path.exists(lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] LoRA file not found:\", lora_path)\n",
    "        \n",
    "        if lora_path.endswith('.safetensors'):\n",
    "            state_dict = load_file(lora_path)\n",
    "        else:\n",
    "            state_dict = torch.load(lora_path, map_location='cpu')\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(mode, state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(f\"[INFO] {mode} LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    device_map=device,\n",
    ")\n",
    "model = NeroModel(\n",
    "    base_model, \n",
    "    L1T1_lora_config=model_configs['L1T1']['lora_config'], \n",
    "    L2T1_lora_config=model_configs['L2T1']['lora_config'], \n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded L1T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L1T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : -0.0011653788387775421\n",
      "- min     : -1.404322624206543\n",
      "- max     : 1.4684089422225952\n",
      "\n",
      "[INFO] L1T1 LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded L1T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L1T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 6.287686119321734e-05\n",
      "- min     : -0.04176201671361923\n",
      "- max     : 0.04242725297808647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded L1T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L1T1')\n",
    "print()\n",
    "\n",
    "model.load_lora_params('L1T1', model_configs['L1T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L1T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L1T1')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded L2T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L2T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 0.0026187323965132236\n",
      "- min     : -1.3684216737747192\n",
      "- max     : 1.397903561592102\n",
      "\n",
      "[INFO] L2T1 LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded L2T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L2T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 2.2152138626552187e-05\n",
      "- min     : -0.06327299773693085\n",
      "- max     : 0.0625513345003128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded L2T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L2T1')\n",
    "print()\n",
    "\n",
    "model.load_lora_params('L2T1', model_configs['L2T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L2T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L2T1')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.freeze_all_except_L2T2_lora()\n",
    "print()\n",
    "\n",
    "# check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gradient checkpointing enabled!\n",
      "\n",
      "Loss: tensor(3.5031, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 25.50963737989262\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"[INFO] Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(hf_data_id, data_dir=hf_data_dir)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total batches: 1250\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"[INFO] Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask):\n",
      "(torch.Size([4, 512]), torch.Size([4, 512]))\n",
      "\n",
      "First batch text:\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "Jamie owns 4 Persian ...\n",
      "\n",
      "Loss: tensor(3.5031, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 25.50963737989262\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(1.5417, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 7.547106782720999\n"
     ]
    }
   ],
   "source": [
    "check_loss_and_grad_norm(model, tokenizer, prompt=first_batch_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3507/3493382525.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  L2T2_scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_3507/3493382525.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  nonlinear_fn_scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791c53407f7748fa8ea4b563290fbbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112919377774233, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250821_173046-n3xedr6v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/n3xedr6v' target=\"_blank\">fresh-morning-181</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/n3xedr6v' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/n3xedr6v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# If `device` is not specified or set to 'auto', use the model's device\n",
    "# if device is None or device == 'auto':\n",
    "device = next(iter(model.parameters())).device\n",
    "\n",
    "# Set up optimizer and gradient scaler\n",
    "# for L2T2 LoRA\n",
    "model.train_L2T2_lora()\n",
    "L2T2_lora_params = [p for n, p in model.named_parameters() if p.requires_grad]\n",
    "L2T2_optimizer = torch.optim.Adam(L2T2_lora_params, lr=lr)\n",
    "L2T2_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# for nonlinear functions\n",
    "model.train_nonlinear_fn()\n",
    "nonlinear_fn_params = [p for n, p in model.named_parameters() if p.requires_grad]\n",
    "nonlinear_fn_optimizer = torch.optim.Adam(nonlinear_fn_params, lr=lr)\n",
    "nonlinear_fn_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Set up LR scheduler\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "if warmup_ratio > 0:\n",
    "    # If `warmup_ratio` > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup # type: ignore\n",
    "    max_optimizer_steps = (max_global_steps // grad_accumulation_steps) // 2 # divide by 2 because we train in 2 modes\n",
    "    num_warmup_steps = int(warmup_ratio * max_optimizer_steps)\n",
    "    L2T2_scheduler = get_cosine_schedule_with_warmup(\n",
    "        L2T2_optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "    nonlinear_fn_scheduler = get_cosine_schedule_with_warmup(\n",
    "        nonlinear_fn_optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "else:\n",
    "    # If `warmup_ratio` is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "    L2T2_scheduler = LambdaLR(L2T2_optimizer, lr_lambda=lambda step: 1.0)\n",
    "    nonlinear_fn_scheduler = LambdaLR(nonlinear_fn_optimizer, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True, # End previous run and start a new one\n",
    "    config=dict(\n",
    "        # Project configuration\n",
    "        seed = seed,\n",
    "        device = device,\n",
    "\n",
    "        # Data configuration\n",
    "        hf_data_id = hf_data_id,\n",
    "        hf_data_dir = hf_data_dir,\n",
    "\n",
    "        # Training configuration\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "        lr = lr,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        checkpoint_steps = checkpoint_steps,\n",
    "        distance_fn = distance_fn,\n",
    "        resume_step = resume_step,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "global_step = resume_step\n",
    "start_epoch = 0\n",
    "\n",
    "def load_trainer_params(target, model, optimizer, scheduler, scaler, checkpoint_dir, device):\n",
    "    # Load Nero parameters\n",
    "    nero_path = os.path.join(checkpoint_dir, f'{target}.safetensors')\n",
    "    # model.load_nero_params(mode, nero_path)\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_dir, f'{target}_optimizer.pt')\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "    \n",
    "    # Move optimizer state to the correct device\n",
    "    for param in optimizer.state:\n",
    "        param_device = param.device\n",
    "        param_dtype = param.dtype\n",
    "        for key, value in optimizer.state[param].items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                optimizer.state[param][key] = value.to(device=param_device, dtype=param_dtype)\n",
    "\n",
    "    # Load scheduler state\n",
    "    scheduler_path = os.path.join(checkpoint_dir, f'{target}_scheduler.pt')\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
    "\n",
    "    # Load scaler state\n",
    "    scaler_path = os.path.join(checkpoint_dir, f'{target}_scaler.pt')\n",
    "    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "\n",
    "if resume_step > 0:\n",
    "    checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{resume_step}')\n",
    "    print(f\"[INFO] Resuming training from checkpoint directory:\", checkpoint_dir)\n",
    "\n",
    "    # Load trainer parameters\n",
    "    load_trainer_params('L2T2_lora', model, L2T2_optimizer, L2T2_scheduler, L2T2_scaler, checkpoint_dir, device)\n",
    "    load_trainer_params('nonlinear_fn', model, nonlinear_fn_optimizer, nonlinear_fn_scheduler, nonlinear_fn_scaler, checkpoint_dir, device)\n",
    "\n",
    "    # Load trainer state\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, 'trainer_state.json')\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        start_epoch = log_history[-1]['epoch'] if log_history else 0\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch} and step {resume_step}.\")\n",
    "\n",
    "    # Load RNG state for reproducibility\n",
    "    rng_path = os.path.join(checkpoint_dir, 'rng_state.pth')\n",
    "    if os.path.exists(rng_path):\n",
    "        rng_state = torch.load(rng_path)\n",
    "        random.setstate(rng_state['python'])\n",
    "        np.random.set_state(rng_state['numpy'])\n",
    "        torch.set_rng_state(rng_state['cpu'])\n",
    "        if torch.cuda.is_available() and rng_state['cuda']:\n",
    "            torch.cuda.set_rng_state_all(rng_state['cuda'])\n",
    "    \n",
    "    if resume_step % grad_accumulation_steps != 0:\n",
    "        print(\"[WARN] Resuming mid-gradient accumulation cycle. Make sure this is intended.\")\n",
    "else:\n",
    "    if push_to_hf:\n",
    "        # If it's new training, create Hugging Face repository\n",
    "        print(f\"[INFO] Creating Hugging Face repository:\", hf_nero_id) # print the link instead\n",
    "        create_repo(repo_id=hf_nero_id, repo_type='model', exist_ok=True)\n",
    "        print(f\"[INFO] Hugging Face repository created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3507/4064395303.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_loss: tensor(6.9696, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "loss: tensor(3.4848, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 0\n",
      "TEST 1\n",
      "TEST 2\n",
      "{n} p_grad_norm: tensor(nan, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(nan, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(nan, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(nan, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(nan, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(nan, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(nan, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(nan, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0032, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.0554, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0053, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.7919, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0125, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(3.8190, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0007, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.1697, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0004, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.1290, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0336, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0251, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0057, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.9523, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0024, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.5888, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0315, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(9.4695, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0005, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.1749, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0003, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.1449, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0294, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(6.3471, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0373, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0099, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0069, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.4398, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0086, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(2.7389, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0011, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.5414, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0003, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.1390, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0066, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(2.5862, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0115, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(2.6845, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0091, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0056, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(2.2359, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0122, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(3.2326, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.3773, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0007, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.2525, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0087, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(2.6182, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0077, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(3.9001, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0121, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0050, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(2.2441, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0082, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.8582, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0017, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.3569, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0004, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.2276, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0098, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(2.6607, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0088, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(3.5535, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0030, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.9042, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0050, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(2.9044, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0059, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.7278, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0023, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.7643, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0005, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.2621, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0071, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.9421, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0117, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(4.3052, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0027, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.6554, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0047, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.8531, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0051, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(1.6738, device='cuda:0')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5034, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0009, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2823, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0044, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.5412, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0086, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(3.3078, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0036, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0035, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.2027, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0058, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(2.2791, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0011, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4389, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1917, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0044, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.6739, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0109, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(3.5562, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6175, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0032, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8884, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0046, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(2.4325, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0012, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8250, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0004, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2105, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0034, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.7367, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0061, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(3.6568, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0029, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6792, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0046, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.3918, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0045, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(2.0827, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6002, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0009, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2812, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0044, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(2.1828, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0115, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(3.6872, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6683, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0023, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8570, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0043, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(2.8584, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5819, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0007, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2903, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0029, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.0134, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0055, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.6496, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0025, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6774, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0022, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.1231, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0035, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.7582, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0017, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.7194, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0006, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2966, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0040, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.5053, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0076, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.9387, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0038, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.9103, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.3540, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0042, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.3621, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0012, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6675, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0004, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1601, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0035, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.1699, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0070, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(2.8794, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0022, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6357, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0024, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8208, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0046, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.4977, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0016, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4284, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0004, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2685, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0066, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(2.3934, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0098, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0042, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.9191, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0050, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(inf, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0072, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.9464, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0005, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1720, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0002, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0506, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0095, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(2.5380, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0058, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.6946, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0023, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6583, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0045, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.1020, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0043, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.7465, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0008, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2598, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0003, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1273, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0050, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.8075, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0031, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.9524, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0029, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8282, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0024, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.7760, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.4239, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3131, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0002, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1053, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.7120, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0022, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5753, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5019, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0039, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5144, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0020, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8073, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0005, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1584, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0002, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0722, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4603, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0011, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4915, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4211, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0016, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.7850, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0022, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4500, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0008, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3365, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0003, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1098, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0024, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5985, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0006, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3975, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5877, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.0470, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.7917, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4275, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0005, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1264, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0020, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5039, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0019, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.7333, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0009, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2642, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0022, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3464, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0020, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8287, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5673, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3152, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0014, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5907, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5055, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2880, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0014, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4944, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0022, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8177, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0006, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1935, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0003, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1161, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5338, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4278, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0019, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4512, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3222, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4763, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0006, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1084, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0004, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1569, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3753, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0008, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2576, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0007, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2324, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5609, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4949, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0007, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2370, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0005, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1395, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0011, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4919, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3388, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0007, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1545, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0012, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3925, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0017, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6003, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4500, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0011, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4733, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0019, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6496, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6165, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2082, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0011, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2994, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0012, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4896, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0012, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3453, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0008, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2742, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0020, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.9979, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0026, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8744, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0010, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2399, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4467, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0009, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4551, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0016, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6641, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4279, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0029, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.7824, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0017, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6710, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0008, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2467, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0016, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5482, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5397, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0003, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1330, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0003, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1023, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4216, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0012, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3719, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0016, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3748, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0019, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8641, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6169, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0011, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3973, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0004, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.1727, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0012, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.3677, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0011, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4539, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0012, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.4425, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0037, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.4411, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0022, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.0272, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0017, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5872, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0009, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2996, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.7359, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0020, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.5679, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8780, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0032, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.6760, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0023, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.4507, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0015, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.9633, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0013, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.2910, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0018, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8851, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0022, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.8020, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0020, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.9364, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0038, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.3880, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(0.0033, device='cuda:1')\n",
      "{n} p_grad_norm: tensor(1.0920, device='cuda:1')\n",
      "target: L2T2_lora, epoch: 0, step: 1, loss: 6.9696, L1T2_lora/lr: 6.4516e-06, nonlinear_fn/lr: 0.0000e+00, grad_norm: nan, grad_norm_clipped: nan\n",
      "_loss: tensor(7.0470, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "loss: tensor(3.5235, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "TEST 0\n",
      "target: nonlinear_fn, epoch: 0, step: 2, loss: 7.0470, L1T2_lora/lr: 6.4516e-06, nonlinear_fn/lr: 0.0000e+00\n",
      "_loss: tensor(7.8375, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "loss: tensor(3.9187, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "TEST 0\n",
      "TEST 1\n",
      "TEST 3\n",
      "target: nonlinear_fn, epoch: 0, step: 3, loss: 7.8375, L1T2_lora/lr: 6.4516e-06, nonlinear_fn/lr: 6.4516e-06, nonlinear_fn/grad_norm: nan, nonlinear_fn/grad_norm_clipped: nan\n",
      "_loss: tensor(7.1845, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "loss: tensor(3.5923, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "TEST 0\n",
      "target: L2T2_lora, epoch: 0, step: 4, loss: 7.1845, L1T2_lora/lr: 6.4516e-06, nonlinear_fn/lr: 6.4516e-06\n",
      "_loss: tensor(7.0451, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "loss: tensor(3.5225, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m grad_accumulation_steps\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL2T2_lora\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mL2T2_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     nonlinear_fn_scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target = 'L2T2_lora' # or 'nonlinear_fn'\n",
    "log_history = []\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of gradient accumulation cycle\n",
    "# This ensures there are no leftover gradients when resuming mid-cycle or after a previous cycle was interrupted\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    L2T2_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Skip previously completed steps\n",
    "        if global_step <= resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        # Stop training if `max_global_steps` reached\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            if target == 'L2T2_lora':\n",
    "                model.train_L2T2_lora()\n",
    "            else:\n",
    "                model.train_nonlinear_fn()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids,\n",
    "                use_cache=False, # Disable cache to avoid conflict with gradient checkpointing\n",
    "            )\n",
    "            \n",
    "            _loss = outputs.loss\n",
    "            print(\"_loss:\", _loss)\n",
    "            loss = _loss / grad_accumulation_steps\n",
    "            print(\"loss:\", loss)\n",
    "\n",
    "        log = {\n",
    "            'target': target,\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "        }\n",
    "\n",
    "        # Backward pass\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        log['loss'] = loss.item() * grad_accumulation_steps\n",
    "        if target == 'L2T2_lora':\n",
    "            L2T2_scaler.scale(loss).backward()\n",
    "        else:\n",
    "            nonlinear_fn_scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update parameters only at the end of gradient accumulation cycle\n",
    "        grad_norm_log = {}\n",
    "        print(\"TEST 0\")\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            print(\"TEST 1\")\n",
    "            if target == 'L2T2_lora':\n",
    "                print(\"TEST 2\")\n",
    "                # Unscale gradients before computing gradient norm and applying clipping\n",
    "                L2T2_scaler.unscale_(L2T2_optimizer)\n",
    "                \n",
    "                named_params = {n: p for n, p in model.named_parameters() if 'L2T2' in n}\n",
    "                compute_named_grad_norm(named_params)\n",
    "\n",
    "                # Compute gradient norm\n",
    "                grad_norm = compute_grad_norm(L2T2_lora_params)\n",
    "                grad_norm_log['grad_norm'] = grad_norm\n",
    "\n",
    "                # Clip gradients\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(L2T2_lora_params, clip_grad_norm)\n",
    "                \n",
    "                # Compute clipped gradient norm\n",
    "                grad_norm_clipped = compute_grad_norm(L2T2_lora_params)\n",
    "                grad_norm_log['grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "                # Update parameters\n",
    "                L2T2_scaler.step(L2T2_optimizer)\n",
    "                L2T2_scaler.update()\n",
    "                L2T2_scheduler.step()\n",
    "\n",
    "                # Zero gradients for the next gradient accumulation cycle\n",
    "                L2T2_optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                print(\"TEST 3\")\n",
    "                # Unscale gradients before computing gradient norm and applying clipping\n",
    "                nonlinear_fn_scaler.unscale_(nonlinear_fn_optimizer)\n",
    "                \n",
    "                # Compute gradient norm\n",
    "                grad_norm = compute_grad_norm(nonlinear_fn_params)\n",
    "                grad_norm_log['nonlinear_fn/grad_norm'] = grad_norm\n",
    "\n",
    "                # Clip gradients\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(nonlinear_fn_params, clip_grad_norm)\n",
    "                \n",
    "                # Compute clipped gradient norm\n",
    "                grad_norm_clipped = compute_grad_norm(nonlinear_fn_params)\n",
    "                grad_norm_log['nonlinear_fn/grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "                # Update parameters\n",
    "                nonlinear_fn_scaler.step(nonlinear_fn_optimizer)\n",
    "                nonlinear_fn_scaler.update()\n",
    "                nonlinear_fn_scheduler.step()\n",
    "\n",
    "                # Zero gradients for the next gradient accumulation cycle\n",
    "                nonlinear_fn_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # After updating parameters, toggle training target\n",
    "            target = 'nonlinear_fn' if target == 'L2T2_lora' else 'L2T2_lora'\n",
    "\n",
    "        # Logging\n",
    "        lr_log = {\n",
    "            'L1T2_lora/lr': L2T2_scheduler.get_last_lr()[0],\n",
    "            'nonlinear_fn/lr': nonlinear_fn_scheduler.get_last_lr()[0]\n",
    "        }\n",
    "        log = {\n",
    "            **log, \n",
    "            **lr_log, \n",
    "            **grad_norm_log,\n",
    "        }\n",
    "        log_history.append(log)\n",
    "        wandb.log(log)\n",
    "        print(\", \".join(\n",
    "            f\"{k}: {format_float(v)}\" if isinstance(v, float) else f\"{k}: {v}\"\n",
    "            for k, v in log.items()\n",
    "        ))\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step > 0 and global_step % checkpoint_steps == 0:\n",
    "            # Create checkpoint directory\n",
    "            checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save Nero parameters, along with optimizer, scheduler, and scaler states\n",
    "            # L1T2_nero_state_dict = {n: p.detach().cpu() for n, p in model.named_parameters() if 'L1T2' in n}\n",
    "            # save_file(L1T2_nero_state_dict, os.path.join(checkpoint_dir, 'L1T2_nero.safetensors'))\n",
    "            # torch.save(L1T2_optimizer.state_dict(), os.path.join(checkpoint_dir, 'L1T2_optimizer.pt'))\n",
    "            # torch.save(L1T2_scheduler.state_dict(), os.path.join(checkpoint_dir, 'L1T2_scheduler.pt'))\n",
    "            # torch.save(L1T2_scaler.state_dict(), os.path.join(checkpoint_dir, 'L1T2_scaler.pt'))\n",
    "\n",
    "            L2T2_nero_state_dict = {n: p.detach().cpu() for n, p in model.named_parameters() if 'L2T2' in n}\n",
    "            save_file(L2T2_nero_state_dict, os.path.join(checkpoint_dir, 'L2T2_nero.safetensors'))\n",
    "            torch.save(L2T2_optimizer.state_dict(), os.path.join(checkpoint_dir, 'L2T2_optimizer.pt'))\n",
    "            torch.save(L2T2_scheduler.state_dict(), os.path.join(checkpoint_dir, 'L2T2_scheduler.pt'))\n",
    "            torch.save(L2T2_scaler.state_dict(), os.path.join(checkpoint_dir, 'L2T2_scaler.pt'))\n",
    "\n",
    "            # Save trainer state for resuming training\n",
    "            trainer_state = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'log_history': log_history,\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'w') as f:\n",
    "                json.dump(trainer_state, f, indent=2)\n",
    "\n",
    "            # Save RNG state for reproducibility\n",
    "            rng_state = {\n",
    "                'python': random.getstate(),\n",
    "                'numpy': np.random.get_state(),\n",
    "                'cpu': torch.get_rng_state(),\n",
    "                'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else [],\n",
    "            }\n",
    "            torch.save(rng_state, os.path.join(checkpoint_dir, 'rng_state.pth'))\n",
    "\n",
    "            # Upload checkpoint directory to Hugging Face repository\n",
    "            if push_to_hf:\n",
    "                upload_folder(\n",
    "                    folder_path=checkpoint_dir,\n",
    "                    repo_id=hf_nero_id,\n",
    "                    path_in_repo=f\"checkpoint-{global_step}\",\n",
    "                    commit_message=f\"Add checkpoint at step {global_step}\",\n",
    "                    repo_type='model',\n",
    "                )\n",
    "        \n",
    "        # Check generated text every `generate_steps`\n",
    "        if global_step > 0 and global_step % generate_steps == 0:\n",
    "            model.set_train_L2T2_nero(False)\n",
    "            generated = generate_text(model, tokenizer, sample_prompt, device=device)\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT\")\n",
    "            print(\"================================\")\n",
    "            print(f\"{'Prompt':<9}:\", sample_prompt)\n",
    "            print(f\"{'Generated':<9}:\", generated)\n",
    "            print()\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
