{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from huggingface_hub import upload_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N(n):\n",
    "    if n >= 1_000_000_000:\n",
    "        return f\"{n/1_000_000_000:.1f}\".rstrip('0').rstrip('.') + 'B'\n",
    "    elif n >= 1_000_000:\n",
    "        return f\"{n/1_000_000:.1f}\".rstrip('0').rstrip('.') + 'M'\n",
    "    elif n >= 1_000:\n",
    "        return f\"{n/1_000:.1f}\".rstrip('0').rstrip('.') + 'K'\n",
    "    else:\n",
    "        return str(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Save directory name: wikipedia_ja_5K_1K_1K_64\n",
      "[CONFIG] Save directory: data/preprocessed/wikipedia_ja_5K_1K_1K_64\n"
     ]
    }
   ],
   "source": [
    "task = 'wikipedia'\n",
    "lang = 'ja'\n",
    "train_size = 5000\n",
    "val_size = 1000\n",
    "test_size = 1000\n",
    "max_seq_length = 64\n",
    "hf_data_id = 'alxxtexxr/Nero-XLT-Dataset'\n",
    "\n",
    "save_dir_name = '_'.join([\n",
    "    task, \n",
    "    lang, \n",
    "    N(train_size), \n",
    "    N(val_size), \n",
    "    N(test_size), \n",
    "    str(max_seq_length),\n",
    "])\n",
    "save_dir = Path('data/preprocessed') / save_dir_name\n",
    "print(\"[CONFIG] Save directory name:\", save_dir_name)\n",
    "print(\"[CONFIG] Save directory:\", save_dir)\n",
    "\n",
    "base_model_name = 'unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(\n",
    "    task: Literal['wikipedia', 'gsm8k'],\n",
    "    lang: str,\n",
    "    train_size: int = 5000,\n",
    "    val_size: int = 1000,\n",
    "    test_size: int = 1000,\n",
    "):\n",
    "    # Set up Hugging Face data configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming to avoid full download\n",
    "    dataset_stream = load_dataset(\n",
    "        data_id, \n",
    "        data_dir=data_dir, \n",
    "        split='train', \n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size + val_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    full_dataset = Dataset.from_list(sliced_data)\n",
    "\n",
    "    # Split into train, validation, and test datasets\n",
    "    train_dataset = full_dataset.select(range(train_size))\n",
    "    val_dataset = full_dataset.select(range(train_size, train_size + val_size))\n",
    "    test_dataset = full_dataset.select(range(train_size + val_size, total_size))\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "def _args(x=None):\n",
    "    return dict(\n",
    "        batched=True,\n",
    "        remove_columns=x,\n",
    "        num_proc=os.cpu_count(),\n",
    "    )\n",
    "\n",
    "def process_gsm8k_dataset(dataset, tokenizer, max_seq_length):\n",
    "    eos_token = tokenizer.eos_token\n",
    "\n",
    "    def format(example):\n",
    "        prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{q}\n",
    "\n",
    "### Answer: \n",
    "{a}\"\"\" + eos_token\n",
    "\n",
    "        return {'text': prompt.format(q=example['question'], a=example['answer'])}\n",
    "\n",
    "    def tokenize(example):\n",
    "        return tokenizer(\n",
    "            example['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_seq_length,\n",
    "        )\n",
    "\n",
    "    formatted = dataset.map(format, **_args(x=dataset.column_names))\n",
    "    tokenized = formatted.map(tokenize, **_args(x=formatted.column_names))\n",
    "    return tokenized\n",
    "\n",
    "def process_wikipedia_dataset(dataset, tokenizer, block_size):\n",
    "    def tokenize(example):\n",
    "        return tokenizer(example['text'])\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated = []\n",
    "        for input_ids in examples['input_ids']:\n",
    "            concatenated += input_ids\n",
    "\n",
    "        total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "        input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_mask = [[1] * block_size for _ in input_ids]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    tokenized = dataset.map(tokenize, **_args(x=dataset.column_names))\n",
    "    grouped = tokenized.map(group_texts, **_args(x=tokenized.column_names))\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bafcdadb5a04c519ca1aefac51c71fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(task, lang, train_size, test_size)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af7a33747b344b4823a5e419a37ea21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (141723 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b70293258bb479caa7f0ab1529217da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dca3ebc75a41bcabbdf496537d3381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341b18aafdc0469085c43900bd32364a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c27c25047044762b0ff80fbb399e792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f26001ce3334872ad0cf8ae1d54f10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if task == 'gsm8k':\n",
    "    train_dataset = process_gsm8k_dataset(datasets['train'], tokenizer, max_seq_length)\n",
    "    val_dataset = process_gsm8k_dataset(datasets['val'], tokenizer, max_seq_length)\n",
    "    test_dataset = process_gsm8k_dataset(datasets['test'], tokenizer, max_seq_length)\n",
    "else:\n",
    "    train_dataset = process_wikipedia_dataset(datasets['train'], tokenizer, max_seq_length)\n",
    "    val_dataset = process_wikipedia_dataset(datasets['val'], tokenizer, max_seq_length)\n",
    "    test_dataset = process_wikipedia_dataset(datasets['test'], tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ade06090064be694be65d2ecf4be3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/444 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0b2ef3b2ab4a39bca1ad8c7e930538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/72 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5339bef429ba4a69bcef989efdd6664f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/66 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 443134\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 71172\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 65808\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset.to_parquet(save_dir / 'train.parquet')\n",
    "val_dataset.to_parquet(save_dir / 'val.parquet')\n",
    "test_dataset.to_parquet(save_dir / 'test.parquet')\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb71184963174bdabc9d919efe5cecdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cd832832524b14a65fa90bd2e4d9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.parquet:   0%|          | 0.00/48.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2a5f93830c406e9591f61f3343ed40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.parquet:   0%|          | 0.00/7.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c893fe04f6a4b169917a9eda2bb2233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.parquet:   0%|          | 0.00/7.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/alxxtexxr/Nero-XLT-Dataset/commit/e3cf25d87a652f29f7d52f9924de906ef5282089', commit_message='Upload preprocessed wikipedia_ja_5K_1K_1K_64 dataset folder', commit_description='', oid='e3cf25d87a652f29f7d52f9924de906ef5282089', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/alxxtexxr/Nero-XLT-Dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='alxxtexxr/Nero-XLT-Dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_folder(\n",
    "    repo_id=hf_data_id,\n",
    "    repo_type='dataset',\n",
    "    folder_path=save_dir,\n",
    "    path_in_repo=save_dir_name,\n",
    "    commit_message=f\"Upload preprocessed {save_dir_name} dataset folder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check\n",
    "# from datasets import load_dataset\n",
    "# load_dataset(hf_data_id, data_dir=save_dir_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
