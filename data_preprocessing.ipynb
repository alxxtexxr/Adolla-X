{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from huggingface_hub import upload_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N(n):\n",
    "    if n >= 1_000_000_000:\n",
    "        return f\"{n/1_000_000_000:.1f}\".rstrip('0').rstrip('.') + 'B'\n",
    "    elif n >= 1_000_000:\n",
    "        return f\"{n/1_000_000:.1f}\".rstrip('0').rstrip('.') + 'M'\n",
    "    elif n >= 1_000:\n",
    "        return f\"{n/1_000:.1f}\".rstrip('0').rstrip('.') + 'K'\n",
    "    else:\n",
    "        return str(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Save directory name: gsm8k_en_5K_1K_1K_1024\n",
      "[CONFIG] Save directory: data/preprocessed/gsm8k_en_5K_1K_1K_1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "task = 'gsm8k'\n",
    "lang = 'en'\n",
    "train_size = 5000\n",
    "val_size = 1000\n",
    "test_size = 1000\n",
    "max_seq_length = 1024\n",
    "hf_data_id = 'alxxtexxr/Nero-XLT-Dataset'\n",
    "\n",
    "save_dir_name = '_'.join([\n",
    "    task, \n",
    "    lang, \n",
    "    N(train_size), \n",
    "    N(val_size), \n",
    "    N(test_size), \n",
    "    str(max_seq_length),\n",
    "])\n",
    "save_dir = Path('data/preprocessed') / save_dir_name\n",
    "print(\"[CONFIG] Save directory name:\", save_dir_name)\n",
    "print(\"[CONFIG] Save directory:\", save_dir)\n",
    "\n",
    "base_model_name = 'unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(\n",
    "    task: Literal['wikipedia', 'gsm8k'],\n",
    "    lang: str,\n",
    "    train_size: int = 5000,\n",
    "    val_size: int = 1000,\n",
    "    test_size: int = 1000,\n",
    "):\n",
    "    # Set up Hugging Face data configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming to avoid full download\n",
    "    dataset_stream = load_dataset(\n",
    "        data_id, \n",
    "        data_dir=data_dir, \n",
    "        split='train', \n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size + val_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    full_dataset = Dataset.from_list(sliced_data)\n",
    "\n",
    "    # Split into train, validation, and test datasets\n",
    "    train_dataset = full_dataset.select(range(train_size))\n",
    "    val_dataset = full_dataset.select(range(train_size, train_size + val_size))\n",
    "    test_dataset = full_dataset.select(range(train_size + val_size, total_size))\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "def _args(x=None):\n",
    "    return dict(\n",
    "        batched=True,\n",
    "        remove_columns=x,\n",
    "        num_proc=os.cpu_count(),\n",
    "    )\n",
    "\n",
    "def process_gsm8k_dataset(dataset, tokenizer, max_seq_length):\n",
    "    eos_token = tokenizer.eos_token\n",
    "\n",
    "    def format(examples):\n",
    "        prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{q}\n",
    "\n",
    "### Answer: \n",
    "{a}\"\"\" + eos_token\n",
    "        \n",
    "        return {'text': [prompt.format(q=q, a=a) for q, a in zip(examples['question'], examples['answer'])]}\n",
    "\n",
    "    def tokenize(example):\n",
    "        return tokenizer(\n",
    "            example['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_seq_length,\n",
    "        )\n",
    "\n",
    "    formatted = dataset.map(format, **_args(x=dataset.column_names))\n",
    "    tokenized = formatted.map(tokenize, **_args(x=formatted.column_names))\n",
    "    return tokenized\n",
    "\n",
    "def process_wikipedia_dataset(dataset, tokenizer, block_size):\n",
    "    def tokenize(example):\n",
    "        return tokenizer(example['text'])\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated = []\n",
    "        for input_ids in examples['input_ids']:\n",
    "            concatenated += input_ids\n",
    "\n",
    "        total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "        input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_mask = [[1] * block_size for _ in input_ids]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "    tokenized = dataset.map(tokenize, **_args(x=dataset.column_names))\n",
    "    grouped = tokenized.map(group_texts, **_args(x=tokenized.column_names))\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(task, lang, train_size, test_size)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9ed329e27647bf961e31e99fd8bfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f8b3cc28314ffd96a2bfc6ed256df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0d880d9c884977b1f6db625541ce62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7664150a04494585839091cb26a028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b55b66cfe4384809cb77d0c9d5356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e893ee5466d4748b02a551672e91c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if task == 'gsm8k':\n",
    "    train_dataset = process_gsm8k_dataset(datasets['train'], tokenizer, max_seq_length)\n",
    "    val_dataset = process_gsm8k_dataset(datasets['val'], tokenizer, max_seq_length)\n",
    "    test_dataset = process_gsm8k_dataset(datasets['test'], tokenizer, max_seq_length)\n",
    "else:\n",
    "    train_dataset = process_wikipedia_dataset(datasets['train'], tokenizer, max_seq_length)\n",
    "    val_dataset = process_wikipedia_dataset(datasets['val'], tokenizer, max_seq_length)\n",
    "    test_dataset = process_wikipedia_dataset(datasets['test'], tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "import numpy as np\n",
    "max([len(np.array(d)[np.array(d) != 128004]) for d in train_dataset['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69cb4ca8c124916b008f91d4e2571bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da39de3cc33c495a9d1f2edf63ca5528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ea7965931441c29e2ac8c6317bc875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset.to_parquet(save_dir / 'train.parquet')\n",
    "val_dataset.to_parquet(save_dir / 'val.parquet')\n",
    "test_dataset.to_parquet(save_dir / 'test.parquet')\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a53da472d2643758d16a15d27606c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.parquet:   0%|          | 0.00/486k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20973877f3c4d72a7fedca67c1ebd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70ef61450bf40e3b46109db42d2672b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.parquet:   0%|          | 0.00/97.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a93c1bbacfd4d478c08e502ddc2fbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.parquet:   0%|          | 0.00/98.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/alxxtexxr/Nero-XLT-Dataset/commit/aad532f04cb03f336f1009a7968f6a46b2c5e383', commit_message='Upload preprocessed gsm8k_en_5K_1K_1K_64 dataset folder', commit_description='', oid='aad532f04cb03f336f1009a7968f6a46b2c5e383', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/alxxtexxr/Nero-XLT-Dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='alxxtexxr/Nero-XLT-Dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_folder(\n",
    "    repo_id=hf_data_id,\n",
    "    repo_type='dataset',\n",
    "    folder_path=save_dir,\n",
    "    path_in_repo=save_dir_name,\n",
    "    commit_message=f\"Upload preprocessed {save_dir_name} dataset folder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check\n",
    "# from datasets import load_dataset\n",
    "# load_dataset(hf_data_id, data_dir=save_dir_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
