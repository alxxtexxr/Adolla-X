{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from typing import Optional\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download, create_repo, upload_folder\n",
    "from safetensors.torch import load_file, save_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(\n",
    "        repo_id: str, \n",
    "        checkpoint: Optional[int], \n",
    "        max_checkpoints: str = 10_000, \n",
    "        checkpoint_steps: str = 25,\n",
    "    ):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_steps) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dir = os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir, checkpoint_dir\n",
    "\n",
    "def check_loss_and_grad_norm(model, tokenizer, prompt=\"Paris is the capital of\"):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    print(\"Loss:\", outputs.loss)\n",
    "\n",
    "    # Backward pass\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "\n",
    "def check_parameter(n, p):\n",
    "    print(f\"- {'name':<8}:\", n)\n",
    "    print(f\"- {'device':<8}:\", p.device)\n",
    "    print(f\"- {'dtype':<8}:\", p.dtype)\n",
    "    print(f\"- {'mean':<8}:\", p.mean().item())\n",
    "    print(f\"- {'min':<8}:\", p.min().item())\n",
    "    print(f\"- {'max':<8}:\", p.max().item())\n",
    "\n",
    "def check_lora_parameters(model, mode=None):\n",
    "    prefix = mode + '_lora' if mode != None else 'lora'\n",
    "    for n, p in model.named_parameters():\n",
    "        if prefix in n:\n",
    "            check_parameter(n, p)\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=32, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def get_task_and_lang_from_repo_id(repo_id: str):\n",
    "    task, lang, _ = repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "    return task, lang\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id: str,\n",
    "    train_size: int = 5000,\n",
    "    test_size: int = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang = get_task_and_lang_from_repo_id(lora_repo_id)\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    \n",
    "    # Load dataset using streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_grad_norm(params):\n",
    "    grad_norm = 0.0\n",
    "    for p in params:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd1222b92a14e6cb87baa89b7dafffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8431eadeac4ca5b75e1c36781f89d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "- source:\n",
      "  - label     : L1T1\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650/adapter_model.safetensors\n",
      "- target:\n",
      "  - label     : L2T1\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650/adapter_model.safetensors\n",
      "\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "device = 'auto'\n",
    "\n",
    "# Data configuration\n",
    "hf_data_id = 'alxxtexxr/Nero-XLT-Dataset'\n",
    "hf_data_dir = 'wikipedia_ja_5K_1K_1K_64'\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 3.0\n",
    "lr = 2e-4\n",
    "warmup_ratio = 0.01\n",
    "# num_warmup_steps = 100\n",
    "checkpoint_steps = 50\n",
    "generate_steps = 50\n",
    "sample_prompt = '日本は'\n",
    "\n",
    "# Model configurations\n",
    "model_configs = {\n",
    "    # L1T1 (Source Language - Source Task)\n",
    "    'source': {\n",
    "        'label': 'L1T1',\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L2T1 (Target Language - Source Task)\n",
    "    'target': {\n",
    "        'label': 'L2T1',\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L1T2 (Source Language - Target Task)\n",
    "    # 'target': {\n",
    "    #     'label': 'L1T2',\n",
    "    #     'hf_lora_id': 'alxxtexxr/L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457',\n",
    "    #     'checkpoint': 1875,\n",
    "    # },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    _, lora_dir = download_hf_model(config['hf_lora_id'], config['checkpoint'])\n",
    "    model_configs[key]['lora_dir'] = lora_dir\n",
    "    model_configs[key]['lora_path'] = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "    model_configs[key]['lora_config'] = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "for key, config in model_configs.items():\n",
    "    print(f\"- {key}:\")\n",
    "    for config_name, config_value in config.items():\n",
    "        if config_name == 'lora_config':\n",
    "            continue\n",
    "        print(f\"{'-':>3} {config_name:<10}: {config_value}\")\n",
    "print()\n",
    "\n",
    "assert model_configs['source']['lora_config'].base_model_name_or_path == model_configs['target']['lora_config'].base_model_name_or_path, \"Base models must be the same\"\n",
    "base_model_name = model_configs['source']['lora_config'].base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating Nero directory: L3.1-8B-wikipedia-ja-5K-1K-1K-64K-Nero-v20250807184323\n",
      "[INFO] Nero directory created!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face configuration\n",
    "hf_nero_id = None\n",
    "resume_step = 0\n",
    "\n",
    "if hf_nero_id is not None and resume_step > 0:\n",
    "    print(f\"[INFO] Downloading Nero checkpoint at step {resume_step} from Hugging Face repository:\", hf_nero_id)\n",
    "    nero_dir, _ = download_hf_model(hf_nero_id, resume_step)\n",
    "    print(f\"[INFO] Nero checkpoint downloaded successfully!\")\n",
    "else:\n",
    "    hf_username = 'alxxtexxr'\n",
    "    nero_dir = f'L3.1-8B-{hf_data_dir.replace(\"_\", \"-\")}K-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    print(f\"[INFO] Creating Nero directory:\", nero_dir)\n",
    "    hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "    os.makedirs(nero_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Nero directory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, mode,\n",
    "                 \n",
    "                 # Teacher LoRA parameters\n",
    "                 teacher_rank, teacher_alpha, teacher_dropout, teacher_bias, teacher_use_rslora,\n",
    "\n",
    "                 # Student LoRA parameters\n",
    "                 student_rank, student_alpha, student_dropout, student_bias, student_use_rslora,\n",
    "                 \n",
    "                 return_hidden_output=False,\n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.mode = mode\n",
    "\n",
    "        self.return_hidden_output = return_hidden_output\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "\n",
    "        # ================================================================================================================================\n",
    "        # Teacher LoRA\n",
    "        # ================================================================================================================================\n",
    "        self.teacher_alpha = teacher_alpha\n",
    "        self.teacher_bias = teacher_bias\n",
    "        self.teacher_scaling = teacher_alpha / math.sqrt(teacher_rank) if teacher_use_rslora else teacher_alpha / teacher_rank\n",
    "        self.teacher_dropout = nn.Dropout(teacher_dropout) if teacher_dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.teacher_lora_A = nn.Linear(in_features, teacher_rank, bias=teacher_bias).to(self.device)  # Projects down\n",
    "        self.teacher_lora_B = nn.Linear(teacher_rank, out_features, bias=teacher_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        teacher_std = 1 / torch.sqrt(torch.tensor(teacher_rank).float())\n",
    "        nn.init.normal_(self.teacher_lora_A.weight, mean=0.0, std=teacher_std)\n",
    "        nn.init.zeros_(self.teacher_lora_B.weight) \n",
    "        \n",
    "        # ================================================================================================================================\n",
    "        # Student LoRA\n",
    "        # ================================================================================================================================\n",
    "        self.student_alpha = student_alpha\n",
    "        self.student_bias = student_bias\n",
    "        self.student_scaling = student_alpha / math.sqrt(student_rank) if student_use_rslora else student_alpha / student_rank\n",
    "        self.student_dropout = nn.Dropout(student_dropout) if student_dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.student_lora_A = nn.Linear(in_features, student_rank, bias=student_bias).to(self.device)  # Projects down\n",
    "        self.student_lora_B = nn.Linear(student_rank, out_features, bias=student_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        student_std = 1 / torch.sqrt(torch.tensor(student_rank).float())\n",
    "        nn.init.normal_(self.student_lora_A.weight, mean=0.0, std=student_std)\n",
    "        nn.init.zeros_(self.student_lora_B.weight) \n",
    "        \n",
    "        # ================================================================================================================================\n",
    "        # Student Nero\n",
    "        # ================================================================================================================================\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.student_nero_A = nn.Linear(out_features, student_rank, bias=student_bias).to(self.device)\n",
    "        self.student_nero_B = nn.Linear(student_rank, out_features, bias=student_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.student_nero_A.weight, mean=0.0, std=student_std)\n",
    "        nn.init.zeros_(self.student_nero_B.weight) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"================================================================\")\n",
    "        # print(self.module_name)\n",
    "        # print(\"================================================================\")\n",
    "\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.mode == 'teacher':\n",
    "            requires_conversion = not torch.is_autocast_enabled()\n",
    "            if requires_conversion:\n",
    "                x = x.to(self.teacher_lora_A.weight.dtype)\n",
    "                \n",
    "            # lora_out = self.teacher_lora_B(self.teacher_lora_A(self.teacher_dropout(x))) * self.teacher_scaling\n",
    "            dropout_out = self.teacher_dropout(x)\n",
    "            lora_A_out = self.teacher_lora_A(dropout_out)\n",
    "            lora_B_out = self.teacher_lora_B(lora_A_out)\n",
    "            lora_out = lora_B_out * self.teacher_scaling\n",
    "            if requires_conversion:\n",
    "                lora_out = lora_out.to(base_out.dtype)\n",
    "                \n",
    "            output = base_out + lora_out\n",
    "\n",
    "            if self.return_hidden_output:\n",
    "                return output, lora_out\n",
    "        else:\n",
    "            requires_conversion = not torch.is_autocast_enabled()\n",
    "            if requires_conversion:\n",
    "                x = x.to(self.student_lora_A.weight.dtype)\n",
    "\n",
    "            # lora_out = self.student_lora_B(self.student_lora_A(self.student_dropout(x))) # * self.scaling\n",
    "            dropout_out = self.student_dropout(x)\n",
    "            lora_A_out = self.student_lora_A(dropout_out)\n",
    "            lora_B_out = self.student_lora_B(lora_A_out)\n",
    "            # lora_out = lora_B_out * self.student_scaling\n",
    "            lora_out = lora_B_out\n",
    "            # if requires_conversion:\n",
    "            #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "            # nero_out = self.student_nero_B(F.relu(self.student_nero_A(lora_out)))\n",
    "            nero_A_out = self.student_nero_A(lora_out)\n",
    "            relu_out = F.relu(nero_A_out)\n",
    "            nero_B_out = self.student_nero_B(relu_out)\n",
    "            nero_out = nero_B_out * self.student_scaling\n",
    "            if requires_conversion:\n",
    "                nero_out = nero_out.to(base_out.dtype)\n",
    "\n",
    "            output = base_out + nero_out\n",
    "\n",
    "            if self.return_hidden_output:\n",
    "                return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_teacher_lora_params(self, state_dict, prefix):\n",
    "        self.teacher_lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.teacher_lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.teacher_bias:\n",
    "            self.teacher_lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.teacher_lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "\n",
    "    def load_student_lora_params(self, state_dict, prefix):\n",
    "        self.student_lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.student_lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.student_bias:\n",
    "            self.student_lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.student_lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "    def load_student_nero_params(self, state_dict, prefix):\n",
    "        self.student_nero_A.weight.data = state_dict[f'{prefix}.student_nero_A.weight'].to(self.device)\n",
    "        self.student_nero_B.weight.data = state_dict[f'{prefix}.student_nero_B.weight'].to(self.device)\n",
    "        if self.student_bias:\n",
    "            self.student_nero_A.bias.data = state_dict[f'{prefix}.student_nero_A.bias'].to(self.device)\n",
    "            self.student_nero_B.bias.data = state_dict[f'{prefix}.student_nero_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 base_model: nn.Module, \n",
    "                 teacher_lora_config: LoraConfig, \n",
    "                 student_lora_config: LoraConfig, \n",
    "                 mode='student',\n",
    "                 return_hidden_outputs: bool=False,\n",
    "                 debug: bool=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.mode = mode\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_hidden_outputs = return_hidden_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(teacher_lora_config, student_lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, teacher_lora_config, student_lora_config):\n",
    "        assert teacher_lora_config.target_modules == student_lora_config.target_modules, \"[ERROR] Teacher and student LoRA configurations must have the same target modules.\"\n",
    "\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in student_lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    base_layer=module,\n",
    "                    mode=self.mode,\n",
    "\n",
    "                    # Teacher LoRA parameters\n",
    "                    teacher_rank=teacher_lora_config.r,\n",
    "                    teacher_alpha=teacher_lora_config.lora_alpha,\n",
    "                    teacher_dropout=teacher_lora_config.lora_dropout,\n",
    "                    teacher_bias=teacher_lora_config.lora_bias,\n",
    "                    teacher_use_rslora=teacher_lora_config.use_rslora,\n",
    "\n",
    "                    # Student LoRA parameters\n",
    "                    student_rank=student_lora_config.r,\n",
    "                    student_alpha=student_lora_config.lora_alpha,\n",
    "                    student_dropout=student_lora_config.lora_dropout,\n",
    "                    student_bias=student_lora_config.lora_bias,\n",
    "                    student_use_rslora=student_lora_config.use_rslora,\n",
    "\n",
    "                    return_hidden_output=self.return_hidden_outputs,\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_mode(self, mode: str, verbose: bool=False):\n",
    "        if mode not in ['teacher', 'student']:\n",
    "            raise ValueError(f\"[ERROR] Invalid mode: {mode}. Must be either 'teacher' or 'student'.\")\n",
    "        self.mode = mode\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.mode = mode\n",
    "        \n",
    "        if mode == 'teacher':\n",
    "            self.freeze_all()\n",
    "        else:\n",
    "            self.freeze_all_except_nero()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Model mode set to '{mode}'.\")\n",
    "    \n",
    "    def set_return_hidden_outputs(self, return_hidden_outputs: bool, verbose: bool=False):\n",
    "        self.return_hidden_outputs = return_hidden_outputs\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_hidden_output = return_hidden_outputs\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Return hidden outputs set to {return_hidden_outputs}.\")\n",
    "    \n",
    "    def freeze_all(self, verbose=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen!\")\n",
    "\n",
    "    def freeze_all_except_nero(self, verbose=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen except Nero layers!\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        print(\"[INFO] All layers are unfrozen!\")\n",
    "    \n",
    "    def load_teacher_lora_params(self, teacher_lora_path):\n",
    "        if not os.path.exists(teacher_lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] Teacher LoRA file not found:\", teacher_lora_path)\n",
    "        \n",
    "        if teacher_lora_path.endswith('.bin'):\n",
    "            state_dict = torch.load(teacher_lora_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(teacher_lora_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_teacher_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] Teacher LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def load_student_lora_params(self, student_lora_path):\n",
    "        if not os.path.exists(student_lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] Student LoRA file not found:\", student_lora_path)\n",
    "        \n",
    "        if student_lora_path.endswith('.bin'):\n",
    "            state_dict = torch.load(student_lora_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(student_lora_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_student_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] Student LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def load_student_nero_params(self, student_nero_path):\n",
    "        if not os.path.exists(student_nero_path):\n",
    "            raise FileNotFoundError(\"[ERROR] student_Nero file not found:\", student_nero_path)\n",
    "        \n",
    "        if student_nero_path.endswith('.bin'):\n",
    "            state_dict = torch.load(student_nero_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(student_nero_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.student_nero_A.weight' in state_dict and f'{nero_layer_name}.student_nero_B.weight' in state_dict:\n",
    "                nero_layer.load_student_nero_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] Student Nero parameters loaded successfully!\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self.return_hidden_outputs:\n",
    "            hidden_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, hidden_out = _out\n",
    "                    hidden_outs[layer_name] = hidden_out # Store hidden_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract hidden_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(*args, **kwargs)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, hidden_outs\n",
    "        \n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    device_map=device,\n",
    ")\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    teacher_lora_config=model_configs['target']['lora_config'], \n",
    "    student_lora_config=model_configs['source']['lora_config'], \n",
    "    mode='student',\n",
    "    return_hidden_outputs=False,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nero_model.set_mode('teacher')\n",
    "\n",
    "# for param in nero_model.base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "        \n",
    "# for nero_layer in nero_model.nero_layers.values():\n",
    "#     for param_name, param in nero_layer.named_parameters():\n",
    "#         if 'teacher_lora_A' in param_name or 'teacher_lora_B' in param_name:\n",
    "#             param.requires_grad = True\n",
    "#         else:\n",
    "#             param.requires_grad = False\n",
    "\n",
    "# check_loss_and_grad_norm(nero_model, tokenizer, prompt=\"Despite the ominous clouds gathering on the horizon and the distant rumble of thunder echoing through the valley, the determined hikers,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded teacher LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.teacher_lora_A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : -0.001495478441938758\n",
      "- min     : -1.6334571838378906\n",
      "- max     : 1.582234501838684\n",
      "\n",
      "[INFO] Teacher LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded teacher LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.teacher_lora_A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 2.2152138626552187e-05\n",
      "- min     : -0.06327299773693085\n",
      "- max     : 0.0625513345003128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded teacher LoRA parameters:\")\n",
    "check_lora_parameters(nero_model, mode='teacher')\n",
    "print()\n",
    "\n",
    "nero_model.load_teacher_lora_params(model_configs['target']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded teacher LoRA parameters:\")\n",
    "check_lora_parameters(nero_model, mode='teacher')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded student LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.student_lora_A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 0.000745462893974036\n",
      "- min     : -1.3428223133087158\n",
      "- max     : 1.3509130477905273\n",
      "\n",
      "[INFO] Student LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded student LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.student_lora_A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 6.287686119321734e-05\n",
      "- min     : -0.04176201671361923\n",
      "- max     : 0.04242725297808647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded student LoRA parameters:\")\n",
    "check_lora_parameters(nero_model, mode='student')\n",
    "print()\n",
    "\n",
    "nero_model.load_student_lora_params(model_configs['source']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded student LoRA parameters:\")\n",
    "check_lora_parameters(nero_model, mode='student')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nero_model.freeze_all_except_nero()\n",
    "print()\n",
    "\n",
    "# check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gradient checkpointing enabled!\n",
      "\n",
      "Loss: tensor(3.4235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Gradient norm: 9.222118304758878\n"
     ]
    }
   ],
   "source": [
    "nero_model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"[INFO] Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 443134\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 71172\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 65808\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(hf_data_id, data_dir=hf_data_dir)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total batches: 110784\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"[INFO] Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask):\n",
      "(torch.Size([4, 64]), torch.Size([4, 64]))\n",
      "\n",
      "First batch text:\n",
      "18030\n",
      " \n",
      " 規格名は。GBKをさらに拡張し、少数民族言語の文字なども含む大規模な文字セットで、GBKに取って代わる正式な国家規格。2000年3月17日に国家質量技術監督局（当 ...\n",
      "\n",
      "Loss: tensor(3.4235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Gradient norm: 9.222118304758878\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn_v4(nero_outs, lora_outs, nero_logits, lora_logits, \n",
    "#                alpha=1.0, beta=0.00015, temperature=2.0):\n",
    "#     assert nero_outs.keys() == lora_outs.keys(), \"`nero_outs` and `lora_outs` must have the same layers.\"\n",
    "#     loss_device = next(iter(nero_outs.values())).device\n",
    "#     total_hidden_loss = torch.tensor(0.0, device=loss_device)\n",
    "\n",
    "#     # --- Hidden representation loss ---\n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name].to(nero_out.device)\n",
    "\n",
    "#         # Scale by sequence length to avoid length-sensitive loss scaling\n",
    "#         hidden_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()), dim=-1).mean()\n",
    "#         total_hidden_loss += hidden_loss\n",
    "\n",
    "#     total_hidden_loss /= len(lora_outs)\n",
    "#     print(\"total_hidden_loss:\", total_hidden_loss)\n",
    "#     total_hidden_loss *= alpha\n",
    "\n",
    "#     # --- Logit KL divergence loss ---\n",
    "#     # Important: apply softmax with temperature for distillation\n",
    "#     logit_loss = F.kl_div(\n",
    "#         F.log_softmax(nero_logits.to(loss_device) / temperature, dim=-1),\n",
    "#         F.softmax(lora_logits.to(loss_device) / temperature, dim=-1),\n",
    "#         reduction='batchmean'\n",
    "#     ) * (temperature ** 2)\n",
    "#     print(\"logit_loss:\", logit_loss)\n",
    "#     logit_loss *= beta\n",
    "\n",
    "#     # --- Final combined loss ---\n",
    "#     total_loss = total_hidden_loss + logit_loss\n",
    "#     return total_loss, total_hidden_loss, logit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.606545329093933\n"
     ]
    }
   ],
   "source": [
    "# def loss_fn_v5(nero_model_outs, lora_model_outs, attention_mask, alpha=0.5, temperature=2.0):\n",
    "#     lm_loss = nero_model_outs.loss\n",
    "#     nero_logits = nero_model_outs.logits  # (batch_size, seq_len, vocab_size)\n",
    "#     lora_logits = lora_model_outs.logits  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "#     # Compute softened distributions\n",
    "#     student_log_probs = F.log_softmax(nero_logits / temperature, dim=-1)\n",
    "#     teacher_probs = F.softmax(lora_logits / temperature, dim=-1)\n",
    "\n",
    "#     # Flatten for masking\n",
    "#     student_log_probs = student_log_probs.view(-1, student_log_probs.size(-1))\n",
    "#     teacher_probs = teacher_probs.view(-1, teacher_probs.size(-1))\n",
    "#     flat_mask = attention_mask.view(-1).unsqueeze(-1).to(dtype=student_log_probs.dtype)\n",
    "\n",
    "#     # Element-wise KL divergence per token\n",
    "#     kl_div = F.kl_div(student_log_probs, teacher_probs, reduction='none').sum(-1)  # (batch * seq)\n",
    "\n",
    "#     # Apply mask and normalize\n",
    "#     masked_kl = (kl_div * flat_mask.squeeze(-1)).sum() / flat_mask.sum()\n",
    "#     logit_loss = masked_kl * (temperature ** 2)\n",
    "\n",
    "#     return (lm_loss * alpha) + (logit_loss * (1 - alpha))\n",
    "\n",
    "def loss_fn_v5(nero_model_outs, lora_model_outs, attention_mask, alpha=0.5, temperature=2.0):\n",
    "    lm_loss = nero_model_outs.loss\n",
    "    nero_logits = nero_model_outs.logits  # (batch_size, seq_len, vocab_size)\n",
    "    lora_logits = lora_model_outs.logits  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    # Compute softened distributions (no clamping)\n",
    "    student_log_probs = F.log_softmax(nero_logits / temperature, dim=-1)\n",
    "    teacher_probs = F.softmax(lora_logits / temperature, dim=-1)\n",
    "\n",
    "    # Flatten logits and attention mask\n",
    "    student_log_probs = student_log_probs.view(-1, student_log_probs.size(-1))\n",
    "    teacher_probs = teacher_probs.view(-1, teacher_probs.size(-1))\n",
    "    flat_mask = attention_mask.view(-1).unsqueeze(-1).to(dtype=student_log_probs.dtype)\n",
    "\n",
    "    # Compute KL divergence for each token (elementwise)\n",
    "    kl_div = F.kl_div(student_log_probs, teacher_probs, reduction='none').sum(-1)  # shape: (batch * seq_len)\n",
    "\n",
    "    # Mask padding positions and normalize\n",
    "    masked_kl = (kl_div * flat_mask.squeeze(-1)).sum() / (flat_mask.sum() + 1e-8)\n",
    "\n",
    "    # Temperature scaling\n",
    "    logit_loss = masked_kl * (temperature ** 2)\n",
    "\n",
    "    # Final loss: blend LM loss and KD loss\n",
    "    total_loss = (lm_loss * alpha) + (logit_loss * (1 - alpha))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# Sanity check\n",
    "first_batch_text = \"Roy Marshall (1930–1992) was a Barbadian cricketer who played in four Test matches for the West Indies and had an extensive domestic career with Hampshire in\"\n",
    "device = next(iter(nero_model.parameters())).device\n",
    "inputs = tokenizer(first_batch_text, return_tensors='pt')\n",
    "\n",
    "nero_model.set_mode('teacher')\n",
    "nero_model.eval()\n",
    "teacher_outs = nero_model(\n",
    "    input_ids=inputs['input_ids'].to(device), \n",
    "    attention_mask=inputs['attention_mask'].to(device),\n",
    ")\n",
    "\n",
    "nero_model.set_mode('student')\n",
    "nero_model.train()\n",
    "student_outs = nero_model(\n",
    "    input_ids=inputs['input_ids'].to(device), \n",
    "    attention_mask=inputs['attention_mask'].to(device),\n",
    "    labels=inputs['input_ids'].to(device), \n",
    ")\n",
    "\n",
    "loss = loss_fn_v5(student_outs, teacher_outs, inputs['attention_mask'].to(device))\n",
    "print(\"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_v6(\n",
    "      nero_model_outs, \n",
    "      lora_model_outs, \n",
    "      nero_hidden_outs,\n",
    "      lora_hidden_outs,\n",
    "      attention_mask, \n",
    "      alpha=0.33, \n",
    "      beta=1.0, \n",
    "      gamma=1.0,\n",
    "      temperature=2.0,\n",
    "    ):\n",
    "    # 1. LM loss\n",
    "    lm_loss = nero_model_outs.loss\n",
    "    device = lm_loss.device\n",
    "    \n",
    "    # 2. Logit distillation loss (KL divergence)\n",
    "    nero_logits = nero_model_outs.logits\n",
    "    lora_logits = lora_model_outs.logits\n",
    "    student_log_probs = F.log_softmax(nero_logits / temperature, dim=-1)\n",
    "    teacher_probs = F.softmax(lora_logits / temperature, dim=-1)\n",
    "    student_log_probs = student_log_probs.view(-1, student_log_probs.size(-1))\n",
    "    teacher_probs = teacher_probs.view(-1, teacher_probs.size(-1))\n",
    "    flat_mask = attention_mask.view(-1).unsqueeze(-1).to(dtype=student_log_probs.dtype)\n",
    "    kl_div = F.kl_div(student_log_probs, teacher_probs, reduction='none').sum(-1)\n",
    "    logit_loss = (kl_div * flat_mask.squeeze(-1)).sum() / (flat_mask.sum() + 1e-8)\n",
    "    logit_loss = logit_loss * (temperature ** 2)\n",
    "\n",
    "    # 3. Representation distillation loss (MSE between last hidden states)\n",
    "    # student_hidden = nero_model_outs.hidden_states[-1]\n",
    "    # teacher_hidden = lora_model_outs.hidden_states[-1]\n",
    "    # mask = attention_mask.unsqueeze(-1).to(dtype=student_hidden.dtype)\n",
    "    # mse_loss = F.mse_loss(student_hidden * mask, teacher_hidden * mask, reduction='sum')\n",
    "    # rep_loss = mse_loss / (mask.sum() + 1e-8)\n",
    "\n",
    "    total_hidden_loss = torch.tensor(0.0, device=device)\n",
    "    for layer_name in lora_hidden_outs.keys():\n",
    "        nero_out = nero_hidden_outs[layer_name]\n",
    "        lora_out = lora_hidden_outs[layer_name].to(nero_out.device)\n",
    "\n",
    "        # MAE Loss\n",
    "        # hidden_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()), dim=-1).mean()\n",
    "        \n",
    "        # MSE Loss\n",
    "        mse_loss = F.mse_loss(nero_out.float(), lora_out.float(), reduction='sum')\n",
    "        hidden_loss = mse_loss / torch.sum(lora_out ** 2)\n",
    "\n",
    "        total_hidden_loss += hidden_loss\n",
    "    total_hidden_loss /= len(lora_hidden_outs)\n",
    "\n",
    "    # Combine all losses\n",
    "    # total_loss = (lm_loss * alpha) + (logit_loss * (1 - alpha)) + (total_hidden_loss * beta)\n",
    "    total_loss = (alpha * lm_loss) + (beta * logit_loss) + (gamma * total_hidden_loss)\n",
    "    return total_loss, lm_loss, logit_loss, total_hidden_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: (tensor(2.7843, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.1330, device='cuda:0', grad_fn=<NllLossBackward0>), tensor(1.0801, device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>), tensor(1.0003, device='cuda:0', grad_fn=<DivBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch_text = \"Roy Marshall (1930–1992) was a Barbadian cricketer who played in four Test matches for the West Indies and had an extensive domestic career with Hampshire in\"\n",
    "device = next(iter(nero_model.parameters())).device\n",
    "inputs = tokenizer(first_batch_text, return_tensors='pt')\n",
    "\n",
    "nero_model.set_return_hidden_outputs(True)\n",
    "\n",
    "nero_model.set_mode('teacher')\n",
    "nero_model.eval()\n",
    "teacher_outs, teacher_hidden_outs = nero_model(\n",
    "    input_ids=inputs['input_ids'].to(device), \n",
    "    attention_mask=inputs['attention_mask'].to(device),\n",
    ")\n",
    "\n",
    "nero_model.set_mode('student')\n",
    "nero_model.train()\n",
    "student_outs, student_hidden_outs = nero_model(\n",
    "    input_ids=inputs['input_ids'].to(device), \n",
    "    attention_mask=inputs['attention_mask'].to(device),\n",
    "    labels=inputs['input_ids'].to(device), \n",
    ")\n",
    "\n",
    "loss = loss_fn_v6(\n",
    "    student_outs, \n",
    "    teacher_outs, \n",
    "    student_hidden_outs,\n",
    "    teacher_hidden_outs,\n",
    "    inputs['attention_mask'].to(device),\n",
    ")\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1479854413.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250807_184359-uyxj9rjh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/uyxj9rjh' target=\"_blank\">visionary-frost-139</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/uyxj9rjh' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/uyxj9rjh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating Hugging Face repository: alxxtexxr/L3.1-8B-wikipedia-ja-5K-1K-1K-64K-Nero-v20250807184323\n",
      "[INFO] Hugging Face repository created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1479854413.py:150: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 1, loss: 2.748265504837036, lm_loss: 2.8924598693847656, logit_loss: 0.7937536239624023, hidden_loss: 1.0, lr: 3.616636528028933e-07, grad_norm: 0.723087389408253, grad_norm_clipped: 0.723087389408253\n",
      "epoch: 0, step: 2, loss: 2.6068525314331055, lm_loss: 2.317270278930664, logit_loss: 0.8421533107757568, hidden_loss: 1.0, lr: 3.616636528028933e-07\n",
      "epoch: 0, step: 3, loss: 2.6405463218688965, lm_loss: 2.8171303272247314, logit_loss: 0.7108933925628662, hidden_loss: 1.0, lr: 7.233273056057866e-07, grad_norm: 1.3192691207366212, grad_norm_clipped: 1.3192691207366212\n",
      "epoch: 0, step: 4, loss: 2.610607385635376, lm_loss: 2.466327428817749, logit_loss: 0.7967228889465332, hidden_loss: 0.9999963641166687, lr: 7.233273056057866e-07\n",
      "epoch: 0, step: 5, loss: 2.7453324794769287, lm_loss: 2.7352848052978516, logit_loss: 0.8426922559738159, hidden_loss: 0.9999961853027344, lr: 1.08499095840868e-06, grad_norm: 1.3787425645496496, grad_norm_clipped: 1.3787425645496496\n",
      "epoch: 0, step: 6, loss: 2.676185131072998, lm_loss: 2.405050277709961, logit_loss: 0.882532000541687, hidden_loss: 0.9999866485595703, lr: 1.08499095840868e-06\n",
      "epoch: 0, step: 7, loss: 2.6148033142089844, lm_loss: 2.713625907897949, logit_loss: 0.7193199992179871, hidden_loss: 0.9999865293502808, lr: 1.4466546112115732e-06, grad_norm: 1.372301528567013, grad_norm_clipped: 1.372301528567013\n",
      "epoch: 0, step: 8, loss: 2.6357595920562744, lm_loss: 2.8539483547210693, logit_loss: 0.6939860582351685, hidden_loss: 0.999970555305481, lr: 1.4466546112115732e-06\n",
      "epoch: 0, step: 9, loss: 2.452915668487549, lm_loss: 2.7180473804473877, logit_loss: 0.5559865236282349, hidden_loss: 0.9999734163284302, lr: 1.8083182640144668e-06, grad_norm: 1.2382610550028244, grad_norm_clipped: 1.2382610550028244\n",
      "epoch: 0, step: 10, loss: 2.567495822906494, lm_loss: 2.5061769485473633, logit_loss: 0.7405067682266235, hidden_loss: 0.9999507069587708, lr: 1.8083182640144668e-06\n",
      "epoch: 0, step: 11, loss: 2.567293643951416, lm_loss: 3.1023619174957275, logit_loss: 0.543561577796936, hidden_loss: 0.9999526739120483, lr: 2.16998191681736e-06, grad_norm: 1.2475195339221257, grad_norm_clipped: 1.2475195339221257\n",
      "epoch: 0, step: 12, loss: 2.457409143447876, lm_loss: 2.78953218460083, logit_loss: 0.5369347333908081, hidden_loss: 0.9999287724494934, lr: 2.16998191681736e-06\n",
      "epoch: 0, step: 13, loss: 2.7049992084503174, lm_loss: 2.634997844696045, logit_loss: 0.8355242013931274, hidden_loss: 0.9999256730079651, lr: 2.531645569620253e-06, grad_norm: 1.2473316610270209, grad_norm_clipped: 1.2473316610270209\n",
      "epoch: 0, step: 14, loss: 2.5181632041931152, lm_loss: 2.4517300128936768, logit_loss: 0.7091919183731079, hidden_loss: 0.9999004602432251, lr: 2.531645569620253e-06\n",
      "epoch: 0, step: 15, loss: 2.4086947441101074, lm_loss: 2.594900608062744, logit_loss: 0.5524801015853882, hidden_loss: 0.9998973608016968, lr: 2.8933092224231464e-06, grad_norm: 1.1750237192651383, grad_norm_clipped: 1.1750237192651383\n",
      "epoch: 0, step: 16, loss: 2.6828696727752686, lm_loss: 3.008826732635498, logit_loss: 0.6900943517684937, hidden_loss: 0.9998624324798584, lr: 2.8933092224231464e-06\n",
      "epoch: 0, step: 17, loss: 2.3664932250976562, lm_loss: 2.4261105060577393, logit_loss: 0.5660157203674316, hidden_loss: 0.999860942363739, lr: 3.25497287522604e-06, grad_norm: 1.1952339727944437, grad_norm_clipped: 1.1952339727944437\n",
      "epoch: 0, step: 18, loss: 2.641991376876831, lm_loss: 2.436283588409424, logit_loss: 0.8381907343864441, hidden_loss: 0.9998270869255066, lr: 3.25497287522604e-06\n",
      "epoch: 0, step: 19, loss: 2.388582229614258, lm_loss: 2.501335859298706, logit_loss: 0.5633186101913452, hidden_loss: 0.9998228549957275, lr: 3.6166365280289336e-06, grad_norm: 1.254199036035338, grad_norm_clipped: 1.254199036035338\n",
      "epoch: 0, step: 20, loss: 3.0819613933563232, lm_loss: 2.8361828327178955, logit_loss: 1.1462512016296387, hidden_loss: 0.9997698664665222, lr: 3.6166365280289336e-06\n",
      "epoch: 0, step: 21, loss: 2.8033902645111084, lm_loss: 2.858259916305542, logit_loss: 0.8603900074958801, hidden_loss: 0.9997744560241699, lr: 3.978300180831827e-06, grad_norm: 1.5929391632643823, grad_norm_clipped: 1.5929391632643823\n",
      "epoch: 0, step: 22, loss: 2.757463216781616, lm_loss: 2.966063976287842, logit_loss: 0.7789413928985596, hidden_loss: 0.9997207522392273, lr: 3.978300180831827e-06\n",
      "epoch: 0, step: 23, loss: 2.448542594909668, lm_loss: 2.425098419189453, logit_loss: 0.6485280990600586, hidden_loss: 0.9997320175170898, lr: 4.33996383363472e-06, grad_norm: 1.3157772833518866, grad_norm_clipped: 1.3157772833518866\n",
      "epoch: 0, step: 24, loss: 2.588205337524414, lm_loss: 2.8388473987579346, logit_loss: 0.65171217918396, hidden_loss: 0.9996734261512756, lr: 4.33996383363472e-06\n",
      "epoch: 0, step: 25, loss: 2.4712400436401367, lm_loss: 2.5877737998962402, logit_loss: 0.6176077127456665, hidden_loss: 0.9996668100357056, lr: 4.701627486437614e-06, grad_norm: 1.2200899591380339, grad_norm_clipped: 1.2200899591380339\n",
      "epoch: 0, step: 26, loss: 2.4777822494506836, lm_loss: 2.607008218765259, logit_loss: 0.6178612112998962, hidden_loss: 0.9996082186698914, lr: 4.701627486437614e-06\n",
      "epoch: 0, step: 27, loss: 2.5633859634399414, lm_loss: 2.7194974422454834, logit_loss: 0.6663497686386108, hidden_loss: 0.9996021389961243, lr: 5.063291139240506e-06, grad_norm: 1.2357265064618141, grad_norm_clipped: 1.2357265064618141\n",
      "epoch: 0, step: 28, loss: 2.448493719100952, lm_loss: 2.44915509223938, logit_loss: 0.6407289505004883, hidden_loss: 0.9995435476303101, lr: 5.063291139240506e-06\n",
      "epoch: 0, step: 29, loss: 2.7360825538635254, lm_loss: 2.685899257659912, logit_loss: 0.8501859903335571, hidden_loss: 0.9995496273040771, lr: 5.4249547920433995e-06, grad_norm: 1.2526224083030866, grad_norm_clipped: 1.2526224083030866\n",
      "epoch: 0, step: 30, loss: 2.625162124633789, lm_loss: 3.1477150917053223, logit_loss: 0.586958110332489, hidden_loss: 0.9994580149650574, lr: 5.4249547920433995e-06\n",
      "epoch: 0, step: 31, loss: 2.556884288787842, lm_loss: 2.755831003189087, logit_loss: 0.6479759216308594, hidden_loss: 0.999484121799469, lr: 5.786618444846293e-06, grad_norm: 1.208255199852779, grad_norm_clipped: 1.208255199852779\n",
      "epoch: 0, step: 32, loss: 2.493553638458252, lm_loss: 2.7871572971343994, logit_loss: 0.5743769407272339, hidden_loss: 0.9994145631790161, lr: 5.786618444846293e-06\n",
      "epoch: 0, step: 33, loss: 2.400865077972412, lm_loss: 2.451165199279785, logit_loss: 0.5925614833831787, hidden_loss: 0.999419093132019, lr: 6.148282097649186e-06, grad_norm: 1.1779712496863954, grad_norm_clipped: 1.1779712496863954\n",
      "epoch: 0, step: 34, loss: 2.7266459465026855, lm_loss: 2.6967623233795166, logit_loss: 0.8374079465866089, hidden_loss: 0.9993063807487488, lr: 6.148282097649186e-06\n",
      "epoch: 0, step: 35, loss: 2.629692792892456, lm_loss: 3.2121260166168213, logit_loss: 0.5703989863395691, hidden_loss: 0.9992921948432922, lr: 6.50994575045208e-06, grad_norm: 1.3139767716084814, grad_norm_clipped: 1.3139767716084814\n",
      "epoch: 0, step: 36, loss: 2.5037856101989746, lm_loss: 2.9441440105438232, logit_loss: 0.5330125093460083, hidden_loss: 0.999205470085144, lr: 6.50994575045208e-06\n",
      "epoch: 0, step: 37, loss: 2.520738124847412, lm_loss: 2.7140145301818848, logit_loss: 0.6258875131607056, hidden_loss: 0.9992257356643677, lr: 6.871609403254974e-06, grad_norm: 1.2197902916997274, grad_norm_clipped: 1.2197902916997274\n",
      "epoch: 0, step: 38, loss: 2.3933675289154053, lm_loss: 2.450066089630127, logit_loss: 0.5857144594192505, hidden_loss: 0.9991312623023987, lr: 6.871609403254974e-06\n",
      "epoch: 0, step: 39, loss: 2.744797468185425, lm_loss: 3.2644262313842773, logit_loss: 0.6683902144432068, hidden_loss: 0.9991465210914612, lr: 7.233273056057867e-06, grad_norm: 1.381096211046361, grad_norm_clipped: 1.381096211046361\n",
      "epoch: 0, step: 40, loss: 2.5334792137145996, lm_loss: 2.6739614009857178, logit_loss: 0.6520743370056152, hidden_loss: 0.9989974498748779, lr: 7.233273056057867e-06\n",
      "epoch: 0, step: 41, loss: 2.573098659515381, lm_loss: 2.8005528450012207, logit_loss: 0.6498562097549438, hidden_loss: 0.999059796333313, lr: 7.5949367088607605e-06, grad_norm: 1.2136754233749132, grad_norm_clipped: 1.2136754233749132\n",
      "epoch: 0, step: 42, loss: 2.449335813522339, lm_loss: 2.581049919128418, logit_loss: 0.5987030863761902, hidden_loss: 0.998886227607727, lr: 7.5949367088607605e-06\n",
      "epoch: 0, step: 43, loss: 2.5364623069763184, lm_loss: 2.9868555068969727, logit_loss: 0.5518704056739807, hidden_loss: 0.9989296197891235, lr: 7.956600361663654e-06, grad_norm: 1.1399161163624911, grad_norm_clipped: 1.1399161163624911\n",
      "epoch: 0, step: 44, loss: 2.5349044799804688, lm_loss: 2.866133451461792, logit_loss: 0.5903069972991943, hidden_loss: 0.9987733960151672, lr: 7.956600361663654e-06\n",
      "epoch: 0, step: 45, loss: 2.7318990230560303, lm_loss: 2.873565196990967, logit_loss: 0.7847651839256287, hidden_loss: 0.998857319355011, lr: 8.318264014466546e-06, grad_norm: 1.255392315406281, grad_norm_clipped: 1.255392315406281\n",
      "epoch: 0, step: 46, loss: 2.4657905101776123, lm_loss: 2.384188652038574, logit_loss: 0.6802325248718262, hidden_loss: 0.9987757802009583, lr: 8.318264014466546e-06\n",
      "epoch: 0, step: 47, loss: 2.477242946624756, lm_loss: 2.8292055130004883, logit_loss: 0.5449080467224121, hidden_loss: 0.9986971020698547, lr: 8.67992766726944e-06, grad_norm: 1.1180081223057212, grad_norm_clipped: 1.1180081223057212\n",
      "epoch: 0, step: 48, loss: 2.476987838745117, lm_loss: 2.7096316814422607, logit_loss: 0.5841858386993408, hidden_loss: 0.9986234307289124, lr: 8.67992766726944e-06\n",
      "epoch: 0, step: 49, loss: 2.341425657272339, lm_loss: 2.409180164337158, logit_loss: 0.5478450059890747, hidden_loss: 0.9985511302947998, lr: 9.041591320072333e-06, grad_norm: 1.1372204591717168, grad_norm_clipped: 1.1372204591717168\n",
      "epoch: 0, step: 50, loss: 2.7247848510742188, lm_loss: 2.6924197673797607, logit_loss: 0.8377562761306763, hidden_loss: 0.9985300302505493, lr: 9.041591320072333e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc866213b514897b5251666eb12303c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/88.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5332aa94838e48449c0bbffc9025063c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c712445caad4488a2c3af809670b019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04dfa9ad19d49d58a202808745efb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/177M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d7025aa08e4aaf9fc99424e8d7dfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f855b5e9a44049df969e68b079c8e501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : 日本は\n",
      "Generated: 日本は、世界の他の国々と比較して、人口が多い国で、国土面積が小さい国である。日本は、人口が多い国\n",
      "\n",
      "epoch: 0, step: 51, loss: 2.395453453063965, lm_loss: 2.406221866607666, logit_loss: 0.6029367446899414, hidden_loss: 0.99846351146698, lr: 9.403254972875227e-06, grad_norm: 1.190116266288135, grad_norm_clipped: 1.190116266288135\n",
      "epoch: 0, step: 52, loss: 2.6948933601379395, lm_loss: 2.6254611015319824, logit_loss: 0.83009272813797, hidden_loss: 0.99839848279953, lr: 9.403254972875227e-06\n",
      "epoch: 0, step: 53, loss: 2.8036134243011475, lm_loss: 2.3813555240631104, logit_loss: 1.019332766532898, hidden_loss: 0.9984333515167236, lr: 9.76491862567812e-06, grad_norm: 1.5427950808324384, grad_norm_clipped: 1.5427950808324384\n",
      "epoch: 0, step: 54, loss: 2.5087358951568604, lm_loss: 2.9238173961639404, logit_loss: 0.5457111597061157, hidden_loss: 0.9981648921966553, lr: 9.76491862567812e-06\n",
      "epoch: 0, step: 55, loss: 2.66558837890625, lm_loss: 2.6368231773376465, logit_loss: 0.7971600294113159, hidden_loss: 0.9982767105102539, lr: 1.0126582278481012e-05, grad_norm: 1.2574527918094303, grad_norm_clipped: 1.2574527918094303\n",
      "epoch: 0, step: 56, loss: 2.499131202697754, lm_loss: 2.6853268146514893, logit_loss: 0.6148659586906433, hidden_loss: 0.9981073141098022, lr: 1.0126582278481012e-05\n",
      "epoch: 0, step: 57, loss: 2.4715611934661865, lm_loss: 2.576597213745117, logit_loss: 0.6232064366340637, hidden_loss: 0.9980775713920593, lr: 1.0488245931283906e-05, grad_norm: 1.1650428705076623, grad_norm_clipped: 1.1650428705076623\n",
      "epoch: 0, step: 58, loss: 2.463207721710205, lm_loss: 2.3737800121307373, logit_loss: 0.6818587183952332, hidden_loss: 0.9980014562606812, lr: 1.0488245931283906e-05\n",
      "epoch: 0, step: 59, loss: 2.310695171356201, lm_loss: 2.431062698364258, logit_loss: 0.5105351209640503, hidden_loss: 0.9979093074798584, lr: 1.0849909584086799e-05, grad_norm: 1.1324652676305687, grad_norm_clipped: 1.1324652676305687\n",
      "epoch: 0, step: 60, loss: 2.4539430141448975, lm_loss: 2.728346109390259, logit_loss: 0.55595862865448, hidden_loss: 0.9976301193237305, lr: 1.0849909584086799e-05\n",
      "epoch: 0, step: 61, loss: 2.4212207794189453, lm_loss: 2.2301511764526367, logit_loss: 0.6874943971633911, hidden_loss: 0.9977765083312988, lr: 1.1211573236889693e-05, grad_norm: 1.1711050163618124, grad_norm_clipped: 1.1711050163618124\n",
      "epoch: 0, step: 62, loss: 2.546043872833252, lm_loss: 2.610307455062866, logit_loss: 0.6869512796401978, hidden_loss: 0.9976912140846252, lr: 1.1211573236889693e-05\n",
      "epoch: 0, step: 63, loss: 2.677732229232788, lm_loss: 2.5132646560668945, logit_loss: 0.8507304191589355, hidden_loss: 0.9976244568824768, lr: 1.1573236889692586e-05, grad_norm: 1.2722931202848209, grad_norm_clipped: 1.2722931202848209\n",
      "epoch: 0, step: 64, loss: 2.6325461864471436, lm_loss: 2.7622437477111816, logit_loss: 0.7234726548194885, hidden_loss: 0.9975330829620361, lr: 1.1573236889692586e-05\n",
      "epoch: 0, step: 65, loss: 2.605522394180298, lm_loss: 3.0595383644104004, logit_loss: 0.5984399318695068, hidden_loss: 0.997434675693512, lr: 1.193490054249548e-05, grad_norm: 1.1658814152317587, grad_norm_clipped: 1.1658814152317587\n",
      "epoch: 0, step: 66, loss: 2.7120983600616455, lm_loss: 2.887399911880493, logit_loss: 0.7619888782501221, hidden_loss: 0.9972675442695618, lr: 1.193490054249548e-05\n",
      "epoch: 0, step: 67, loss: 2.4988367557525635, lm_loss: 2.6083803176879883, logit_loss: 0.6408019065856934, hidden_loss: 0.9972694516181946, lr: 1.2296564195298372e-05, grad_norm: 1.2282874774327168, grad_norm_clipped: 1.2282874774327168\n",
      "epoch: 0, step: 68, loss: 2.5930299758911133, lm_loss: 2.836886167526245, logit_loss: 0.6598243713378906, hidden_loss: 0.9970332384109497, lr: 1.2296564195298372e-05\n",
      "epoch: 0, step: 69, loss: 2.4928975105285645, lm_loss: 2.038215160369873, logit_loss: 0.823016881942749, hidden_loss: 0.9972697496414185, lr: 1.2658227848101267e-05, grad_norm: 1.177912041246066, grad_norm_clipped: 1.177912041246066\n",
      "epoch: 0, step: 70, loss: 2.63789439201355, lm_loss: 2.3130674362182617, logit_loss: 0.8774887323379517, hidden_loss: 0.9970933794975281, lr: 1.2658227848101267e-05\n",
      "epoch: 0, step: 71, loss: 2.562018394470215, lm_loss: 2.5725133419036865, logit_loss: 0.7159888744354248, hidden_loss: 0.9971000552177429, lr: 1.301989150090416e-05, grad_norm: 1.26442565003543, grad_norm_clipped: 1.26442565003543\n",
      "epoch: 0, step: 72, loss: 2.4850106239318848, lm_loss: 2.819936513900757, logit_loss: 0.5576428174972534, hidden_loss: 0.9967888593673706, lr: 1.301989150090416e-05\n",
      "epoch: 0, step: 73, loss: 2.5180141925811768, lm_loss: 2.782684087753296, logit_loss: 0.6029891967773438, hidden_loss: 0.9967392086982727, lr: 1.3381555153707053e-05, grad_norm: 1.1635661471068737, grad_norm_clipped: 1.1635661471068737\n",
      "epoch: 0, step: 74, loss: 2.4626641273498535, lm_loss: 2.5872066020965576, logit_loss: 0.6122216582298279, hidden_loss: 0.9966643452644348, lr: 1.3381555153707053e-05\n",
      "epoch: 0, step: 75, loss: 2.473322629928589, lm_loss: 2.604916572570801, logit_loss: 0.6170825958251953, hidden_loss: 0.9966174960136414, lr: 1.3743218806509948e-05, grad_norm: 1.1505177223132967, grad_norm_clipped: 1.1505177223132967\n",
      "epoch: 0, step: 76, loss: 2.6749234199523926, lm_loss: 3.1054482460021973, logit_loss: 0.653753399848938, hidden_loss: 0.9963720440864563, lr: 1.3743218806509948e-05\n",
      "epoch: 0, step: 77, loss: 2.519103527069092, lm_loss: 2.7024405002593994, logit_loss: 0.6307927370071411, hidden_loss: 0.9965053796768188, lr: 1.410488245931284e-05, grad_norm: 1.1722603556895357, grad_norm_clipped: 1.1722603556895357\n",
      "epoch: 0, step: 78, loss: 2.5440468788146973, lm_loss: 2.748065948486328, logit_loss: 0.6409862041473389, hidden_loss: 0.996198832988739, lr: 1.410488245931284e-05\n",
      "epoch: 0, step: 79, loss: 2.42838978767395, lm_loss: 2.6124603748321533, logit_loss: 0.5701624751091003, hidden_loss: 0.9961152672767639, lr: 1.4466546112115734e-05, grad_norm: 1.1486316676793689, grad_norm_clipped: 1.1486316676793689\n",
      "epoch: 0, step: 80, loss: 2.320112705230713, lm_loss: 2.4121947288513184, logit_loss: 0.5280086398124695, hidden_loss: 0.9960797429084778, lr: 1.4466546112115734e-05\n",
      "epoch: 0, step: 81, loss: 2.2803850173950195, lm_loss: 2.0428383350372314, logit_loss: 0.6102302074432373, hidden_loss: 0.9960182905197144, lr: 1.4828209764918627e-05, grad_norm: 1.1357584477330724, grad_norm_clipped: 1.1357584477330724\n",
      "epoch: 0, step: 82, loss: 2.5645527839660645, lm_loss: 2.9411075115203857, logit_loss: 0.5982393026351929, hidden_loss: 0.9957481622695923, lr: 1.4828209764918627e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Error detected in MmBackward0. No forward pass information available. Enable detect anomaly during forward pass for more information. (Triggered internally at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:100.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'MmBackward0' returned nan values in its 1th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1479854413.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Update parameters only at the end of gradient accumulation cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'MmBackward0' returned nan values in its 1th output."
     ]
    }
   ],
   "source": [
    "# Set model to training mode\n",
    "nero_model.set_mode('student')\n",
    "nero_model.train()\n",
    "\n",
    "# If `device` is not specified or set to 'auto', use model's device\n",
    "# if device is None or device == 'auto':\n",
    "device = next(iter(nero_model.parameters())).device\n",
    "\n",
    "# Set up optimizer and gradient scaler\n",
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr, foreach=False)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Set up LR scheduler\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "if warmup_ratio > 0:\n",
    "# if num_warmup_steps > 0:\n",
    "    # If `warmup_ratio` > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup # type: ignore\n",
    "    max_optimizer_steps = max_global_steps // grad_accumulation_steps\n",
    "    num_warmup_steps = int(warmup_ratio * max_optimizer_steps)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "else:\n",
    "    # If `warmup_ratio` is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True, # End previous run and start a new one\n",
    "    config=dict(\n",
    "        # Project configuration\n",
    "        seed = seed,\n",
    "        # target_lang=target_lang,\n",
    "        # target_task=target_task,\n",
    "        device = device,\n",
    "\n",
    "        # Data configuration\n",
    "        # train_size = train_size,\n",
    "        # test_size = test_size,\n",
    "        # max_seq_length = max_seq_length,\n",
    "\n",
    "        # Training configuration\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        resume_step = resume_step,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "        lr = lr,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        # num_warmup_steps = num_warmup_steps,\n",
    "        checkpoint_steps = checkpoint_steps,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "global_step = resume_step\n",
    "start_epoch = 0\n",
    "\n",
    "if resume_step > 0:\n",
    "    checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{resume_step}')\n",
    "    print(f\"[INFO] Resuming training from checkpoint directory:\", checkpoint_dir)\n",
    "\n",
    "    # Load Nero parameters\n",
    "    nero_path = os.path.join(checkpoint_dir, 'adapter_model.safetensors')\n",
    "    nero_model.load_nero_params(nero_path)\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_dir, 'optimizer.pt')\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "    \n",
    "    # Move optimizer state to the correct device\n",
    "    for param in optimizer.state:\n",
    "        param_device = param.device\n",
    "        param_dtype = param.dtype\n",
    "        for key, value in optimizer.state[param].items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                optimizer.state[param][key] = value.to(device=param_device, dtype=param_dtype)\n",
    "\n",
    "    # Load scheduler state\n",
    "    scheduler_path = os.path.join(checkpoint_dir, 'scheduler.pt')\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
    "\n",
    "    # Load scaler state\n",
    "    scaler_path = os.path.join(checkpoint_dir, 'scaler.pt')\n",
    "    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "\n",
    "    # Load trainer state\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, 'trainer_state.json')\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        start_epoch = log_history[-1]['epoch'] if log_history else 0\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch} and step {resume_step}.\")\n",
    "\n",
    "    # Load RNG state for reproducibility\n",
    "    rng_path = os.path.join(checkpoint_dir, 'rng_state.pth')\n",
    "    if os.path.exists(rng_path):\n",
    "        rng_state = torch.load(rng_path)\n",
    "        random.setstate(rng_state['python'])\n",
    "        np.random.set_state(rng_state['numpy'])\n",
    "        torch.set_rng_state(rng_state['cpu'])\n",
    "        if torch.cuda.is_available() and rng_state['cuda']:\n",
    "            torch.cuda.set_rng_state_all(rng_state['cuda'])\n",
    "    \n",
    "    if resume_step % grad_accumulation_steps != 0:\n",
    "        print(\"[WARN] Resuming mid-gradient accumulation cycle. Make sure this is intended.\")\n",
    "else:\n",
    "    # If it's new training, create Hugging Face repository\n",
    "    print(f\"[INFO] Creating Hugging Face repository:\", hf_nero_id) # print the link instead\n",
    "    create_repo(repo_id=hf_nero_id, repo_type='model', exist_ok=True)\n",
    "    print(f\"[INFO] Hugging Face repository created successfully!\")\n",
    "\n",
    "log_history = []\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of gradient accumulation cycle\n",
    "# This ensures there are no leftover gradients when resuming mid-cycle or after a previous cycle was interrupted\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Skip previously completed steps\n",
    "        if global_step <= resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        # Stop training if `max_global_steps` reached\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            nero_model.set_return_hidden_outputs(True)\n",
    "\n",
    "            # Forward pass for teacher\n",
    "            nero_model.set_mode('teacher')\n",
    "            nero_model.eval()\n",
    "            teacher_model_outs, teacher_hidden_outs = nero_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Forward pass for student\n",
    "            nero_model.set_mode('student')\n",
    "            nero_model.train()\n",
    "            student_model_outs, student_hidden_outs = nero_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids,\n",
    "            )\n",
    "\n",
    "            nero_model.set_return_hidden_outputs(False)\n",
    "\n",
    "            # Compute loss\n",
    "            # _loss = loss_fn_v5(student_model_outs, teacher_model_outs, attention_mask)\n",
    "            total_loss, lm_loss, logit_loss, hidden_loss = loss_fn_v6(\n",
    "                student_model_outs, \n",
    "                teacher_model_outs, \n",
    "                student_hidden_outs,\n",
    "                teacher_hidden_outs,\n",
    "                attention_mask,\n",
    "            )\n",
    "            loss = total_loss / grad_accumulation_steps\n",
    "\n",
    "        log = {\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "            'loss': loss.item() * grad_accumulation_steps,\n",
    "            'lm_loss': lm_loss.item(),\n",
    "            'logit_loss': logit_loss.item(),\n",
    "            'hidden_loss': hidden_loss.item(),\n",
    "        }\n",
    "\n",
    "        # Backward pass\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update parameters only at the end of gradient accumulation cycle\n",
    "        grad_norm_log = {}\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            # Unscale gradients before computing gradient norm and applying clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            \n",
    "            # Compute gradient norm\n",
    "            grad_norm = compute_grad_norm(nero_params)\n",
    "            grad_norm_log['grad_norm'] = grad_norm\n",
    "\n",
    "            # Clip gradients\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(nero_params, clip_grad_norm)\n",
    "            \n",
    "            # Compute clipped gradient norm\n",
    "            grad_norm_clipped = compute_grad_norm(nero_params)\n",
    "            grad_norm_log['grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "            # Update parameters\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Zero gradients for the next gradient accumulation cycle\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Logging\n",
    "        log = {\n",
    "            **log, \n",
    "            'lr': scheduler.get_last_lr()[0], \n",
    "            **grad_norm_log,\n",
    "        }\n",
    "        log_history.append(log)\n",
    "        wandb.log(log)\n",
    "        print(\", \".join(f\"{k}: {v}\" for k, v in log.items()))\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step > 0 and global_step % checkpoint_steps == 0:\n",
    "            # Create checkpoint directory\n",
    "            checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save Nero parameters, along with optimizer, scheduler, and scaler states\n",
    "            nero_state_dict = {n: p.detach().cpu() for n, p in nero_model.named_parameters() if p.requires_grad}\n",
    "            save_file(nero_state_dict, os.path.join(checkpoint_dir, 'adapter_model.safetensors'))  # NEW\n",
    "            torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, 'optimizer.pt'))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(checkpoint_dir, 'scheduler.pt'))\n",
    "            torch.save(scaler.state_dict(), os.path.join(checkpoint_dir, 'scaler.pt'))\n",
    "\n",
    "            # Save trainer state for resuming training\n",
    "            trainer_state = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'log_history': log_history,\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'w') as f:\n",
    "                json.dump(trainer_state, f, indent=2)\n",
    "\n",
    "            # Save RNG state for reproducibility\n",
    "            rng_state = {\n",
    "                'python': random.getstate(),\n",
    "                'numpy': np.random.get_state(),\n",
    "                'cpu': torch.get_rng_state(),\n",
    "                'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else [],\n",
    "            }\n",
    "            torch.save(rng_state, os.path.join(checkpoint_dir, 'rng_state.pth'))\n",
    "\n",
    "            # Upload checkpoint directory to Hugging Face repository\n",
    "            upload_folder(\n",
    "                folder_path=checkpoint_dir,\n",
    "                repo_id=hf_nero_id,\n",
    "                path_in_repo=f\"checkpoint-{global_step}\",\n",
    "                commit_message=f\"Add checkpoint at step {global_step}\",\n",
    "                repo_type='model',\n",
    "            )\n",
    "        \n",
    "        # Check generated text every `generate_steps`\n",
    "        if global_step > 0 and global_step % generate_steps == 0:\n",
    "            nero_model.set_return_hidden_outputs(False)\n",
    "            nero_model.set_mode('student')\n",
    "            generated = generate_text(nero_model, tokenizer, sample_prompt, device=device)\n",
    "            print()\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT\")\n",
    "            print(\"================================\")\n",
    "            print(f\"{'Prompt':<9}:\", sample_prompt)\n",
    "            print(f\"{'Generated':<9}:\", generated)\n",
    "            print()\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
