{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kill all processess on GPU\n",
    "!fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if 'COLAB_' not in ''.join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks and Kaggle notebooks!\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "    %pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    %pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.11 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel, UnslothTrainer, UnslothTrainingArguments, is_bf16_supported\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume from checkpoint: False\n",
      "Project name: L3.1-8B-wikipedia-id-LoRA-v20250401105723\n",
      "Hub model ID: alxxtexxr/L3.1-8B-wikipedia-id-LoRA-v20250401105723\n"
     ]
    }
   ],
   "source": [
    "# Project configs\n",
    "seed = 69\n",
    "lang = 'id' # 'en' | 'id'\n",
    "task = 'wikipedia' # 'wikipedia' | 'gsm8k'\n",
    "\n",
    "# Data Configs\n",
    "max_data_length = 2500\n",
    "max_seq_length = 1024\n",
    "test_size = 0.2 # 2500 * 0.2 = 500 test data\n",
    "hf_data_id = 'wikimedia/wikipedia' # 'wikimedia/wikipedia' | 'openai/gsm8k'\n",
    "hf_data_dir = '20231101.id' # 'wikipedia': '20231101.en' | '20231101.id' || 'gsm8k': 'main'\n",
    "hf_data_split = f'train[:{max_data_length}]'\n",
    "\n",
    "# Model configs\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# LoRA configs\n",
    "lora_target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "\n",
    "resume_from_checkpoint = False\n",
    "if resume_from_checkpoint:\n",
    "    hub_model_id = ''\n",
    "    project_name = hub_model_id.split('/')[-1]\n",
    "    model_name = project_name\n",
    "\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(repo_id=hub_model_id, local_dir=model_name)\n",
    "else:\n",
    "    model_name = 'unsloth/Meta-Llama-3.1-8B'\n",
    "    project_name = f'L3.1-8B-{task}-{lang}-LoRA-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    hub_model_id = f'alxxtexxr/{project_name}'\n",
    "print(\"Resume from checkpoint:\", resume_from_checkpoint)\n",
    "print(\"Project name:\", project_name)\n",
    "print(\"Hub model ID:\", hub_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ec763c55744092b66c359f5b2952d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46984b9308ca4c52808fd0dc28280725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee066419baa4cf69d84db9b27050405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f95af3b40c4118a7804dbbd161ed61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40814dc20a954463b95950a3256341c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    random_state=seed,\n",
    "    target_modules=lora_target_modules,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,   \n",
    "    lora_dropout=0, # Supports any, but = 0 is optimized\n",
    "    bias='none',    # Supports any, but = 'none' is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=False, # True or 'unsloth' for very long context\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b8eee14cc844beae93f8130a504d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/131k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0512dc0cc041b9b19565ad45a66763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/267M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854aab95d4934fb4b59bfd9fc9920c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/146M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce07eab95a4473fbc5f1c6b75133c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/170M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b210832b10f47c8ab04d500e53cef4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f61d9b264c6422eb2f384f03c8a8ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(hf_data_id, data_dir=hf_data_dir, split=hf_data_split)\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "def format_gsm8k_prompts(examples):\n",
    "    gsm8k_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{question}\n",
    "\n",
    "### Answer: \n",
    "{answer}\"\"\" + eos_token\n",
    "    \n",
    "    return {'text': [gsm8k_prompt.format(question=question, answer=answer) for question, answer in zip(examples['question'], examples['answer'])]}\n",
    "\n",
    "def format_prompts(examples):\n",
    "    return {'text': [example + eos_token for example in examples['text']]}\n",
    "\n",
    "if task == 'gsm8k':\n",
    "    dataset = dataset.map(format_gsm8k_prompts, batched=True)\n",
    "else:\n",
    "    dataset = dataset.map(format_prompts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'url', 'title', 'text'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_split = dataset.train_test_split(test_size=test_size)\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Martha Christina Tiahahu () adalah seorang gadis dari desa Abubu,Nusalaut, Maluku Tengah. Pada usia 17 tahun, ia ikut mengangkat senjata melawan tentara Belanda. Ayahnya adalah Kapitan Paulus Tiahahu, seorang kapitan dari negeri Abubu yang membantu Thomas Matulessy dalam Perang Pattimura pada 1817.\n",
      "\n",
      "M.C. Tiahahu merupakan seorang pejuang kemerdekaan. Ketika ikut dalam pertempuran melawan tentara Belanda saat Perang Pattimura (1817), ia masih remaja. Keberaniannya terkenal di kalangan pejuang, masyarakat luas, dan bahkan musuh-musuhnya.\n",
      "\n",
      "Sejak awal perjuangan, ia selalu ikut mengambil bagian dan pantang mundur. Dengan rambut panjangnya yang terurai ke belakang serta berikat kepala sehelai kain berang (merah), ia setia mendampingi ayahnya dalam setiap pertempuran, baik di Pulau Nusalaut maupun di Pulau Saparua. Siang dan malam ia selalu hadir dan ikut dalam pembuatan kubu-kubu pertahanan. Ia juga membangkitkan semangat kaum wanita di sekitarnya agar ikut membantu kaum pria di setiap medan pertempuran.\n",
      "\n",
      "Di dalam pertempuran yang sengit di Desa Ouw â€“ Ullath jasirah tenggara Pulau Saparua yang tampak betapa hebat srikandi ini menggempur musuh bersama para pejuang rakyat. Namun akhirnya karena tidak seimbang dalam persenjataan, tipu daya musuh dan pengkhianatan, para tokoh pejuang dapat ditangkap dan menjalani hukuman. Ada yang harus mati digantung dan ada yang dibuang ke Pulau Jawa. Kapitan Paulus Tiahahu divonis hukum mati tembak. Martha Christina Tiahahu berjuang untuk melepaskan ayahnya dari hukuman mati, tetapi ia tidak berdaya dan meneruskan bergerilyanya di hutan, tetapi akhirnya tertangkap dan hendak diasingkan ke Pulau Jawa. Saat itulah ia jatuh sakit, namun ia menolak diobati oleh orang Belanda.\n",
      "\n",
      "Di Kapal Perang Eversten, Martha Christina Tiahahu menemui ajalnya dan dengan penghormatan militer jasadnya diluncurkan di Laut Banda tepatnya di antara Pulau Buru dan Pulau Manipa pada tanggal 2 Januari 1818. Untuk menghargai jasa dan pengorbanannya, Martha Christina Tiahahu dikukuhkan sebagai Pahlawan Kemerdekaan Nasional oleh Pemerintah Republik Indonesia.\n",
      "\n",
      "Biografi \n",
      "Tiahahu lahir di desa Santiago de AbÃºbu di Pulau Nusalaut, dekat Maluku, pada 4 Januari 1800. Ayahnya adalah Kapten Paulus Tiahahu dari klan Soa Uluputi. Setelah ibunya meninggal saat dia masih bayi, Tiahahu dibesarkan oleh ayahnya. Sebagai seorang anak, dia keras kepala dan mengikuti ayahnya ke mana pun dia pergi, kadang-kadang bergabung dengannya dalam merencanakan serangan.\n",
      "\n",
      "Mulai tahun 1817, Tiahahu bergabung dengan ayahnya dalam perang gerilya melawan pemerintah kolonial Belanda. Mereka juga mendukung tentara Pattimura. Dia melihat beberapa pertempuran. Dalam pertempuran di Pulau Saparua, pasukan itu membunuh komandan Belanda Richement dan melukai Komandan penggantinya Meyer. Dalam pertempuran lain, dia dan pasukannya berhasil membakar Benteng Duurstede hingga rata dengan tanah. Selama pertempuran, dia dikatakan melempar batu ke pasukan Belanda jika tentaranya kehabisan amunisi, sementara catatan lain mengatakan dia menggunakan tombak. Setelah Vermeulen Kringer mengambil alih militer Belanda di Maluku, Tiahahu, ayahnya, dan Pattimura ditangkap pada Oktober 1817.\n",
      "\n",
      "Dibawa dengan HNLMS Evertsen ke Nusalaut, Tiahahu adalah satu-satunya prajurit yang ditangkap yang tidak dihukum; ini karena usianya yang masih muda. Setelah beberapa waktu ditahan di Fort Beverwijk, tempat ayahnya dieksekusi, pada akhir tahun 1817 Tiahahu dibebaskan. Dia terus berjuang melawan Belanda.\n",
      "\n",
      "Dalam penyisiran pada bulan Desember 1817 Tiahahu dan beberapa mantan pemberontak lainnya ditangkap. Gerilyawan yang ditangkap ditempatkan di Evertsen untuk diangkut ke Jawa; mereka dimaksudkan untuk digunakan sebagai tenaga kerja budak di perkebunan kopi di sana. Namun, dalam perjalanan Tiahahu jatuh sakit. Menolak obat dan makanan, dia meninggal pada 2 Januari 1818 ketika kapal sedang menyeberangi Laut Banda; dia menerima penguburan di laut hari itu juga.\n",
      "\n",
      "Perjuangan \n",
      "Martha Christina Tiahahu dilahirkan di Abubu Nusalaut pada tanggal 4 Januari 1800 merupakan anak sulung dari Kapitan Paulus Tiahahu dan masih berusia 17 tahun ketika mengikuti jejak ayahnya memimpin perlawanan di Pulau Nusalaut. Pada waktu yang sama Kapitan Pattimura sedang mengangkat senjata melawan kekuasaan Belanda di Saparua. Perlawanan di Saparua menjalar ke Nusalaut dan daerah sekitarnya.\n",
      "\n",
      "Pada waktu itu, sebagian pasukan rakyat bersama para raja dan patih bergerak ke Saparua untuk membantu perjuangan Kapitan Pattimura sehingga tindakan Belanda yang akan mengambil alih Benteng Beverwijk luput dari perhatian. Guru Soselissa yang memihak Belanda melakukan kontak dengan musuh mengatas-namakan rakyat menyatakan menyerah kepada Belanda. Tanggal 10 Oktober 1817 Benteng Beverwijk jatuh ke tangan Belanda tanpa perlawanan. Sementara itu, di Saparua pertempuran demi pertempuran terus berkobar. Karena semakin berkurangnya persediaan peluru dan mesiu pasukan rakyat mundur ke pegunungan Ulath-Ouw. Di antara pasukan itu terdapat pula Martha Christina Tiahahu beserta para raja dan patih dari Nusalaut.\n",
      "\n",
      "Tanggal 11 Oktober 1817 pasukan Belanda di bawah pimpinan Richemont bergerak ke Ulath, tetapi berhasil dipukul mundur oleh pasukan rakyat. Dengan kekuatan 100 orang prajurit, Meyer beserta Richemont kembali ke Ulath. Pertempuran berkobar kembali, korban berjatuhan di kedua belah pihak.\n",
      "\n",
      "Dalam pertempuran ini Richemont tertembak mati. Meyer dan pasukannya bertahan di tanjakan negeri Ouw. Dari segala penjuru pasukan rakyat mengepung, sorak sorai pasukan bercakalele. Di tengah keganasan pertempuran itu muncul seorang gadis remaja bercakalele menantang peluru musuh. Dia adalah putri Nusahalawano, Martha Christina Tiahahu, srikandi berambut panjang terurai ke belakang dengan sehelai kain berang (kain merah) terikat di kepala.\n",
      "\n",
      "Dengan mendampingi sang ayah dan memberikan kobaran semangat kepada pasukan Nusalaut untuk menghancurkan musuh, Marta Christina telah memberi semangat kepada kaum perempuan dari Ulath dan Ouw untuk turut mendampingi kaum laki-laki di medan pertempuran. Baru di medan ini Belanda berhadapan dengan kaum perempuan fanatik yang turut bertempur. Pertempuran semakin sengit katika sebuah peluru pasukan rakyat mengenai leher Meyer, Vermeulen Kringer mengambil alih komando setelah Meyer diangkat ke atas kapal Eversten.\n",
      "\n",
      "Tanggal 12 Oktober 1817 Vermeulen Kringer memerintahkan serangan umum terhadap pasukan rakyat, ketika pasukan rakyat membalas serangan yang begitu hebat ini dengan lemparan batu, para opsir Belanda menyadari bahwa persediaan peluru pasukan rakyat telah habis. Vermeulen Kringer memberi komando untuk keluar dari kubu-kubu dan kembali melancarkan serangan dengan sangkur terhunus. Pasukan rakyat mundur dan bertahan di hutan, seluruh negeri Ulath dan Ouw diratakan dengan tanah, semua yang ada dibakar dan dirampok habis-habisan.\n",
      "\n",
      "Martha Christina dan sang ayah serta beberapa tokoh pejuang lainnya tertangkap dan dibawa ke dalam kapal Eversten. Di dalam kapal ini para tawanan dari Jasirah Tenggara bertemu dengan Kapitan Pattimura dan tawanan lainnya. Mereka diinterogasi oleh Buyskes dan dijatuhi hukuman. Karena masih sangat muda, Buyskes membebaskan Martha Christina Tiahahu dari hukuman, tetapi sang ayah, Kapitan Paulus Tiahahu tetap dijatuhi hukuman mati. Mendengar keputusan tersebut, Martha Christina Tiahahu memandang sekitar pasukan Belanda dengan tatapan sayu namun kuat yang menandakan keharuan mendalam terhadap sang ayah. Tiba-tiba Martha Christina Tiahahu merebahkan diri di depan Buyskes memohonkan ampun bagi sang ayah yang sudah tua, tetapi semua itu sia-sia.\n",
      "\n",
      "Tanggal 16 Oktober 1817 Martha Christina Tiahahu beserta sang Ayah dibawa ke Nusalaut dan ditahan di benteng Beverwijk sambil menunggu pelaksanaan eksekusi mati bagi ayahnya. Martha Christina Tiahahu mendampingi sang Ayah pada waktu memasuki tempat eksekusi, kemudian Martha Christina Tiahahu dibawa kembali ke dalam benteng Beverwijk dan tinggal bersama guru Soselissa.\n",
      "\n",
      "Sepeninggal ayahnya, Martha Christina Tiahahu masuk ke dalam hutan dan berkeliaran seperti orang kehilangan akal. Hal ini membuat kesehatannya terganggu.\n",
      "\n",
      "Dalam suatu Operasi Pembersihan pada bulan Desember 1817 Martha Christina Tiahahu beserta 39 orang lainnya tertangkap dan dibawa dengan kapal Eversten ke Pulau Jawa untuk dipekerjakan secara paksa di perkebunan kopi.\n",
      "\n",
      "Selama di atas kapal ini kondisi kesehatan Martha Christina Tiahahu semakin memburuk, ia menolak makan dan pengobatan. Akhirnya pada tanggal 2 Januari 1818, selepas Tanjung Alang, Martha Christina Tiahahu menghembuskan napas yang terakhir. Jenazah Martha Christina Tiahahu disemayamkan dengan penghormatan militer ke Laut Banda.\n",
      "\n",
      "Berdasarkan Surat Keputusan Presiden Republik Indonesia Nomor 012/TK/Tahun 1969, tanggal 20 Mei 1969, Martha Christina Tiahahu secara resmi diakui sebagai pahlawan nasional.\n",
      "\n",
      "Referensi\n",
      "\n",
      "Pranala luar \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "Tokoh Maluku\n",
      "Pahlawan nasional Indonesia\n",
      "Daftar pahlawan nasional Indonesia yang beragama Kristen<|end_of_text|>\n",
      "================================================================\n",
      "Ranah Internet tingkat teratas () merupakan rujukan kepada huruf-huruf terakhir setelah tanda titik dalam sebuah nama domain. Misalnya www.google.com memiliki domain teratas .com (atau COM, karena nama domain tidak mempermasalahkan huruf besar atau kecil).\n",
      "\n",
      "Otoritas Nomor yang Ditugaskan Internet (IANA) saat ini mengklasifikasikan Ranah Internet tingkat teratas ke dalam 3 jenis:\n",
      " Ranah internet tingkat atas kode negara (ccTLD)\n",
      " Dipergunakan untuk kode negara atau wilayah dependensi. Terdiri dari 2 huruf, misalnya.id untuk Indonesia, dan .jp untuk Jepang\n",
      "\n",
      " Ranah internet tingkat atas generik (gTLD)\n",
      " Dipergunakan oleh macam-macam organisasi (sebagai contoh, .com untuk organisasi komersial). Domain ini terdiri dari 3 huruf atau lebih. Sebagian besar gTLD tersedia untuk dapat digunakan secara luas, tetapi untuk alasan historis, .mil (militer Amerika Serikat) dan .gov (Pemerintahan Federal Amerika Serikat) dibatasi dan hanya dapat digunakan oleh kedua otoritas tersebut. Domain-domain dalam gTLD disubklasifikasikan ke dalam ranah yang disponsori (Ranah internet tingkat teratas bersponsor (sTLD)), misalnya .aero, .coop dan .museum, dan ranah yang tidak disponsori (Ranah internet tingkat atas yang tidak bersponsor (uTLD)), misalnya .biz, .info, .name, dan .pro.\n",
      " Ranah internet tingkat atas infrastruktur\n",
      " Satu-satunya yang diterima adalah .arpa. Sementara domain .root ada tetapi tanpa kejelasan mengenai untuk apa keberadaannya.\n",
      "\n",
      "TLD umum \n",
      " .aero: industri pesawat terbang\n",
      " .arpa: Address and Routing Parameter Area\n",
      " .biz: bisnis\n",
      " .com: komersial\n",
      " .coop: koperasi\n",
      " .info: informasi\n",
      " .int: internasional\n",
      " .jobs: sumber daya manusia\n",
      " .museum: museum\n",
      " .name: nama perorangan\n",
      " .net: jaringan\n",
      " .org: organisasi\n",
      " .page: -\n",
      " .pro: profesi\n",
      " .site: umum\n",
      " .space: ruang kreatif\n",
      " .travel: industri wisata\n",
      " .tv: televisi\n",
      ".xyz: umum\n",
      "\n",
      "TLD eksklusif Amerika Serikat \n",
      " .edu: pendidikan (eksklusif untuk Departement Pendidikan Amerika Serikat)\n",
      " .gov: pemerintah (eksklusif untuk pemerintah Amerika Serikat)\n",
      " .mil: militer (eksklusif untuk militer Amerika Serikat)\n",
      "\n",
      "Walau demikian, nama ranah .edu digunakan pula dalam praktiknya untuk beberapa situs Indonesia.cara.\n",
      "\n",
      "TLD di Indonesia \n",
      " .id: domain utama negara Indonesia\n",
      " .go.id: untuk organisasi kepemerintahan di Indonesia\n",
      " .mil.id: penggunaan secara khusus oleh militer Indonesia\n",
      " .co.id: bagi perusahaan atau lembaga komersial Indonesia\n",
      " .or.id: untuk organisasi nirlaba Indonesia\n",
      " .web.id: didesignasi untuk badan informal maupun pribadi warga indonesia\n",
      " .my.id: untuk badan informal maupun pribadi warga indonesia\n",
      " .net.id: untuk organisasi / ranah umum untuk situs Internet Indonesia\n",
      " .war.net.id: diperuntukkan warnet (warung Internet)\n",
      " .sch.id: ranah khusus untuk lembaga sekolah di Indonesia\n",
      " .ac.id: diperuntukkan bagi lembaga akademik (Universitas, Akademi, Sekolah Tinggi) di Indonesia\n",
      " desa.id: untuk situs Desa di Indonesia\n",
      "\n",
      "Kode TLD seluruh dunia\n",
      "\n",
      "Pranala luar \n",
      "  Pendaftaran nama ranah Indonesia Registrasi ranah .id \n",
      "  Informasi IANA tentang TLD http://www.iana.org/domain-names.htm\n",
      "  Survei Domain Internet memberikan kita gambaran mengenai seberapa sering sebuah TLD digunakan: http://www.isc.org/ops/ds/reports/2004-01/dist-bynum.php \n",
      "  Pendaftaran Domain Internet Indonesia GT88\n",
      "\n",
      "Ranah internet tingkat teratas\n",
      "Daftar negara<|end_of_text|>\n",
      "================================================================\n",
      "Bumi atau dunia dibagi menjadi 24 zona waktu yang berbeda-beda, sesuai letak daerah tersebut. Waktu universal yang menjadi pautan adalah waktu GMT (Greenwich Mean Time), waktu yang ada di Greenwich, Inggris. Zona waktu biasanya dipautkan pada GMT. Zona waktu adalah wilayah dunia yang mengamati waktu standar yang seragam untuk tujuan hukum, komersial, dan sosial. Zona waktu cenderung mengikuti batas negara dan subdivisinya daripada garis bujur, karena nyaman bagi daerah-daerah dalam komunikasi komersial atau lainnya untuk menjaga waktu yang sama.\n",
      "\n",
      "Sebagian besar zona waktu di darat diimbangi dari Waktu Universal Terkoordinasi (UTC) dengan jumlah keseluruhan jam (UTC âˆ’ 12: 00 hingga UTC + 14: 00), tetapi beberapa zona diimbangi oleh 30 atau 45 menit (misalnya Newfoundland Waktu Standar adalah UTC âˆ’ 03:30, Waktu Standar Nepal adalah UTC + 05:45, Waktu Standar India adalah UTC + 05:30 dan Waktu Standar Myanmar adalah UTC + 06:30).\n",
      "\n",
      "Pembeda \n",
      "\n",
      "Beberapa negara dengan garis lintang dan zona sedang yang lebih tinggi menggunakan waktu musim panas untuk sebagian tahun, biasanya dengan menyesuaikan waktu jam lokal dengan satu jam. Banyak zona waktu daratan condong ke arah barat zona waktu bahari yang sesuai. Ini juga menciptakan efek penghematan waktu siang hari secara permanen.\n",
      "\n",
      "Perhitungan zona waktu \n",
      "Zona waktu dihitung berdasarkan garis bujur sebagai berikut:\n",
      " 360Â° = 24 jam\n",
      " 15Â° = 1 jam\n",
      " 1Â° = 4 menit\n",
      "\n",
      "Konversi untuk kompas kutub: 1Â° adalah 60 menit (60'). Contoh:\n",
      " 105.5Â° = 105Â°30'\n",
      " 90.25Â° = 90Â°15'\n",
      " 60.75Â° = 60Â°45'\n",
      "\n",
      "Catatan\n",
      "\n",
      "Lihat pula \n",
      " Zona waktu Indonesia<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "for row in dataset_split['train'][:3][\"text\"]:\n",
    "    print(\"================================================================\")\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b12b3ea9499492c984cf5009d33e355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_split['train'],\n",
    "    # eval_dataset=dataset_split['test'],\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=8,\n",
    "\n",
    "    args=TrainingArguments(\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        # max_steps=3, # For debugging\n",
    "        warmup_ratio=0.05,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type='cosine',\n",
    "        optim='paged_adamw_8bit', # 'paged_adamw_8bit' | 'adamw_8bit'\n",
    "        weight_decay=0.00,\n",
    "        max_grad_norm=0.3,\n",
    "        fp16=(not is_bf16_supported()),\n",
    "        bf16=is_bf16_supported(),\n",
    "\n",
    "        # Eval arguments\n",
    "        # eval_strategy='steps',\n",
    "        # eval_steps=10,\n",
    "        \n",
    "        # Logging arguments\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=1,\n",
    "        # logging_first_step=True,\n",
    "        report_to=['tensorboard', 'wandb'],\n",
    "\n",
    "        # Saving arguments\n",
    "        save_strategy='steps',\n",
    "        save_steps=100,\n",
    "        # save_steps=1, # For debugging\n",
    "        save_total_limit=5, # 1 best + 4 recent checkpoints. Warning: It doesn't work\n",
    "        \n",
    "        # With load_best_model_at_end=True, your save_strategy will be ignored and default to eval_strategy.\n",
    "        # So you will find one checkpoint at the end of each epoch.\n",
    "        # https://discuss.huggingface.co/t/trainer-not-saving-after-save-steps/5464\n",
    "        # load_best_model_at_end=True, \n",
    "\n",
    "        output_dir=project_name,\n",
    "        hub_model_id=hub_model_id,\n",
    "        push_to_hub=True,\n",
    "\n",
    "        hub_strategy='all_checkpoints',\n",
    "        hub_always_push=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "5.67 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 3 | Total steps = 750\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 20,971,520/8,000,000,000 (0.26% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250401_110047-ummo33o2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/huggingface/runs/ummo33o2' target=\"_blank\">L3.1-8B-wikipedia-id-LoRA-v20250401105723</a></strong> to <a href='https://wandb.ai/alimtegar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/huggingface' target=\"_blank\">https://wandb.ai/alimtegar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/huggingface/runs/ummo33o2' target=\"_blank\">https://wandb.ai/alimtegar/huggingface/runs/ummo33o2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/750 50:54 < 5:08:19, 0.03 it/s, Epoch 0.43/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.724700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.824500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.865800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.908800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://miserably-adapted-crow.ngrok-free.app/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
