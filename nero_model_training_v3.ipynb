{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download, create_repo, upload_folder\n",
    "from safetensors.torch import load_file, save_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(\n",
    "        repo_id, \n",
    "        checkpoint=None, \n",
    "        max_checkpoints=10_000, \n",
    "        checkpoint_steps=25,\n",
    "    ):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_steps) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dir = os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir, checkpoint_dir\n",
    "\n",
    "def check_loss_and_grad_norm(model, tokenizer, prompt=\"Paris is the capital of\"):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    print(\"Loss:\", outputs.loss)\n",
    "\n",
    "    # Backward pass\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for p in [p for n, p in model.named_parameters() if p.requires_grad]:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=32, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id,\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang, _ = lora_repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    split = f'train[:{(train_size+test_size)}]'\n",
    "\n",
    "    # Load dataset\n",
    "    # TODO: Use streaming to not download the entire dataset\n",
    "    # dataset = load_dataset(data_id, data_dir=data_dir, split=split)\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_grad_norm(params):\n",
    "    grad_norm = 0.0\n",
    "    for p in params:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b1d5fea2254dda9fa92d160d8e7ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fac633e86ed418b905ec3ab4505c1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e7dbd8bd39429cb7896d62f4b0dd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/871 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026eb6f30d504322bb326130da268a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2a4fb7a7f34bacba1babc45dab8aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3419420aad44443f96989607d5b55e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42144562d34141a09fc43adfbc0d11c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a73af9e81f243eeafa6c6399603fdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/871 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fd338875c84a1bae22f05d80a1be6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/43.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f49433b03d4a55a615c09c25db9fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df18f62d54804b1495664812ee477c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e14947f13f471ca94ab928d1800c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82186493f53a439ba56ab3befa96a347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c5a843bdc5437e8715e638470221b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916d8c0ab66c4a3fad55f45115a35d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8df6f702506455a96cb0808f6b8d068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871e165139534dd7b3db24e84e998f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513db27bdb204bf99a83d081bae6d1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)t.tfevents.1751286655.ec3c369f91a7.385.0:   0%|          | 0.00/402k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a27adf6d7b6433f81b7215ef68ae611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78da5b48210a4fc0a34de41370b9e159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e51b34fc2f4f27801a4f0445e923a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7161114045c94da6a2985ffb34e8dc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c4a3e9f2294a428515f59a0e67ea6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc56a8a0a559446ba83b65e84c4c8b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "target_lang = 'ja' # 'en' | 'id' | 'es'\n",
    "target_task = 'wikipedia' # 'wikipedia' | 'gsm8k'\n",
    "device = 'auto' # 'cpu' | 'cuda' | 'auto'\n",
    "\n",
    "# Data configuration\n",
    "train_size = 5000\n",
    "test_size = 0 # Temporary\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 5.0\n",
    "lr = 2e-4\n",
    "warmup_ratio = 0.05\n",
    "checkpoint_steps = 50\n",
    "generate_steps = 50\n",
    "sample_prompt = '日本は'\n",
    "\n",
    "# Model configurations\n",
    "hf_lora_id = 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650'\n",
    "checkpoint = 650\n",
    "\n",
    "_, lora_checkpoint_dir = download_hf_model(hf_lora_id, checkpoint)\n",
    "lora_path = os.path.join(lora_checkpoint_dir, 'adapter_model.safetensors')\n",
    "lora_config = LoraConfig.from_pretrained(lora_checkpoint_dir)\n",
    "\n",
    "base_model_name = lora_config.base_model_name_or_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "print(\"[CONFIG] Base model name:\", base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating Nero directory: L3.1-8B-wikipedia-ja-5K-Nero-v20250805174511\n",
      "[INFO] Nero directory created!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face configuration\n",
    "hf_nero_id = None\n",
    "resume_step = 0\n",
    "\n",
    "if hf_nero_id is not None and resume_step > 0:\n",
    "    print(f\"[INFO] Downloading Nero checkpoint at step {resume_step} from Hugging Face repository:\", hf_nero_id)\n",
    "    nero_dir, _ = download_hf_model(hf_nero_id, resume_step)\n",
    "    print(f\"[INFO] Nero checkpoint downloaded successfully!\")\n",
    "else:\n",
    "    hf_username = 'alxxtexxr'\n",
    "    nero_dir = f'L3.1-8B-{target_task}-{target_lang}-{train_size//1000}K-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    print(f\"[INFO] Creating Nero directory:\", nero_dir)\n",
    "    hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "    os.makedirs(nero_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Nero directory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4020a1da2b1c4274a37bfccb56177171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56638c1ec98f4728a3af943d4ce7294d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4063513bd1d24fb094358511d8f186ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 \n",
    "                 # For debugging \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.nero_bias = nero_bias\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # For debugging\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.normal_(self.nero_B.weight, mean=0.0, std=std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) # * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        nero_out = self.nero_B(F.relu(self.nero_A(lora_out)))\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "\n",
    "        output = base_out + (nero_out * self.scaling)\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "    def load_nero_params(self, state_dict, prefix):\n",
    "        self.nero_A.weight.data = state_dict[f'{prefix}.nero_A.weight'].to(self.device)\n",
    "        self.nero_B.weight.data = state_dict[f'{prefix}.nero_B.weight'].to(self.device)\n",
    "        if self.nero_bias:\n",
    "            self.nero_A.bias.data = state_dict[f'{prefix}.nero_A.bias'].to(self.device)\n",
    "            self.nero_B.bias.data = state_dict[f'{prefix}.nero_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_return_nero_outputs(self, return_nero_outputs: bool):\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_nero_output = return_nero_outputs\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        print(\"[INFO] All layers are frozen except Nero layers!\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        print(\"[INFO] All layers are unfrozen!\")\n",
    "    \n",
    "    def load_lora_params(self, lora_path):\n",
    "        if not os.path.exists(lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] LoRA file not found:\", lora_path)\n",
    "        \n",
    "        if lora_path.endswith('.bin'):\n",
    "            state_dict = torch.load(lora_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(lora_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def load_nero_params(self, nero_path):\n",
    "        if not os.path.exists(lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] Nero file not found:\", lora_path)\n",
    "        \n",
    "        if nero_path.endswith('.bin'):\n",
    "            state_dict = torch.load(nero_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(nero_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.nero_A.weight' in state_dict and f'{nero_layer_name}.nero_B.weight' in state_dict:\n",
    "                nero_layer.load_nero_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] Nero parameters loaded successfully!\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(*args, **kwargs)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device)\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    lora_config, \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=False,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] All layers are frozen except Nero layers!\n",
      "\n",
      "Loss: tensor(12.9127, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 8.642618892672493\n"
     ]
    }
   ],
   "source": [
    "nero_model.freeze_all_except_nero()\n",
    "print()\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 0.004079858772456646\n",
      "- Min     : -1.4468613862991333\n",
      "- Max     : 1.3746638298034668\n",
      "\n",
      "[INFO] LoRA parameters loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n",
      "\n",
      "Loss: tensor(13.4193, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 9.637150062361702\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "nero_model.load_lora_params(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gradient checkpointing enabled!\n",
      "\n",
      "Loss: tensor(13.4193, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 9.637150062361702\n"
     ]
    }
   ],
   "source": [
    "nero_model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"[INFO] Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3db26392d24a85b4420737a4704f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_hf_dataset(\n",
    "    target_lang, \n",
    "    target_task, \n",
    "    train_size=train_size, \n",
    "    test_size=test_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae798aa60df4d659b826619e0fc7f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (141723 > 131072). Running this sequence through the model will result in indexing errors\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7060fc3ca04043b5c830266ef6ecd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dataset_map_config(remove_columns=None):\n",
    "    return dict(\n",
    "        batched=True, \n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=os.cpu_count(), # Use all available CPUs\n",
    "    )\n",
    "\n",
    "if target_task == 'gsm8k':\n",
    "    eos_token = tokenizer.eos_token\n",
    "    \n",
    "    def format_gsm8k_prompt(example):\n",
    "        gsm8k_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{question}\n",
    "\n",
    "### Answer: \n",
    "{answer}\"\"\" + eos_token\n",
    "\n",
    "        return {'text': gsm8k_prompt.format(\n",
    "            question=example['question'], \n",
    "            answer=example['answer'],\n",
    "        )}\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_seq_length,\n",
    "        )\n",
    "\n",
    "    def add_labels(example):\n",
    "        example['labels'] = example['input_ids'].copy()\n",
    "        return example\n",
    "\n",
    "    dataset_formatted = dataset.map(\n",
    "        format_gsm8k_prompt, \n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_tokenized = dataset_formatted.map(\n",
    "        tokenize_fn, \n",
    "        **get_dataset_map_config(remove_columns=dataset_formatted.column_names),\n",
    "    )\n",
    "    dataset_final = dataset_tokenized.map(\n",
    "        add_labels,\n",
    "        **get_dataset_map_config(remove_columns=dataset_tokenized.column_names),\n",
    "    )\n",
    "else:\n",
    "    block_size = max_seq_length\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(example['text'])\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated = []\n",
    "        for input_ids in examples['input_ids']:\n",
    "            concatenated += input_ids\n",
    "\n",
    "        total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "        input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_mask = [[1] * block_size for _ in input_ids]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "    dataset_tokenized = dataset.map(\n",
    "        tokenize_fn,\n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_final = dataset_tokenized.map(\n",
    "        group_texts, \n",
    "        **get_dataset_map_config(remove_columns=dataset_tokenized.column_names),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total batches: 6923\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset_final, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"[INFO] Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask, labels):\n",
      "(torch.Size([4, 1024]), torch.Size([4, 1024]), torch.Size([4, 1024]))\n",
      "\n",
      "First batch text:\n",
      "くる息子を、その日のために一人で育てることを決意する。\n",
      " 木彫りの僧侶\n",
      " 声 - 松尾銀三\n",
      " 旺気楼に祀られている像。元は人間であったが、青龍の見立てを受けている最中に邪気を受け、木彫りの像に変わって ...\n",
      "\n",
      "Loss: tensor(13.9221, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 2.0530590763437186\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask, labels):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "    first_batch['labels'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer, prompt=first_batch_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674/2625844535.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f8263050af4a2c99b7a608383608ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112639466667436, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250805_174730-73g804sa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/73g804sa' target=\"_blank\">likely-sun-94</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/73g804sa' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/73g804sa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating Hugging Face repository: alxxtexxr/L3.1-8B-wikipedia-ja-5K-Nero-v20250805174511\n",
      "[INFO] Hugging Face repository created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674/2625844535.py:148: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 1, loss: 14.206133842468262, lr: 1.1560693641618497e-06, grad_norm: 1.1859621034997168, grad_norm_clipped: 1.1859621034997168\n",
      "epoch: 0, step: 2, loss: 14.073974609375, lr: 1.1560693641618497e-06\n",
      "epoch: 0, step: 3, loss: 14.092690467834473, lr: 2.3121387283236993e-06, grad_norm: 2.1565261534406677, grad_norm_clipped: 2.1565261534406677\n",
      "epoch: 0, step: 4, loss: 14.026183128356934, lr: 2.3121387283236993e-06\n",
      "epoch: 0, step: 5, loss: 13.972622871398926, lr: 3.468208092485549e-06, grad_norm: 2.1121887367775036, grad_norm_clipped: 2.1121887367775036\n",
      "epoch: 0, step: 6, loss: 14.047845840454102, lr: 3.468208092485549e-06\n",
      "epoch: 0, step: 7, loss: 14.141060829162598, lr: 4.624277456647399e-06, grad_norm: 2.404423062023708, grad_norm_clipped: 2.404423062023708\n",
      "epoch: 0, step: 8, loss: 14.005545616149902, lr: 4.624277456647399e-06\n",
      "epoch: 0, step: 9, loss: 14.026354789733887, lr: 5.780346820809249e-06, grad_norm: 2.0687781371260408, grad_norm_clipped: 2.0687781371260408\n",
      "epoch: 0, step: 10, loss: 13.996689796447754, lr: 5.780346820809249e-06\n",
      "epoch: 0, step: 11, loss: 13.99870777130127, lr: 6.936416184971098e-06, grad_norm: 2.2039188710745377, grad_norm_clipped: 2.2039188710745377\n",
      "epoch: 0, step: 12, loss: 14.051534652709961, lr: 6.936416184971098e-06\n",
      "epoch: 0, step: 13, loss: 14.178183555603027, lr: 8.092485549132949e-06, grad_norm: 2.7912562390461066, grad_norm_clipped: 2.7912562390461066\n",
      "epoch: 0, step: 14, loss: 13.988080024719238, lr: 8.092485549132949e-06\n",
      "epoch: 0, step: 15, loss: 13.883214950561523, lr: 9.248554913294797e-06, grad_norm: 2.4092466988722236, grad_norm_clipped: 2.4092466988722236\n",
      "epoch: 0, step: 16, loss: 14.017509460449219, lr: 9.248554913294797e-06\n",
      "epoch: 0, step: 17, loss: 14.016289710998535, lr: 1.0404624277456647e-05, grad_norm: 2.2733361520062494, grad_norm_clipped: 2.2733361520062494\n",
      "epoch: 0, step: 18, loss: 14.021740913391113, lr: 1.0404624277456647e-05\n",
      "epoch: 0, step: 19, loss: 13.997714042663574, lr: 1.1560693641618498e-05, grad_norm: 2.5783630742162615, grad_norm_clipped: 2.5783630742162615\n",
      "epoch: 0, step: 20, loss: 13.963146209716797, lr: 1.1560693641618498e-05\n",
      "epoch: 0, step: 21, loss: 13.858067512512207, lr: 1.2716763005780346e-05, grad_norm: 2.2003760513611748, grad_norm_clipped: 2.2003760513611748\n",
      "epoch: 0, step: 22, loss: 14.016595840454102, lr: 1.2716763005780346e-05\n",
      "epoch: 0, step: 23, loss: 13.963501930236816, lr: 1.3872832369942197e-05, grad_norm: 2.309401882562086, grad_norm_clipped: 2.309401882562086\n",
      "epoch: 0, step: 24, loss: 13.921771049499512, lr: 1.3872832369942197e-05\n",
      "epoch: 0, step: 25, loss: 13.88796615600586, lr: 1.5028901734104049e-05, grad_norm: 2.1855061276913124, grad_norm_clipped: 2.1855061276913124\n",
      "epoch: 0, step: 26, loss: 13.913122177124023, lr: 1.5028901734104049e-05\n",
      "epoch: 0, step: 27, loss: 13.716583251953125, lr: 1.6184971098265897e-05, grad_norm: 2.184244457820199, grad_norm_clipped: 2.184244457820199\n",
      "epoch: 0, step: 28, loss: 13.860694885253906, lr: 1.6184971098265897e-05\n",
      "epoch: 0, step: 29, loss: 13.862215042114258, lr: 1.7341040462427746e-05, grad_norm: 2.0273114287697607, grad_norm_clipped: 2.0273114287697607\n",
      "epoch: 0, step: 30, loss: 14.004227638244629, lr: 1.7341040462427746e-05\n",
      "epoch: 0, step: 31, loss: 13.880746841430664, lr: 1.8497109826589594e-05, grad_norm: 2.105872001932364, grad_norm_clipped: 2.105872001932364\n",
      "epoch: 0, step: 32, loss: 13.831145286560059, lr: 1.8497109826589594e-05\n",
      "epoch: 0, step: 35, loss: 13.741365432739258, lr: 2.0809248554913295e-05, grad_norm: 2.0028135114526293, grad_norm_clipped: 2.0028135114526293\n",
      "epoch: 0, step: 36, loss: 13.677966117858887, lr: 2.0809248554913295e-05\n",
      "epoch: 0, step: 37, loss: 13.754268646240234, lr: 2.1965317919075147e-05, grad_norm: 2.2397029589764887, grad_norm_clipped: 2.2397029589764887\n",
      "epoch: 0, step: 39, loss: 13.748753547668457, lr: 2.3121387283236996e-05, grad_norm: 2.090443701458973, grad_norm_clipped: 2.090443701458973\n",
      "epoch: 0, step: 40, loss: 13.717106819152832, lr: 2.3121387283236996e-05\n",
      "epoch: 0, step: 41, loss: 13.6516752243042, lr: 2.4277456647398844e-05, grad_norm: 2.7337323783334555, grad_norm_clipped: 2.7337323783334555\n",
      "epoch: 0, step: 42, loss: 13.530241966247559, lr: 2.4277456647398844e-05\n",
      "epoch: 0, step: 43, loss: 13.619300842285156, lr: 2.5433526011560693e-05, grad_norm: 2.0875138022860313, grad_norm_clipped: 2.0875138022860313\n",
      "epoch: 0, step: 44, loss: 13.612296104431152, lr: 2.5433526011560693e-05\n",
      "epoch: 0, step: 45, loss: 13.665183067321777, lr: 2.658959537572254e-05, grad_norm: 1.9809877186307043, grad_norm_clipped: 1.9809877186307043\n",
      "epoch: 0, step: 46, loss: 13.66300106048584, lr: 2.658959537572254e-05\n",
      "epoch: 0, step: 47, loss: 13.483794212341309, lr: 2.7745664739884393e-05, grad_norm: 2.238262516508169, grad_norm_clipped: 2.238262516508169\n",
      "epoch: 0, step: 48, loss: 13.380632400512695, lr: 2.7745664739884393e-05\n",
      "epoch: 0, step: 49, loss: 13.460066795349121, lr: 2.8901734104046245e-05, grad_norm: 2.065044889087774, grad_norm_clipped: 2.065044889087774\n",
      "epoch: 0, step: 50, loss: 13.429543495178223, lr: 2.8901734104046245e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258531ca4b7d45998eeb77b59feb1cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0a53094bf444b5ba412aaa32d766c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/188M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84342e07718949a484c20648d5c9c25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e79335f66b42f9ad63d3abdc2f2000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/93.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e50f7e4d684a5ab8bfe03e545a5432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108df56f49a14d96bbd1f93bd9700563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : 日本は\n",
      "Generated: 日本はivesyn مک.syntaxistonRY.syntax.syntaxadians.syntaxrynúčastivesivesadianihatrynRY natsieองจาก McL مک El McLsieadiansองจากúčastRYiami Flux\n",
      "\n",
      "epoch: 0, step: 51, loss: 13.465641975402832, lr: 3.0057803468208097e-05, grad_norm: 2.2052367795871057, grad_norm_clipped: 2.2052367795871057\n",
      "epoch: 0, step: 52, loss: 13.470633506774902, lr: 3.0057803468208097e-05\n",
      "epoch: 0, step: 53, loss: 13.301692008972168, lr: 3.1213872832369946e-05, grad_norm: 2.2361969397974732, grad_norm_clipped: 2.2361969397974732\n",
      "epoch: 0, step: 54, loss: 13.442814826965332, lr: 3.1213872832369946e-05\n",
      "epoch: 0, step: 55, loss: 13.381743431091309, lr: 3.2369942196531794e-05, grad_norm: 2.000464217754044, grad_norm_clipped: 2.000464217754044\n",
      "epoch: 0, step: 56, loss: 13.18196964263916, lr: 3.2369942196531794e-05\n",
      "epoch: 0, step: 57, loss: 13.35496711730957, lr: 3.352601156069364e-05, grad_norm: 2.13352416352135, grad_norm_clipped: 2.13352416352135\n",
      "epoch: 0, step: 58, loss: 13.331205368041992, lr: 3.352601156069364e-05\n",
      "epoch: 0, step: 59, loss: 13.135034561157227, lr: 3.468208092485549e-05, grad_norm: 1.9265461342342147, grad_norm_clipped: 1.9265461342342147\n",
      "epoch: 0, step: 60, loss: 13.101040840148926, lr: 3.468208092485549e-05\n",
      "epoch: 0, step: 61, loss: 13.183296203613281, lr: 3.583815028901734e-05, grad_norm: 2.1896755865123065, grad_norm_clipped: 2.1896755865123065\n",
      "epoch: 0, step: 62, loss: 13.054239273071289, lr: 3.583815028901734e-05\n",
      "epoch: 0, step: 63, loss: 13.185364723205566, lr: 3.699421965317919e-05, grad_norm: 2.309111772402644, grad_norm_clipped: 2.309111772402644\n",
      "epoch: 0, step: 64, loss: 13.189902305603027, lr: 3.699421965317919e-05\n",
      "epoch: 0, step: 65, loss: 13.282912254333496, lr: 3.815028901734104e-05, grad_norm: 1.8679069362085137, grad_norm_clipped: 1.8679069362085137\n",
      "epoch: 0, step: 66, loss: 13.133049964904785, lr: 3.815028901734104e-05\n",
      "epoch: 0, step: 67, loss: 13.234095573425293, lr: 3.930635838150289e-05, grad_norm: 1.8741533731666782, grad_norm_clipped: 1.8741533731666782\n",
      "epoch: 0, step: 68, loss: 13.005383491516113, lr: 3.930635838150289e-05\n",
      "epoch: 0, step: 69, loss: 13.043532371520996, lr: 4.046242774566474e-05, grad_norm: 1.9356563033811034, grad_norm_clipped: 1.9356563033811034\n",
      "epoch: 0, step: 70, loss: 12.900616645812988, lr: 4.046242774566474e-05\n",
      "epoch: 0, step: 71, loss: 12.85816478729248, lr: 4.161849710982659e-05, grad_norm: 1.913673927549016, grad_norm_clipped: 1.913673927549016\n",
      "epoch: 0, step: 72, loss: 12.852256774902344, lr: 4.161849710982659e-05\n",
      "epoch: 0, step: 73, loss: 13.048439979553223, lr: 4.2774566473988445e-05, grad_norm: 1.8936542969151795, grad_norm_clipped: 1.8936542969151795\n",
      "epoch: 0, step: 74, loss: 12.881246566772461, lr: 4.2774566473988445e-05\n",
      "epoch: 0, step: 75, loss: 12.683371543884277, lr: 4.3930635838150294e-05, grad_norm: 1.929478310915793, grad_norm_clipped: 1.929478310915793\n",
      "epoch: 0, step: 76, loss: 12.849372863769531, lr: 4.3930635838150294e-05\n",
      "epoch: 0, step: 77, loss: 12.817928314208984, lr: 4.508670520231214e-05, grad_norm: 1.9340804670023084, grad_norm_clipped: 1.9340804670023084\n",
      "epoch: 0, step: 78, loss: 12.684371948242188, lr: 4.508670520231214e-05\n",
      "epoch: 0, step: 79, loss: 12.663043975830078, lr: 4.624277456647399e-05, grad_norm: 1.9226775077184075, grad_norm_clipped: 1.9226775077184075\n",
      "epoch: 0, step: 80, loss: 12.601306915283203, lr: 4.624277456647399e-05\n",
      "epoch: 0, step: 81, loss: 12.762557029724121, lr: 4.739884393063584e-05, grad_norm: 1.8544391045749453, grad_norm_clipped: 1.8544391045749453\n",
      "epoch: 0, step: 82, loss: 12.171802520751953, lr: 4.739884393063584e-05\n",
      "epoch: 0, step: 83, loss: 12.524392127990723, lr: 4.855491329479769e-05, grad_norm: 2.291176033080804, grad_norm_clipped: 2.291176033080804\n",
      "epoch: 0, step: 84, loss: 12.387322425842285, lr: 4.855491329479769e-05\n",
      "epoch: 0, step: 85, loss: 12.566266059875488, lr: 4.971098265895954e-05, grad_norm: 1.9073099539205056, grad_norm_clipped: 1.9073099539205056\n",
      "epoch: 0, step: 86, loss: 12.33020305633545, lr: 4.971098265895954e-05\n",
      "epoch: 0, step: 87, loss: 12.45130443572998, lr: 5.0867052023121385e-05, grad_norm: 1.8566419185514427, grad_norm_clipped: 1.8566419185514427\n",
      "epoch: 0, step: 88, loss: 12.265963554382324, lr: 5.0867052023121385e-05\n",
      "epoch: 0, step: 89, loss: 12.228435516357422, lr: 5.2023121387283234e-05, grad_norm: 1.856122530131939, grad_norm_clipped: 1.856122530131939\n",
      "epoch: 0, step: 90, loss: 12.286319732666016, lr: 5.2023121387283234e-05\n",
      "epoch: 0, step: 91, loss: 12.341452598571777, lr: 5.317919075144508e-05, grad_norm: 1.8033257788087882, grad_norm_clipped: 1.8033257788087882\n",
      "epoch: 0, step: 92, loss: 12.192258834838867, lr: 5.317919075144508e-05\n",
      "epoch: 0, step: 93, loss: 12.252216339111328, lr: 5.433526011560693e-05, grad_norm: 1.8000550420391697, grad_norm_clipped: 1.8000550420391697\n",
      "epoch: 0, step: 94, loss: 12.129652976989746, lr: 5.433526011560693e-05\n",
      "epoch: 0, step: 95, loss: 12.345612525939941, lr: 5.5491329479768787e-05, grad_norm: 1.7422423199059383, grad_norm_clipped: 1.7422423199059383\n",
      "epoch: 0, step: 96, loss: 12.21363639831543, lr: 5.5491329479768787e-05\n",
      "epoch: 0, step: 97, loss: 11.985485076904297, lr: 5.664739884393064e-05, grad_norm: 1.7164772838732254, grad_norm_clipped: 1.7164772838732254\n",
      "epoch: 0, step: 98, loss: 12.040678977966309, lr: 5.664739884393064e-05\n",
      "epoch: 0, step: 99, loss: 12.122004508972168, lr: 5.780346820809249e-05, grad_norm: 1.7591220417265558, grad_norm_clipped: 1.7591220417265558\n",
      "epoch: 0, step: 100, loss: 11.965411186218262, lr: 5.780346820809249e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e284a149cb245108138abf78ba83289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4d97c4cabd45b28f9aab14304a7128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/188M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151dd2d6438a46d7ba05f4ebe2757614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c182ec42c92414cb6a61083e0352a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/93.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23fa8ee35f5482e8e62aed8350f341e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2190bbd4fda4d279bafda9619a87971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : 日本は\n",
      "Generated: 日本は EmbRYRYúčast338、účastynRWラ、、、 Flux032garyRYsie32RY Esper Flux EsperRY、ivesúčast、RY03208 Esper\n",
      "\n",
      "epoch: 0, step: 101, loss: 11.77790641784668, lr: 5.895953757225434e-05, grad_norm: 1.8393874360646352, grad_norm_clipped: 1.8393874360646352\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 169\u001b[0m\n\u001b[1;32m    162\u001b[0m log \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m: global_step,\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m grad_accumulation_steps,\n\u001b[1;32m    166\u001b[0m }\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Update parameters only at the end of gradient accumulation cycle\u001b[39;00m\n\u001b[1;32m    172\u001b[0m grad_norm_log \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# If `device` is not specified or set to 'auto', use model's device\n",
    "if device is None or device == 'auto':\n",
    "    device = next(iter(nero_model.parameters())).device\n",
    "\n",
    "# Set up optimizer and gradient scaler\n",
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr, foreach=False)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Set up LR scheduler\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "if warmup_ratio > 0:\n",
    "    # If `warmup_ratio` > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup # type: ignore\n",
    "    max_optimizer_steps = max_global_steps // grad_accumulation_steps\n",
    "    warmup_steps = int(warmup_ratio * max_optimizer_steps)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "else:\n",
    "    # If `warmup_ratio` is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True, # End previous run and start a new one\n",
    "    config=dict(\n",
    "        # Project configuration\n",
    "        seed = seed,\n",
    "        target_lang=target_lang,\n",
    "        target_task=target_task,\n",
    "        device = device,\n",
    "\n",
    "        # Data configuration\n",
    "        train_size = train_size,\n",
    "        test_size = test_size,\n",
    "        max_seq_length = max_seq_length,\n",
    "\n",
    "        # Training configuration\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        resume_step = resume_step,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "        lr = lr,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        checkpoint_steps = checkpoint_steps,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "global_step = resume_step\n",
    "start_epoch = 0\n",
    "\n",
    "if resume_step > 0:\n",
    "    checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{resume_step}')\n",
    "    print(f\"[INFO] Resuming training from checkpoint directory:\", checkpoint_dir)\n",
    "\n",
    "    # Load Nero parameters\n",
    "    nero_path = os.path.join(checkpoint_dir, 'adapter_model.safetensors')\n",
    "    nero_model.load_nero_params(nero_path)\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_dir, 'optimizer.pt')\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "    \n",
    "    # Move optimizer state to the correct device\n",
    "    for param in optimizer.state:\n",
    "        param_device = param.device\n",
    "        param_dtype = param.dtype\n",
    "        for key, value in optimizer.state[param].items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                optimizer.state[param][key] = value.to(device=param_device, dtype=param_dtype)\n",
    "\n",
    "    # Load scheduler state\n",
    "    scheduler_path = os.path.join(checkpoint_dir, 'scheduler.pt')\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
    "\n",
    "    # Load scaler state\n",
    "    scaler_path = os.path.join(checkpoint_dir, 'scaler.pt')\n",
    "    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "\n",
    "    # Load trainer state\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, 'trainer_state.json')\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        start_epoch = log_history[-1]['epoch'] if log_history else 0\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch} and step {resume_step}.\")\n",
    "\n",
    "    # Load RNG state for reproducibility\n",
    "    rng_path = os.path.join(checkpoint_dir, 'rng_state.pth')\n",
    "    if os.path.exists(rng_path):\n",
    "        rng_state = torch.load(rng_path)\n",
    "        random.setstate(rng_state['python'])\n",
    "        np.random.set_state(rng_state['numpy'])\n",
    "        torch.set_rng_state(rng_state['cpu'])\n",
    "        if torch.cuda.is_available() and rng_state['cuda']:\n",
    "            torch.cuda.set_rng_state_all(rng_state['cuda'])\n",
    "    \n",
    "    if resume_step % grad_accumulation_steps != 0:\n",
    "        print(\"[WARN] Resuming mid-gradient accumulation cycle. Make sure this is intended.\")\n",
    "else:\n",
    "    # If it's new training, create Hugging Face repository\n",
    "    print(f\"[INFO] Creating Hugging Face repository:\", hf_nero_id) # print the link instead\n",
    "    create_repo(repo_id=hf_nero_id, repo_type='model', exist_ok=True)\n",
    "    print(f\"[INFO] Hugging Face repository created successfully!\")\n",
    "\n",
    "# Set model to training mode\n",
    "nero_model.train()\n",
    "\n",
    "log_history = []\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of gradient accumulation cycle\n",
    "# This ensures there are no leftover gradients when resuming mid-cycle or after a previous cycle was interrupted\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Skip previously completed steps\n",
    "        if global_step <= resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        # Stop training if `max_global_steps` reached\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            outputs = nero_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "\n",
    "                # Disable cache to avoid conflict with gradient checkpointing\n",
    "                use_cache=False, \n",
    "            )\n",
    "\n",
    "            # Compute loss\n",
    "            loss = outputs.loss / grad_accumulation_steps\n",
    "\n",
    "        log = {\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "            'loss': loss.item() * grad_accumulation_steps,\n",
    "        }\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update parameters only at the end of gradient accumulation cycle\n",
    "        grad_norm_log = {}\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            # Unscale gradients before computing gradient norm and applying clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # Compute gradient norm\n",
    "            grad_norm = compute_grad_norm(nero_params)\n",
    "            grad_norm_log['grad_norm'] = grad_norm\n",
    "\n",
    "            # Clip gradients\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(nero_params, clip_grad_norm)\n",
    "            \n",
    "            # Compute clipped gradient norm\n",
    "            grad_norm_clipped = compute_grad_norm(nero_params)\n",
    "            grad_norm_log['grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "            # Update parameters\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Zero gradients for the next gradient accumulation cycle\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Logging\n",
    "        log = {\n",
    "            **log, \n",
    "            'lr': scheduler.get_last_lr()[0], \n",
    "            **grad_norm_log,\n",
    "        }\n",
    "        log_history.append(log)\n",
    "        wandb.log(log)\n",
    "        print(\", \".join(f\"{k}: {v}\" for k, v in log.items()))\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step > 0 and global_step % checkpoint_steps == 0:\n",
    "            # Create checkpoint directory\n",
    "            checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save Nero parameters, along with optimizer, scheduler, and scaler states\n",
    "            nero_state_dict = {n: p.detach().cpu() for n, p in nero_model.named_parameters() if p.requires_grad}\n",
    "            save_file(nero_state_dict, os.path.join(checkpoint_dir, 'adapter_model.safetensors'))  # NEW\n",
    "            torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, 'optimizer.pt'))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(checkpoint_dir, 'scheduler.pt'))\n",
    "            torch.save(scaler.state_dict(), os.path.join(checkpoint_dir, 'scaler.pt'))\n",
    "\n",
    "            # Save trainer state for resuming training\n",
    "            trainer_state = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'log_history': log_history,\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'w') as f:\n",
    "                json.dump(trainer_state, f, indent=2)\n",
    "\n",
    "            # Save RNG state for reproducibility\n",
    "            rng_state = {\n",
    "                'python': random.getstate(),\n",
    "                'numpy': np.random.get_state(),\n",
    "                'cpu': torch.get_rng_state(),\n",
    "                'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else [],\n",
    "            }\n",
    "            torch.save(rng_state, os.path.join(checkpoint_dir, 'rng_state.pth'))\n",
    "\n",
    "            # Upload checkpoint directory to Hugging Face repository\n",
    "            upload_folder(\n",
    "                folder_path=checkpoint_dir,\n",
    "                repo_id=hf_nero_id,\n",
    "                path_in_repo=f\"checkpoint-{global_step}\",\n",
    "                commit_message=f\"Add checkpoint at step {global_step}\",\n",
    "                repo_type='model',\n",
    "            )\n",
    "        \n",
    "        # Check generated text every `generate_steps`\n",
    "        if global_step > 0 and global_step % generate_steps == 0:\n",
    "            generated = generate_text(nero_model, tokenizer, sample_prompt, device=device)\n",
    "            print()\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT\")\n",
    "            print(\"================================\")\n",
    "            print(f\"{'Prompt':<9}:\", sample_prompt)\n",
    "            print(f\"{'Generated':<9}:\", generated)\n",
    "            print()\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
