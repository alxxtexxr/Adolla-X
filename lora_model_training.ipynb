{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if 'COLAB_' not in ''.join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    %pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, UnslothTrainer, UnslothTrainingArguments, is_bf16_supported\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project configs\n",
    "seed = 69\n",
    "lang = 'id' # 'en' | 'id'\n",
    "task = 'wikipedia' # 'wikipedia' | 'gsm8k'\n",
    "\n",
    "# Data Configs\n",
    "train_size = 5000\n",
    "test_size = 1000\n",
    "max_seq_length = 1024\n",
    "hf_data_id = 'wikimedia/wikipedia' # 'wikimedia/wikipedia' | 'openai/gsm8k'\n",
    "hf_data_dir = '20231101.id' # 'wikipedia': '20231101.en' | '20231101.id' || 'gsm8k': 'main'\n",
    "hf_data_split = f'train[:{(train_size+test_size)}]'\n",
    "\n",
    "# Model configs\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# LoRA configs\n",
    "lora_target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "\n",
    "resume_model_id = 'alxxtexxr/L3.1-8B-wikipedia-id-8K-LoRA-v20250627140551'\n",
    "resume_from_checkpoint = bool(resume_model_id)\n",
    "if resume_from_checkpoint:\n",
    "    hub_model_id = resume_model_id\n",
    "    project_name = hub_model_id.split('/')[-1]\n",
    "    model_name = project_name\n",
    "\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(repo_id=hub_model_id, local_dir=model_name)\n",
    "else:\n",
    "    model_name = 'unsloth/Meta-Llama-3.1-8B'\n",
    "    project_name = f'L3.1-8B-{task}-{lang}-{train_size}K-LoRA-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    hub_model_id = f'alxxtexxr/{project_name}'\n",
    "print(\"Resume from checkpoint:\", resume_from_checkpoint)\n",
    "print(\"Project name:\", project_name)\n",
    "print(\"Hugging Face model ID:\", hub_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    random_state=seed,\n",
    "    target_modules=lora_target_modules,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,   \n",
    "    lora_dropout=0, # Supports any, but = 0 is optimized\n",
    "    bias='none',    # Supports any, but = 'none' is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=False, # True or 'unsloth' for very long context\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(hf_data_id, data_dir=hf_data_dir, split=hf_data_split)\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "def format_gsm8k_prompts(examples):\n",
    "    gsm8k_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{question}\n",
    "\n",
    "### Answer: \n",
    "{answer}\"\"\" + eos_token\n",
    "    \n",
    "    return {'text': [gsm8k_prompt.format(question=question, answer=answer) for question, answer in zip(examples['question'], examples['answer'])]}\n",
    "\n",
    "def format_prompts(examples):\n",
    "    return {'text': [example + eos_token for example in examples['text']]}\n",
    "\n",
    "if task == 'gsm8k':\n",
    "    dataset = dataset.map(format_gsm8k_prompts, batched=True)\n",
    "else:\n",
    "    dataset = dataset.map(format_prompts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset.train_test_split(train_size=train_size, test_size=test_size, seed=seed)\n",
    "print(dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for row in dataset_split['train'][:3][\"text\"]:\n",
    "    print(\"================================================================\")\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_split['train'],\n",
    "    # eval_dataset=dataset_split['test'],\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=8,\n",
    "\n",
    "    args=TrainingArguments(\n",
    "        seed=seed,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        # max_steps=3, # For debugging\n",
    "        warmup_ratio=0.05,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type='cosine',\n",
    "        optim='paged_adamw_8bit', # 'paged_adamw_8bit' | 'adamw_8bit'\n",
    "        weight_decay=0.00,\n",
    "        max_grad_norm=0.3,\n",
    "        fp16=(not is_bf16_supported()),\n",
    "        bf16=is_bf16_supported(),\n",
    "\n",
    "        # Eval arguments\n",
    "        # eval_strategy='steps',\n",
    "        # eval_steps=10,\n",
    "        \n",
    "        # Logging arguments\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=1,\n",
    "        # logging_first_step=True,\n",
    "        report_to=['tensorboard', 'wandb'],\n",
    "\n",
    "        # Saving arguments\n",
    "        save_strategy='steps',\n",
    "        save_steps=50,\n",
    "        # save_steps=1, # For debugging\n",
    "        save_total_limit=5, # 1 best + 4 recent checkpoints. Warning: It doesn't work\n",
    "        \n",
    "        # With load_best_model_at_end=True, your save_strategy will be ignored and default to eval_strategy.\n",
    "        # So you will find one checkpoint at the end of each epoch.\n",
    "        # https://discuss.huggingface.co/t/trainer-not-saving-after-save-steps/5464\n",
    "        # load_best_model_at_end=True, \n",
    "\n",
    "        output_dir=project_name,\n",
    "        hub_model_id=hub_model_id,\n",
    "        push_to_hub=True,\n",
    "\n",
    "        hub_strategy='all_checkpoints',\n",
    "        hub_always_push=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
