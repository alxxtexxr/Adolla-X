{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kill all processess on GPU\n",
    "!fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required libraries (optimized for Colab/Kaggle notebooks)\n",
    "import os\n",
    "if 'COLAB_' not in ''.join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks and Kaggle notebooks!\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "    %pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    %pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project config\n",
    "seed = 69 # Nice.\n",
    "\n",
    "# Model config\n",
    "max_seq_length = 1024\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# LoRA config\n",
    "hf_lora_id = 'alxxtexxr/L3.1-8B-wikipedia-en-LoRA-v20250305134947'\n",
    "lora_dir = hf_lora_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained LoRA\n",
    "snapshot_download(\n",
    "    repo_id=hf_lora_id, \n",
    "    local_dir=lora_dir, \n",
    "    # ignore_patterns='checkpoint-*/*',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_custom_modules': None,\n",
      " 'alpha_pattern': {},\n",
      " 'auto_mapping': None,\n",
      " 'base_model_name_or_path': 'unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit',\n",
      " 'bias': 'none',\n",
      " 'eva_config': None,\n",
      " 'exclude_modules': None,\n",
      " 'fan_in_fan_out': False,\n",
      " 'inference_mode': True,\n",
      " 'init_lora_weights': True,\n",
      " 'layer_replication': None,\n",
      " 'layers_pattern': None,\n",
      " 'layers_to_transform': None,\n",
      " 'loftq_config': {},\n",
      " 'lora_alpha': 16,\n",
      " 'lora_bias': False,\n",
      " 'lora_dropout': 0,\n",
      " 'megatron_config': None,\n",
      " 'megatron_core': 'megatron.core',\n",
      " 'modules_to_save': None,\n",
      " 'peft_type': <PeftType.LORA: 'LORA'>,\n",
      " 'r': 8,\n",
      " 'rank_pattern': {},\n",
      " 'revision': None,\n",
      " 'runtime_config': LoraRuntimeConfig(ephemeral_gpu_offload=False),\n",
      " 'target_modules': {'down_proj',\n",
      "                    'gate_proj',\n",
      "                    'k_proj',\n",
      "                    'o_proj',\n",
      "                    'q_proj',\n",
      "                    'up_proj',\n",
      "                    'v_proj'},\n",
      " 'task_type': 'CAUSAL_LM',\n",
      " 'use_dora': False,\n",
      " 'use_rslora': False}\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig.from_pretrained(lora_dir)\n",
    "pprint(lora_config.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae862a2a86249db91bde8090c4c6191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeeed7bb8f2e42f586e4fe62a72b06d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d912914a8c4b98a5156cdc7d8b0429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('unsloth/Meta-Llama-3.1-8B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preheat the oven to 350 degrees and place the cookie dough on a baking sheet.  Bake for 10-12 minutes, until the edges are just barely golden.  Remove from the oven and allow to cool for 5 minutes.  Remove the cookies from the baking sheet and place on a wire rack\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=50)\n",
    "print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
