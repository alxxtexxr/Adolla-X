{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download, create_repo, upload_folder\n",
    "from safetensors.torch import load_file, save_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(\n",
    "        repo_id, \n",
    "        checkpoint=None, \n",
    "        max_checkpoints=10_000, \n",
    "        checkpoint_steps=25,\n",
    "    ):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_steps) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dir = os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir, checkpoint_dir\n",
    "\n",
    "def check_loss_and_grad_norm(model, tokenizer, prompt=\"Paris is the capital of\"):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    print(\"Loss:\", outputs.loss)\n",
    "\n",
    "    # Backward pass\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for p in [p for n, p in model.named_parameters() if p.requires_grad]:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=32, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id,\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang, _ = lora_repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    split = f'train[:{(train_size+test_size)}]'\n",
    "\n",
    "    # Load dataset\n",
    "    # TODO: Use streaming to not download the entire dataset\n",
    "    # dataset = load_dataset(data_id, data_dir=data_dir, split=split)\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_grad_norm(params):\n",
    "    grad_norm = 0.0\n",
    "    for p in params:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c7a70d59be4e7d828db14f373238e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3852cab82cb43c1bfc779a4877cdb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5686f25fd2684acea27ab735be41902d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adad2753786244d2b1695c909bc73a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb6cc5f12e64155a076cba441d682c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/871 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e3c1e20d3c4cf18dc6167eab52b0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f32a730a362451f9b7ac55c79b6cb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1e1e7b5e15473fa86102300fb5e687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/43.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39fac2cc9124b8f88af8bca682a58f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/871 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b8e4c18f4548549ee2b9999710c6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7851632de824bdfbd39081940d319c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d06b9f0d8304237a9751ca7cc286f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd445771ec7f434aaf1f513b7e2e8d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c87bc96e5ae42bab04ce95d28551779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f0c5ca7e854db7b36de4839123be04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b158e028012449280390d22231ba13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a324e4ab214bfc9351c1fd35d3a2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)t.tfevents.1751286655.ec3c369f91a7.385.0:   0%|          | 0.00/402k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e845e4683f04daca79c506f895fa3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bdbaaf7fcc45089d2bbc69dc9d7cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd01586686045069924207865e48762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec283a0944c48f09445df174336116b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b78cf9b70924e928947ffac7f981bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28602905de0e4278865c1dbc67758b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b6837edf3d4b5db8306d2fb4331784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "target_lang = 'ja' # 'en' | 'id' | 'es'\n",
    "target_task = 'wikipedia' # 'wikipedia' | 'gsm8k'\n",
    "device = 'auto' # 'cpu' | 'cuda' | 'auto'\n",
    "\n",
    "# Data configuration\n",
    "train_size = 5000\n",
    "test_size = 0 # Temporary\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 5.0\n",
    "lr = 2e-4\n",
    "warmup_ratio = 0.05\n",
    "checkpoint_steps = 50\n",
    "generate_steps = 50\n",
    "sample_prompt = '日本は'\n",
    "\n",
    "# Model configurations\n",
    "hf_lora_id = 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650'\n",
    "checkpoint = 650\n",
    "\n",
    "_, lora_checkpoint_dir = download_hf_model(hf_lora_id, checkpoint)\n",
    "lora_path = os.path.join(lora_checkpoint_dir, 'adapter_model.safetensors')\n",
    "lora_config = LoraConfig.from_pretrained(lora_checkpoint_dir)\n",
    "\n",
    "base_model_name = lora_config.base_model_name_or_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "print(\"[CONFIG] Base model name:\", base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading Nero checkpoint at step 250 from Hugging Face repository: alxxtexxr/L3.1-8B-wikipedia-ja-5K-Nero-v20250805100041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fc1eb54aa24189a546f59c258ffd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb43f89fba5452aa048f9b78dec3f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f2611783054e90a00a3d535673c048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d323ed0ff31147659094f9a877b885cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a1cd667d3d488791603c80bbc9036d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ebc11355814d5eb6816914e1399a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c281e71c8b4ac0854d7d9cbad754d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/93.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0f7457a5a44b9ba562ae1698dcd00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/188M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Nero checkpoint downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face configuration\n",
    "hf_nero_id = 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-Nero-v20250805100041'\n",
    "resume_step = 250\n",
    "\n",
    "if hf_nero_id is not None and resume_step > 0:\n",
    "    print(f\"[INFO] Downloading Nero checkpoint at step {resume_step} from Hugging Face repository:\", hf_nero_id)\n",
    "    nero_dir, _ = download_hf_model(hf_nero_id, resume_step)\n",
    "    print(f\"[INFO] Nero checkpoint downloaded successfully!\")\n",
    "else:\n",
    "    hf_username = 'alxxtexxr'\n",
    "    nero_dir = f'L3.1-8B-{target_task}-{target_lang}-{train_size//1000}K-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    print(f\"[INFO] Creating Nero directory:\", nero_dir)\n",
    "    hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "    os.makedirs(nero_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Nero directory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee9030a385f46c986952f2512da4aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d9bc9c208e48c7815bfb0dbca1e48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e759ac7dce0409fbc91e09b3a6d3a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 \n",
    "                 # For debugging \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.nero_bias = nero_bias\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # For debugging\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.nero_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        nero_out = F.relu(self.nero_B(self.nero_A(self.dropout(lora_out))) * self.scaling)\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "\n",
    "        output = base_out + nero_out\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "    def load_nero_params(self, state_dict, prefix):\n",
    "        self.nero_A.weight.data = state_dict[f'{prefix}.nero_A.weight'].to(self.device)\n",
    "        self.nero_B.weight.data = state_dict[f'{prefix}.nero_B.weight'].to(self.device)\n",
    "        if self.nero_bias:\n",
    "            self.nero_A.bias.data = state_dict[f'{prefix}.nero_A.bias'].to(self.device)\n",
    "            self.nero_B.bias.data = state_dict[f'{prefix}.nero_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_return_nero_outputs(self, return_nero_outputs: bool):\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_nero_output = return_nero_outputs\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        print(\"[INFO] All layers are frozen except Nero layers!\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        print(\"[INFO] All layers are unfrozen!\")\n",
    "    \n",
    "    def load_lora_params(self, lora_path):\n",
    "        if not os.path.exists(lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] LoRA file not found:\", lora_path)\n",
    "        \n",
    "        if lora_path.endswith('.bin'):\n",
    "            state_dict = torch.load(lora_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(lora_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def load_nero_params(self, nero_path):\n",
    "        if not os.path.exists(lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] Nero file not found:\", lora_path)\n",
    "        \n",
    "        if nero_path.endswith('.bin'):\n",
    "            state_dict = torch.load(nero_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(nero_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.nero_A.weight' in state_dict and f'{nero_layer_name}.nero_B.weight' in state_dict:\n",
    "                nero_layer.load_nero_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] Nero parameters loaded successfully!\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(*args, **kwargs)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device)\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    lora_config, \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=False,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] All layers are frozen except Nero layers!\n",
      "\n",
      "Loss: tensor(14.7169, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 1.605727714927968\n"
     ]
    }
   ],
   "source": [
    "nero_model.freeze_all_except_nero()\n",
    "print()\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : -0.0013564765686169267\n",
      "- Min     : -1.302886724472046\n",
      "- Max     : 1.3925155401229858\n",
      "\n",
      "[INFO] LoRA parameters loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n",
      "\n",
      "Loss: tensor(14.7169, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 1.84030896092776\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "nero_model.load_lora_params(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gradient checkpointing enabled!\n",
      "\n",
      "Loss: tensor(14.7169, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 1.84030896092776\n"
     ]
    }
   ],
   "source": [
    "nero_model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"[INFO] Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3c7f3ed861452ca90ce7ad721de5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_hf_dataset(\n",
    "    target_lang, \n",
    "    target_task, \n",
    "    train_size=train_size, \n",
    "    test_size=test_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bc419ad442479d8235adb5ab534f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (141723 > 131072). Running this sequence through the model will result in indexing errors\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ad6a65491b44478924ede22501dd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dataset_map_config(remove_columns=None):\n",
    "    return dict(\n",
    "        batched=True, \n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "if target_task == 'gsm8k':\n",
    "    eos_token = tokenizer.eos_token\n",
    "    \n",
    "    def format_gsm8k_prompt(example):\n",
    "        gsm8k_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{question}\n",
    "\n",
    "### Answer: \n",
    "{answer}\"\"\" + eos_token\n",
    "\n",
    "        return {'text': gsm8k_prompt.format(\n",
    "            question=example['question'], \n",
    "            answer=example['answer'],\n",
    "        )}\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_seq_length,\n",
    "        )\n",
    "\n",
    "    def add_labels(example):\n",
    "        example['labels'] = example['input_ids'].copy()\n",
    "        return example\n",
    "\n",
    "    dataset_formatted = dataset.map(\n",
    "        format_gsm8k_prompt, \n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_tokenized = dataset_formatted.map(\n",
    "        tokenize_fn, \n",
    "        **get_dataset_map_config(remove_columns=dataset_formatted.column_names),\n",
    "    )\n",
    "    dataset_final = dataset_tokenized.map(\n",
    "        add_labels,\n",
    "        **get_dataset_map_config(remove_columns=dataset_tokenized.column_names),\n",
    "    )\n",
    "else:\n",
    "    block_size = max_seq_length\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(example['text'])\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated = []\n",
    "        for input_ids in examples['input_ids']:\n",
    "            concatenated += input_ids\n",
    "\n",
    "        total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "        input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_mask = [[1] * block_size for _ in input_ids]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "    dataset_tokenized = dataset.map(\n",
    "        tokenize_fn,\n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_final = dataset_tokenized.map(\n",
    "        group_texts, \n",
    "        **get_dataset_map_config(remove_columns=dataset_tokenized.column_names),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total batches: 6923\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset_final, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"[INFO] Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask, labels):\n",
      "(torch.Size([4, 1024]), torch.Size([4, 1024]), torch.Size([4, 1024]))\n",
      "\n",
      "First batch text:\n",
      "念日）。\n",
      " 1929年 - 説教強盗・妻木松吉を東京で逮捕。\n",
      "\n",
      " 1934年 - レオポルド3世がベルギー王に即位。\n",
      " 1939年 - 理化学研究所が原子実験室を設置。\n",
      " 1941年 - グレン・シー ...\n",
      "\n",
      "Loss: tensor(13.9196, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 0.5556781421938904\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask, labels):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "    first_batch['labels'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer, prompt=first_batch_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'日本は Destruction_<? � Destruction\\\\controllersutzeremouth\\\\controllers Destructionemouth � sna �姫 Destruction Destruction �470_rewrite �onse470 Destruction � �_rewriteonse � Destruction � �470'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check generated text before training\n",
    "generate_text(nero_model, tokenizer=tokenizer, prompt=sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_673/1577867366.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250805_134934-5e5e6knx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/5e5e6knx' target=\"_blank\">vague-plasma-90</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/5e5e6knx' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/5e5e6knx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Resuming training from checkpoint directory: L3.1-8B-wikipedia-ja-5K-Nero-v20250805100041/checkpoint-250\n",
      "[INFO] Nero parameters loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_673/1577867366.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
      "/tmp/ipykernel_673/1577867366.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
      "/tmp/ipykernel_673/1577867366.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
      "/tmp/ipykernel_673/1577867366.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rng_state = torch.load(rng_path)\n",
      "/tmp/ipykernel_673/1577867366.py:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Resuming training from epoch 0 and step 250.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 250, loss: 9.157166481018066, lr: 0.00014450867052023122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705e82db97694b1cb603daf7b734d40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/188M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81033dd89f64130813c838c7e79c594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e7c93a440b4539a65518210b3e9378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : 日本は\n",
      "Generated: 日本は、）のしたの 、（、、、を、する、 するのは がのした、をは日いは、 した\n",
      "\n",
      "epoch: 0, step: 251, loss: 9.033392906188965, lr: 0.00014566473988439306, grad_norm: 0.9216304610373316, grad_norm_clipped: 0.9216304610373316\n",
      "epoch: 0, step: 252, loss: 8.98396110534668, lr: 0.00014566473988439306\n",
      "epoch: 0, step: 253, loss: 8.885972023010254, lr: 0.00014682080924855492, grad_norm: 1.1127268578301117, grad_norm_clipped: 1.1127268578301117\n",
      "epoch: 0, step: 254, loss: 9.002128601074219, lr: 0.00014682080924855492\n",
      "epoch: 0, step: 255, loss: 8.513808250427246, lr: 0.00014797687861271676, grad_norm: 0.9086245812910376, grad_norm_clipped: 0.9086245812910376\n",
      "epoch: 0, step: 256, loss: 8.59604549407959, lr: 0.00014797687861271676\n",
      "epoch: 0, step: 257, loss: 8.558557510375977, lr: 0.00014913294797687862, grad_norm: 1.0900000290564018, grad_norm_clipped: 1.0900000290564018\n",
      "epoch: 0, step: 258, loss: 8.637701988220215, lr: 0.00014913294797687862\n",
      "epoch: 0, step: 259, loss: 8.540786743164062, lr: 0.00015028901734104045, grad_norm: 0.9091613816614863, grad_norm_clipped: 0.9091613816614863\n",
      "epoch: 0, step: 260, loss: 8.778623580932617, lr: 0.00015028901734104045\n",
      "epoch: 0, step: 261, loss: 9.290138244628906, lr: 0.00015144508670520231, grad_norm: 2.0956607182584284, grad_norm_clipped: 2.0956607182584284\n",
      "epoch: 0, step: 262, loss: 8.830960273742676, lr: 0.00015144508670520231\n",
      "epoch: 0, step: 263, loss: 8.519754409790039, lr: 0.00015260115606936415, grad_norm: 0.9159126336229969, grad_norm_clipped: 0.9159126336229969\n",
      "epoch: 0, step: 264, loss: 8.50416088104248, lr: 0.00015260115606936415\n",
      "epoch: 0, step: 265, loss: 8.16535758972168, lr: 0.000153757225433526, grad_norm: 1.2120909423571469, grad_norm_clipped: 1.2120909423571469\n"
     ]
    }
   ],
   "source": [
    "# If `device` is not specified or set to 'auto', use model's device\n",
    "if device is None or device == 'auto':\n",
    "    device = next(iter(nero_model.parameters())).device\n",
    "\n",
    "# Set up optimizer and gradient scaler\n",
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr, foreach=False)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Set up LR scheduler\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "if warmup_ratio > 0:\n",
    "    # If `warmup_ratio` > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup # type: ignore\n",
    "    max_optimizer_steps = max_global_steps // grad_accumulation_steps\n",
    "    warmup_steps = int(warmup_ratio * max_optimizer_steps)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "else:\n",
    "    # If `warmup_ratio` is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True, # End previous run and start a new one\n",
    "    config=dict(\n",
    "        # Project configuration\n",
    "        seed = seed,\n",
    "        target_lang=target_lang,\n",
    "        target_task=target_task,\n",
    "        device = device,\n",
    "\n",
    "        # Data configuration\n",
    "        train_size = train_size,\n",
    "        test_size = test_size,\n",
    "        max_seq_length = max_seq_length,\n",
    "\n",
    "        # Training configuration\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        resume_step = resume_step,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "        lr = lr,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        checkpoint_steps = checkpoint_steps,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "global_step = resume_step\n",
    "start_epoch = 0\n",
    "\n",
    "if resume_step > 0:\n",
    "    checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{resume_step}')\n",
    "    print(f\"[INFO] Resuming training from checkpoint directory:\", checkpoint_dir)\n",
    "\n",
    "    # Load Nero parameters\n",
    "    nero_path = os.path.join(checkpoint_dir, 'adapter_model.safetensors')\n",
    "    nero_model.load_nero_params(nero_path)\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_dir, 'optimizer.pt')\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "    \n",
    "    # Move optimizer state to the correct device\n",
    "    for param in optimizer.state:\n",
    "        param_device = param.device\n",
    "        param_dtype = param.dtype\n",
    "        for key, value in optimizer.state[param].items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                optimizer.state[param][key] = value.to(device=param_device, dtype=param_dtype)\n",
    "\n",
    "    # Load scheduler state\n",
    "    scheduler_path = os.path.join(checkpoint_dir, 'scheduler.pt')\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
    "\n",
    "    # Load scaler state\n",
    "    scaler_path = os.path.join(checkpoint_dir, 'scaler.pt')\n",
    "    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "\n",
    "    # Load trainer state\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, 'trainer_state.json')\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        start_epoch = log_history[-1]['epoch'] if log_history else 0\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch} and step {resume_step}.\")\n",
    "\n",
    "    # Load RNG state for reproducibility\n",
    "    rng_path = os.path.join(checkpoint_dir, 'rng_state.pth')\n",
    "    if os.path.exists(rng_path):\n",
    "        rng_state = torch.load(rng_path)\n",
    "        random.setstate(rng_state['python'])\n",
    "        np.random.set_state(rng_state['numpy'])\n",
    "        torch.set_rng_state(rng_state['cpu'])\n",
    "        if torch.cuda.is_available() and rng_state['cuda']:\n",
    "            torch.cuda.set_rng_state_all(rng_state['cuda'])\n",
    "    \n",
    "    if resume_step % grad_accumulation_steps != 0:\n",
    "        print(\"[WARN] Resuming mid-gradient accumulation cycle. Make sure this is intended.\")\n",
    "else:\n",
    "    # If it's new training, create Hugging Face repository\n",
    "    print(f\"[INFO] Creating Hugging Face repository:\", hf_nero_id) # print the link instead\n",
    "    create_repo(repo_id=hf_nero_id, repo_type='model', exist_ok=True)\n",
    "    print(f\"[INFO] Hugging Face repository created successfully!\")\n",
    "\n",
    "# Set model to training mode\n",
    "nero_model.train()\n",
    "\n",
    "log_history = []\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of gradient accumulation cycle\n",
    "# This ensures there are no leftover gradients when resuming mid-cycle or after a previous cycle was interrupted\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Skip previously completed steps\n",
    "        if global_step < resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "        \n",
    "        # Stop training if `max_global_steps` reached\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            outputs = nero_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "\n",
    "                # Disable cache to avoid conflict with gradient checkpointing\n",
    "                use_cache=False, \n",
    "            )\n",
    "\n",
    "            # Compute loss\n",
    "            loss = outputs.loss / grad_accumulation_steps\n",
    "\n",
    "        log = {\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "            'loss': loss.item() * grad_accumulation_steps,\n",
    "        }\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update parameters only at the end of gradient accumulation cycle\n",
    "        grad_norm_log = {}\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            # Unscale gradients before computing gradient norm and applying clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # Compute gradient norm\n",
    "            grad_norm = compute_grad_norm(nero_params)\n",
    "            grad_norm_log['grad_norm'] = grad_norm\n",
    "\n",
    "            # Clip gradients\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(nero_params, clip_grad_norm)\n",
    "            \n",
    "            # Compute clipped gradient norm\n",
    "            grad_norm_clipped = compute_grad_norm(nero_params)\n",
    "            grad_norm_log['grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "            # Update parameters\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Zero gradients for the next gradient accumulation cycle\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Logging\n",
    "        log = {\n",
    "            **log, \n",
    "            'lr': scheduler.get_last_lr()[0], \n",
    "            **grad_norm_log,\n",
    "        }\n",
    "        log_history.append(log)\n",
    "        wandb.log(log)\n",
    "        print(\", \".join(f\"{k}: {v}\" for k, v in log.items()))\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step > 0 and global_step % checkpoint_steps == 0:\n",
    "            # Create checkpoint directory\n",
    "            checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save Nero parameters, along with optimizer, scheduler, and scaler states\n",
    "            nero_state_dict = {n: p.detach().cpu() for n, p in nero_model.named_parameters() if p.requires_grad}\n",
    "            save_file(nero_state_dict, os.path.join(checkpoint_dir, 'adapter_model.safetensors'))  # NEW\n",
    "            torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, 'optimizer.pt'))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(checkpoint_dir, 'scheduler.pt'))\n",
    "            torch.save(scaler.state_dict(), os.path.join(checkpoint_dir, 'scaler.pt'))\n",
    "\n",
    "            # Save trainer state for resuming training\n",
    "            trainer_state = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'log_history': log_history,\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'w') as f:\n",
    "                json.dump(trainer_state, f, indent=2)\n",
    "\n",
    "            # Save RNG state for reproducibility\n",
    "            rng_state = {\n",
    "                'python': random.getstate(),\n",
    "                'numpy': np.random.get_state(),\n",
    "                'cpu': torch.get_rng_state(),\n",
    "                'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else [],\n",
    "            }\n",
    "            torch.save(rng_state, os.path.join(checkpoint_dir, 'rng_state.pth'))\n",
    "\n",
    "            # Upload checkpoint directory to Hugging Face repository\n",
    "            upload_folder(\n",
    "                folder_path=checkpoint_dir,\n",
    "                repo_id=hf_nero_id,\n",
    "                path_in_repo=f\"checkpoint-{global_step}\",\n",
    "                commit_message=f\"Add checkpoint at step {global_step}\",\n",
    "                repo_type='model',\n",
    "            )\n",
    "        \n",
    "        # Check generated text every `generate_steps`\n",
    "        if global_step > 0 and global_step % generate_steps == 0:\n",
    "            generated = generate_text(nero_model, tokenizer, sample_prompt, device=device)\n",
    "            print()\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT\")\n",
    "            print(\"================================\")\n",
    "            print(f\"{'Prompt':<9}:\", sample_prompt)\n",
    "            print(f\"{'Generated':<9}:\", generated)\n",
    "            print()\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
