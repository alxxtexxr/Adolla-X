{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(repo_id, checkpoint=None, max_checkpoints=2000, checkpoint_step=25):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_step) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        return os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, skip_special_tokens=True):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'].to(device), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=skip_special_tokens)[0])\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id,\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang, _ = lora_repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    split = f'train[:{(train_size+test_size)}]'\n",
    "\n",
    "    # Load dataset\n",
    "    # TODO: Use streaming to not download the entire dataset\n",
    "    # dataset = load_dataset(data_id, data_dir=data_dir, split=split)\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf94e72bb43249649e464ee8dcdc5a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f8b245995d48d5b5e9bddd9c7a5757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "- source:\n",
      "  - label     : L1T1\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650/adapter_model.safetensors\n",
      "- target:\n",
      "  - label     : L2T1\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650/adapter_model.safetensors\n",
      "\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "lora_model_device = 'cuda:0'\n",
    "nero_model_device = 'cuda:1'\n",
    "\n",
    "# Training configuration\n",
    "block_size = 96\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = 100\n",
    "resume_step = 0\n",
    "lr = 1e-4\n",
    "\n",
    "model_configs = {\n",
    "    # L1T1 (Source Language - Source Task)\n",
    "    'source': {\n",
    "        'label': 'L1T1',\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L2T1 (Target Language - Source Task)\n",
    "    'target': {\n",
    "        'label': 'L2T1',\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L1T2 (Source Language - Target Task)\n",
    "    # 'target': {\n",
    "    #     'label': 'L1T2',\n",
    "    #     'hf_lora_id': 'alxxtexxr/L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457',\n",
    "    #     'checkpoint': 1875,\n",
    "    # },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    lora_dir = download_hf_model(config['hf_lora_id'], config['checkpoint'])\n",
    "    model_configs[key]['lora_dir'] = lora_dir\n",
    "    model_configs[key]['lora_path'] = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "    model_configs[key]['lora_config'] = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "for key, config in model_configs.items():\n",
    "    print(f\"- {key}:\")\n",
    "    for config_name, config_value in config.items():\n",
    "        if config_name == 'lora_config':\n",
    "            continue\n",
    "        print(f\"{'-':>3} {config_name:<10}: {config_value}\")\n",
    "print()\n",
    "\n",
    "assert model_configs['source']['lora_config'].base_model_name_or_path == model_configs['target']['lora_config'].base_model_name_or_path, \"Base models must be the same\"\n",
    "base_model_name = model_configs['source']['lora_config'].base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/bnb.py\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 return_lora_output=False, debug=False):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_lora_output = return_lora_output\n",
    "        self.debug = debug\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"================================================================\")\n",
    "            print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "            print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        if requires_conversion:\n",
    "            lora_out = lora_out.to(base_out.dtype)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "            print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        output = base_out + lora_out\n",
    "\n",
    "        if self.return_lora_output:\n",
    "            return output, lora_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class LoraModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, \n",
    "                 return_lora_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        self.return_lora_outputs = return_lora_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "    \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                lora_layer = LoraLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    return_lora_output=self.return_lora_outputs,\n",
    "                    debug=self.debug,\n",
    "                )\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = lora_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "\n",
    "    def set_return_lora_outputs(self, return_lora_outputs: bool):\n",
    "        self.return_lora_outputs = return_lora_outputs\n",
    "        for layer in self.lora_layers.values():\n",
    "            layer.return_lora_output = return_lora_outputs\n",
    "\n",
    "    def freeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for lora_layer in self.lora_layers.values():\n",
    "            for param in lora_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_params(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for lora_layer_name, lora_layer in self.lora_layers.items():\n",
    "            lora_layer_name = lora_layer_name.replace('__DOT__', '.')\n",
    "            lora_layer_name = prefix + lora_layer_name\n",
    "            if f'{lora_layer_name}.lora_A.weight' in state_dict and f'{lora_layer_name}.lora_B.weight' in state_dict:\n",
    "                lora_layer.load_lora_params(state_dict, lora_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_lora_outputs:\n",
    "            lora_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, lora_out = _out\n",
    "                    lora_outs[layer_name] = lora_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.lora_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, lora_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_lora_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=lora_model_device)\n",
    "lora_model = LoraModel(\n",
    "    base_lora_model, \n",
    "    model_configs['target']['lora_config'],\n",
    "    return_lora_outputs=True,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : -0.002439698437228799\n",
      "- Min     : -1.4944459199905396\n",
      "- Max     : 1.4414706230163574\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 2.2152138626552187e-05\n",
      "- Min     : -0.06327299773693085\n",
      "- Max     : 0.0625513345003128\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(lora_model)\n",
    "print()\n",
    "\n",
    "lora_model.load_lora_params(model_configs['target']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.freeze_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.train()\n",
    "device = next(lora_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "lora_model_outs = lora_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_lora_logits.requires_grad False\n",
      "_lora_logits.grad_fn None\n",
      "next(iter(_lora_outs.values()).requires_grad False\n",
      "next(iter(_lora_outs.values()).grad_fn None\n"
     ]
    }
   ],
   "source": [
    "_lora_logits = lora_model_outs[0].logits\n",
    "_lora_outs = lora_model_outs[1]\n",
    "print(\"_lora_logits.requires_grad\", _lora_logits.requires_grad)\n",
    "print(\"_lora_logits.grad_fn\", _lora_logits.grad_fn)\n",
    "print(\"next(iter(_lora_outs.values()).requires_grad\", next(iter(_lora_outs.values())).requires_grad)\n",
    "print(\"next(iter(_lora_outs.values()).grad_fn\", next(iter(_lora_outs.values())).grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preheat the oven to 350 degrees and place the cookie dough in the refrigerator for 10 minutes. Remove the cookie dough from the refrigerator and slice into 1/4-inch thick slices. Place the cookie dough slices on a baking sheet lined with parchment paper or a non-stick baking mat. Bake the cookies\n"
     ]
    }
   ],
   "source": [
    "lora_model.eval()\n",
    "lora_model.set_return_lora_outputs(False)\n",
    "generate_text(\n",
    "    lora_model, \n",
    "    tokenizer, \n",
    "    prompt=\"Preheat the oven to 350 degrees and place the cookie dough\",\n",
    ")\n",
    "lora_model.set_return_lora_outputs(True)\n",
    "lora_model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nero Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 # For debugging \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # For debugging\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.nero_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"================================================================\")\n",
    "            print(self.module_name)\n",
    "            print(\"================================================================\")\n",
    "            print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "            print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "            print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # nero_out = F.relu(self.nero_B(self.nero_A(self.dropout(lora_out))) * self.scaling)\n",
    "        nero_dropout_out = self.dropout(lora_out)\n",
    "        nero_A_out = self.nero_A(nero_dropout_out)\n",
    "        nero_B_out = self.nero_B(nero_A_out)\n",
    "        nero_scaling_out = nero_B_out * self.scaling\n",
    "        nero_out = F.relu(nero_scaling_out)\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"nero_out.requires_grad:\", nero_out.requires_grad)\n",
    "            print(\"nero_out.grad_fn:\", nero_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "            nero_out_has_nan = torch.isnan(nero_out).any()\n",
    "            if nero_out_has_nan:\n",
    "                print(\"!!! NERO OUT HAS NAN !!!\")\n",
    "                print(\"nero_out:\")\n",
    "                print(nero_out)\n",
    "                print()\n",
    "                print(\"nero_scaling_out:\")\n",
    "                print(nero_scaling_out)\n",
    "                print()\n",
    "                print(\"nero_B_out:\")\n",
    "                print(nero_B_out)\n",
    "                print()\n",
    "                print(\"nero_A_out:\")\n",
    "                print(nero_A_out)\n",
    "                print()\n",
    "                print(\"nero_dropout_out:\")\n",
    "                print(nero_dropout_out)\n",
    "                print()\n",
    "                print(\"lora_out:\")\n",
    "                print(lora_out)\n",
    "                print()\n",
    "\n",
    "        # Add `base_out` with gradients-detached `nero_out`, \n",
    "        # so that `base_out` does not carry gradients\n",
    "        # nero_out_detached = nero_out.detach()\n",
    "\n",
    "        # if self.debug:\n",
    "        #     print(\"nero_out_detached.requires_grad:\", nero_out_detached.requires_grad)\n",
    "        #     print(\"nero_out_detached.grad_fn:\", nero_out_detached.grad_fn)\n",
    "        #     print()\n",
    "\n",
    "        # output = base_out + nero_out_detached\n",
    "        output = base_out + nero_out\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"output.requires_grad:\", output.requires_grad)\n",
    "            print(\"output.grad_fn:\", output.grad_fn)\n",
    "            print()\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_return_nero_outputs(self, return_nero_outputs: bool):\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_nero_output = return_nero_outputs\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_params(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=nero_model_device)\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    model_configs['source']['lora_config'], \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=True,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : -0.0019485268276184797\n",
      "- Min     : -1.4076730012893677\n",
      "- Max     : 1.3847874402999878\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "nero_model.load_lora_params(model_configs['source']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nero_model.freeze_all_except_nero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nero_model.train()\n",
    "device = next(nero_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "nero_model_outs = nero_model(\n",
    "    input_ids=inputs['input_ids'].to(device), \n",
    "    attention_mask=inputs['attention_mask'].to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_nero_logits.requires_grad True\n",
      "_nero_logits.grad_fn <UnsafeViewBackward0 object at 0x7b51d4ef78e0>\n",
      "next(iter(_nero_outs.values()).requires_grad True\n",
      "next(iter(_nero_outs.values()).grad_fn <ToCopyBackward0 object at 0x7b51d4ef78e0>\n"
     ]
    }
   ],
   "source": [
    "_nero_logits = nero_model_outs[0].logits\n",
    "_nero_outs = nero_model_outs[1]\n",
    "print(\"_nero_logits.requires_grad\", _nero_logits.requires_grad)\n",
    "print(\"_nero_logits.grad_fn\", _nero_logits.grad_fn)\n",
    "print(\"next(iter(_nero_outs.values()).requires_grad\", next(iter(_nero_outs.values())).requires_grad)\n",
    "print(\"next(iter(_nero_outs.values()).grad_fn\", next(iter(_nero_outs.values())).grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preheat the oven to 350 degrees and place the cookie dough Destruction_<?_<?_<? gloss_<? �_<?470_<?_<? сбор_<?_<?_<? �_<? �芳 � сборhton Destruction_<?_<?vak470_<? Shore470_<?onse�姫MOOTH470 �470 gloss Destruction Satoshi470� �_<?_<? Destruction_<? gloss Rica\n"
     ]
    }
   ],
   "source": [
    "nero_model.eval()\n",
    "nero_model.set_return_nero_outputs(False)\n",
    "generate_text(\n",
    "    nero_model, \n",
    "    tokenizer, \n",
    "    prompt=\"Preheat the oven to 350 degrees and place the cookie dough\",\n",
    ")\n",
    "nero_model.set_return_nero_outputs(True)\n",
    "nero_model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_hf_dataset_from_lora(model_configs['target']['hf_lora_id'], test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3416cc8f117c4e929366d92398c3e5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (141723 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8a1a9ff6c14c0f8c40ef5ca7e8f031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'gsm8k' in model_configs['target']['hf_lora_id']:\n",
    "    def format_prompt(example):\n",
    "        gsm8k_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{question}\n",
    "\n",
    "### Answer: \n",
    "{answer}\"\"\"\n",
    "\n",
    "        return {'text': gsm8k_prompt.format(\n",
    "            question=example['question'], \n",
    "            answer=example['answer'],\n",
    "        )}\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=block_size,\n",
    "        )\n",
    "\n",
    "    def add_labels(example):\n",
    "        example['labels'] = example['input_ids'].copy()\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(format_prompt)\n",
    "    dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset.column_names)\n",
    "    dataset = dataset.map(add_labels)\n",
    "else:\n",
    "    eos_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token = eos_token\n",
    "\n",
    "    # Tokenize dataset\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(example['text'])\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        tokenize_fn, \n",
    "        batched=True, \n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # Concatenate all tokens into one long stream, then split into blocks\n",
    "    def group_texts(examples):\n",
    "        concatenated = []\n",
    "        for input_ids in examples['input_ids']:\n",
    "            concatenated += input_ids\n",
    "\n",
    "        total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "        input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_mask = [[1] * block_size for _ in input_ids]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        group_texts, \n",
    "        batched=True, \n",
    "        batch_size=1000,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 73856\n",
      "First batch input IDs shape: torch.Size([4, 96])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"Total batches:\", len(train_loader))\n",
    "print(\"First batch input IDs shape:\", next(iter(train_loader))['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _debugged = False\n",
    "# _layer_name = None\n",
    "# _nero_out = None\n",
    "# _lora_out = None\n",
    "# _mse_loss_unnormed = None\n",
    "# _nero_out_sum = None\n",
    "\n",
    "# def loss_fn_v1(nero_outs, lora_outs):\n",
    "#     assert nero_outs.keys() == lora_outs.keys() # TODO: Print warning message\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name]\n",
    "\n",
    "#         # Normalized MSE loss\n",
    "#         # mse_loss = F.mse_loss(nero_out, lora_out, reduction='sum') / torch.sum(nero_outs[layer_name] ** 2)\n",
    "#         mse_loss_unnormed = F.mse_loss(nero_out, lora_out, reduction='sum')\n",
    "#         nero_out_sum = torch.sum(nero_outs[layer_name] ** 2)\n",
    "#         mse_loss = mse_loss_unnormed / nero_out_sum\n",
    "\n",
    "#         print(\"================================================================\")\n",
    "#         print(layer_name)\n",
    "#         print(\"================================================================\")\n",
    "#         print(\"mse_loss:\", mse_loss)\n",
    "#         print(\"nero_out_sum:\", nero_out_sum)\n",
    "#         print(\"mse_loss_unnormed:\", mse_loss_unnormed)\n",
    "#         print()\n",
    "\n",
    "#         global _debugged\n",
    "#         if (torch.isinf(nero_out_sum) or torch.isnan(mse_loss_unnormed)) and not _debugged:\n",
    "#             global _layer_name\n",
    "#             global _nero_out\n",
    "#             global _lora_out\n",
    "#             global _mse_loss_unnormed\n",
    "#             global _nero_out_sum\n",
    "#             _layer_name = layer_name\n",
    "#             _nero_out = nero_out\n",
    "#             _lora_out = lora_out\n",
    "#             _mse_loss_unnormed = mse_loss_unnormed\n",
    "#             _nero_out_sum = nero_out_sum\n",
    "#             _debugged = True\n",
    "\n",
    "#         if torch.isnan(mse_loss):\n",
    "#             nero_out_has_nan = torch.isnan(nero_out).any()\n",
    "#             lora_out_has_nan = torch.isnan(lora_out).any()\n",
    "\n",
    "#             print(\"nero_out_has_nan:\", nero_out_has_nan)\n",
    "#             print(\"lora_out_has_nan:\", lora_out_has_nan)\n",
    "#             print()\n",
    "\n",
    "#             # if nero_out_has_nan:\n",
    "#             print(\"nero_out:\")\n",
    "#             print(nero_out)\n",
    "#             print()\n",
    "            \n",
    "#             # if lora_out_has_nan:\n",
    "#             print(\"lora_out:\")\n",
    "#             print(lora_out)\n",
    "#             print()\n",
    "\n",
    "#         total_loss += mse_loss\n",
    "\n",
    "#     return total_loss / len(lora_outs)  # Averaging loss across layers\n",
    "\n",
    "# # loss = loss_func_v1(nero_model_outs[1], lora_model_outs[1])\n",
    "# # print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn_v2(nero_outs, lora_outs):\n",
    "#     assert nero_outs.keys() == lora_outs.keys()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         print(\"================================================================\")\n",
    "#         print(layer_name)\n",
    "#         print(\"================================================================\")\n",
    "\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name]\n",
    "\n",
    "#         diff = nero_out - lora_out\n",
    "#         print(\"mean(abs(diff)):\", diff.abs().mean())\n",
    "#         print(\"mean((diff)^2):\", (diff ** 2).mean())\n",
    "#         print()\n",
    "        \n",
    "#         print(\"max abs(diff):\", diff.abs().max().item())\n",
    "#         print(\"mean abs(diff):\", diff.abs().mean().item())\n",
    "#         print(\"std abs(diff):\", diff.abs().std().item())\n",
    "#         print()\n",
    "\n",
    "#         # Use mean MSE to prevent overflow and keep scale uniform\n",
    "#         mse_loss = F.mse_loss(nero_out.float(), lora_out.float(), reduction='mean')  # Compute in float32\n",
    "\n",
    "#         print(\"mse_loss:\", mse_loss)\n",
    "#         print()\n",
    "\n",
    "#         total_loss += mse_loss\n",
    "\n",
    "#     return total_loss / len(lora_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _debugged = False\n",
    "# _nero_out = None\n",
    "# _lora_out = None\n",
    "\n",
    "# def loss_fn_v3(nero_outs, lora_outs, debug=False):\n",
    "#     assert nero_outs.keys() == lora_outs.keys()\n",
    "#     loss_device = next(iter(nero_outs.values())).device\n",
    "#     total_loss = torch.tensor(0.0, device=loss_device)\n",
    "    \n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name].to(nero_out.device)\n",
    "\n",
    "#         # Use mean MAE to prevent overflow and keep scale uniform\n",
    "#         # mae_loss = F.l1_loss(nero_out.float(), lora_out.float(), reduction='mean')  \n",
    "#         # mae_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()))\n",
    "#         mae_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()), dim=-1).mean() # scale by sequence length\n",
    "\n",
    "#         if debug:\n",
    "#             print(\"================================================================\")\n",
    "#             print(layer_name)\n",
    "#             print(\"================================================================\")\n",
    "            \n",
    "#             global _debugged\n",
    "#             if not _debugged:\n",
    "#                 global _nero_out\n",
    "#                 global _lora_out\n",
    "#                 _nero_out = nero_out\n",
    "#                 _lora_out = lora_out\n",
    "#                 _debugged = True\n",
    "\n",
    "#             diff = nero_out - lora_out\n",
    "#             print(\"mean(abs(diff)):\", diff.abs().mean())\n",
    "#             print(\"mean((diff)^2):\", (diff ** 2).mean())\n",
    "#             print()\n",
    "            \n",
    "#             print(\"max abs(diff):\", diff.abs().max().item())\n",
    "#             print(\"mean abs(diff):\", diff.abs().mean().item())\n",
    "#             print(\"std abs(diff):\", diff.abs().std().item())\n",
    "#             print()\n",
    "\n",
    "#             print(\"mae_loss:\", mae_loss)\n",
    "#             print()\n",
    "\n",
    "#         total_loss += mae_loss.to(loss_device)\n",
    "\n",
    "#     return total_loss / len(lora_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_v4(nero_outs, lora_outs, nero_logits, lora_logits, alpha=1.0, beta=0.00015, temperature=2.0, debug=False):\n",
    "    assert nero_outs.keys() == lora_outs.keys()\n",
    "    loss_device = next(iter(nero_outs.values())).device\n",
    "    total_hidden_loss = torch.tensor(0.0, device=loss_device)\n",
    "\n",
    "    # --- Hidden representation loss ---\n",
    "    for layer_name in lora_outs.keys():\n",
    "        nero_out = nero_outs[layer_name]\n",
    "        lora_out = lora_outs[layer_name].to(nero_out.device)\n",
    "\n",
    "        # Scale by sequence length to avoid length-sensitive loss scaling\n",
    "        hidden_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()), dim=-1).mean()\n",
    "\n",
    "        if debug:\n",
    "            print(\"Layer:\", layer_name)\n",
    "            print(\"hidden_loss:\", hidden_loss.item())\n",
    "\n",
    "        total_hidden_loss += hidden_loss\n",
    "\n",
    "    total_hidden_loss /= len(lora_outs)\n",
    "    total_hidden_loss *= alpha\n",
    "\n",
    "    # --- Logit KL divergence loss ---\n",
    "    # Important: apply softmax with temperature for distillation\n",
    "    logit_loss = F.kl_div(\n",
    "        F.log_softmax(nero_logits.to(loss_device) / temperature, dim=-1),\n",
    "        F.softmax(lora_logits.to(loss_device) / temperature, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    logit_loss *= beta\n",
    "\n",
    "    if debug:\n",
    "        print(\"Logit KL loss:\", logit_loss.item())\n",
    "\n",
    "    # --- Final combined loss ---\n",
    "    total_loss = total_hidden_loss + logit_loss\n",
    "    return total_loss, total_hidden_loss, logit_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3478, device='cuda:1', grad_fn=<AddBackward0>),\n",
       " tensor(0.1808, device='cuda:1', grad_fn=<MulBackward0>),\n",
       " tensor(0.1670, device='cuda:1', dtype=torch.float16, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn_v4(\n",
    "    nero_outs=_nero_outs,\n",
    "    lora_outs=_lora_outs,\n",
    "    nero_logits=_nero_logits,\n",
    "    lora_logits=_lora_logits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2581/1359036346.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b02aba99e71429393506a39209e70d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112385333338656, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250801_152523-h76w80ck</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/h76w80ck' target=\"_blank\">fresh-totem-44</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/h76w80ck' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/h76w80ck</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2581/1359036346.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/1, step: 0/100, loss: 0.3967, hidden_loss: 0.1817, logit_loss: 0.2150\n",
      "epoch: 0/1, step: 1/100, loss: 0.3899, hidden_loss: 0.1816, logit_loss: 0.2083\n",
      "epoch: 0/1, step: 2/100, loss: 0.3874, hidden_loss: 0.1813, logit_loss: 0.2061\n",
      "epoch: 0/1, step: 3/100, loss: 0.3853, hidden_loss: 0.1809, logit_loss: 0.2043\n",
      "epoch: 0/1, step: 4/100, loss: 0.4259, hidden_loss: 0.1812, logit_loss: 0.2447\n",
      "epoch: 0/1, step: 5/100, loss: 0.3534, hidden_loss: 0.1810, logit_loss: 0.1724\n",
      "epoch: 0/1, step: 6/100, loss: 0.3629, hidden_loss: 0.1808, logit_loss: 0.1821\n",
      "epoch: 0/1, step: 7/100, loss: 0.3890, hidden_loss: 0.1806, logit_loss: 0.2084\n",
      "epoch: 0/1, step: 8/100, loss: 0.3553, hidden_loss: 0.1804, logit_loss: 0.1749\n",
      "epoch: 0/1, step: 9/100, loss: 0.3580, hidden_loss: 0.1803, logit_loss: 0.1777\n",
      "epoch: 0/1, step: 10/100, loss: 0.4100, hidden_loss: 0.1806, logit_loss: 0.2294\n",
      "epoch: 0/1, step: 11/100, loss: 0.3599, hidden_loss: 0.1801, logit_loss: 0.1799\n",
      "epoch: 0/1, step: 12/100, loss: 0.3570, hidden_loss: 0.1801, logit_loss: 0.1769\n",
      "epoch: 0/1, step: 13/100, loss: 0.3660, hidden_loss: 0.1802, logit_loss: 0.1858\n",
      "epoch: 0/1, step: 14/100, loss: 0.3582, hidden_loss: 0.1798, logit_loss: 0.1784\n",
      "epoch: 0/1, step: 15/100, loss: 0.3795, hidden_loss: 0.1797, logit_loss: 0.1998\n",
      "epoch: 0/1, step: 16/100, loss: 0.3578, hidden_loss: 0.1794, logit_loss: 0.1784\n",
      "epoch: 0/1, step: 17/100, loss: 0.3605, hidden_loss: 0.1793, logit_loss: 0.1812\n",
      "epoch: 0/1, step: 18/100, loss: 0.3647, hidden_loss: 0.1790, logit_loss: 0.1857\n",
      "epoch: 0/1, step: 19/100, loss: 0.4106, hidden_loss: 0.1791, logit_loss: 0.2316\n",
      "epoch: 0/1, step: 20/100, loss: 0.3895, hidden_loss: 0.1788, logit_loss: 0.2107\n",
      "epoch: 0/1, step: 21/100, loss: 0.3477, hidden_loss: 0.1786, logit_loss: 0.1691\n",
      "epoch: 0/1, step: 22/100, loss: 0.3527, hidden_loss: 0.1784, logit_loss: 0.1743\n",
      "epoch: 0/1, step: 23/100, loss: 0.3778, hidden_loss: 0.1785, logit_loss: 0.1993\n",
      "epoch: 0/1, step: 24/100, loss: 0.3519, hidden_loss: 0.1781, logit_loss: 0.1738\n",
      "epoch: 0/1, step: 25/100, loss: 0.3625, hidden_loss: 0.1780, logit_loss: 0.1845\n",
      "epoch: 0/1, step: 26/100, loss: 0.3548, hidden_loss: 0.1776, logit_loss: 0.1772\n",
      "epoch: 0/1, step: 27/100, loss: 0.3595, hidden_loss: 0.1776, logit_loss: 0.1820\n",
      "epoch: 0/1, step: 28/100, loss: 0.3535, hidden_loss: 0.1772, logit_loss: 0.1763\n",
      "epoch: 0/1, step: 29/100, loss: 0.3578, hidden_loss: 0.1775, logit_loss: 0.1803\n",
      "epoch: 0/1, step: 30/100, loss: 0.3750, hidden_loss: 0.1769, logit_loss: 0.1981\n",
      "epoch: 0/1, step: 31/100, loss: 0.3570, hidden_loss: 0.1767, logit_loss: 0.1803\n",
      "epoch: 0/1, step: 32/100, loss: 0.3638, hidden_loss: 0.1766, logit_loss: 0.1871\n",
      "epoch: 0/1, step: 33/100, loss: 0.3756, hidden_loss: 0.1764, logit_loss: 0.1992\n",
      "epoch: 0/1, step: 34/100, loss: 0.3708, hidden_loss: 0.1761, logit_loss: 0.1948\n",
      "epoch: 0/1, step: 35/100, loss: 0.3622, hidden_loss: 0.1758, logit_loss: 0.1864\n",
      "epoch: 0/1, step: 36/100, loss: 0.3606, hidden_loss: 0.1757, logit_loss: 0.1849\n",
      "epoch: 0/1, step: 37/100, loss: 0.3513, hidden_loss: 0.1753, logit_loss: 0.1760\n",
      "epoch: 0/1, step: 38/100, loss: 0.3620, hidden_loss: 0.1753, logit_loss: 0.1867\n",
      "epoch: 0/1, step: 39/100, loss: 0.3500, hidden_loss: 0.1749, logit_loss: 0.1750\n",
      "epoch: 0/1, step: 40/100, loss: 0.3646, hidden_loss: 0.1748, logit_loss: 0.1898\n",
      "epoch: 0/1, step: 41/100, loss: 0.3675, hidden_loss: 0.1743, logit_loss: 0.1932\n",
      "epoch: 0/1, step: 42/100, loss: 0.3337, hidden_loss: 0.1741, logit_loss: 0.1596\n",
      "epoch: 0/1, step: 43/100, loss: 0.3562, hidden_loss: 0.1741, logit_loss: 0.1821\n",
      "epoch: 0/1, step: 44/100, loss: 0.3547, hidden_loss: 0.1734, logit_loss: 0.1813\n",
      "epoch: 0/1, step: 45/100, loss: 0.3346, hidden_loss: 0.1732, logit_loss: 0.1614\n",
      "epoch: 0/1, step: 46/100, loss: 0.3567, hidden_loss: 0.1727, logit_loss: 0.1839\n",
      "epoch: 0/1, step: 47/100, loss: 0.3499, hidden_loss: 0.1724, logit_loss: 0.1775\n",
      "epoch: 0/1, step: 48/100, loss: 0.3266, hidden_loss: 0.1722, logit_loss: 0.1543\n",
      "epoch: 0/1, step: 49/100, loss: 0.3402, hidden_loss: 0.1719, logit_loss: 0.1683\n",
      "epoch: 0/1, step: 50/100, loss: 0.3611, hidden_loss: 0.1716, logit_loss: 0.1895\n",
      "epoch: 0/1, step: 51/100, loss: 0.3417, hidden_loss: 0.1712, logit_loss: 0.1704\n",
      "epoch: 0/1, step: 52/100, loss: 0.3328, hidden_loss: 0.1706, logit_loss: 0.1622\n",
      "epoch: 0/1, step: 53/100, loss: 0.3580, hidden_loss: 0.1702, logit_loss: 0.1878\n",
      "epoch: 0/1, step: 54/100, loss: 0.3475, hidden_loss: 0.1702, logit_loss: 0.1773\n",
      "epoch: 0/1, step: 55/100, loss: 0.3236, hidden_loss: 0.1693, logit_loss: 0.1542\n",
      "epoch: 0/1, step: 56/100, loss: 0.3278, hidden_loss: 0.1690, logit_loss: 0.1588\n",
      "epoch: 0/1, step: 57/100, loss: 0.3350, hidden_loss: 0.1685, logit_loss: 0.1666\n",
      "epoch: 0/1, step: 58/100, loss: 0.3251, hidden_loss: 0.1680, logit_loss: 0.1572\n",
      "epoch: 0/1, step: 59/100, loss: 0.3199, hidden_loss: 0.1675, logit_loss: 0.1523\n",
      "epoch: 0/1, step: 60/100, loss: 0.3218, hidden_loss: 0.1673, logit_loss: 0.1545\n",
      "epoch: 0/1, step: 61/100, loss: 0.3350, hidden_loss: 0.1666, logit_loss: 0.1684\n",
      "epoch: 0/1, step: 62/100, loss: 0.3306, hidden_loss: 0.1660, logit_loss: 0.1646\n",
      "epoch: 0/1, step: 63/100, loss: 0.3141, hidden_loss: 0.1652, logit_loss: 0.1489\n",
      "epoch: 0/1, step: 64/100, loss: 0.3456, hidden_loss: 0.1649, logit_loss: 0.1807\n",
      "epoch: 0/1, step: 65/100, loss: 0.3487, hidden_loss: 0.1642, logit_loss: 0.1844\n",
      "epoch: 0/1, step: 66/100, loss: 0.3322, hidden_loss: 0.1635, logit_loss: 0.1687\n",
      "epoch: 0/1, step: 67/100, loss: 0.3427, hidden_loss: 0.1630, logit_loss: 0.1797\n",
      "epoch: 0/1, step: 68/100, loss: 0.3134, hidden_loss: 0.1624, logit_loss: 0.1510\n",
      "epoch: 0/1, step: 69/100, loss: 0.3189, hidden_loss: 0.1617, logit_loss: 0.1573\n",
      "epoch: 0/1, step: 70/100, loss: 0.3194, hidden_loss: 0.1612, logit_loss: 0.1582\n",
      "epoch: 0/1, step: 71/100, loss: 0.2876, hidden_loss: 0.1601, logit_loss: 0.1275\n",
      "epoch: 0/1, step: 72/100, loss: 0.2996, hidden_loss: 0.1594, logit_loss: 0.1402\n",
      "epoch: 0/1, step: 73/100, loss: 0.3067, hidden_loss: 0.1590, logit_loss: 0.1477\n",
      "epoch: 0/1, step: 74/100, loss: 0.2964, hidden_loss: 0.1582, logit_loss: 0.1382\n",
      "epoch: 0/1, step: 75/100, loss: 0.2811, hidden_loss: 0.1572, logit_loss: 0.1240\n",
      "epoch: 0/1, step: 76/100, loss: 0.2856, hidden_loss: 0.1566, logit_loss: 0.1290\n",
      "epoch: 0/1, step: 77/100, loss: 0.2858, hidden_loss: 0.1558, logit_loss: 0.1300\n",
      "epoch: 0/1, step: 78/100, loss: 0.2922, hidden_loss: 0.1553, logit_loss: 0.1369\n",
      "epoch: 0/1, step: 79/100, loss: 0.2800, hidden_loss: 0.1547, logit_loss: 0.1253\n",
      "epoch: 0/1, step: 80/100, loss: 0.3029, hidden_loss: 0.1536, logit_loss: 0.1493\n",
      "epoch: 0/1, step: 81/100, loss: 0.2820, hidden_loss: 0.1530, logit_loss: 0.1290\n",
      "epoch: 0/1, step: 82/100, loss: 0.3063, hidden_loss: 0.1523, logit_loss: 0.1539\n",
      "epoch: 0/1, step: 83/100, loss: 0.2719, hidden_loss: 0.1516, logit_loss: 0.1203\n",
      "epoch: 0/1, step: 84/100, loss: 0.2767, hidden_loss: 0.1510, logit_loss: 0.1257\n",
      "epoch: 0/1, step: 85/100, loss: 0.2907, hidden_loss: 0.1506, logit_loss: 0.1402\n",
      "epoch: 0/1, step: 86/100, loss: 0.2630, hidden_loss: 0.1497, logit_loss: 0.1133\n",
      "epoch: 0/1, step: 87/100, loss: 0.2737, hidden_loss: 0.1485, logit_loss: 0.1251\n",
      "epoch: 0/1, step: 88/100, loss: 0.2663, hidden_loss: 0.1477, logit_loss: 0.1186\n",
      "epoch: 0/1, step: 89/100, loss: 0.2723, hidden_loss: 0.1472, logit_loss: 0.1251\n",
      "epoch: 0/1, step: 90/100, loss: 0.2719, hidden_loss: 0.1464, logit_loss: 0.1255\n",
      "epoch: 0/1, step: 91/100, loss: 0.3046, hidden_loss: 0.1455, logit_loss: 0.1590\n",
      "epoch: 0/1, step: 92/100, loss: 0.2562, hidden_loss: 0.1443, logit_loss: 0.1119\n",
      "epoch: 0/1, step: 93/100, loss: 0.2731, hidden_loss: 0.1436, logit_loss: 0.1296\n",
      "epoch: 0/1, step: 94/100, loss: 0.2893, hidden_loss: 0.1429, logit_loss: 0.1463\n",
      "epoch: 0/1, step: 95/100, loss: 0.2606, hidden_loss: 0.1421, logit_loss: 0.1185\n",
      "epoch: 0/1, step: 96/100, loss: 0.2758, hidden_loss: 0.1410, logit_loss: 0.1348\n",
      "epoch: 0/1, step: 97/100, loss: 0.2758, hidden_loss: 0.1403, logit_loss: 0.1356\n",
      "epoch: 0/1, step: 98/100, loss: 0.2569, hidden_loss: 0.1393, logit_loss: 0.1176\n",
      "epoch: 0/1, step: 99/100, loss: 0.2466, hidden_loss: 0.1378, logit_loss: 0.1088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b4277b410a43008d8a8e8ecc2ebb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.024 MB of 0.024 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>hidden_loss</td><td>██████████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁</td></tr><tr><td>logit_loss</td><td>▆█▄▆▅▅▅▅▆▆▆▄▅▅▄▆▆▅▅▄▅▄▄▃▃▄▅▃▃▄▂▂▃▃▂▂▄▃▁▁</td></tr><tr><td>loss</td><td>▇▆█▅▆▅▅▆▅▅▆▅▅▅▆▆▅▆▆▅▅▅▄▄▄▅▅▃▃▂▃▂▂▃▂▃▁▃▂▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>hidden_loss</td><td>0.13782</td></tr><tr><td>logit_loss</td><td>0.10883</td></tr><tr><td>loss</td><td>0.24665</td></tr><tr><td>step</td><td>99</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-totem-44</strong> at: <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/h76w80ck' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/h76w80ck</a><br/> View project at: <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250801_152523-h76w80ck/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    config=dict(\n",
    "        seed = seed,\n",
    "        lora_model_device = lora_model_device,\n",
    "        nero_model_device = nero_model_device,\n",
    "        block_size = block_size,\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        resume_step = resume_step,\n",
    "        lr = lr,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Ensure model devices are set if not specified\n",
    "if nero_model_device is None:\n",
    "    nero_model_device = next(iter(nero_model.parameters())).device\n",
    "if lora_model_device is None:\n",
    "    lora_model_device = next(iter(lora_model.parameters())).device\n",
    "\n",
    "global_step = 0\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "done = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        if global_step < resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        # Flush gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs to devices\n",
    "        nero_input_ids = batch['input_ids'].to(nero_model_device)\n",
    "        nero_attention_mask = batch['attention_mask'].to(nero_model_device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass for nero\n",
    "            nero_model_outs, nero_outs = nero_model(\n",
    "                input_ids=nero_input_ids,\n",
    "                attention_mask=nero_attention_mask\n",
    "            )\n",
    "\n",
    "            # Forward pass for lora\n",
    "            lora_input_ids = nero_input_ids.to(lora_model_device)\n",
    "            lora_attention_mask = nero_attention_mask.to(lora_model_device)\n",
    "            lora_model_outs, lora_outs = lora_model(\n",
    "                input_ids=lora_input_ids,\n",
    "                attention_mask=lora_attention_mask\n",
    "            )\n",
    "\n",
    "            # Loss computation\n",
    "            loss, hidden_loss, logit_loss = loss_fn_v4(\n",
    "                nero_outs, \n",
    "                lora_outs, \n",
    "                nero_model_outs.logits, \n",
    "                lora_model_outs.logits,\n",
    "                debug=False,\n",
    "            )\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Compute gradient norm\n",
    "        # total_norm = 0.0\n",
    "        # for p in nero_params:\n",
    "        #     param_norm = p.grad.data.norm(2)\n",
    "        #     total_norm += param_norm.item() ** 2\n",
    "        # total_norm = total_norm ** 0.5\n",
    "        # print(f\"Gradient norm: {total_norm:.4f}\")\n",
    "        \n",
    "        # Update parameters\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Logging\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "            'loss': loss.item(),\n",
    "            'hidden_loss': hidden_loss.item(),\n",
    "            'logit_loss': logit_loss.item(),\n",
    "        })\n",
    "        print(f\"epoch: {epoch}/{num_epochs}, step: {global_step}/{max_global_steps}, loss: {loss.item():.4f}, hidden_loss: {hidden_loss.item():.4f}, logit_loss: {logit_loss.item():.4f}\")\n",
    "\n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nero parameters saved to nero_params_L1T1_to_L1T2.pth\n"
     ]
    }
   ],
   "source": [
    "# Save Nero parameters\n",
    "nero_params_path = f\"nero_params_L1T1_to_{model_configs['target']['label']}.pth\"\n",
    "lora_state_dict = {k: v for k, v in nero_model.state_dict().items() if 'nero_' in k}\n",
    "torch.save(lora_state_dict, nero_params_path)\n",
    "print(\"Nero parameters saved to: \", nero_params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "空が青い年575の年757年3766年が年年77の年637のに7年年、7の年3のの年5年77577年7年7年\n"
     ]
    }
   ],
   "source": [
    "nero_model.eval()\n",
    "nero_model.set_return_nero_outputs(False)\n",
    "generate_text(\n",
    "    nero_model, \n",
    "    tokenizer, \n",
    "    prompt=\"空が青い\",\n",
    ")\n",
    "nero_model.set_return_nero_outputs(True)\n",
    "nero_model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
