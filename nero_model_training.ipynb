{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (2.0.2)\n",
      "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from xformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->xformers) (3.0.2)\n",
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-si30wbv4/unsloth_e57d9cf3736a484baa1b311060f91a1a\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-si30wbv4/unsloth_e57d9cf3736a484baa1b311060f91a1a\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 5266ead104938c4908c7f2d2a60526555faf7e85\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: bitsandbytes>=0.45.5 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.46.1)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.2)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.7.11 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2025.7.11)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
      "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-0.9.27-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.54.0)\n",
      "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.0)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.16.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (5.29.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.34.1)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.2)\n",
      "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (25.1.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.7.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2025.7.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.20.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Downloading tyro-0.9.27-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shtab, msgspec, tyro\n",
      "Successfully installed msgspec-0.19.0 shtab-1.7.2 tyro-0.9.27\n"
     ]
    }
   ],
   "source": [
    "%pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(repo_id, checkpoint):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, 2000, 25) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    if checkpoint:\n",
    "        return os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, skip_special_tokens=True):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'].to(device), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=skip_special_tokens)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "{'L1T1': {'checkpoint': 650,\n",
      "          'lora_dir': 'L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650',\n",
      "          'lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650'},\n",
      " 'L2T1': {'checkpoint': 650,\n",
      "          'lora_dir': 'L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650',\n",
      "          'lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629'}}\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "device = 'cuda'\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 1024\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_configs = {\n",
    "    'L1T1': {\n",
    "        'lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "    'L2T1': {\n",
    "        'lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    model_configs[key]['lora_dir'] = download_hf_model(config['lora_id'], config['checkpoint'])\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "pprint(model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/bnb.py\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.11: Fast Llama patching. Transformers: 4.54.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "class LoraLayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha, dropout, lora_bias, use_rslora, return_lora_output=False):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_lora_output = return_lora_output\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        if requires_conversion:\n",
    "            lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        output = base_out + lora_out\n",
    "\n",
    "        if self.return_lora_output:\n",
    "            return output, lora_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_weights(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class LoraModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, return_lora_outputs=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        self.return_lora_outputs = return_lora_outputs\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "    \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                lora_layer = LoraLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    return_lora_output=self.return_lora_outputs,\n",
    "                )\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = lora_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "\n",
    "    def freeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for lora_layer in self.lora_layers.values():\n",
    "            for param in lora_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_weights(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for lora_layer_name, lora_layer in self.lora_layers.items():\n",
    "            lora_layer_name = lora_layer_name.replace('__DOT__', '.')\n",
    "            lora_layer_name = prefix + lora_layer_name\n",
    "            if f'{lora_layer_name}.lora_A.weight' in state_dict and f'{lora_layer_name}.lora_B.weight' in state_dict:\n",
    "                lora_layer.load_lora_weights(state_dict, lora_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_lora_outputs:\n",
    "            lora_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, lora_out = _out\n",
    "                    lora_outs[layer_name] = lora_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.lora_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, lora_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_model1, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name='unsloth/Meta-Llama-3.1-8B',\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "lora_config = LoraConfig.from_pretrained(model_configs['L1T1']['lora_dir'])\n",
    "lora_model = LoraModel(base_model1, lora_config, return_lora_outputs=True)\n",
    "# lora_model.freeze_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.train()\n",
    "# lora_model.gradient_checkpointing_enable() # Fix error: 'LlamaDecoderLayer' object has no attribute '_gradient_checkpointing_func'\n",
    "device = next(lora_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "lora_model_outs = lora_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers__DOT__0__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__self_attn__DOT__q_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__self_attn__DOT__k_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__self_attn__DOT__v_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__self_attn__DOT__o_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__mlp__DOT__gate_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__mlp__DOT__down_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model_outs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6079538176\n",
      "2097152\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "if 'lora_model' in globals():\n",
    "    lora_model.to('cpu')\n",
    "    del lora_model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : -0.00032052432652562857\n",
      "- Min     : -1.5238006114959717\n",
      "- Max     : 1.4642316102981567\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(lora_model)\n",
    "print()\n",
    "\n",
    "lora_path = os.path.join(model_configs['L1T1']['lora_dir'], 'adapter_model.safetensors')\n",
    "lora_model.load_lora_weights(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_text(lora_model, tokenizer, prompt=\"Preheat the oven to 350 degrees and place the cookie dough\", skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nero Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.nero_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        print(\"================================================================\")\n",
    "        print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "        print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "        print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "        print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "        print()\n",
    "\n",
    "        nero_out = F.relu(self.nero_B(self.nero_A(self.dropout(lora_out))) * self.scaling)\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "        self.last_nero_out = nero_out\n",
    "\n",
    "        print(\"nero_out.requires_grad:\", nero_out.requires_grad)\n",
    "        print(\"nero_out.grad_fn:\", nero_out.grad_fn)\n",
    "        print(\"================================================================\")\n",
    "        print()\n",
    "\n",
    "        output = base_out + nero_out\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_weights(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        self.lora_A.weight.requires_grad = False\n",
    "        self.lora_B.weight.requires_grad = False\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "            self.lora_A.bias.requires_grad = False\n",
    "            self.lora_B.bias.requires_grad = False\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_weights(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_weights(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "# base_model2, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name='unsloth/Meta-Llama-3.1-8B',\n",
    "#     max_seq_length=max_seq_length,\n",
    "#     dtype=dtype,\n",
    "#     load_in_4bit=load_in_4bit,\n",
    "# )\n",
    "# lora_config = LoraConfig.from_pretrained(model_configs['L2T1']['lora_dir'])\n",
    "# nero_model = NeroModel(base_model2, lora_config, nero_bias=True, \n",
    "#                        return_nero_outputs=True\n",
    "#                        )\n",
    "# nero_model.freeze_except_nero()\n",
    "# print(nero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained(model_configs['L2T1']['lora_dir'])\n",
    "base_model = AutoModelForCausalLM.from_pretrained(lora_config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_config.base_model_name_or_path)\n",
    "nero_model = NeroModel(\n",
    "    base_model, \n",
    "    lora_config, \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0892de10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf086cf2b0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf086ce710>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0892e470>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf086ce710>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf086cf2b0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0892fac0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf086cf2b0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf086ce710>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0892e9c0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dc30>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890db10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0932fdf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890d3c0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890eb90>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf093d4050>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890eb90>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890d3c0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf093d4270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890d3c0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890eb90>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0882f460>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0882f240>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0882f020>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0882df20>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x79bf0890e230>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x79bf0890da50>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x79bf0890e230>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0882e360>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0882d7b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0882c7c0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0882d040>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0845c160>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0845ff00>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0845fce0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081cfdf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081cf570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081cf790>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf92bec160>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ec160>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ec8d0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ecaf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ecf30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ed150>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ed370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081edbf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ee030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ee250>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ee470>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ee8b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081eead0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081eecf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ef570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081ef9b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081efbd0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf081efdf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee8270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee8490>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee86b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee8f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee9370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee9590>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee97b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee9bf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffee9e10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffeea030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffeea8b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffeeacf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffeeaf10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffeeb130>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffeeb570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffeeb790>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffeeb9b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffef8270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffef86b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffef88d0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffef8af0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffef8f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffef9150>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffef9370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffef9bf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefa030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefa250>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefa470>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefa8b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefaad0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefacf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefb570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefb9b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefbbd0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beffefbdf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841c270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841c490>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ff10>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841c6b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ff10>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841cf30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841d370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841d590>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841d7b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841dbf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841de10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890c1f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841e030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e230>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890c1f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841e8b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e230>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841ecf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841ef10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841f130>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841f570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841f790>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0841f9b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08408270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084086b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084088d0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08408af0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08408f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08409150>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08409370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08409bf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840a030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840a250>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840a470>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840a8b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840aad0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840acf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840b570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840b9b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840bbd0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0840bdf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08414270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08414490>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084146b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08414f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08415370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08415590>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084157b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08415bf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08415e10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08416030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084168b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08416cf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08416f10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08417130>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08417570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08417790>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084179b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08448270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084486b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084488d0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08448af0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08448f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08449150>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08449370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08449bf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844a030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844a250>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844a470>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844a8b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844aad0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844acf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844b570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844b9b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844bbd0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf0844bdf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08434270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08434490>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084346b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08434f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08435370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08435590>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084357b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08435bf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08435e10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08436030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084368b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08436cf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08436f10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08437130>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08437570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf08437790>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79bf084379b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff900270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9006b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9008d0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff900af0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff900f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff901150>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff901370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff901bf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff902030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff902250>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff902470>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9028b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff902ad0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff902cf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff903570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9039b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff903bd0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff903df0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90c270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90c490>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90c6b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90cf30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90d370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90d590>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90d7b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90dbf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90de10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90e030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90e8b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90ecf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90ef10>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90f130>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90f570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90f790>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff90f9b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff914270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9146b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9148d0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff914af0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff914f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff915150>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff915370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff915bf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff916030>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff916250>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff916470>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9168b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff916ad0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890dba0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff916cf0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890dba0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff917570>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9179b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff917bd0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff917df0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff920270>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff920490>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890cb80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9206b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890cb80>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff920f30>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da80>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890da50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff921370>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890da50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ce50>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff921590>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890ce50>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890e6b0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <torch.autograd.function.MatMul4BitBackward object at 0x79beff9217b0>\n",
      "self.lora_A.weight.requires_grad: True\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: True\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x79bf0890e6b0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x79bf0890ce50>\n"
     ]
    }
   ],
   "source": [
    "nero_model.train()\n",
    "# nero_model.gradient_checkpointing_enable() # Fix error: 'LlamaDecoderLayer' object has no attribute '_gradient_checkpointing_func'\n",
    "device = next(nero_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "nero_model_outs = nero_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.6890, 0.6992,  ..., 0.0000, 0.0000, 0.5366],\n",
       "         [0.0000, 0.6890, 0.6992,  ..., 0.0000, 0.0000, 0.5366],\n",
       "         [0.0000, 0.6890, 0.6992,  ..., 0.0000, 0.0000, 0.5366],\n",
       "         ...,\n",
       "         [0.0000, 0.6890, 0.6992,  ..., 0.0000, 0.0000, 0.5366],\n",
       "         [0.0000, 0.6890, 0.6992,  ..., 0.0000, 0.0000, 0.5366],\n",
       "         [0.0000, 0.6890, 0.6992,  ..., 0.0000, 0.0000, 0.5366]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nero_model_outs[1]['layers__DOT__0__DOT__self_attn__DOT__q_proj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.embed_tokens.weight False\n",
      "base_model.model.layers.0.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.0.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.0.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.0.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.0.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.0.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.0.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.0.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.0.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.0.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.0.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.0.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.0.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.0.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.0.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.0.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.0.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.0.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.0.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.0.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.0.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.0.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.0.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.0.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.0.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.0.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.0.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.0.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.0.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.0.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.0.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.0.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.0.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.0.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.0.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.0.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.0.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.0.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.0.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.0.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.0.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.0.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.0.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.0.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.0.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.0.input_layernorm.weight True\n",
      "base_model.model.layers.0.post_attention_layernorm.weight True\n",
      "base_model.model.layers.1.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.1.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.1.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.1.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.1.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.1.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.1.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.1.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.1.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.1.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.1.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.1.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.1.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.1.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.1.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.1.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.1.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.1.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.1.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.1.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.1.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.1.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.1.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.1.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.1.mlp.gate_proj.base_layer.weight True\n",
      "base_model.model.layers.1.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.1.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.1.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.1.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.1.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.1.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.1.mlp.up_proj.base_layer.weight True\n",
      "base_model.model.layers.1.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.1.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.1.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.1.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.1.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.1.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.1.mlp.down_proj.base_layer.weight True\n",
      "base_model.model.layers.1.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.1.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.1.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.1.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.1.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.1.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.1.input_layernorm.weight True\n",
      "base_model.model.layers.1.post_attention_layernorm.weight True\n",
      "base_model.model.layers.2.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.2.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.2.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.2.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.2.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.2.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.2.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.2.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.2.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.2.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.2.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.2.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.2.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.2.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.2.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.2.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.2.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.2.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.2.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.2.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.2.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.2.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.2.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.2.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.2.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.2.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.2.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.2.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.2.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.2.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.2.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.2.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.2.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.2.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.2.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.2.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.2.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.2.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.2.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.2.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.2.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.2.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.2.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.2.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.2.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.2.input_layernorm.weight True\n",
      "base_model.model.layers.2.post_attention_layernorm.weight True\n",
      "base_model.model.layers.3.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.3.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.3.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.3.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.3.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.3.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.3.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.3.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.3.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.3.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.3.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.3.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.3.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.3.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.3.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.3.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.3.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.3.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.3.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.3.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.3.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.3.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.3.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.3.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.3.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.3.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.3.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.3.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.3.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.3.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.3.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.3.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.3.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.3.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.3.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.3.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.3.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.3.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.3.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.3.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.3.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.3.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.3.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.3.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.3.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.3.input_layernorm.weight True\n",
      "base_model.model.layers.3.post_attention_layernorm.weight True\n",
      "base_model.model.layers.4.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.4.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.4.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.4.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.4.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.4.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.4.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.4.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.4.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.4.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.4.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.4.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.4.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.4.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.4.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.4.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.4.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.4.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.4.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.4.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.4.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.4.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.4.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.4.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.4.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.4.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.4.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.4.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.4.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.4.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.4.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.4.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.4.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.4.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.4.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.4.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.4.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.4.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.4.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.4.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.4.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.4.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.4.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.4.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.4.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.4.input_layernorm.weight True\n",
      "base_model.model.layers.4.post_attention_layernorm.weight True\n",
      "base_model.model.layers.5.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.5.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.5.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.5.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.5.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.5.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.5.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.5.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.5.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.5.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.5.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.5.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.5.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.5.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.5.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.5.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.5.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.5.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.5.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.5.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.5.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.5.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.5.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.5.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.5.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.5.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.5.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.5.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.5.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.5.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.5.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.5.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.5.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.5.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.5.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.5.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.5.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.5.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.5.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.5.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.5.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.5.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.5.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.5.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.5.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.5.input_layernorm.weight True\n",
      "base_model.model.layers.5.post_attention_layernorm.weight True\n",
      "base_model.model.layers.6.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.6.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.6.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.6.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.6.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.6.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.6.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.6.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.6.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.6.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.6.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.6.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.6.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.6.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.6.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.6.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.6.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.6.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.6.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.6.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.6.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.6.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.6.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.6.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.6.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.6.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.6.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.6.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.6.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.6.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.6.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.6.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.6.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.6.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.6.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.6.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.6.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.6.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.6.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.6.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.6.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.6.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.6.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.6.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.6.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.6.input_layernorm.weight True\n",
      "base_model.model.layers.6.post_attention_layernorm.weight True\n",
      "base_model.model.layers.7.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.7.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.7.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.7.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.7.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.7.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.7.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.7.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.7.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.7.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.7.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.7.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.7.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.7.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.7.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.7.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.7.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.7.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.7.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.7.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.7.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.7.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.7.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.7.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.7.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.7.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.7.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.7.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.7.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.7.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.7.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.7.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.7.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.7.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.7.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.7.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.7.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.7.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.7.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.7.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.7.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.7.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.7.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.7.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.7.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.7.input_layernorm.weight True\n",
      "base_model.model.layers.7.post_attention_layernorm.weight True\n",
      "base_model.model.layers.8.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.8.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.8.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.8.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.8.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.8.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.8.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.8.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.8.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.8.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.8.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.8.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.8.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.8.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.8.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.8.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.8.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.8.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.8.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.8.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.8.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.8.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.8.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.8.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.8.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.8.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.8.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.8.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.8.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.8.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.8.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.8.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.8.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.8.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.8.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.8.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.8.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.8.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.8.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.8.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.8.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.8.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.8.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.8.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.8.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.8.input_layernorm.weight True\n",
      "base_model.model.layers.8.post_attention_layernorm.weight True\n",
      "base_model.model.layers.9.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.9.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.9.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.9.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.9.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.9.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.9.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.9.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.9.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.9.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.9.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.9.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.9.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.9.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.9.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.9.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.9.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.9.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.9.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.9.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.9.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.9.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.9.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.9.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.9.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.9.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.9.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.9.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.9.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.9.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.9.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.9.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.9.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.9.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.9.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.9.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.9.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.9.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.9.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.9.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.9.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.9.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.9.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.9.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.9.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.9.input_layernorm.weight True\n",
      "base_model.model.layers.9.post_attention_layernorm.weight True\n",
      "base_model.model.layers.10.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.10.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.10.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.10.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.10.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.10.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.10.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.10.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.10.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.10.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.10.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.10.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.10.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.10.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.10.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.10.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.10.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.10.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.10.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.10.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.10.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.10.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.10.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.10.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.10.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.10.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.10.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.10.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.10.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.10.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.10.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.10.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.10.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.10.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.10.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.10.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.10.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.10.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.10.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.10.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.10.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.10.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.10.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.10.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.10.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.10.input_layernorm.weight True\n",
      "base_model.model.layers.10.post_attention_layernorm.weight True\n",
      "base_model.model.layers.11.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.11.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.11.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.11.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.11.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.11.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.11.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.11.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.11.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.11.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.11.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.11.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.11.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.11.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.11.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.11.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.11.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.11.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.11.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.11.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.11.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.11.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.11.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.11.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.11.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.11.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.11.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.11.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.11.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.11.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.11.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.11.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.11.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.11.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.11.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.11.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.11.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.11.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.11.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.11.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.11.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.11.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.11.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.11.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.11.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.11.input_layernorm.weight True\n",
      "base_model.model.layers.11.post_attention_layernorm.weight True\n",
      "base_model.model.layers.12.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.12.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.12.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.12.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.12.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.12.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.12.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.12.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.12.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.12.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.12.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.12.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.12.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.12.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.12.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.12.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.12.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.12.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.12.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.12.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.12.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.12.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.12.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.12.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.12.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.12.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.12.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.12.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.12.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.12.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.12.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.12.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.12.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.12.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.12.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.12.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.12.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.12.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.12.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.12.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.12.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.12.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.12.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.12.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.12.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.12.input_layernorm.weight True\n",
      "base_model.model.layers.12.post_attention_layernorm.weight True\n",
      "base_model.model.layers.13.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.13.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.13.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.13.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.13.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.13.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.13.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.13.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.13.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.13.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.13.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.13.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.13.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.13.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.13.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.13.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.13.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.13.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.13.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.13.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.13.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.13.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.13.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.13.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.13.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.13.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.13.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.13.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.13.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.13.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.13.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.13.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.13.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.13.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.13.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.13.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.13.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.13.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.13.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.13.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.13.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.13.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.13.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.13.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.13.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.13.input_layernorm.weight True\n",
      "base_model.model.layers.13.post_attention_layernorm.weight True\n",
      "base_model.model.layers.14.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.14.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.14.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.14.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.14.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.14.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.14.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.14.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.14.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.14.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.14.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.14.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.14.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.14.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.14.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.14.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.14.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.14.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.14.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.14.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.14.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.14.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.14.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.14.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.14.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.14.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.14.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.14.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.14.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.14.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.14.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.14.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.14.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.14.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.14.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.14.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.14.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.14.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.14.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.14.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.14.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.14.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.14.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.14.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.14.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.14.input_layernorm.weight True\n",
      "base_model.model.layers.14.post_attention_layernorm.weight True\n",
      "base_model.model.layers.15.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.15.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.15.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.15.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.15.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.15.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.15.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.15.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.15.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.15.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.15.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.15.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.15.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.15.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.15.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.15.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.15.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.15.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.15.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.15.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.15.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.15.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.15.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.15.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.15.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.15.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.15.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.15.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.15.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.15.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.15.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.15.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.15.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.15.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.15.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.15.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.15.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.15.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.15.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.15.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.15.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.15.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.15.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.15.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.15.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.15.input_layernorm.weight True\n",
      "base_model.model.layers.15.post_attention_layernorm.weight True\n",
      "base_model.model.layers.16.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.16.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.16.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.16.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.16.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.16.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.16.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.16.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.16.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.16.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.16.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.16.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.16.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.16.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.16.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.16.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.16.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.16.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.16.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.16.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.16.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.16.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.16.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.16.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.16.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.16.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.16.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.16.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.16.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.16.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.16.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.16.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.16.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.16.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.16.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.16.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.16.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.16.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.16.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.16.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.16.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.16.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.16.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.16.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.16.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.16.input_layernorm.weight True\n",
      "base_model.model.layers.16.post_attention_layernorm.weight True\n",
      "base_model.model.layers.17.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.17.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.17.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.17.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.17.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.17.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.17.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.17.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.17.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.17.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.17.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.17.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.17.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.17.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.17.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.17.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.17.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.17.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.17.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.17.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.17.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.17.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.17.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.17.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.17.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.17.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.17.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.17.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.17.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.17.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.17.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.17.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.17.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.17.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.17.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.17.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.17.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.17.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.17.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.17.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.17.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.17.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.17.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.17.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.17.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.17.input_layernorm.weight True\n",
      "base_model.model.layers.17.post_attention_layernorm.weight True\n",
      "base_model.model.layers.18.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.18.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.18.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.18.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.18.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.18.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.18.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.18.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.18.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.18.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.18.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.18.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.18.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.18.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.18.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.18.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.18.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.18.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.18.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.18.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.18.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.18.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.18.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.18.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.18.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.18.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.18.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.18.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.18.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.18.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.18.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.18.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.18.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.18.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.18.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.18.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.18.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.18.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.18.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.18.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.18.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.18.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.18.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.18.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.18.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.18.input_layernorm.weight True\n",
      "base_model.model.layers.18.post_attention_layernorm.weight True\n",
      "base_model.model.layers.19.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.19.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.19.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.19.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.19.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.19.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.19.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.19.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.19.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.19.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.19.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.19.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.19.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.19.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.19.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.19.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.19.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.19.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.19.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.19.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.19.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.19.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.19.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.19.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.19.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.19.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.19.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.19.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.19.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.19.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.19.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.19.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.19.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.19.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.19.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.19.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.19.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.19.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.19.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.19.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.19.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.19.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.19.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.19.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.19.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.19.input_layernorm.weight True\n",
      "base_model.model.layers.19.post_attention_layernorm.weight True\n",
      "base_model.model.layers.20.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.20.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.20.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.20.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.20.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.20.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.20.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.20.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.20.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.20.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.20.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.20.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.20.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.20.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.20.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.20.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.20.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.20.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.20.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.20.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.20.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.20.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.20.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.20.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.20.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.20.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.20.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.20.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.20.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.20.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.20.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.20.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.20.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.20.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.20.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.20.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.20.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.20.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.20.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.20.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.20.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.20.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.20.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.20.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.20.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.20.input_layernorm.weight True\n",
      "base_model.model.layers.20.post_attention_layernorm.weight True\n",
      "base_model.model.layers.21.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.21.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.21.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.21.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.21.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.21.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.21.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.21.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.21.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.21.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.21.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.21.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.21.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.21.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.21.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.21.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.21.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.21.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.21.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.21.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.21.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.21.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.21.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.21.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.21.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.21.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.21.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.21.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.21.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.21.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.21.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.21.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.21.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.21.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.21.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.21.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.21.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.21.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.21.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.21.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.21.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.21.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.21.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.21.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.21.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.21.input_layernorm.weight True\n",
      "base_model.model.layers.21.post_attention_layernorm.weight True\n",
      "base_model.model.layers.22.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.22.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.22.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.22.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.22.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.22.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.22.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.22.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.22.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.22.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.22.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.22.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.22.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.22.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.22.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.22.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.22.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.22.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.22.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.22.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.22.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.22.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.22.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.22.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.22.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.22.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.22.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.22.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.22.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.22.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.22.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.22.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.22.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.22.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.22.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.22.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.22.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.22.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.22.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.22.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.22.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.22.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.22.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.22.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.22.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.22.input_layernorm.weight True\n",
      "base_model.model.layers.22.post_attention_layernorm.weight True\n",
      "base_model.model.layers.23.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.23.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.23.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.23.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.23.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.23.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.23.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.23.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.23.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.23.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.23.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.23.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.23.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.23.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.23.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.23.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.23.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.23.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.23.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.23.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.23.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.23.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.23.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.23.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.23.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.23.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.23.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.23.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.23.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.23.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.23.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.23.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.23.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.23.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.23.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.23.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.23.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.23.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.23.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.23.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.23.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.23.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.23.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.23.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.23.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.23.input_layernorm.weight True\n",
      "base_model.model.layers.23.post_attention_layernorm.weight True\n",
      "base_model.model.layers.24.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.24.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.24.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.24.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.24.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.24.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.24.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.24.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.24.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.24.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.24.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.24.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.24.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.24.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.24.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.24.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.24.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.24.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.24.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.24.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.24.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.24.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.24.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.24.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.24.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.24.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.24.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.24.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.24.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.24.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.24.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.24.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.24.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.24.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.24.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.24.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.24.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.24.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.24.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.24.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.24.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.24.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.24.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.24.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.24.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.24.input_layernorm.weight True\n",
      "base_model.model.layers.24.post_attention_layernorm.weight True\n",
      "base_model.model.layers.25.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.25.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.25.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.25.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.25.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.25.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.25.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.25.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.25.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.25.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.25.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.25.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.25.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.25.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.25.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.25.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.25.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.25.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.25.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.25.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.25.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.25.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.25.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.25.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.25.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.25.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.25.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.25.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.25.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.25.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.25.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.25.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.25.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.25.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.25.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.25.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.25.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.25.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.25.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.25.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.25.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.25.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.25.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.25.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.25.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.25.input_layernorm.weight True\n",
      "base_model.model.layers.25.post_attention_layernorm.weight True\n",
      "base_model.model.layers.26.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.26.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.26.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.26.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.26.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.26.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.26.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.26.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.26.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.26.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.26.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.26.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.26.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.26.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.26.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.26.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.26.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.26.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.26.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.26.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.26.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.26.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.26.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.26.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.26.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.26.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.26.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.26.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.26.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.26.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.26.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.26.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.26.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.26.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.26.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.26.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.26.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.26.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.26.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.26.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.26.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.26.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.26.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.26.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.26.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.26.input_layernorm.weight True\n",
      "base_model.model.layers.26.post_attention_layernorm.weight True\n",
      "base_model.model.layers.27.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.27.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.27.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.27.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.27.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.27.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.27.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.27.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.27.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.27.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.27.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.27.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.27.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.27.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.27.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.27.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.27.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.27.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.27.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.27.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.27.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.27.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.27.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.27.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.27.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.27.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.27.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.27.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.27.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.27.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.27.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.27.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.27.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.27.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.27.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.27.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.27.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.27.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.27.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.27.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.27.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.27.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.27.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.27.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.27.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.27.input_layernorm.weight True\n",
      "base_model.model.layers.27.post_attention_layernorm.weight True\n",
      "base_model.model.layers.28.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.28.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.28.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.28.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.28.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.28.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.28.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.28.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.28.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.28.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.28.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.28.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.28.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.28.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.28.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.28.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.28.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.28.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.28.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.28.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.28.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.28.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.28.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.28.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.28.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.28.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.28.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.28.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.28.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.28.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.28.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.28.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.28.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.28.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.28.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.28.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.28.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.28.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.28.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.28.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.28.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.28.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.28.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.28.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.28.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.28.input_layernorm.weight True\n",
      "base_model.model.layers.28.post_attention_layernorm.weight True\n",
      "base_model.model.layers.29.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.29.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.29.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.29.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.29.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.29.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.29.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.29.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.29.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.29.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.29.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.29.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.29.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.29.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.29.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.29.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.29.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.29.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.29.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.29.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.29.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.29.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.29.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.29.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.29.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.29.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.29.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.29.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.29.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.29.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.29.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.29.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.29.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.29.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.29.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.29.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.29.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.29.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.29.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.29.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.29.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.29.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.29.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.29.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.29.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.29.input_layernorm.weight True\n",
      "base_model.model.layers.29.post_attention_layernorm.weight True\n",
      "base_model.model.layers.30.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.30.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.30.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.30.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.30.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.30.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.30.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.30.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.30.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.30.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.30.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.30.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.30.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.30.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.30.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.30.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.30.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.30.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.30.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.30.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.30.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.30.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.30.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.30.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.30.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.30.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.30.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.30.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.30.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.30.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.30.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.30.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.30.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.30.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.30.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.30.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.30.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.30.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.30.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.30.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.30.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.30.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.30.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.30.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.30.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.30.input_layernorm.weight True\n",
      "base_model.model.layers.30.post_attention_layernorm.weight True\n",
      "base_model.model.layers.31.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_A.weight True\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_B.weight True\n",
      "base_model.model.layers.31.self_attn.q_proj.nero_A.weight True\n",
      "base_model.model.layers.31.self_attn.q_proj.nero_A.bias True\n",
      "base_model.model.layers.31.self_attn.q_proj.nero_B.weight True\n",
      "base_model.model.layers.31.self_attn.q_proj.nero_B.bias True\n",
      "base_model.model.layers.31.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.layers.31.self_attn.k_proj.lora_A.weight True\n",
      "base_model.model.layers.31.self_attn.k_proj.lora_B.weight True\n",
      "base_model.model.layers.31.self_attn.k_proj.nero_A.weight True\n",
      "base_model.model.layers.31.self_attn.k_proj.nero_A.bias True\n",
      "base_model.model.layers.31.self_attn.k_proj.nero_B.weight True\n",
      "base_model.model.layers.31.self_attn.k_proj.nero_B.bias True\n",
      "base_model.model.layers.31.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_A.weight True\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_B.weight True\n",
      "base_model.model.layers.31.self_attn.v_proj.nero_A.weight True\n",
      "base_model.model.layers.31.self_attn.v_proj.nero_A.bias True\n",
      "base_model.model.layers.31.self_attn.v_proj.nero_B.weight True\n",
      "base_model.model.layers.31.self_attn.v_proj.nero_B.bias True\n",
      "base_model.model.layers.31.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.layers.31.self_attn.o_proj.lora_A.weight True\n",
      "base_model.model.layers.31.self_attn.o_proj.lora_B.weight True\n",
      "base_model.model.layers.31.self_attn.o_proj.nero_A.weight True\n",
      "base_model.model.layers.31.self_attn.o_proj.nero_A.bias True\n",
      "base_model.model.layers.31.self_attn.o_proj.nero_B.weight True\n",
      "base_model.model.layers.31.self_attn.o_proj.nero_B.bias True\n",
      "base_model.model.layers.31.mlp.gate_proj.base_layer.weight False\n",
      "base_model.model.layers.31.mlp.gate_proj.lora_A.weight True\n",
      "base_model.model.layers.31.mlp.gate_proj.lora_B.weight True\n",
      "base_model.model.layers.31.mlp.gate_proj.nero_A.weight True\n",
      "base_model.model.layers.31.mlp.gate_proj.nero_A.bias True\n",
      "base_model.model.layers.31.mlp.gate_proj.nero_B.weight True\n",
      "base_model.model.layers.31.mlp.gate_proj.nero_B.bias True\n",
      "base_model.model.layers.31.mlp.up_proj.base_layer.weight False\n",
      "base_model.model.layers.31.mlp.up_proj.lora_A.weight True\n",
      "base_model.model.layers.31.mlp.up_proj.lora_B.weight True\n",
      "base_model.model.layers.31.mlp.up_proj.nero_A.weight True\n",
      "base_model.model.layers.31.mlp.up_proj.nero_A.bias True\n",
      "base_model.model.layers.31.mlp.up_proj.nero_B.weight True\n",
      "base_model.model.layers.31.mlp.up_proj.nero_B.bias True\n",
      "base_model.model.layers.31.mlp.down_proj.base_layer.weight False\n",
      "base_model.model.layers.31.mlp.down_proj.lora_A.weight True\n",
      "base_model.model.layers.31.mlp.down_proj.lora_B.weight True\n",
      "base_model.model.layers.31.mlp.down_proj.nero_A.weight True\n",
      "base_model.model.layers.31.mlp.down_proj.nero_A.bias True\n",
      "base_model.model.layers.31.mlp.down_proj.nero_B.weight True\n",
      "base_model.model.layers.31.mlp.down_proj.nero_B.bias True\n",
      "base_model.model.layers.31.input_layernorm.weight True\n",
      "base_model.model.layers.31.post_attention_layernorm.weight True\n",
      "base_model.model.norm.weight True\n",
      "base_model.lm_head.weight True\n"
     ]
    }
   ],
   "source": [
    "for n, p in nero_model.named_parameters():\n",
    "    print(n, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 0.0012095558922737837\n",
      "- Min     : -1.3954026699066162\n",
      "- Max     : 1.4998902082443237\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 2.2152138626552187e-05\n",
      "- Min     : -0.06327299773693085\n",
      "- Max     : 0.0625513345003128\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "lora_path = os.path.join(model_configs['L2T1']['lora_dir'], 'adapter_model.safetensors')\n",
    "nero_model.load_lora_weights(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in nero_model.named_parameters():\n",
    "#     if p.requires_grad and 'nero' not in n:\n",
    "#         print(n, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nero_model.freeze_all_except_nero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in nero_model.named_parameters():\n",
    "    if p.requires_grad and 'nero' not in n:\n",
    "        print(n, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth.models.loader import FastLlamaModel\n",
    "# use_gradient_checkpointing = 'unsloth'\n",
    "# nero_model_patched = FastLlamaModel.patch_peft_model(nero_model, use_gradient_checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038788fd0>\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789120>\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038788fd0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789120>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038788fd0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789120>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878ac20>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878ac20>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038788fd0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038788fd0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038788fd0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038788fd0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038a52e60>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038a51150>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038a52e60>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789060>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789060>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789090>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789090>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789000>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789000>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789000>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789000>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789000>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789000>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789000>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789000>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038789000>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789000>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038789000>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bee0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bee0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a2038b491e0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a2038b49fc0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a2038b491e0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bf70>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bf70>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bf70>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bf70>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bf70>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bf70>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878a950>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878bf70>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878a950>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878bf70>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a20387890f0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a203878aad0>\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n",
      "self.lora_A.weight.requires_grad: False\n",
      "self.lora_A.weight.grad_fn: None\n",
      "self.lora_B.weight.requires_grad: False\n",
      "self.lora_B.weight.grad_fn: None\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x7a203878aad0>\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x7a20387890f0>\n"
     ]
    }
   ],
   "source": [
    "nero_model.train()\n",
    "# nero_model.gradient_checkpointing_enable() # Fix error: 'LlamaDecoderLayer' object has no attribute '_gradient_checkpointing_func'\n",
    "device = next(nero_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "nero_model_outs = nero_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers__DOT__0__DOT__self_attn__DOT__q_proj': tensor([[[0.0340, 0.2380, 0.4421,  ..., 0.3149, 0.0000, 0.2181],\n",
       "          [0.0340, 0.2380, 0.4421,  ..., 0.3149, 0.0000, 0.2181],\n",
       "          [0.0340, 0.2380, 0.4421,  ..., 0.3149, 0.0000, 0.2181],\n",
       "          ...,\n",
       "          [0.0340, 0.2380, 0.4421,  ..., 0.3149, 0.0000, 0.2181],\n",
       "          [0.0340, 0.2380, 0.4421,  ..., 0.3149, 0.0000, 0.2181],\n",
       "          [0.0340, 0.2380, 0.4421,  ..., 0.3149, 0.0000, 0.2181]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__self_attn__DOT__k_proj': tensor([[[0.0462, 0.0000, 0.2678,  ..., 0.3589, 0.2004, 0.1290],\n",
       "          [0.0462, 0.0000, 0.2678,  ..., 0.3589, 0.2004, 0.1290],\n",
       "          [0.0462, 0.0000, 0.2678,  ..., 0.3589, 0.2004, 0.1290],\n",
       "          ...,\n",
       "          [0.0462, 0.0000, 0.2678,  ..., 0.3589, 0.2004, 0.1290],\n",
       "          [0.0462, 0.0000, 0.2678,  ..., 0.3589, 0.2004, 0.1290],\n",
       "          [0.0462, 0.0000, 0.2678,  ..., 0.3589, 0.2004, 0.1290]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.6709, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6709, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6709, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6709, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6709, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6709, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__self_attn__DOT__o_proj': tensor([[[0.3696, 0.0000, 0.1254,  ..., 0.0000, 0.2158, 0.5337],\n",
       "          [0.3696, 0.0000, 0.1254,  ..., 0.0000, 0.2158, 0.5337],\n",
       "          [0.3696, 0.0000, 0.1254,  ..., 0.0000, 0.2158, 0.5337],\n",
       "          ...,\n",
       "          [0.3696, 0.0000, 0.1254,  ..., 0.0000, 0.2158, 0.5337],\n",
       "          [0.3696, 0.0000, 0.1254,  ..., 0.0000, 0.2158, 0.5337],\n",
       "          [0.3696, 0.0000, 0.1254,  ..., 0.0000, 0.2158, 0.5337]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__mlp__DOT__gate_proj': tensor([[[0.0528, 0.0000, 0.0000,  ..., 0.0000, 0.1246, 0.0000],\n",
       "          [0.0528, 0.0000, 0.0000,  ..., 0.0000, 0.1246, 0.0000],\n",
       "          [0.0528, 0.0000, 0.0000,  ..., 0.0000, 0.1246, 0.0000],\n",
       "          ...,\n",
       "          [0.0528, 0.0000, 0.0000,  ..., 0.0000, 0.1246, 0.0000],\n",
       "          [0.0528, 0.0000, 0.0000,  ..., 0.0000, 0.1246, 0.0000],\n",
       "          [0.0528, 0.0000, 0.0000,  ..., 0.0000, 0.1246, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0000, 0.4917,  ..., 0.4805, 0.6230, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4917,  ..., 0.4805, 0.6230, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4917,  ..., 0.4805, 0.6230, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.4917,  ..., 0.4805, 0.6230, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4917,  ..., 0.4805, 0.6230, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4917,  ..., 0.4805, 0.6230, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__0__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.4917, 0.3079,  ..., 0.1500, 0.5620, 0.1628],\n",
       "          [0.0000, 0.4917, 0.3079,  ..., 0.1500, 0.5620, 0.1628],\n",
       "          [0.0000, 0.4917, 0.3079,  ..., 0.1500, 0.5620, 0.1628],\n",
       "          ...,\n",
       "          [0.0000, 0.4917, 0.3079,  ..., 0.1500, 0.5620, 0.1628],\n",
       "          [0.0000, 0.4917, 0.3079,  ..., 0.1500, 0.5620, 0.1628],\n",
       "          [0.0000, 0.4917, 0.3079,  ..., 0.1500, 0.5620, 0.1628]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__self_attn__DOT__q_proj': tensor([[[0.2769, 0.4292, 0.1138,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2769, 0.4292, 0.1138,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2769, 0.4292, 0.1138,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.2769, 0.4292, 0.1138,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2769, 0.4292, 0.1138,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2769, 0.4292, 0.1138,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.5850, 0.1323,  ..., 0.6108, 0.0000, 0.5347],\n",
       "          [0.0000, 0.5850, 0.1323,  ..., 0.6108, 0.0000, 0.5347],\n",
       "          [0.0000, 0.5850, 0.1323,  ..., 0.6108, 0.0000, 0.5347],\n",
       "          ...,\n",
       "          [0.0000, 0.5850, 0.1323,  ..., 0.6108, 0.0000, 0.5347],\n",
       "          [0.0000, 0.5850, 0.1323,  ..., 0.6108, 0.0000, 0.5347],\n",
       "          [0.0000, 0.5850, 0.1323,  ..., 0.6108, 0.0000, 0.5347]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.4355,  ..., 0.0000, 0.0000, 0.6113],\n",
       "          [0.0000, 0.0000, 0.4355,  ..., 0.0000, 0.0000, 0.6113],\n",
       "          [0.0000, 0.0000, 0.4355,  ..., 0.0000, 0.0000, 0.6113],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.4355,  ..., 0.0000, 0.0000, 0.6113],\n",
       "          [0.0000, 0.0000, 0.4355,  ..., 0.0000, 0.0000, 0.6113],\n",
       "          [0.0000, 0.0000, 0.4355,  ..., 0.0000, 0.0000, 0.6113]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__mlp__DOT__gate_proj': tensor([[[0.5454, 0.0000, 0.4424,  ..., 0.0000, 0.1907, 0.0000],\n",
       "          [0.5454, 0.0000, 0.4424,  ..., 0.0000, 0.1907, 0.0000],\n",
       "          [0.5454, 0.0000, 0.4424,  ..., 0.0000, 0.1907, 0.0000],\n",
       "          ...,\n",
       "          [0.5454, 0.0000, 0.4424,  ..., 0.0000, 0.1907, 0.0000],\n",
       "          [0.5454, 0.0000, 0.4424,  ..., 0.0000, 0.1907, 0.0000],\n",
       "          [0.5454, 0.0000, 0.4424,  ..., 0.0000, 0.1907, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__mlp__DOT__up_proj': tensor([[[0.0590, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.7036],\n",
       "          [0.0590, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.7036],\n",
       "          [0.0590, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.7036],\n",
       "          ...,\n",
       "          [0.0590, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.7036],\n",
       "          [0.0590, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.7036],\n",
       "          [0.0590, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.7036]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__1__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.1599, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1599, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1599, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.1599, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1599, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1599, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__self_attn__DOT__q_proj': tensor([[[0.5435, 0.0000, 0.3374,  ..., 0.0000, 0.0000, 0.3833],\n",
       "          [0.5435, 0.0000, 0.3374,  ..., 0.0000, 0.0000, 0.3833],\n",
       "          [0.5435, 0.0000, 0.3374,  ..., 0.0000, 0.0000, 0.3833],\n",
       "          ...,\n",
       "          [0.5435, 0.0000, 0.3374,  ..., 0.0000, 0.0000, 0.3833],\n",
       "          [0.5435, 0.0000, 0.3374,  ..., 0.0000, 0.0000, 0.3833],\n",
       "          [0.5435, 0.0000, 0.3374,  ..., 0.0000, 0.0000, 0.3833]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__self_attn__DOT__k_proj': tensor([[[0.0919, 0.5576, 0.0000,  ..., 0.0000, 0.6875, 0.0000],\n",
       "          [0.0919, 0.5576, 0.0000,  ..., 0.0000, 0.6875, 0.0000],\n",
       "          [0.0919, 0.5576, 0.0000,  ..., 0.0000, 0.6875, 0.0000],\n",
       "          ...,\n",
       "          [0.0919, 0.5576, 0.0000,  ..., 0.0000, 0.6875, 0.0000],\n",
       "          [0.0919, 0.5576, 0.0000,  ..., 0.0000, 0.6875, 0.0000],\n",
       "          [0.0919, 0.5576, 0.0000,  ..., 0.0000, 0.6875, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__self_attn__DOT__v_proj': tensor([[[0.5981, 0.1797, 0.0341,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5981, 0.1797, 0.0341,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5981, 0.1797, 0.0341,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5981, 0.1797, 0.0341,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5981, 0.1797, 0.0341,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5981, 0.1797, 0.0341,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__self_attn__DOT__o_proj': tensor([[[0.3870, 0.0000, 0.0000,  ..., 0.0000, 0.1130, 0.4814],\n",
       "          [0.3870, 0.0000, 0.0000,  ..., 0.0000, 0.1130, 0.4814],\n",
       "          [0.3870, 0.0000, 0.0000,  ..., 0.0000, 0.1130, 0.4814],\n",
       "          ...,\n",
       "          [0.3870, 0.0000, 0.0000,  ..., 0.0000, 0.1130, 0.4814],\n",
       "          [0.3870, 0.0000, 0.0000,  ..., 0.0000, 0.1130, 0.4814],\n",
       "          [0.3870, 0.0000, 0.0000,  ..., 0.0000, 0.1130, 0.4814]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.6079, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6079, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6079, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6079, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6079, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6079, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0000, 0.1616,  ..., 0.0000, 0.0308, 0.4177],\n",
       "          [0.0000, 0.0000, 0.1616,  ..., 0.0000, 0.0308, 0.4177],\n",
       "          [0.0000, 0.0000, 0.1616,  ..., 0.0000, 0.0308, 0.4177],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.1616,  ..., 0.0000, 0.0308, 0.4177],\n",
       "          [0.0000, 0.0000, 0.1616,  ..., 0.0000, 0.0308, 0.4177],\n",
       "          [0.0000, 0.0000, 0.1616,  ..., 0.0000, 0.0308, 0.4177]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__2__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.4421, 0.0000,  ..., 0.5747, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4421, 0.0000,  ..., 0.5747, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4421, 0.0000,  ..., 0.5747, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.4421, 0.0000,  ..., 0.5747, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4421, 0.0000,  ..., 0.5747, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4421, 0.0000,  ..., 0.5747, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3440],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3440],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3440],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3440],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3440],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3440]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.6089, 0.0000,  ..., 0.0000, 0.2305, 0.0000],\n",
       "          [0.0000, 0.6089, 0.0000,  ..., 0.0000, 0.2305, 0.0000],\n",
       "          [0.0000, 0.6089, 0.0000,  ..., 0.0000, 0.2305, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.6089, 0.0000,  ..., 0.0000, 0.2305, 0.0000],\n",
       "          [0.0000, 0.6089, 0.0000,  ..., 0.0000, 0.2305, 0.0000],\n",
       "          [0.0000, 0.6089, 0.0000,  ..., 0.0000, 0.2305, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.1268, 0.0000,  ..., 0.0000, 0.6738, 0.2057],\n",
       "          [0.0000, 0.1268, 0.0000,  ..., 0.0000, 0.6738, 0.2057],\n",
       "          [0.0000, 0.1268, 0.0000,  ..., 0.0000, 0.6738, 0.2057],\n",
       "          ...,\n",
       "          [0.0000, 0.1268, 0.0000,  ..., 0.0000, 0.6738, 0.2057],\n",
       "          [0.0000, 0.1268, 0.0000,  ..., 0.0000, 0.6738, 0.2057],\n",
       "          [0.0000, 0.1268, 0.0000,  ..., 0.0000, 0.6738, 0.2057]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0000, 0.3047,  ..., 0.0000, 0.6660, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3047,  ..., 0.0000, 0.6660, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3047,  ..., 0.0000, 0.6660, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.3047,  ..., 0.0000, 0.6660, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3047,  ..., 0.0000, 0.6660, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3047,  ..., 0.0000, 0.6660, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__mlp__DOT__gate_proj': tensor([[[0.5659, 0.0000, 0.1061,  ..., 0.4250, 0.1735, 0.4453],\n",
       "          [0.5659, 0.0000, 0.1061,  ..., 0.4250, 0.1735, 0.4453],\n",
       "          [0.5659, 0.0000, 0.1061,  ..., 0.4250, 0.1735, 0.4453],\n",
       "          ...,\n",
       "          [0.5659, 0.0000, 0.1061,  ..., 0.4250, 0.1735, 0.4453],\n",
       "          [0.5659, 0.0000, 0.1061,  ..., 0.4250, 0.1735, 0.4453],\n",
       "          [0.5659, 0.0000, 0.1061,  ..., 0.4250, 0.1735, 0.4453]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.5459, 0.2247,  ..., 0.0749, 0.3774, 0.0000],\n",
       "          [0.0000, 0.5459, 0.2247,  ..., 0.0749, 0.3774, 0.0000],\n",
       "          [0.0000, 0.5459, 0.2247,  ..., 0.0749, 0.3774, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.5459, 0.2247,  ..., 0.0749, 0.3774, 0.0000],\n",
       "          [0.0000, 0.5459, 0.2247,  ..., 0.0749, 0.3774, 0.0000],\n",
       "          [0.0000, 0.5459, 0.2247,  ..., 0.0749, 0.3774, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__3__DOT__mlp__DOT__down_proj': tensor([[[0.5547, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5547, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5547, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5547, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5547, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5547, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.5605, 0.1801, 0.2233],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5605, 0.1801, 0.2233],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5605, 0.1801, 0.2233],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5605, 0.1801, 0.2233],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5605, 0.1801, 0.2233],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5605, 0.1801, 0.2233]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__self_attn__DOT__k_proj': tensor([[[0.6387, 0.0000, 0.3718,  ..., 0.0000, 0.2656, 0.0000],\n",
       "          [0.6387, 0.0000, 0.3718,  ..., 0.0000, 0.2656, 0.0000],\n",
       "          [0.6387, 0.0000, 0.3718,  ..., 0.0000, 0.2656, 0.0000],\n",
       "          ...,\n",
       "          [0.6387, 0.0000, 0.3718,  ..., 0.0000, 0.2656, 0.0000],\n",
       "          [0.6387, 0.0000, 0.3718,  ..., 0.0000, 0.2656, 0.0000],\n",
       "          [0.6387, 0.0000, 0.3718,  ..., 0.0000, 0.2656, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.4006,  ..., 0.2927, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4006,  ..., 0.2927, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4006,  ..., 0.2927, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.4006,  ..., 0.2927, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4006,  ..., 0.2927, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4006,  ..., 0.2927, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.4194, 0.0011,  ..., 0.0000, 0.1666, 0.0000],\n",
       "          [0.0000, 0.4194, 0.0011,  ..., 0.0000, 0.1666, 0.0000],\n",
       "          [0.0000, 0.4194, 0.0011,  ..., 0.0000, 0.1666, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.4194, 0.0011,  ..., 0.0000, 0.1666, 0.0000],\n",
       "          [0.0000, 0.4194, 0.0011,  ..., 0.0000, 0.1666, 0.0000],\n",
       "          [0.0000, 0.4194, 0.0011,  ..., 0.0000, 0.1666, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__mlp__DOT__gate_proj': tensor([[[0.4165, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.1438],\n",
       "          [0.4165, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.1438],\n",
       "          [0.4165, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.1438],\n",
       "          ...,\n",
       "          [0.4165, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.1438],\n",
       "          [0.4165, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.1438],\n",
       "          [0.4165, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.1438]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.6274, 0.3284,  ..., 0.4927, 0.0000, 0.4844],\n",
       "          [0.0000, 0.6274, 0.3284,  ..., 0.4927, 0.0000, 0.4844],\n",
       "          [0.0000, 0.6274, 0.3284,  ..., 0.4927, 0.0000, 0.4844],\n",
       "          ...,\n",
       "          [0.0000, 0.6274, 0.3284,  ..., 0.4927, 0.0000, 0.4844],\n",
       "          [0.0000, 0.6274, 0.3284,  ..., 0.4927, 0.0000, 0.4844],\n",
       "          [0.0000, 0.6274, 0.3284,  ..., 0.4927, 0.0000, 0.4844]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__4__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.5083, 0.1677,  ..., 0.0000, 0.0000, 0.1475],\n",
       "          [0.0000, 0.5083, 0.1677,  ..., 0.0000, 0.0000, 0.1475],\n",
       "          [0.0000, 0.5083, 0.1677,  ..., 0.0000, 0.0000, 0.1475],\n",
       "          ...,\n",
       "          [0.0000, 0.5083, 0.1677,  ..., 0.0000, 0.0000, 0.1475],\n",
       "          [0.0000, 0.5083, 0.1677,  ..., 0.0000, 0.0000, 0.1475],\n",
       "          [0.0000, 0.5083, 0.1677,  ..., 0.0000, 0.0000, 0.1475]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.5781, 0.0000,  ..., 0.0000, 0.0000, 0.1490],\n",
       "          [0.0000, 0.5781, 0.0000,  ..., 0.0000, 0.0000, 0.1490],\n",
       "          [0.0000, 0.5781, 0.0000,  ..., 0.0000, 0.0000, 0.1490],\n",
       "          ...,\n",
       "          [0.0000, 0.5781, 0.0000,  ..., 0.0000, 0.0000, 0.1490],\n",
       "          [0.0000, 0.5781, 0.0000,  ..., 0.0000, 0.0000, 0.1490],\n",
       "          [0.0000, 0.5781, 0.0000,  ..., 0.0000, 0.0000, 0.1490]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.6699, 0.2568,  ..., 0.0000, 0.0000, 0.4331],\n",
       "          [0.0000, 0.6699, 0.2568,  ..., 0.0000, 0.0000, 0.4331],\n",
       "          [0.0000, 0.6699, 0.2568,  ..., 0.0000, 0.0000, 0.4331],\n",
       "          ...,\n",
       "          [0.0000, 0.6699, 0.2568,  ..., 0.0000, 0.0000, 0.4331],\n",
       "          [0.0000, 0.6699, 0.2568,  ..., 0.0000, 0.0000, 0.4331],\n",
       "          [0.0000, 0.6699, 0.2568,  ..., 0.0000, 0.0000, 0.4331]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__self_attn__DOT__v_proj': tensor([[[0.0499, 0.0773, 0.0000,  ..., 0.1115, 0.1120, 0.0000],\n",
       "          [0.0499, 0.0773, 0.0000,  ..., 0.1115, 0.1120, 0.0000],\n",
       "          [0.0499, 0.0773, 0.0000,  ..., 0.1115, 0.1120, 0.0000],\n",
       "          ...,\n",
       "          [0.0499, 0.0773, 0.0000,  ..., 0.1115, 0.1120, 0.0000],\n",
       "          [0.0499, 0.0773, 0.0000,  ..., 0.1115, 0.1120, 0.0000],\n",
       "          [0.0499, 0.0773, 0.0000,  ..., 0.1115, 0.1120, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__self_attn__DOT__o_proj': tensor([[[0.5864, 0.0000, 0.0000,  ..., 0.0000, 0.0709, 0.0000],\n",
       "          [0.5864, 0.0000, 0.0000,  ..., 0.0000, 0.0709, 0.0000],\n",
       "          [0.5864, 0.0000, 0.0000,  ..., 0.0000, 0.0709, 0.0000],\n",
       "          ...,\n",
       "          [0.5864, 0.0000, 0.0000,  ..., 0.0000, 0.0709, 0.0000],\n",
       "          [0.5864, 0.0000, 0.0000,  ..., 0.0000, 0.0709, 0.0000],\n",
       "          [0.5864, 0.0000, 0.0000,  ..., 0.0000, 0.0709, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.3799, 0.4858, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3799, 0.4858, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3799, 0.4858, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3799, 0.4858, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3799, 0.4858, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3799, 0.4858, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.6216, 0.0000,  ..., 0.0000, 0.0000, 0.0281],\n",
       "          [0.0000, 0.6216, 0.0000,  ..., 0.0000, 0.0000, 0.0281],\n",
       "          [0.0000, 0.6216, 0.0000,  ..., 0.0000, 0.0000, 0.0281],\n",
       "          ...,\n",
       "          [0.0000, 0.6216, 0.0000,  ..., 0.0000, 0.0000, 0.0281],\n",
       "          [0.0000, 0.6216, 0.0000,  ..., 0.0000, 0.0000, 0.0281],\n",
       "          [0.0000, 0.6216, 0.0000,  ..., 0.0000, 0.0000, 0.0281]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__5__DOT__mlp__DOT__down_proj': tensor([[[0.2571, 0.0000, 0.4792,  ..., 0.0000, 0.4812, 0.0000],\n",
       "          [0.2571, 0.0000, 0.4792,  ..., 0.0000, 0.4812, 0.0000],\n",
       "          [0.2571, 0.0000, 0.4792,  ..., 0.0000, 0.4812, 0.0000],\n",
       "          ...,\n",
       "          [0.2571, 0.0000, 0.4792,  ..., 0.0000, 0.4812, 0.0000],\n",
       "          [0.2571, 0.0000, 0.4792,  ..., 0.0000, 0.4812, 0.0000],\n",
       "          [0.2571, 0.0000, 0.4792,  ..., 0.0000, 0.4812, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.3708, 0.0000,  ..., 0.0000, 0.1066, 0.4724],\n",
       "          [0.0000, 0.3708, 0.0000,  ..., 0.0000, 0.1066, 0.4724],\n",
       "          [0.0000, 0.3708, 0.0000,  ..., 0.0000, 0.1066, 0.4724],\n",
       "          ...,\n",
       "          [0.0000, 0.3708, 0.0000,  ..., 0.0000, 0.1066, 0.4724],\n",
       "          [0.0000, 0.3708, 0.0000,  ..., 0.0000, 0.1066, 0.4724],\n",
       "          [0.0000, 0.3708, 0.0000,  ..., 0.0000, 0.1066, 0.4724]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__self_attn__DOT__k_proj': tensor([[[0.6953, 0.3521, 0.0000,  ..., 0.0000, 0.0000, 0.5630],\n",
       "          [0.6953, 0.3521, 0.0000,  ..., 0.0000, 0.0000, 0.5630],\n",
       "          [0.6953, 0.3521, 0.0000,  ..., 0.0000, 0.0000, 0.5630],\n",
       "          ...,\n",
       "          [0.6953, 0.3521, 0.0000,  ..., 0.0000, 0.0000, 0.5630],\n",
       "          [0.6953, 0.3521, 0.0000,  ..., 0.0000, 0.0000, 0.5630],\n",
       "          [0.6953, 0.3521, 0.0000,  ..., 0.0000, 0.0000, 0.5630]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__self_attn__DOT__v_proj': tensor([[[0.1735, 0.2598, 0.4966,  ..., 0.3232, 0.5938, 0.0000],\n",
       "          [0.1735, 0.2598, 0.4966,  ..., 0.3232, 0.5938, 0.0000],\n",
       "          [0.1735, 0.2598, 0.4966,  ..., 0.3232, 0.5938, 0.0000],\n",
       "          ...,\n",
       "          [0.1735, 0.2598, 0.4966,  ..., 0.3232, 0.5938, 0.0000],\n",
       "          [0.1735, 0.2598, 0.4966,  ..., 0.3232, 0.5938, 0.0000],\n",
       "          [0.1735, 0.2598, 0.4966,  ..., 0.3232, 0.5938, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__self_attn__DOT__o_proj': tensor([[[0.3938, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3938, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3938, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.3938, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3938, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3938, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__mlp__DOT__gate_proj': tensor([[[0.1412, 0.4214, 0.5996,  ..., 0.2239, 0.0627, 0.0000],\n",
       "          [0.1412, 0.4214, 0.5996,  ..., 0.2239, 0.0627, 0.0000],\n",
       "          [0.1412, 0.4214, 0.5996,  ..., 0.2239, 0.0627, 0.0000],\n",
       "          ...,\n",
       "          [0.1412, 0.4214, 0.5996,  ..., 0.2239, 0.0627, 0.0000],\n",
       "          [0.1412, 0.4214, 0.5996,  ..., 0.2239, 0.0627, 0.0000],\n",
       "          [0.1412, 0.4214, 0.5996,  ..., 0.2239, 0.0627, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0709, 0.0000,  ..., 0.0930, 0.6582, 0.0000],\n",
       "          [0.0000, 0.0709, 0.0000,  ..., 0.0930, 0.6582, 0.0000],\n",
       "          [0.0000, 0.0709, 0.0000,  ..., 0.0930, 0.6582, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0709, 0.0000,  ..., 0.0930, 0.6582, 0.0000],\n",
       "          [0.0000, 0.0709, 0.0000,  ..., 0.0930, 0.6582, 0.0000],\n",
       "          [0.0000, 0.0709, 0.0000,  ..., 0.0930, 0.6582, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__6__DOT__mlp__DOT__down_proj': tensor([[[0.4355, 0.0000, 0.3152,  ..., 0.5322, 0.0000, 0.0000],\n",
       "          [0.4355, 0.0000, 0.3152,  ..., 0.5322, 0.0000, 0.0000],\n",
       "          [0.4355, 0.0000, 0.3152,  ..., 0.5322, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.4355, 0.0000, 0.3152,  ..., 0.5322, 0.0000, 0.0000],\n",
       "          [0.4355, 0.0000, 0.3152,  ..., 0.5322, 0.0000, 0.0000],\n",
       "          [0.4355, 0.0000, 0.3152,  ..., 0.5322, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__self_attn__DOT__q_proj': tensor([[[0.5371, 0.0000, 0.0000,  ..., 0.5098, 0.0000, 0.0000],\n",
       "          [0.5371, 0.0000, 0.0000,  ..., 0.5098, 0.0000, 0.0000],\n",
       "          [0.5371, 0.0000, 0.0000,  ..., 0.5098, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5371, 0.0000, 0.0000,  ..., 0.5098, 0.0000, 0.0000],\n",
       "          [0.5371, 0.0000, 0.0000,  ..., 0.5098, 0.0000, 0.0000],\n",
       "          [0.5371, 0.0000, 0.0000,  ..., 0.5098, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__self_attn__DOT__k_proj': tensor([[[0.6025, 0.0000, 0.4111,  ..., 0.0000, 0.6362, 0.6191],\n",
       "          [0.6025, 0.0000, 0.4111,  ..., 0.0000, 0.6362, 0.6191],\n",
       "          [0.6025, 0.0000, 0.4111,  ..., 0.0000, 0.6362, 0.6191],\n",
       "          ...,\n",
       "          [0.6025, 0.0000, 0.4111,  ..., 0.0000, 0.6362, 0.6191],\n",
       "          [0.6025, 0.0000, 0.4111,  ..., 0.0000, 0.6362, 0.6191],\n",
       "          [0.6025, 0.0000, 0.4111,  ..., 0.0000, 0.6362, 0.6191]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__self_attn__DOT__v_proj': tensor([[[0.6533, 0.6479, 0.4346,  ..., 0.0809, 0.0000, 0.0000],\n",
       "          [0.6533, 0.6479, 0.4346,  ..., 0.0809, 0.0000, 0.0000],\n",
       "          [0.6533, 0.6479, 0.4346,  ..., 0.0809, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.6533, 0.6479, 0.4346,  ..., 0.0809, 0.0000, 0.0000],\n",
       "          [0.6533, 0.6479, 0.4346,  ..., 0.0809, 0.0000, 0.0000],\n",
       "          [0.6533, 0.6479, 0.4346,  ..., 0.0809, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__self_attn__DOT__o_proj': tensor([[[0.4167, 0.0000, 0.6914,  ..., 0.0000, 0.3386, 0.0000],\n",
       "          [0.4167, 0.0000, 0.6914,  ..., 0.0000, 0.3386, 0.0000],\n",
       "          [0.4167, 0.0000, 0.6914,  ..., 0.0000, 0.3386, 0.0000],\n",
       "          ...,\n",
       "          [0.4167, 0.0000, 0.6914,  ..., 0.0000, 0.3386, 0.0000],\n",
       "          [0.4167, 0.0000, 0.6914,  ..., 0.0000, 0.3386, 0.0000],\n",
       "          [0.4167, 0.0000, 0.6914,  ..., 0.0000, 0.3386, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.1736, 0.0000, 0.2817],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1736, 0.0000, 0.2817],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1736, 0.0000, 0.2817],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1736, 0.0000, 0.2817],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1736, 0.0000, 0.2817],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1736, 0.0000, 0.2817]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__mlp__DOT__up_proj': tensor([[[0.0347, 0.0932, 0.2430,  ..., 0.0000, 0.0000, 0.3301],\n",
       "          [0.0347, 0.0932, 0.2430,  ..., 0.0000, 0.0000, 0.3301],\n",
       "          [0.0347, 0.0932, 0.2430,  ..., 0.0000, 0.0000, 0.3301],\n",
       "          ...,\n",
       "          [0.0347, 0.0932, 0.2430,  ..., 0.0000, 0.0000, 0.3301],\n",
       "          [0.0347, 0.0932, 0.2430,  ..., 0.0000, 0.0000, 0.3301],\n",
       "          [0.0347, 0.0932, 0.2430,  ..., 0.0000, 0.0000, 0.3301]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__7__DOT__mlp__DOT__down_proj': tensor([[[0.5293, 0.2126, 0.0000,  ..., 0.3298, 0.6592, 0.0000],\n",
       "          [0.5293, 0.2126, 0.0000,  ..., 0.3298, 0.6592, 0.0000],\n",
       "          [0.5293, 0.2126, 0.0000,  ..., 0.3298, 0.6592, 0.0000],\n",
       "          ...,\n",
       "          [0.5293, 0.2126, 0.0000,  ..., 0.3298, 0.6592, 0.0000],\n",
       "          [0.5293, 0.2126, 0.0000,  ..., 0.3298, 0.6592, 0.0000],\n",
       "          [0.5293, 0.2126, 0.0000,  ..., 0.3298, 0.6592, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__self_attn__DOT__q_proj': tensor([[[0.3159, 0.0000, 0.5308,  ..., 0.3748, 0.0000, 0.0228],\n",
       "          [0.3159, 0.0000, 0.5308,  ..., 0.3748, 0.0000, 0.0228],\n",
       "          [0.3159, 0.0000, 0.5308,  ..., 0.3748, 0.0000, 0.0228],\n",
       "          ...,\n",
       "          [0.3159, 0.0000, 0.5308,  ..., 0.3748, 0.0000, 0.0228],\n",
       "          [0.3159, 0.0000, 0.5308,  ..., 0.3748, 0.0000, 0.0228],\n",
       "          [0.3159, 0.0000, 0.5308,  ..., 0.3748, 0.0000, 0.0228]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__self_attn__DOT__k_proj': tensor([[[0.5381, 0.4346, 0.6382,  ..., 0.0000, 0.1240, 0.0000],\n",
       "          [0.5381, 0.4346, 0.6382,  ..., 0.0000, 0.1240, 0.0000],\n",
       "          [0.5381, 0.4346, 0.6382,  ..., 0.0000, 0.1240, 0.0000],\n",
       "          ...,\n",
       "          [0.5381, 0.4346, 0.6382,  ..., 0.0000, 0.1240, 0.0000],\n",
       "          [0.5381, 0.4346, 0.6382,  ..., 0.0000, 0.1240, 0.0000],\n",
       "          [0.5381, 0.4346, 0.6382,  ..., 0.0000, 0.1240, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1962, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1962, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1962, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1962, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1962, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1962, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.2878, 0.4751,  ..., 0.2637, 0.0690, 0.4978],\n",
       "          [0.0000, 0.2878, 0.4751,  ..., 0.2637, 0.0690, 0.4978],\n",
       "          [0.0000, 0.2878, 0.4751,  ..., 0.2637, 0.0690, 0.4978],\n",
       "          ...,\n",
       "          [0.0000, 0.2878, 0.4751,  ..., 0.2637, 0.0690, 0.4978],\n",
       "          [0.0000, 0.2878, 0.4751,  ..., 0.2637, 0.0690, 0.4978],\n",
       "          [0.0000, 0.2878, 0.4751,  ..., 0.2637, 0.0690, 0.4978]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.1066, 0.0000,  ..., 0.0663, 0.0000, 0.0624],\n",
       "          [0.0000, 0.1066, 0.0000,  ..., 0.0663, 0.0000, 0.0624],\n",
       "          [0.0000, 0.1066, 0.0000,  ..., 0.0663, 0.0000, 0.0624],\n",
       "          ...,\n",
       "          [0.0000, 0.1066, 0.0000,  ..., 0.0663, 0.0000, 0.0624],\n",
       "          [0.0000, 0.1066, 0.0000,  ..., 0.0663, 0.0000, 0.0624],\n",
       "          [0.0000, 0.1066, 0.0000,  ..., 0.0663, 0.0000, 0.0624]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0000, 0.6841,  ..., 0.3782, 0.6040, 0.0284],\n",
       "          [0.0000, 0.0000, 0.6841,  ..., 0.3782, 0.6040, 0.0284],\n",
       "          [0.0000, 0.0000, 0.6841,  ..., 0.3782, 0.6040, 0.0284],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.6841,  ..., 0.3782, 0.6040, 0.0284],\n",
       "          [0.0000, 0.0000, 0.6841,  ..., 0.3782, 0.6040, 0.0284],\n",
       "          [0.0000, 0.0000, 0.6841,  ..., 0.3782, 0.6040, 0.0284]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__8__DOT__mlp__DOT__down_proj': tensor([[[0.3970, 0.3452, 0.0000,  ..., 0.0000, 0.2976, 0.6309],\n",
       "          [0.3970, 0.3452, 0.0000,  ..., 0.0000, 0.2976, 0.6309],\n",
       "          [0.3970, 0.3452, 0.0000,  ..., 0.0000, 0.2976, 0.6309],\n",
       "          ...,\n",
       "          [0.3970, 0.3452, 0.0000,  ..., 0.0000, 0.2976, 0.6309],\n",
       "          [0.3970, 0.3452, 0.0000,  ..., 0.0000, 0.2976, 0.6309],\n",
       "          [0.3970, 0.3452, 0.0000,  ..., 0.0000, 0.2976, 0.6309]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__self_attn__DOT__q_proj': tensor([[[0.4092, 0.3918, 0.5269,  ..., 0.0000, 0.4348, 0.0000],\n",
       "          [0.4092, 0.3918, 0.5269,  ..., 0.0000, 0.4348, 0.0000],\n",
       "          [0.4092, 0.3918, 0.5269,  ..., 0.0000, 0.4348, 0.0000],\n",
       "          ...,\n",
       "          [0.4092, 0.3918, 0.5269,  ..., 0.0000, 0.4348, 0.0000],\n",
       "          [0.4092, 0.3918, 0.5269,  ..., 0.0000, 0.4348, 0.0000],\n",
       "          [0.4092, 0.3918, 0.5269,  ..., 0.0000, 0.4348, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__self_attn__DOT__k_proj': tensor([[[0.3384, 0.0550, 0.4167,  ..., 0.5332, 0.2798, 0.0000],\n",
       "          [0.3384, 0.0550, 0.4167,  ..., 0.5332, 0.2798, 0.0000],\n",
       "          [0.3384, 0.0550, 0.4167,  ..., 0.5332, 0.2798, 0.0000],\n",
       "          ...,\n",
       "          [0.3384, 0.0550, 0.4167,  ..., 0.5332, 0.2798, 0.0000],\n",
       "          [0.3384, 0.0550, 0.4167,  ..., 0.5332, 0.2798, 0.0000],\n",
       "          [0.3384, 0.0550, 0.4167,  ..., 0.5332, 0.2798, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__self_attn__DOT__v_proj': tensor([[[0.5273, 0.0000, 0.0000,  ..., 0.6826, 0.0000, 0.0000],\n",
       "          [0.5273, 0.0000, 0.0000,  ..., 0.6826, 0.0000, 0.0000],\n",
       "          [0.5273, 0.0000, 0.0000,  ..., 0.6826, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5273, 0.0000, 0.0000,  ..., 0.6826, 0.0000, 0.0000],\n",
       "          [0.5273, 0.0000, 0.0000,  ..., 0.6826, 0.0000, 0.0000],\n",
       "          [0.5273, 0.0000, 0.0000,  ..., 0.6826, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0175, 0.0934,  ..., 0.6060, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0175, 0.0934,  ..., 0.6060, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0175, 0.0934,  ..., 0.6060, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0175, 0.0934,  ..., 0.6060, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0175, 0.0934,  ..., 0.6060, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0175, 0.0934,  ..., 0.6060, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.6167,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6167,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6167,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.6167,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6167,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6167,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__mlp__DOT__up_proj': tensor([[[0.4263, 0.5854, 0.3809,  ..., 0.0000, 0.2734, 0.6885],\n",
       "          [0.4263, 0.5854, 0.3809,  ..., 0.0000, 0.2734, 0.6885],\n",
       "          [0.4263, 0.5854, 0.3809,  ..., 0.0000, 0.2734, 0.6885],\n",
       "          ...,\n",
       "          [0.4263, 0.5854, 0.3809,  ..., 0.0000, 0.2734, 0.6885],\n",
       "          [0.4263, 0.5854, 0.3809,  ..., 0.0000, 0.2734, 0.6885],\n",
       "          [0.4263, 0.5854, 0.3809,  ..., 0.0000, 0.2734, 0.6885]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__9__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.5093, 0.0000,  ..., 0.6216, 0.0090, 0.4983],\n",
       "          [0.0000, 0.5093, 0.0000,  ..., 0.6216, 0.0090, 0.4983],\n",
       "          [0.0000, 0.5093, 0.0000,  ..., 0.6216, 0.0090, 0.4983],\n",
       "          ...,\n",
       "          [0.0000, 0.5093, 0.0000,  ..., 0.6216, 0.0090, 0.4983],\n",
       "          [0.0000, 0.5093, 0.0000,  ..., 0.6216, 0.0090, 0.4983],\n",
       "          [0.0000, 0.5093, 0.0000,  ..., 0.6216, 0.0090, 0.4983]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__self_attn__DOT__q_proj': tensor([[[0.5322, 0.1921, 0.0000,  ..., 0.0000, 0.1464, 0.0000],\n",
       "          [0.5322, 0.1921, 0.0000,  ..., 0.0000, 0.1464, 0.0000],\n",
       "          [0.5322, 0.1921, 0.0000,  ..., 0.0000, 0.1464, 0.0000],\n",
       "          ...,\n",
       "          [0.5322, 0.1921, 0.0000,  ..., 0.0000, 0.1464, 0.0000],\n",
       "          [0.5322, 0.1921, 0.0000,  ..., 0.0000, 0.1464, 0.0000],\n",
       "          [0.5322, 0.1921, 0.0000,  ..., 0.0000, 0.1464, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.1693, 0.0000,  ..., 0.4727, 0.0000, 0.0726],\n",
       "          [0.0000, 0.1693, 0.0000,  ..., 0.4727, 0.0000, 0.0726],\n",
       "          [0.0000, 0.1693, 0.0000,  ..., 0.4727, 0.0000, 0.0726],\n",
       "          ...,\n",
       "          [0.0000, 0.1693, 0.0000,  ..., 0.4727, 0.0000, 0.0726],\n",
       "          [0.0000, 0.1693, 0.0000,  ..., 0.4727, 0.0000, 0.0726],\n",
       "          [0.0000, 0.1693, 0.0000,  ..., 0.4727, 0.0000, 0.0726]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.7065, 0.1465,  ..., 0.6851, 0.0000, 0.0000],\n",
       "          [0.0000, 0.7065, 0.1465,  ..., 0.6851, 0.0000, 0.0000],\n",
       "          [0.0000, 0.7065, 0.1465,  ..., 0.6851, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.7065, 0.1465,  ..., 0.6851, 0.0000, 0.0000],\n",
       "          [0.0000, 0.7065, 0.1465,  ..., 0.6851, 0.0000, 0.0000],\n",
       "          [0.0000, 0.7065, 0.1465,  ..., 0.6851, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.3652, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3652, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3652, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.3652, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3652, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3652, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.3137, 0.6440],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.3137, 0.6440],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.3137, 0.6440],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.3137, 0.6440],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.3137, 0.6440],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2426, 0.3137, 0.6440]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__mlp__DOT__up_proj': tensor([[[0.1191, 0.5908, 0.0000,  ..., 0.4243, 0.0000, 0.0000],\n",
       "          [0.1191, 0.5908, 0.0000,  ..., 0.4243, 0.0000, 0.0000],\n",
       "          [0.1191, 0.5908, 0.0000,  ..., 0.4243, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1191, 0.5908, 0.0000,  ..., 0.4243, 0.0000, 0.0000],\n",
       "          [0.1191, 0.5908, 0.0000,  ..., 0.4243, 0.0000, 0.0000],\n",
       "          [0.1191, 0.5908, 0.0000,  ..., 0.4243, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__10__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.4275, 0.0000,  ..., 0.3818, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4275, 0.0000,  ..., 0.3818, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4275, 0.0000,  ..., 0.3818, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.4275, 0.0000,  ..., 0.3818, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4275, 0.0000,  ..., 0.3818, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4275, 0.0000,  ..., 0.3818, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1864, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1864, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1864, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1864, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1864, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1864, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.3748, 0.0985,  ..., 0.1498, 0.0846, 0.0509],\n",
       "          [0.0000, 0.3748, 0.0985,  ..., 0.1498, 0.0846, 0.0509],\n",
       "          [0.0000, 0.3748, 0.0985,  ..., 0.1498, 0.0846, 0.0509],\n",
       "          ...,\n",
       "          [0.0000, 0.3748, 0.0985,  ..., 0.1498, 0.0846, 0.0509],\n",
       "          [0.0000, 0.3748, 0.0985,  ..., 0.1498, 0.0846, 0.0509],\n",
       "          [0.0000, 0.3748, 0.0985,  ..., 0.1498, 0.0846, 0.0509]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__self_attn__DOT__v_proj': tensor([[[0.1854, 0.2325, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1854, 0.2325, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1854, 0.2325, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1854, 0.2325, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1854, 0.2325, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1854, 0.2325, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.1791, 0.0000,  ..., 0.5488, 0.0000, 0.0575],\n",
       "          [0.0000, 0.1791, 0.0000,  ..., 0.5488, 0.0000, 0.0575],\n",
       "          [0.0000, 0.1791, 0.0000,  ..., 0.5488, 0.0000, 0.0575],\n",
       "          ...,\n",
       "          [0.0000, 0.1791, 0.0000,  ..., 0.5488, 0.0000, 0.0575],\n",
       "          [0.0000, 0.1791, 0.0000,  ..., 0.5488, 0.0000, 0.0575],\n",
       "          [0.0000, 0.1791, 0.0000,  ..., 0.5488, 0.0000, 0.0575]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.2201, 0.5469,  ..., 0.0000, 0.0879, 0.3301],\n",
       "          [0.0000, 0.2201, 0.5469,  ..., 0.0000, 0.0879, 0.3301],\n",
       "          [0.0000, 0.2201, 0.5469,  ..., 0.0000, 0.0879, 0.3301],\n",
       "          ...,\n",
       "          [0.0000, 0.2201, 0.5469,  ..., 0.0000, 0.0879, 0.3301],\n",
       "          [0.0000, 0.2201, 0.5469,  ..., 0.0000, 0.0879, 0.3301],\n",
       "          [0.0000, 0.2201, 0.5469,  ..., 0.0000, 0.0879, 0.3301]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__mlp__DOT__up_proj': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__11__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2983, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2983, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2983, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2983, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2983, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2983, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__self_attn__DOT__q_proj': tensor([[[0.4443, 0.0358, 0.0460,  ..., 0.7021, 0.5376, 0.5640],\n",
       "          [0.4443, 0.0358, 0.0460,  ..., 0.7021, 0.5376, 0.5640],\n",
       "          [0.4443, 0.0358, 0.0460,  ..., 0.7021, 0.5376, 0.5640],\n",
       "          ...,\n",
       "          [0.4443, 0.0358, 0.0460,  ..., 0.7021, 0.5376, 0.5640],\n",
       "          [0.4443, 0.0358, 0.0460,  ..., 0.7021, 0.5376, 0.5640],\n",
       "          [0.4443, 0.0358, 0.0460,  ..., 0.7021, 0.5376, 0.5640]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.2617, 0.4583,  ..., 0.5674, 0.6162, 0.1965],\n",
       "          [0.0000, 0.2617, 0.4583,  ..., 0.5674, 0.6162, 0.1965],\n",
       "          [0.0000, 0.2617, 0.4583,  ..., 0.5674, 0.6162, 0.1965],\n",
       "          ...,\n",
       "          [0.0000, 0.2617, 0.4583,  ..., 0.5674, 0.6162, 0.1965],\n",
       "          [0.0000, 0.2617, 0.4583,  ..., 0.5674, 0.6162, 0.1965],\n",
       "          [0.0000, 0.2617, 0.4583,  ..., 0.5674, 0.6162, 0.1965]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.5425, 0.0000,  ..., 0.0000, 0.5923, 0.5098],\n",
       "          [0.0000, 0.5425, 0.0000,  ..., 0.0000, 0.5923, 0.5098],\n",
       "          [0.0000, 0.5425, 0.0000,  ..., 0.0000, 0.5923, 0.5098],\n",
       "          ...,\n",
       "          [0.0000, 0.5425, 0.0000,  ..., 0.0000, 0.5923, 0.5098],\n",
       "          [0.0000, 0.5425, 0.0000,  ..., 0.0000, 0.5923, 0.5098],\n",
       "          [0.0000, 0.5425, 0.0000,  ..., 0.0000, 0.5923, 0.5098]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.1973, 0.1340,  ..., 0.0000, 0.0000, 0.1949],\n",
       "          [0.0000, 0.1973, 0.1340,  ..., 0.0000, 0.0000, 0.1949],\n",
       "          [0.0000, 0.1973, 0.1340,  ..., 0.0000, 0.0000, 0.1949],\n",
       "          ...,\n",
       "          [0.0000, 0.1973, 0.1340,  ..., 0.0000, 0.0000, 0.1949],\n",
       "          [0.0000, 0.1973, 0.1340,  ..., 0.0000, 0.0000, 0.1949],\n",
       "          [0.0000, 0.1973, 0.1340,  ..., 0.0000, 0.0000, 0.1949]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__mlp__DOT__gate_proj': tensor([[[0.1260, 0.3979, 0.6421,  ..., 0.0000, 0.0000, 0.2373],\n",
       "          [0.1260, 0.3979, 0.6421,  ..., 0.0000, 0.0000, 0.2373],\n",
       "          [0.1260, 0.3979, 0.6421,  ..., 0.0000, 0.0000, 0.2373],\n",
       "          ...,\n",
       "          [0.1260, 0.3979, 0.6421,  ..., 0.0000, 0.0000, 0.2373],\n",
       "          [0.1260, 0.3979, 0.6421,  ..., 0.0000, 0.0000, 0.2373],\n",
       "          [0.1260, 0.3979, 0.6421,  ..., 0.0000, 0.0000, 0.2373]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.4851, 0.0000,  ..., 0.0000, 0.1534, 0.0000],\n",
       "          [0.0000, 0.4851, 0.0000,  ..., 0.0000, 0.1534, 0.0000],\n",
       "          [0.0000, 0.4851, 0.0000,  ..., 0.0000, 0.1534, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.4851, 0.0000,  ..., 0.0000, 0.1534, 0.0000],\n",
       "          [0.0000, 0.4851, 0.0000,  ..., 0.0000, 0.1534, 0.0000],\n",
       "          [0.0000, 0.4851, 0.0000,  ..., 0.0000, 0.1534, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__12__DOT__mlp__DOT__down_proj': tensor([[[0.0274, 0.0000, 0.0000,  ..., 0.3904, 0.3367, 0.0000],\n",
       "          [0.0274, 0.0000, 0.0000,  ..., 0.3904, 0.3367, 0.0000],\n",
       "          [0.0274, 0.0000, 0.0000,  ..., 0.3904, 0.3367, 0.0000],\n",
       "          ...,\n",
       "          [0.0274, 0.0000, 0.0000,  ..., 0.3904, 0.3367, 0.0000],\n",
       "          [0.0274, 0.0000, 0.0000,  ..., 0.3904, 0.3367, 0.0000],\n",
       "          [0.0274, 0.0000, 0.0000,  ..., 0.3904, 0.3367, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2230, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2230, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2230, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2230, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2230, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2230, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.5693, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5693, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5693, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.5693, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5693, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5693, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.1519, 0.0000,  ..., 0.0000, 0.5239, 0.0000],\n",
       "          [0.0000, 0.1519, 0.0000,  ..., 0.0000, 0.5239, 0.0000],\n",
       "          [0.0000, 0.1519, 0.0000,  ..., 0.0000, 0.5239, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.1519, 0.0000,  ..., 0.0000, 0.5239, 0.0000],\n",
       "          [0.0000, 0.1519, 0.0000,  ..., 0.0000, 0.5239, 0.0000],\n",
       "          [0.0000, 0.1519, 0.0000,  ..., 0.0000, 0.5239, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.2886, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.2886, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.2886, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.2886, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.2886, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.2886, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__mlp__DOT__gate_proj': tensor([[[0.2856, 0.0000, 0.2413,  ..., 0.0895, 0.0000, 0.3625],\n",
       "          [0.2856, 0.0000, 0.2413,  ..., 0.0895, 0.0000, 0.3625],\n",
       "          [0.2856, 0.0000, 0.2413,  ..., 0.0895, 0.0000, 0.3625],\n",
       "          ...,\n",
       "          [0.2856, 0.0000, 0.2413,  ..., 0.0895, 0.0000, 0.3625],\n",
       "          [0.2856, 0.0000, 0.2413,  ..., 0.0895, 0.0000, 0.3625],\n",
       "          [0.2856, 0.0000, 0.2413,  ..., 0.0895, 0.0000, 0.3625]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__mlp__DOT__up_proj': tensor([[[0.3054, 0.4841, 0.0000,  ..., 0.4436, 0.0000, 0.3867],\n",
       "          [0.3054, 0.4841, 0.0000,  ..., 0.4436, 0.0000, 0.3867],\n",
       "          [0.3054, 0.4841, 0.0000,  ..., 0.4436, 0.0000, 0.3867],\n",
       "          ...,\n",
       "          [0.3054, 0.4841, 0.0000,  ..., 0.4436, 0.0000, 0.3867],\n",
       "          [0.3054, 0.4841, 0.0000,  ..., 0.4436, 0.0000, 0.3867],\n",
       "          [0.3054, 0.4841, 0.0000,  ..., 0.4436, 0.0000, 0.3867]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__13__DOT__mlp__DOT__down_proj': tensor([[[0.5029, 0.1368, 0.0000,  ..., 0.0000, 0.0000, 0.2805],\n",
       "          [0.5029, 0.1368, 0.0000,  ..., 0.0000, 0.0000, 0.2805],\n",
       "          [0.5029, 0.1368, 0.0000,  ..., 0.0000, 0.0000, 0.2805],\n",
       "          ...,\n",
       "          [0.5029, 0.1368, 0.0000,  ..., 0.0000, 0.0000, 0.2805],\n",
       "          [0.5029, 0.1368, 0.0000,  ..., 0.0000, 0.0000, 0.2805],\n",
       "          [0.5029, 0.1368, 0.0000,  ..., 0.0000, 0.0000, 0.2805]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.1959, 0.0000,  ..., 0.0000, 0.0000, 0.6689],\n",
       "          [0.0000, 0.1959, 0.0000,  ..., 0.0000, 0.0000, 0.6689],\n",
       "          [0.0000, 0.1959, 0.0000,  ..., 0.0000, 0.0000, 0.6689],\n",
       "          ...,\n",
       "          [0.0000, 0.1959, 0.0000,  ..., 0.0000, 0.0000, 0.6689],\n",
       "          [0.0000, 0.1959, 0.0000,  ..., 0.0000, 0.0000, 0.6689],\n",
       "          [0.0000, 0.1959, 0.0000,  ..., 0.0000, 0.0000, 0.6689]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.2585, 0.0417,  ..., 0.2278, 0.0000, 0.5010],\n",
       "          [0.0000, 0.2585, 0.0417,  ..., 0.2278, 0.0000, 0.5010],\n",
       "          [0.0000, 0.2585, 0.0417,  ..., 0.2278, 0.0000, 0.5010],\n",
       "          ...,\n",
       "          [0.0000, 0.2585, 0.0417,  ..., 0.2278, 0.0000, 0.5010],\n",
       "          [0.0000, 0.2585, 0.0417,  ..., 0.2278, 0.0000, 0.5010],\n",
       "          [0.0000, 0.2585, 0.0417,  ..., 0.2278, 0.0000, 0.5010]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.7070,  ..., 0.0000, 0.4414, 0.0000],\n",
       "          [0.0000, 0.0000, 0.7070,  ..., 0.0000, 0.4414, 0.0000],\n",
       "          [0.0000, 0.0000, 0.7070,  ..., 0.0000, 0.4414, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.7070,  ..., 0.0000, 0.4414, 0.0000],\n",
       "          [0.0000, 0.0000, 0.7070,  ..., 0.0000, 0.4414, 0.0000],\n",
       "          [0.0000, 0.0000, 0.7070,  ..., 0.0000, 0.4414, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0981, 0.0000,  ..., 0.0000, 0.1787, 0.0000],\n",
       "          [0.0000, 0.0981, 0.0000,  ..., 0.0000, 0.1787, 0.0000],\n",
       "          [0.0000, 0.0981, 0.0000,  ..., 0.0000, 0.1787, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0981, 0.0000,  ..., 0.0000, 0.1787, 0.0000],\n",
       "          [0.0000, 0.0981, 0.0000,  ..., 0.0000, 0.1787, 0.0000],\n",
       "          [0.0000, 0.0981, 0.0000,  ..., 0.0000, 0.1787, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__mlp__DOT__gate_proj': tensor([[[0.2439, 0.0000, 0.3091,  ..., 0.0000, 0.0000, 0.1530],\n",
       "          [0.2439, 0.0000, 0.3091,  ..., 0.0000, 0.0000, 0.1530],\n",
       "          [0.2439, 0.0000, 0.3091,  ..., 0.0000, 0.0000, 0.1530],\n",
       "          ...,\n",
       "          [0.2439, 0.0000, 0.3091,  ..., 0.0000, 0.0000, 0.1530],\n",
       "          [0.2439, 0.0000, 0.3091,  ..., 0.0000, 0.0000, 0.1530],\n",
       "          [0.2439, 0.0000, 0.3091,  ..., 0.0000, 0.0000, 0.1530]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__mlp__DOT__up_proj': tensor([[[0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3074],\n",
       "          [0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3074],\n",
       "          [0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3074],\n",
       "          ...,\n",
       "          [0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3074],\n",
       "          [0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3074],\n",
       "          [0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3074]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__14__DOT__mlp__DOT__down_proj': tensor([[[0.5278, 0.0000, 0.0000,  ..., 0.1036, 0.0000, 0.0000],\n",
       "          [0.5278, 0.0000, 0.0000,  ..., 0.1036, 0.0000, 0.0000],\n",
       "          [0.5278, 0.0000, 0.0000,  ..., 0.1036, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5278, 0.0000, 0.0000,  ..., 0.1036, 0.0000, 0.0000],\n",
       "          [0.5278, 0.0000, 0.0000,  ..., 0.1036, 0.0000, 0.0000],\n",
       "          [0.5278, 0.0000, 0.0000,  ..., 0.1036, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__self_attn__DOT__q_proj': tensor([[[0.4077, 0.0000, 0.0000,  ..., 0.0419, 0.1744, 0.0529],\n",
       "          [0.4077, 0.0000, 0.0000,  ..., 0.0419, 0.1744, 0.0529],\n",
       "          [0.4077, 0.0000, 0.0000,  ..., 0.0419, 0.1744, 0.0529],\n",
       "          ...,\n",
       "          [0.4077, 0.0000, 0.0000,  ..., 0.0419, 0.1744, 0.0529],\n",
       "          [0.4077, 0.0000, 0.0000,  ..., 0.0419, 0.1744, 0.0529],\n",
       "          [0.4077, 0.0000, 0.0000,  ..., 0.0419, 0.1744, 0.0529]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.0927, 0.0334,  ..., 0.1564, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0927, 0.0334,  ..., 0.1564, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0927, 0.0334,  ..., 0.1564, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0927, 0.0334,  ..., 0.1564, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0927, 0.0334,  ..., 0.1564, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0927, 0.0334,  ..., 0.1564, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__self_attn__DOT__v_proj': tensor([[[0.2646, 0.5571, 0.4729,  ..., 0.0778, 0.0000, 0.0000],\n",
       "          [0.2646, 0.5571, 0.4729,  ..., 0.0778, 0.0000, 0.0000],\n",
       "          [0.2646, 0.5571, 0.4729,  ..., 0.0778, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.2646, 0.5571, 0.4729,  ..., 0.0778, 0.0000, 0.0000],\n",
       "          [0.2646, 0.5571, 0.4729,  ..., 0.0778, 0.0000, 0.0000],\n",
       "          [0.2646, 0.5571, 0.4729,  ..., 0.0778, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__self_attn__DOT__o_proj': tensor([[[0.0376, 0.0000, 0.3762,  ..., 0.2358, 0.0000, 0.3579],\n",
       "          [0.0376, 0.0000, 0.3762,  ..., 0.2358, 0.0000, 0.3579],\n",
       "          [0.0376, 0.0000, 0.3762,  ..., 0.2358, 0.0000, 0.3579],\n",
       "          ...,\n",
       "          [0.0376, 0.0000, 0.3762,  ..., 0.2358, 0.0000, 0.3579],\n",
       "          [0.0376, 0.0000, 0.3762,  ..., 0.2358, 0.0000, 0.3579],\n",
       "          [0.0376, 0.0000, 0.3762,  ..., 0.2358, 0.0000, 0.3579]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__mlp__DOT__gate_proj': tensor([[[0.3469, 0.0416, 0.0000,  ..., 0.3342, 0.2074, 0.6060],\n",
       "          [0.3469, 0.0416, 0.0000,  ..., 0.3342, 0.2074, 0.6060],\n",
       "          [0.3469, 0.0416, 0.0000,  ..., 0.3342, 0.2074, 0.6060],\n",
       "          ...,\n",
       "          [0.3469, 0.0416, 0.0000,  ..., 0.3342, 0.2074, 0.6060],\n",
       "          [0.3469, 0.0416, 0.0000,  ..., 0.3342, 0.2074, 0.6060],\n",
       "          [0.3469, 0.0416, 0.0000,  ..., 0.3342, 0.2074, 0.6060]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__mlp__DOT__up_proj': tensor([[[0.3936, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1284],\n",
       "          [0.3936, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1284],\n",
       "          [0.3936, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1284],\n",
       "          ...,\n",
       "          [0.3936, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1284],\n",
       "          [0.3936, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1284],\n",
       "          [0.3936, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1284]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__15__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.0000, 0.4016,  ..., 0.2167, 0.2329, 0.1164],\n",
       "          [0.0000, 0.0000, 0.4016,  ..., 0.2167, 0.2329, 0.1164],\n",
       "          [0.0000, 0.0000, 0.4016,  ..., 0.2167, 0.2329, 0.1164],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.4016,  ..., 0.2167, 0.2329, 0.1164],\n",
       "          [0.0000, 0.0000, 0.4016,  ..., 0.2167, 0.2329, 0.1164],\n",
       "          [0.0000, 0.0000, 0.4016,  ..., 0.2167, 0.2329, 0.1164]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__self_attn__DOT__q_proj': tensor([[[0.6064, 0.0444, 0.3843,  ..., 0.0000, 0.5161, 0.0000],\n",
       "          [0.6064, 0.0444, 0.3843,  ..., 0.0000, 0.5161, 0.0000],\n",
       "          [0.6064, 0.0444, 0.3843,  ..., 0.0000, 0.5161, 0.0000],\n",
       "          ...,\n",
       "          [0.6064, 0.0444, 0.3843,  ..., 0.0000, 0.5161, 0.0000],\n",
       "          [0.6064, 0.0444, 0.3843,  ..., 0.0000, 0.5161, 0.0000],\n",
       "          [0.6064, 0.0444, 0.3843,  ..., 0.0000, 0.5161, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__self_attn__DOT__k_proj': tensor([[[0.0287, 0.0150, 0.0000,  ..., 0.0000, 0.0860, 0.0000],\n",
       "          [0.0287, 0.0150, 0.0000,  ..., 0.0000, 0.0860, 0.0000],\n",
       "          [0.0287, 0.0150, 0.0000,  ..., 0.0000, 0.0860, 0.0000],\n",
       "          ...,\n",
       "          [0.0287, 0.0150, 0.0000,  ..., 0.0000, 0.0860, 0.0000],\n",
       "          [0.0287, 0.0150, 0.0000,  ..., 0.0000, 0.0860, 0.0000],\n",
       "          [0.0287, 0.0150, 0.0000,  ..., 0.0000, 0.0860, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__self_attn__DOT__v_proj': tensor([[[0.4609, 0.1137, 0.0000,  ..., 0.0000, 0.0000, 0.1892],\n",
       "          [0.4609, 0.1137, 0.0000,  ..., 0.0000, 0.0000, 0.1892],\n",
       "          [0.4609, 0.1137, 0.0000,  ..., 0.0000, 0.0000, 0.1892],\n",
       "          ...,\n",
       "          [0.4609, 0.1137, 0.0000,  ..., 0.0000, 0.0000, 0.1892],\n",
       "          [0.4609, 0.1137, 0.0000,  ..., 0.0000, 0.0000, 0.1892],\n",
       "          [0.4609, 0.1137, 0.0000,  ..., 0.0000, 0.0000, 0.1892]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0559, 0.0000,  ..., 0.4619, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0559, 0.0000,  ..., 0.4619, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0559, 0.0000,  ..., 0.4619, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0559, 0.0000,  ..., 0.4619, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0559, 0.0000,  ..., 0.4619, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0559, 0.0000,  ..., 0.4619, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__mlp__DOT__gate_proj': tensor([[[0.5020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__mlp__DOT__up_proj': tensor([[[0.3069, 0.1609, 0.3523,  ..., 0.4966, 0.0000, 0.0000],\n",
       "          [0.3069, 0.1609, 0.3523,  ..., 0.4966, 0.0000, 0.0000],\n",
       "          [0.3069, 0.1609, 0.3523,  ..., 0.4966, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.3069, 0.1609, 0.3523,  ..., 0.4966, 0.0000, 0.0000],\n",
       "          [0.3069, 0.1609, 0.3523,  ..., 0.4966, 0.0000, 0.0000],\n",
       "          [0.3069, 0.1609, 0.3523,  ..., 0.4966, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__16__DOT__mlp__DOT__down_proj': tensor([[[0.4990, 0.3896, 0.0000,  ..., 0.0000, 0.1051, 0.0000],\n",
       "          [0.4990, 0.3896, 0.0000,  ..., 0.0000, 0.1051, 0.0000],\n",
       "          [0.4990, 0.3896, 0.0000,  ..., 0.0000, 0.1051, 0.0000],\n",
       "          ...,\n",
       "          [0.4990, 0.3896, 0.0000,  ..., 0.0000, 0.1051, 0.0000],\n",
       "          [0.4990, 0.3896, 0.0000,  ..., 0.0000, 0.1051, 0.0000],\n",
       "          [0.4990, 0.3896, 0.0000,  ..., 0.0000, 0.1051, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.2607, 0.6968,  ..., 0.6221, 0.5737, 0.0000],\n",
       "          [0.0000, 0.2607, 0.6968,  ..., 0.6221, 0.5737, 0.0000],\n",
       "          [0.0000, 0.2607, 0.6968,  ..., 0.6221, 0.5737, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.2607, 0.6968,  ..., 0.6221, 0.5737, 0.0000],\n",
       "          [0.0000, 0.2607, 0.6968,  ..., 0.6221, 0.5737, 0.0000],\n",
       "          [0.0000, 0.2607, 0.6968,  ..., 0.6221, 0.5737, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__self_attn__DOT__k_proj': tensor([[[0.4226, 0.1707, 0.6621,  ..., 0.0000, 0.3921, 0.5439],\n",
       "          [0.4226, 0.1707, 0.6621,  ..., 0.0000, 0.3921, 0.5439],\n",
       "          [0.4226, 0.1707, 0.6621,  ..., 0.0000, 0.3921, 0.5439],\n",
       "          ...,\n",
       "          [0.4226, 0.1707, 0.6621,  ..., 0.0000, 0.3921, 0.5439],\n",
       "          [0.4226, 0.1707, 0.6621,  ..., 0.0000, 0.3921, 0.5439],\n",
       "          [0.4226, 0.1707, 0.6621,  ..., 0.0000, 0.3921, 0.5439]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5576, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5576, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5576, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5576, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5576, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5576, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0829, 0.0000,  ..., 0.0000, 0.5776, 0.0000],\n",
       "          [0.0000, 0.0829, 0.0000,  ..., 0.0000, 0.5776, 0.0000],\n",
       "          [0.0000, 0.0829, 0.0000,  ..., 0.0000, 0.5776, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0829, 0.0000,  ..., 0.0000, 0.5776, 0.0000],\n",
       "          [0.0000, 0.0829, 0.0000,  ..., 0.0000, 0.5776, 0.0000],\n",
       "          [0.0000, 0.0829, 0.0000,  ..., 0.0000, 0.5776, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.1987],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.1987],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.1987],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.1987],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.1987],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.1987]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.4631, 0.0000,  ..., 0.0000, 0.5591, 0.6411],\n",
       "          [0.0000, 0.4631, 0.0000,  ..., 0.0000, 0.5591, 0.6411],\n",
       "          [0.0000, 0.4631, 0.0000,  ..., 0.0000, 0.5591, 0.6411],\n",
       "          ...,\n",
       "          [0.0000, 0.4631, 0.0000,  ..., 0.0000, 0.5591, 0.6411],\n",
       "          [0.0000, 0.4631, 0.0000,  ..., 0.0000, 0.5591, 0.6411],\n",
       "          [0.0000, 0.4631, 0.0000,  ..., 0.0000, 0.5591, 0.6411]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__17__DOT__mlp__DOT__down_proj': tensor([[[0.3672, 0.0000, 0.5771,  ..., 0.2450, 0.0000, 0.0000],\n",
       "          [0.3672, 0.0000, 0.5771,  ..., 0.2450, 0.0000, 0.0000],\n",
       "          [0.3672, 0.0000, 0.5771,  ..., 0.2450, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.3672, 0.0000, 0.5771,  ..., 0.2450, 0.0000, 0.0000],\n",
       "          [0.3672, 0.0000, 0.5771,  ..., 0.2450, 0.0000, 0.0000],\n",
       "          [0.3672, 0.0000, 0.5771,  ..., 0.2450, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__self_attn__DOT__q_proj': tensor([[[0.4971, 0.4998, 0.0864,  ..., 0.1088, 0.6177, 0.6836],\n",
       "          [0.4971, 0.4998, 0.0864,  ..., 0.1088, 0.6177, 0.6836],\n",
       "          [0.4971, 0.4998, 0.0864,  ..., 0.1088, 0.6177, 0.6836],\n",
       "          ...,\n",
       "          [0.4971, 0.4998, 0.0864,  ..., 0.1088, 0.6177, 0.6836],\n",
       "          [0.4971, 0.4998, 0.0864,  ..., 0.1088, 0.6177, 0.6836],\n",
       "          [0.4971, 0.4998, 0.0864,  ..., 0.1088, 0.6177, 0.6836]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.0000, 0.0048,  ..., 0.4089, 0.3538, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0048,  ..., 0.4089, 0.3538, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0048,  ..., 0.4089, 0.3538, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0048,  ..., 0.4089, 0.3538, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0048,  ..., 0.4089, 0.3538, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0048,  ..., 0.4089, 0.3538, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.1805],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.1805],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.1805],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.1805],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.1805],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1438, 0.0000, 0.1805]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__self_attn__DOT__o_proj': tensor([[[0.6646, 0.4829, 0.0196,  ..., 0.7031, 0.4639, 0.0000],\n",
       "          [0.6646, 0.4829, 0.0196,  ..., 0.7031, 0.4639, 0.0000],\n",
       "          [0.6646, 0.4829, 0.0196,  ..., 0.7031, 0.4639, 0.0000],\n",
       "          ...,\n",
       "          [0.6646, 0.4829, 0.0196,  ..., 0.7031, 0.4639, 0.0000],\n",
       "          [0.6646, 0.4829, 0.0196,  ..., 0.7031, 0.4639, 0.0000],\n",
       "          [0.6646, 0.4829, 0.0196,  ..., 0.7031, 0.4639, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__mlp__DOT__gate_proj': tensor([[[0.6982, 0.0000, 0.0000,  ..., 0.5327, 0.1610, 0.0000],\n",
       "          [0.6982, 0.0000, 0.0000,  ..., 0.5327, 0.1610, 0.0000],\n",
       "          [0.6982, 0.0000, 0.0000,  ..., 0.5327, 0.1610, 0.0000],\n",
       "          ...,\n",
       "          [0.6982, 0.0000, 0.0000,  ..., 0.5327, 0.1610, 0.0000],\n",
       "          [0.6982, 0.0000, 0.0000,  ..., 0.5327, 0.1610, 0.0000],\n",
       "          [0.6982, 0.0000, 0.0000,  ..., 0.5327, 0.1610, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.2920, 0.4656,  ..., 0.0000, 0.0000, 0.1216],\n",
       "          [0.0000, 0.2920, 0.4656,  ..., 0.0000, 0.0000, 0.1216],\n",
       "          [0.0000, 0.2920, 0.4656,  ..., 0.0000, 0.0000, 0.1216],\n",
       "          ...,\n",
       "          [0.0000, 0.2920, 0.4656,  ..., 0.0000, 0.0000, 0.1216],\n",
       "          [0.0000, 0.2920, 0.4656,  ..., 0.0000, 0.0000, 0.1216],\n",
       "          [0.0000, 0.2920, 0.4656,  ..., 0.0000, 0.0000, 0.1216]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__18__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.0000, 0.3721,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3721,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3721,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.3721,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3721,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3721,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.3503, 0.5576, 0.6382],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3503, 0.5576, 0.6382],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3503, 0.5576, 0.6382],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3503, 0.5576, 0.6382],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3503, 0.5576, 0.6382],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3503, 0.5576, 0.6382]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.2605, 0.1121, 0.2551],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2605, 0.1121, 0.2551],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2605, 0.1121, 0.2551],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2605, 0.1121, 0.2551],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2605, 0.1121, 0.2551],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2605, 0.1121, 0.2551]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.5469,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5469,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5469,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.5469,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5469,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5469,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1993],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1993],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1993],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1993],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1993],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1993]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.1237,  ..., 0.5693, 0.6255, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1237,  ..., 0.5693, 0.6255, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1237,  ..., 0.5693, 0.6255, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.1237,  ..., 0.5693, 0.6255, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1237,  ..., 0.5693, 0.6255, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1237,  ..., 0.5693, 0.6255, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__mlp__DOT__up_proj': tensor([[[0.0646, 0.3005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0646, 0.3005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0646, 0.3005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0646, 0.3005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0646, 0.3005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0646, 0.3005, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__19__DOT__mlp__DOT__down_proj': tensor([[[0.1091, 0.3384, 0.1620,  ..., 0.6440, 0.2380, 0.0000],\n",
       "          [0.1091, 0.3384, 0.1620,  ..., 0.6440, 0.2380, 0.0000],\n",
       "          [0.1091, 0.3384, 0.1620,  ..., 0.6440, 0.2380, 0.0000],\n",
       "          ...,\n",
       "          [0.1091, 0.3384, 0.1620,  ..., 0.6440, 0.2380, 0.0000],\n",
       "          [0.1091, 0.3384, 0.1620,  ..., 0.6440, 0.2380, 0.0000],\n",
       "          [0.1091, 0.3384, 0.1620,  ..., 0.6440, 0.2380, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__self_attn__DOT__q_proj': tensor([[[0.2942, 0.0000, 0.0000,  ..., 0.0000, 0.2449, 0.5752],\n",
       "          [0.2942, 0.0000, 0.0000,  ..., 0.0000, 0.2449, 0.5752],\n",
       "          [0.2942, 0.0000, 0.0000,  ..., 0.0000, 0.2449, 0.5752],\n",
       "          ...,\n",
       "          [0.2942, 0.0000, 0.0000,  ..., 0.0000, 0.2449, 0.5752],\n",
       "          [0.2942, 0.0000, 0.0000,  ..., 0.0000, 0.2449, 0.5752],\n",
       "          [0.2942, 0.0000, 0.0000,  ..., 0.0000, 0.2449, 0.5752]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.0427, 0.4441,  ..., 0.2629, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0427, 0.4441,  ..., 0.2629, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0427, 0.4441,  ..., 0.2629, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0427, 0.4441,  ..., 0.2629, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0427, 0.4441,  ..., 0.2629, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0427, 0.4441,  ..., 0.2629, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.2678, 0.1223,  ..., 0.2844, 0.4268, 0.0000],\n",
       "          [0.0000, 0.2678, 0.1223,  ..., 0.2844, 0.4268, 0.0000],\n",
       "          [0.0000, 0.2678, 0.1223,  ..., 0.2844, 0.4268, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.2678, 0.1223,  ..., 0.2844, 0.4268, 0.0000],\n",
       "          [0.0000, 0.2678, 0.1223,  ..., 0.2844, 0.4268, 0.0000],\n",
       "          [0.0000, 0.2678, 0.1223,  ..., 0.2844, 0.4268, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.4890, 0.0000,  ..., 0.0000, 0.4751, 0.0837],\n",
       "          [0.0000, 0.4890, 0.0000,  ..., 0.0000, 0.4751, 0.0837],\n",
       "          [0.0000, 0.4890, 0.0000,  ..., 0.0000, 0.4751, 0.0837],\n",
       "          ...,\n",
       "          [0.0000, 0.4890, 0.0000,  ..., 0.0000, 0.4751, 0.0837],\n",
       "          [0.0000, 0.4890, 0.0000,  ..., 0.0000, 0.4751, 0.0837],\n",
       "          [0.0000, 0.4890, 0.0000,  ..., 0.0000, 0.4751, 0.0837]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__mlp__DOT__up_proj': tensor([[[0.4319, 0.2062, 0.0000,  ..., 0.0000, 0.4939, 0.0000],\n",
       "          [0.4319, 0.2062, 0.0000,  ..., 0.0000, 0.4939, 0.0000],\n",
       "          [0.4319, 0.2062, 0.0000,  ..., 0.0000, 0.4939, 0.0000],\n",
       "          ...,\n",
       "          [0.4319, 0.2062, 0.0000,  ..., 0.0000, 0.4939, 0.0000],\n",
       "          [0.4319, 0.2062, 0.0000,  ..., 0.0000, 0.4939, 0.0000],\n",
       "          [0.4319, 0.2062, 0.0000,  ..., 0.0000, 0.4939, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__20__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.0000, 0.5747,  ..., 0.3450, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5747,  ..., 0.3450, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5747,  ..., 0.3450, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.5747,  ..., 0.3450, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5747,  ..., 0.3450, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5747,  ..., 0.3450, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.0000, 0.6865,  ..., 0.4055, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6865,  ..., 0.4055, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6865,  ..., 0.4055, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.6865,  ..., 0.4055, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6865,  ..., 0.4055, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.6865,  ..., 0.4055, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__self_attn__DOT__v_proj': tensor([[[0.3259, 0.0000, 0.3457,  ..., 0.1059, 0.0000, 0.0000],\n",
       "          [0.3259, 0.0000, 0.3457,  ..., 0.1059, 0.0000, 0.0000],\n",
       "          [0.3259, 0.0000, 0.3457,  ..., 0.1059, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.3259, 0.0000, 0.3457,  ..., 0.1059, 0.0000, 0.0000],\n",
       "          [0.3259, 0.0000, 0.3457,  ..., 0.1059, 0.0000, 0.0000],\n",
       "          [0.3259, 0.0000, 0.3457,  ..., 0.1059, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__self_attn__DOT__o_proj': tensor([[[0.3403, 0.0202, 0.0000,  ..., 0.1078, 0.2859, 0.0000],\n",
       "          [0.3403, 0.0202, 0.0000,  ..., 0.1078, 0.2859, 0.0000],\n",
       "          [0.3403, 0.0202, 0.0000,  ..., 0.1078, 0.2859, 0.0000],\n",
       "          ...,\n",
       "          [0.3403, 0.0202, 0.0000,  ..., 0.1078, 0.2859, 0.0000],\n",
       "          [0.3403, 0.0202, 0.0000,  ..., 0.1078, 0.2859, 0.0000],\n",
       "          [0.3403, 0.0202, 0.0000,  ..., 0.1078, 0.2859, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__mlp__DOT__gate_proj': tensor([[[0.1776, 0.0491, 0.0000,  ..., 0.2332, 0.0000, 0.0000],\n",
       "          [0.1776, 0.0491, 0.0000,  ..., 0.2332, 0.0000, 0.0000],\n",
       "          [0.1776, 0.0491, 0.0000,  ..., 0.2332, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1776, 0.0491, 0.0000,  ..., 0.2332, 0.0000, 0.0000],\n",
       "          [0.1776, 0.0491, 0.0000,  ..., 0.2332, 0.0000, 0.0000],\n",
       "          [0.1776, 0.0491, 0.0000,  ..., 0.2332, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0000, 0.2072,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2072,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2072,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.2072,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2072,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2072,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__21__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.6143, 0.0000,  ..., 0.0847, 0.4976, 0.1016],\n",
       "          [0.0000, 0.6143, 0.0000,  ..., 0.0847, 0.4976, 0.1016],\n",
       "          [0.0000, 0.6143, 0.0000,  ..., 0.0847, 0.4976, 0.1016],\n",
       "          ...,\n",
       "          [0.0000, 0.6143, 0.0000,  ..., 0.0847, 0.4976, 0.1016],\n",
       "          [0.0000, 0.6143, 0.0000,  ..., 0.0847, 0.4976, 0.1016],\n",
       "          [0.0000, 0.6143, 0.0000,  ..., 0.0847, 0.4976, 0.1016]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.5366,  ..., 0.6079, 0.2167, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5366,  ..., 0.6079, 0.2167, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5366,  ..., 0.6079, 0.2167, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.5366,  ..., 0.6079, 0.2167, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5366,  ..., 0.6079, 0.2167, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5366,  ..., 0.6079, 0.2167, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.5674, 0.2263,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5674, 0.2263,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5674, 0.2263,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.5674, 0.2263,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5674, 0.2263,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5674, 0.2263,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__self_attn__DOT__v_proj': tensor([[[0.2520, 0.0000, 0.4131,  ..., 0.0000, 0.0351, 0.5659],\n",
       "          [0.2520, 0.0000, 0.4131,  ..., 0.0000, 0.0351, 0.5659],\n",
       "          [0.2520, 0.0000, 0.4131,  ..., 0.0000, 0.0351, 0.5659],\n",
       "          ...,\n",
       "          [0.2520, 0.0000, 0.4131,  ..., 0.0000, 0.0351, 0.5659],\n",
       "          [0.2520, 0.0000, 0.4131,  ..., 0.0000, 0.0351, 0.5659],\n",
       "          [0.2520, 0.0000, 0.4131,  ..., 0.0000, 0.0351, 0.5659]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.6440, 0.0123, 0.1412],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6440, 0.0123, 0.1412],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6440, 0.0123, 0.1412],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6440, 0.0123, 0.1412],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6440, 0.0123, 0.1412],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6440, 0.0123, 0.1412]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__mlp__DOT__gate_proj': tensor([[[0.0116, 0.2151, 0.0000,  ..., 0.1786, 0.0000, 0.0000],\n",
       "          [0.0116, 0.2151, 0.0000,  ..., 0.1786, 0.0000, 0.0000],\n",
       "          [0.0116, 0.2151, 0.0000,  ..., 0.1786, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0116, 0.2151, 0.0000,  ..., 0.1786, 0.0000, 0.0000],\n",
       "          [0.0116, 0.2151, 0.0000,  ..., 0.1786, 0.0000, 0.0000],\n",
       "          [0.0116, 0.2151, 0.0000,  ..., 0.1786, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0264, 0.0000,  ..., 0.0806, 0.3677, 0.0000],\n",
       "          [0.0000, 0.0264, 0.0000,  ..., 0.0806, 0.3677, 0.0000],\n",
       "          [0.0000, 0.0264, 0.0000,  ..., 0.0806, 0.3677, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0264, 0.0000,  ..., 0.0806, 0.3677, 0.0000],\n",
       "          [0.0000, 0.0264, 0.0000,  ..., 0.0806, 0.3677, 0.0000],\n",
       "          [0.0000, 0.0264, 0.0000,  ..., 0.0806, 0.3677, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__22__DOT__mlp__DOT__down_proj': tensor([[[0.0858, 0.0000, 0.0108,  ..., 0.0046, 0.5762, 0.0000],\n",
       "          [0.0858, 0.0000, 0.0108,  ..., 0.0046, 0.5762, 0.0000],\n",
       "          [0.0858, 0.0000, 0.0108,  ..., 0.0046, 0.5762, 0.0000],\n",
       "          ...,\n",
       "          [0.0858, 0.0000, 0.0108,  ..., 0.0046, 0.5762, 0.0000],\n",
       "          [0.0858, 0.0000, 0.0108,  ..., 0.0046, 0.5762, 0.0000],\n",
       "          [0.0858, 0.0000, 0.0108,  ..., 0.0046, 0.5762, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.6172, 0.0087, 0.5903],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6172, 0.0087, 0.5903],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6172, 0.0087, 0.5903],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6172, 0.0087, 0.5903],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6172, 0.0087, 0.5903],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.6172, 0.0087, 0.5903]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.4907, 0.0000,  ..., 0.0000, 0.0000, 0.4370],\n",
       "          [0.0000, 0.4907, 0.0000,  ..., 0.0000, 0.0000, 0.4370],\n",
       "          [0.0000, 0.4907, 0.0000,  ..., 0.0000, 0.0000, 0.4370],\n",
       "          ...,\n",
       "          [0.0000, 0.4907, 0.0000,  ..., 0.0000, 0.0000, 0.4370],\n",
       "          [0.0000, 0.4907, 0.0000,  ..., 0.0000, 0.0000, 0.4370],\n",
       "          [0.0000, 0.4907, 0.0000,  ..., 0.0000, 0.0000, 0.4370]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0223, 0.5693,  ..., 0.5806, 0.4150, 0.5610],\n",
       "          [0.0000, 0.0223, 0.5693,  ..., 0.5806, 0.4150, 0.5610],\n",
       "          [0.0000, 0.0223, 0.5693,  ..., 0.5806, 0.4150, 0.5610],\n",
       "          ...,\n",
       "          [0.0000, 0.0223, 0.5693,  ..., 0.5806, 0.4150, 0.5610],\n",
       "          [0.0000, 0.0223, 0.5693,  ..., 0.5806, 0.4150, 0.5610],\n",
       "          [0.0000, 0.0223, 0.5693,  ..., 0.5806, 0.4150, 0.5610]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.3171, 0.2546,  ..., 0.0000, 0.1749, 0.1826],\n",
       "          [0.0000, 0.3171, 0.2546,  ..., 0.0000, 0.1749, 0.1826],\n",
       "          [0.0000, 0.3171, 0.2546,  ..., 0.0000, 0.1749, 0.1826],\n",
       "          ...,\n",
       "          [0.0000, 0.3171, 0.2546,  ..., 0.0000, 0.1749, 0.1826],\n",
       "          [0.0000, 0.3171, 0.2546,  ..., 0.0000, 0.1749, 0.1826],\n",
       "          [0.0000, 0.3171, 0.2546,  ..., 0.0000, 0.1749, 0.1826]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__mlp__DOT__gate_proj': tensor([[[0.6851, 0.3403, 0.3674,  ..., 0.2465, 0.0000, 0.0000],\n",
       "          [0.6851, 0.3403, 0.3674,  ..., 0.2465, 0.0000, 0.0000],\n",
       "          [0.6851, 0.3403, 0.3674,  ..., 0.2465, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.6851, 0.3403, 0.3674,  ..., 0.2465, 0.0000, 0.0000],\n",
       "          [0.6851, 0.3403, 0.3674,  ..., 0.2465, 0.0000, 0.0000],\n",
       "          [0.6851, 0.3403, 0.3674,  ..., 0.2465, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0000, 0.4570,  ..., 0.6909, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4570,  ..., 0.6909, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4570,  ..., 0.6909, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.4570,  ..., 0.6909, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4570,  ..., 0.6909, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4570,  ..., 0.6909, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__23__DOT__mlp__DOT__down_proj': tensor([[[0.1884, 0.0000, 0.1759,  ..., 0.2235, 0.0000, 0.2571],\n",
       "          [0.1884, 0.0000, 0.1759,  ..., 0.2235, 0.0000, 0.2571],\n",
       "          [0.1884, 0.0000, 0.1759,  ..., 0.2235, 0.0000, 0.2571],\n",
       "          ...,\n",
       "          [0.1884, 0.0000, 0.1759,  ..., 0.2235, 0.0000, 0.2571],\n",
       "          [0.1884, 0.0000, 0.1759,  ..., 0.2235, 0.0000, 0.2571],\n",
       "          [0.1884, 0.0000, 0.1759,  ..., 0.2235, 0.0000, 0.2571]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.2600, 0.6753, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2600, 0.6753, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2600, 0.6753, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2600, 0.6753, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2600, 0.6753, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.2600, 0.6753, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.0000, 0.4180,  ..., 0.6680, 0.5732, 0.2678],\n",
       "          [0.0000, 0.0000, 0.4180,  ..., 0.6680, 0.5732, 0.2678],\n",
       "          [0.0000, 0.0000, 0.4180,  ..., 0.6680, 0.5732, 0.2678],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.4180,  ..., 0.6680, 0.5732, 0.2678],\n",
       "          [0.0000, 0.0000, 0.4180,  ..., 0.6680, 0.5732, 0.2678],\n",
       "          [0.0000, 0.0000, 0.4180,  ..., 0.6680, 0.5732, 0.2678]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__self_attn__DOT__v_proj': tensor([[[0.2327, 0.0000, 0.4529,  ..., 0.0000, 0.5420, 0.0000],\n",
       "          [0.2327, 0.0000, 0.4529,  ..., 0.0000, 0.5420, 0.0000],\n",
       "          [0.2327, 0.0000, 0.4529,  ..., 0.0000, 0.5420, 0.0000],\n",
       "          ...,\n",
       "          [0.2327, 0.0000, 0.4529,  ..., 0.0000, 0.5420, 0.0000],\n",
       "          [0.2327, 0.0000, 0.4529,  ..., 0.0000, 0.5420, 0.0000],\n",
       "          [0.2327, 0.0000, 0.4529,  ..., 0.0000, 0.5420, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0000, 0.2168,  ..., 0.1315, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2168,  ..., 0.1315, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2168,  ..., 0.1315, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.2168,  ..., 0.1315, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2168,  ..., 0.1315, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2168,  ..., 0.1315, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__mlp__DOT__gate_proj': tensor([[[0.1238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3188],\n",
       "          [0.1238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3188],\n",
       "          [0.1238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3188],\n",
       "          ...,\n",
       "          [0.1238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3188],\n",
       "          [0.1238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3188],\n",
       "          [0.1238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3188]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__mlp__DOT__up_proj': tensor([[[0.3506, 0.5791, 0.4744,  ..., 0.0000, 0.1709, 0.0000],\n",
       "          [0.3506, 0.5791, 0.4744,  ..., 0.0000, 0.1709, 0.0000],\n",
       "          [0.3506, 0.5791, 0.4744,  ..., 0.0000, 0.1709, 0.0000],\n",
       "          ...,\n",
       "          [0.3506, 0.5791, 0.4744,  ..., 0.0000, 0.1709, 0.0000],\n",
       "          [0.3506, 0.5791, 0.4744,  ..., 0.0000, 0.1709, 0.0000],\n",
       "          [0.3506, 0.5791, 0.4744,  ..., 0.0000, 0.1709, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__24__DOT__mlp__DOT__down_proj': tensor([[[0.4097, 0.5151, 0.6440,  ..., 0.0000, 0.0000, 0.4258],\n",
       "          [0.4097, 0.5151, 0.6440,  ..., 0.0000, 0.0000, 0.4258],\n",
       "          [0.4097, 0.5151, 0.6440,  ..., 0.0000, 0.0000, 0.4258],\n",
       "          ...,\n",
       "          [0.4097, 0.5151, 0.6440,  ..., 0.0000, 0.0000, 0.4258],\n",
       "          [0.4097, 0.5151, 0.6440,  ..., 0.0000, 0.0000, 0.4258],\n",
       "          [0.4097, 0.5151, 0.6440,  ..., 0.0000, 0.0000, 0.4258]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__self_attn__DOT__q_proj': tensor([[[0.4136, 0.1906, 0.0000,  ..., 0.0977, 0.0000, 0.0000],\n",
       "          [0.4136, 0.1906, 0.0000,  ..., 0.0977, 0.0000, 0.0000],\n",
       "          [0.4136, 0.1906, 0.0000,  ..., 0.0977, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.4136, 0.1906, 0.0000,  ..., 0.0977, 0.0000, 0.0000],\n",
       "          [0.4136, 0.1906, 0.0000,  ..., 0.0977, 0.0000, 0.0000],\n",
       "          [0.4136, 0.1906, 0.0000,  ..., 0.0977, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__self_attn__DOT__k_proj': tensor([[[0.0411, 0.6870, 0.0000,  ..., 0.2139, 0.0000, 0.0437],\n",
       "          [0.0411, 0.6870, 0.0000,  ..., 0.2139, 0.0000, 0.0437],\n",
       "          [0.0411, 0.6870, 0.0000,  ..., 0.2139, 0.0000, 0.0437],\n",
       "          ...,\n",
       "          [0.0411, 0.6870, 0.0000,  ..., 0.2139, 0.0000, 0.0437],\n",
       "          [0.0411, 0.6870, 0.0000,  ..., 0.2139, 0.0000, 0.0437],\n",
       "          [0.0411, 0.6870, 0.0000,  ..., 0.2139, 0.0000, 0.0437]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__self_attn__DOT__v_proj': tensor([[[0.4551, 0.0000, 0.0000,  ..., 0.7012, 0.5552, 0.0000],\n",
       "          [0.4551, 0.0000, 0.0000,  ..., 0.7012, 0.5552, 0.0000],\n",
       "          [0.4551, 0.0000, 0.0000,  ..., 0.7012, 0.5552, 0.0000],\n",
       "          ...,\n",
       "          [0.4551, 0.0000, 0.0000,  ..., 0.7012, 0.5552, 0.0000],\n",
       "          [0.4551, 0.0000, 0.0000,  ..., 0.7012, 0.5552, 0.0000],\n",
       "          [0.4551, 0.0000, 0.0000,  ..., 0.7012, 0.5552, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0000, 0.4067,  ..., 0.2539, 0.6152, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4067,  ..., 0.2539, 0.6152, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4067,  ..., 0.2539, 0.6152, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.4067,  ..., 0.2539, 0.6152, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4067,  ..., 0.2539, 0.6152, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4067,  ..., 0.2539, 0.6152, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.2378, 0.0737,  ..., 0.4583, 0.4180, 0.1252],\n",
       "          [0.0000, 0.2378, 0.0737,  ..., 0.4583, 0.4180, 0.1252],\n",
       "          [0.0000, 0.2378, 0.0737,  ..., 0.4583, 0.4180, 0.1252],\n",
       "          ...,\n",
       "          [0.0000, 0.2378, 0.0737,  ..., 0.4583, 0.4180, 0.1252],\n",
       "          [0.0000, 0.2378, 0.0737,  ..., 0.4583, 0.4180, 0.1252],\n",
       "          [0.0000, 0.2378, 0.0737,  ..., 0.4583, 0.4180, 0.1252]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__mlp__DOT__up_proj': tensor([[[0.1318, 0.2108, 0.0000,  ..., 0.0000, 0.2461, 0.3484],\n",
       "          [0.1318, 0.2108, 0.0000,  ..., 0.0000, 0.2461, 0.3484],\n",
       "          [0.1318, 0.2108, 0.0000,  ..., 0.0000, 0.2461, 0.3484],\n",
       "          ...,\n",
       "          [0.1318, 0.2108, 0.0000,  ..., 0.0000, 0.2461, 0.3484],\n",
       "          [0.1318, 0.2108, 0.0000,  ..., 0.0000, 0.2461, 0.3484],\n",
       "          [0.1318, 0.2108, 0.0000,  ..., 0.0000, 0.2461, 0.3484]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__25__DOT__mlp__DOT__down_proj': tensor([[[0.1064, 0.1509, 0.0000,  ..., 0.0000, 0.3962, 0.0000],\n",
       "          [0.1064, 0.1509, 0.0000,  ..., 0.0000, 0.3962, 0.0000],\n",
       "          [0.1064, 0.1509, 0.0000,  ..., 0.0000, 0.3962, 0.0000],\n",
       "          ...,\n",
       "          [0.1064, 0.1509, 0.0000,  ..., 0.0000, 0.3962, 0.0000],\n",
       "          [0.1064, 0.1509, 0.0000,  ..., 0.0000, 0.3962, 0.0000],\n",
       "          [0.1064, 0.1509, 0.0000,  ..., 0.0000, 0.3962, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__self_attn__DOT__q_proj': tensor([[[0.1794, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1794, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1794, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1794, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1794, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1794, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__self_attn__DOT__k_proj': tensor([[[0.6362, 0.3110, 0.0000,  ..., 0.0000, 0.0000, 0.3333],\n",
       "          [0.6362, 0.3110, 0.0000,  ..., 0.0000, 0.0000, 0.3333],\n",
       "          [0.6362, 0.3110, 0.0000,  ..., 0.0000, 0.0000, 0.3333],\n",
       "          ...,\n",
       "          [0.6362, 0.3110, 0.0000,  ..., 0.0000, 0.0000, 0.3333],\n",
       "          [0.6362, 0.3110, 0.0000,  ..., 0.0000, 0.0000, 0.3333],\n",
       "          [0.6362, 0.3110, 0.0000,  ..., 0.0000, 0.0000, 0.3333]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.0000, 0.5430,  ..., 0.6118, 0.0485, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5430,  ..., 0.6118, 0.0485, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5430,  ..., 0.6118, 0.0485, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.5430,  ..., 0.6118, 0.0485, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5430,  ..., 0.6118, 0.0485, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5430,  ..., 0.6118, 0.0485, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.3599, 0.0000,  ..., 0.1436, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3599, 0.0000,  ..., 0.1436, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3599, 0.0000,  ..., 0.1436, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.3599, 0.0000,  ..., 0.1436, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3599, 0.0000,  ..., 0.1436, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3599, 0.0000,  ..., 0.1436, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.2448,  ..., 0.0288, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2448,  ..., 0.0288, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2448,  ..., 0.0288, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.2448,  ..., 0.0288, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2448,  ..., 0.0288, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2448,  ..., 0.0288, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__mlp__DOT__up_proj': tensor([[[0.1840, 0.0000, 0.6152,  ..., 0.0468, 0.0000, 0.1119],\n",
       "          [0.1840, 0.0000, 0.6152,  ..., 0.0468, 0.0000, 0.1119],\n",
       "          [0.1840, 0.0000, 0.6152,  ..., 0.0468, 0.0000, 0.1119],\n",
       "          ...,\n",
       "          [0.1840, 0.0000, 0.6152,  ..., 0.0468, 0.0000, 0.1119],\n",
       "          [0.1840, 0.0000, 0.6152,  ..., 0.0468, 0.0000, 0.1119],\n",
       "          [0.1840, 0.0000, 0.6152,  ..., 0.0468, 0.0000, 0.1119]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__26__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.7031, 0.6353,  ..., 0.1400, 0.1598, 0.0000],\n",
       "          [0.0000, 0.7031, 0.6353,  ..., 0.1400, 0.1598, 0.0000],\n",
       "          [0.0000, 0.7031, 0.6353,  ..., 0.1400, 0.1598, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.7031, 0.6353,  ..., 0.1400, 0.1598, 0.0000],\n",
       "          [0.0000, 0.7031, 0.6353,  ..., 0.1400, 0.1598, 0.0000],\n",
       "          [0.0000, 0.7031, 0.6353,  ..., 0.1400, 0.1598, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__self_attn__DOT__q_proj': tensor([[[0.0578, 0.0000, 0.0000,  ..., 0.1246, 0.1488, 0.0000],\n",
       "          [0.0578, 0.0000, 0.0000,  ..., 0.1246, 0.1488, 0.0000],\n",
       "          [0.0578, 0.0000, 0.0000,  ..., 0.1246, 0.1488, 0.0000],\n",
       "          ...,\n",
       "          [0.0578, 0.0000, 0.0000,  ..., 0.1246, 0.1488, 0.0000],\n",
       "          [0.0578, 0.0000, 0.0000,  ..., 0.1246, 0.1488, 0.0000],\n",
       "          [0.0578, 0.0000, 0.0000,  ..., 0.1246, 0.1488, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__self_attn__DOT__k_proj': tensor([[[0.6606, 0.0000, 0.0000,  ..., 0.0000, 0.1271, 0.5854],\n",
       "          [0.6606, 0.0000, 0.0000,  ..., 0.0000, 0.1271, 0.5854],\n",
       "          [0.6606, 0.0000, 0.0000,  ..., 0.0000, 0.1271, 0.5854],\n",
       "          ...,\n",
       "          [0.6606, 0.0000, 0.0000,  ..., 0.0000, 0.1271, 0.5854],\n",
       "          [0.6606, 0.0000, 0.0000,  ..., 0.0000, 0.1271, 0.5854],\n",
       "          [0.6606, 0.0000, 0.0000,  ..., 0.0000, 0.1271, 0.5854]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__self_attn__DOT__v_proj': tensor([[[0.2288, 0.0000, 0.0000,  ..., 0.4236, 0.0000, 0.0000],\n",
       "          [0.2288, 0.0000, 0.0000,  ..., 0.4236, 0.0000, 0.0000],\n",
       "          [0.2288, 0.0000, 0.0000,  ..., 0.4236, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.2288, 0.0000, 0.0000,  ..., 0.4236, 0.0000, 0.0000],\n",
       "          [0.2288, 0.0000, 0.0000,  ..., 0.4236, 0.0000, 0.0000],\n",
       "          [0.2288, 0.0000, 0.0000,  ..., 0.4236, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__self_attn__DOT__o_proj': tensor([[[0.7051, 0.0000, 0.0000,  ..., 0.0000, 0.6172, 0.0000],\n",
       "          [0.7051, 0.0000, 0.0000,  ..., 0.0000, 0.6172, 0.0000],\n",
       "          [0.7051, 0.0000, 0.0000,  ..., 0.0000, 0.6172, 0.0000],\n",
       "          ...,\n",
       "          [0.7051, 0.0000, 0.0000,  ..., 0.0000, 0.6172, 0.0000],\n",
       "          [0.7051, 0.0000, 0.0000,  ..., 0.0000, 0.6172, 0.0000],\n",
       "          [0.7051, 0.0000, 0.0000,  ..., 0.0000, 0.6172, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.3638, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3638, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3638, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.3638, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3638, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3638, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__mlp__DOT__up_proj': tensor([[[0.3103, 0.0745, 0.5078,  ..., 0.0000, 0.4324, 0.0000],\n",
       "          [0.3103, 0.0745, 0.5078,  ..., 0.0000, 0.4324, 0.0000],\n",
       "          [0.3103, 0.0745, 0.5078,  ..., 0.0000, 0.4324, 0.0000],\n",
       "          ...,\n",
       "          [0.3103, 0.0745, 0.5078,  ..., 0.0000, 0.4324, 0.0000],\n",
       "          [0.3103, 0.0745, 0.5078,  ..., 0.0000, 0.4324, 0.0000],\n",
       "          [0.3103, 0.0745, 0.5078,  ..., 0.0000, 0.4324, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__27__DOT__mlp__DOT__down_proj': tensor([[[0.1332, 0.0446, 0.6396,  ..., 0.0000, 0.0000, 0.0302],\n",
       "          [0.1332, 0.0446, 0.6396,  ..., 0.0000, 0.0000, 0.0302],\n",
       "          [0.1332, 0.0446, 0.6396,  ..., 0.0000, 0.0000, 0.0302],\n",
       "          ...,\n",
       "          [0.1332, 0.0446, 0.6396,  ..., 0.0000, 0.0000, 0.0302],\n",
       "          [0.1332, 0.0446, 0.6396,  ..., 0.0000, 0.0000, 0.0302],\n",
       "          [0.1332, 0.0446, 0.6396,  ..., 0.0000, 0.0000, 0.0302]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__self_attn__DOT__q_proj': tensor([[[0.0000, 0.5713, 0.0000,  ..., 0.0000, 0.5640, 0.4834],\n",
       "          [0.0000, 0.5713, 0.0000,  ..., 0.0000, 0.5640, 0.4834],\n",
       "          [0.0000, 0.5713, 0.0000,  ..., 0.0000, 0.5640, 0.4834],\n",
       "          ...,\n",
       "          [0.0000, 0.5713, 0.0000,  ..., 0.0000, 0.5640, 0.4834],\n",
       "          [0.0000, 0.5713, 0.0000,  ..., 0.0000, 0.5640, 0.4834],\n",
       "          [0.0000, 0.5713, 0.0000,  ..., 0.0000, 0.5640, 0.4834]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__self_attn__DOT__k_proj': tensor([[[0.6450, 0.0000, 0.2612,  ..., 0.0000, 0.0811, 0.2988],\n",
       "          [0.6450, 0.0000, 0.2612,  ..., 0.0000, 0.0811, 0.2988],\n",
       "          [0.6450, 0.0000, 0.2612,  ..., 0.0000, 0.0811, 0.2988],\n",
       "          ...,\n",
       "          [0.6450, 0.0000, 0.2612,  ..., 0.0000, 0.0811, 0.2988],\n",
       "          [0.6450, 0.0000, 0.2612,  ..., 0.0000, 0.0811, 0.2988],\n",
       "          [0.6450, 0.0000, 0.2612,  ..., 0.0000, 0.0811, 0.2988]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__self_attn__DOT__v_proj': tensor([[[0.6133, 0.4543, 0.0000,  ..., 0.0000, 0.5132, 0.0000],\n",
       "          [0.6133, 0.4543, 0.0000,  ..., 0.0000, 0.5132, 0.0000],\n",
       "          [0.6133, 0.4543, 0.0000,  ..., 0.0000, 0.5132, 0.0000],\n",
       "          ...,\n",
       "          [0.6133, 0.4543, 0.0000,  ..., 0.0000, 0.5132, 0.0000],\n",
       "          [0.6133, 0.4543, 0.0000,  ..., 0.0000, 0.5132, 0.0000],\n",
       "          [0.6133, 0.4543, 0.0000,  ..., 0.0000, 0.5132, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__self_attn__DOT__o_proj': tensor([[[0.5742, 0.0000, 0.0000,  ..., 0.4014, 0.2739, 0.6992],\n",
       "          [0.5742, 0.0000, 0.0000,  ..., 0.4014, 0.2739, 0.6992],\n",
       "          [0.5742, 0.0000, 0.0000,  ..., 0.4014, 0.2739, 0.6992],\n",
       "          ...,\n",
       "          [0.5742, 0.0000, 0.0000,  ..., 0.4014, 0.2739, 0.6992],\n",
       "          [0.5742, 0.0000, 0.0000,  ..., 0.4014, 0.2739, 0.6992],\n",
       "          [0.5742, 0.0000, 0.0000,  ..., 0.4014, 0.2739, 0.6992]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.3499, 0.0000,  ..., 0.3740, 0.0000, 0.3113],\n",
       "          [0.0000, 0.3499, 0.0000,  ..., 0.3740, 0.0000, 0.3113],\n",
       "          [0.0000, 0.3499, 0.0000,  ..., 0.3740, 0.0000, 0.3113],\n",
       "          ...,\n",
       "          [0.0000, 0.3499, 0.0000,  ..., 0.3740, 0.0000, 0.3113],\n",
       "          [0.0000, 0.3499, 0.0000,  ..., 0.3740, 0.0000, 0.3113],\n",
       "          [0.0000, 0.3499, 0.0000,  ..., 0.3740, 0.0000, 0.3113]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0000, 0.3567,  ..., 0.0000, 0.4131, 0.0323],\n",
       "          [0.0000, 0.0000, 0.3567,  ..., 0.0000, 0.4131, 0.0323],\n",
       "          [0.0000, 0.0000, 0.3567,  ..., 0.0000, 0.4131, 0.0323],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.3567,  ..., 0.0000, 0.4131, 0.0323],\n",
       "          [0.0000, 0.0000, 0.3567,  ..., 0.0000, 0.4131, 0.0323],\n",
       "          [0.0000, 0.0000, 0.3567,  ..., 0.0000, 0.4131, 0.0323]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__28__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0060, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0060, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0060, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0060, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0060, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0060, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__self_attn__DOT__q_proj': tensor([[[0.5386, 0.6655, 0.5391,  ..., 0.0000, 0.4961, 0.0000],\n",
       "          [0.5386, 0.6655, 0.5391,  ..., 0.0000, 0.4961, 0.0000],\n",
       "          [0.5386, 0.6655, 0.5391,  ..., 0.0000, 0.4961, 0.0000],\n",
       "          ...,\n",
       "          [0.5386, 0.6655, 0.5391,  ..., 0.0000, 0.4961, 0.0000],\n",
       "          [0.5386, 0.6655, 0.5391,  ..., 0.0000, 0.4961, 0.0000],\n",
       "          [0.5386, 0.6655, 0.5391,  ..., 0.0000, 0.4961, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__self_attn__DOT__k_proj': tensor([[[0.0000, 0.3574, 0.0000,  ..., 0.0583, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3574, 0.0000,  ..., 0.0583, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3574, 0.0000,  ..., 0.0583, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.3574, 0.0000,  ..., 0.0583, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3574, 0.0000,  ..., 0.0583, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3574, 0.0000,  ..., 0.0583, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__self_attn__DOT__v_proj': tensor([[[0.0000, 0.4050, 0.4963,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4050, 0.4963,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4050, 0.4963,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.4050, 0.4963,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4050, 0.4963,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4050, 0.4963,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__self_attn__DOT__o_proj': tensor([[[0.1377, 0.1111, 0.4700,  ..., 0.0000, 0.3379, 0.0000],\n",
       "          [0.1377, 0.1111, 0.4700,  ..., 0.0000, 0.3379, 0.0000],\n",
       "          [0.1377, 0.1111, 0.4700,  ..., 0.0000, 0.3379, 0.0000],\n",
       "          ...,\n",
       "          [0.1377, 0.1111, 0.4700,  ..., 0.0000, 0.3379, 0.0000],\n",
       "          [0.1377, 0.1111, 0.4700,  ..., 0.0000, 0.3379, 0.0000],\n",
       "          [0.1377, 0.1111, 0.4700,  ..., 0.0000, 0.3379, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.5757, 0.0000,  ..., 0.1605, 0.2378, 0.0000],\n",
       "          [0.0000, 0.5757, 0.0000,  ..., 0.1605, 0.2378, 0.0000],\n",
       "          [0.0000, 0.5757, 0.0000,  ..., 0.1605, 0.2378, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.5757, 0.0000,  ..., 0.1605, 0.2378, 0.0000],\n",
       "          [0.0000, 0.5757, 0.0000,  ..., 0.1605, 0.2378, 0.0000],\n",
       "          [0.0000, 0.5757, 0.0000,  ..., 0.1605, 0.2378, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.5776, 0.6543,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5776, 0.6543,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5776, 0.6543,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.5776, 0.6543,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5776, 0.6543,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5776, 0.6543,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__29__DOT__mlp__DOT__down_proj': tensor([[[0.0000, 0.2773, 0.0184,  ..., 0.6528, 0.2910, 0.0000],\n",
       "          [0.0000, 0.2773, 0.0184,  ..., 0.6528, 0.2910, 0.0000],\n",
       "          [0.0000, 0.2773, 0.0184,  ..., 0.6528, 0.2910, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.2773, 0.0184,  ..., 0.6528, 0.2910, 0.0000],\n",
       "          [0.0000, 0.2773, 0.0184,  ..., 0.6528, 0.2910, 0.0000],\n",
       "          [0.0000, 0.2773, 0.0184,  ..., 0.6528, 0.2910, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__self_attn__DOT__q_proj': tensor([[[0.1864, 0.2202, 0.0000,  ..., 0.0000, 0.0718, 0.1506],\n",
       "          [0.1864, 0.2202, 0.0000,  ..., 0.0000, 0.0718, 0.1506],\n",
       "          [0.1864, 0.2202, 0.0000,  ..., 0.0000, 0.0718, 0.1506],\n",
       "          ...,\n",
       "          [0.1864, 0.2202, 0.0000,  ..., 0.0000, 0.0718, 0.1506],\n",
       "          [0.1864, 0.2202, 0.0000,  ..., 0.0000, 0.0718, 0.1506],\n",
       "          [0.1864, 0.2202, 0.0000,  ..., 0.0000, 0.0718, 0.1506]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__self_attn__DOT__k_proj': tensor([[[0.5938, 0.0000, 0.0000,  ..., 0.2688, 0.0000, 0.4915],\n",
       "          [0.5938, 0.0000, 0.0000,  ..., 0.2688, 0.0000, 0.4915],\n",
       "          [0.5938, 0.0000, 0.0000,  ..., 0.2688, 0.0000, 0.4915],\n",
       "          ...,\n",
       "          [0.5938, 0.0000, 0.0000,  ..., 0.2688, 0.0000, 0.4915],\n",
       "          [0.5938, 0.0000, 0.0000,  ..., 0.2688, 0.0000, 0.4915],\n",
       "          [0.5938, 0.0000, 0.0000,  ..., 0.2688, 0.0000, 0.4915]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__self_attn__DOT__v_proj': tensor([[[0.1077, 0.2090, 0.0000,  ..., 0.3958, 0.4426, 0.0000],\n",
       "          [0.1077, 0.2090, 0.0000,  ..., 0.3958, 0.4426, 0.0000],\n",
       "          [0.1077, 0.2090, 0.0000,  ..., 0.3958, 0.4426, 0.0000],\n",
       "          ...,\n",
       "          [0.1077, 0.2090, 0.0000,  ..., 0.3958, 0.4426, 0.0000],\n",
       "          [0.1077, 0.2090, 0.0000,  ..., 0.3958, 0.4426, 0.0000],\n",
       "          [0.1077, 0.2090, 0.0000,  ..., 0.3958, 0.4426, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__self_attn__DOT__o_proj': tensor([[[0.6821, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3201],\n",
       "          [0.6821, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3201],\n",
       "          [0.6821, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3201],\n",
       "          ...,\n",
       "          [0.6821, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3201],\n",
       "          [0.6821, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3201],\n",
       "          [0.6821, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3201]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__mlp__DOT__gate_proj': tensor([[[0.0038, 0.0000, 0.0000,  ..., 0.0929, 0.6177, 0.0000],\n",
       "          [0.0038, 0.0000, 0.0000,  ..., 0.0929, 0.6177, 0.0000],\n",
       "          [0.0038, 0.0000, 0.0000,  ..., 0.0929, 0.6177, 0.0000],\n",
       "          ...,\n",
       "          [0.0038, 0.0000, 0.0000,  ..., 0.0929, 0.6177, 0.0000],\n",
       "          [0.0038, 0.0000, 0.0000,  ..., 0.0929, 0.6177, 0.0000],\n",
       "          [0.0038, 0.0000, 0.0000,  ..., 0.0929, 0.6177, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__mlp__DOT__up_proj': tensor([[[0.6235, 0.0000, 0.4946,  ..., 0.5244, 0.0000, 0.2983],\n",
       "          [0.6235, 0.0000, 0.4946,  ..., 0.5244, 0.0000, 0.2983],\n",
       "          [0.6235, 0.0000, 0.4946,  ..., 0.5244, 0.0000, 0.2983],\n",
       "          ...,\n",
       "          [0.6235, 0.0000, 0.4946,  ..., 0.5244, 0.0000, 0.2983],\n",
       "          [0.6235, 0.0000, 0.4946,  ..., 0.5244, 0.0000, 0.2983],\n",
       "          [0.6235, 0.0000, 0.4946,  ..., 0.5244, 0.0000, 0.2983]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__30__DOT__mlp__DOT__down_proj': tensor([[[0.0681, 0.6963, 0.5923,  ..., 0.2532, 0.6309, 0.0000],\n",
       "          [0.0681, 0.6963, 0.5923,  ..., 0.2532, 0.6309, 0.0000],\n",
       "          [0.0681, 0.6963, 0.5923,  ..., 0.2532, 0.6309, 0.0000],\n",
       "          ...,\n",
       "          [0.0681, 0.6963, 0.5923,  ..., 0.2532, 0.6309, 0.0000],\n",
       "          [0.0681, 0.6963, 0.5923,  ..., 0.2532, 0.6309, 0.0000],\n",
       "          [0.0681, 0.6963, 0.5923,  ..., 0.2532, 0.6309, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__self_attn__DOT__q_proj': tensor([[[0.6841, 0.5161, 0.0000,  ..., 0.2766, 0.0000, 0.0000],\n",
       "          [0.6841, 0.5161, 0.0000,  ..., 0.2766, 0.0000, 0.0000],\n",
       "          [0.6841, 0.5161, 0.0000,  ..., 0.2766, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.6841, 0.5161, 0.0000,  ..., 0.2766, 0.0000, 0.0000],\n",
       "          [0.6841, 0.5161, 0.0000,  ..., 0.2766, 0.0000, 0.0000],\n",
       "          [0.6841, 0.5161, 0.0000,  ..., 0.2766, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__self_attn__DOT__k_proj': tensor([[[0.3755, 0.0000, 0.1017,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3755, 0.0000, 0.1017,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3755, 0.0000, 0.1017,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.3755, 0.0000, 0.1017,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3755, 0.0000, 0.1017,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3755, 0.0000, 0.1017,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__self_attn__DOT__v_proj': tensor([[[0.6504, 0.0000, 0.0000,  ..., 0.6675, 0.0000, 0.0000],\n",
       "          [0.6504, 0.0000, 0.0000,  ..., 0.6675, 0.0000, 0.0000],\n",
       "          [0.6504, 0.0000, 0.0000,  ..., 0.6675, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.6504, 0.0000, 0.0000,  ..., 0.6675, 0.0000, 0.0000],\n",
       "          [0.6504, 0.0000, 0.0000,  ..., 0.6675, 0.0000, 0.0000],\n",
       "          [0.6504, 0.0000, 0.0000,  ..., 0.6675, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__self_attn__DOT__o_proj': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.5532, 0.1116, 0.2766],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5532, 0.1116, 0.2766],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5532, 0.1116, 0.2766],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5532, 0.1116, 0.2766],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5532, 0.1116, 0.2766],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.5532, 0.1116, 0.2766]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__mlp__DOT__gate_proj': tensor([[[0.0000, 0.0000, 0.4104,  ..., 0.4229, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4104,  ..., 0.4229, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4104,  ..., 0.4229, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.4104,  ..., 0.4229, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4104,  ..., 0.4229, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4104,  ..., 0.4229, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__mlp__DOT__up_proj': tensor([[[0.0000, 0.0392, 0.0000,  ..., 0.0631, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0392, 0.0000,  ..., 0.0631, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0392, 0.0000,  ..., 0.0631, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0392, 0.0000,  ..., 0.0631, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0392, 0.0000,  ..., 0.0631, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0392, 0.0000,  ..., 0.0631, 0.0000, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>),\n",
       " 'layers__DOT__31__DOT__mlp__DOT__down_proj': tensor([[[0.4084, 0.0591, 0.3184,  ..., 0.0000, 0.2106, 0.0000],\n",
       "          [0.4084, 0.0591, 0.3184,  ..., 0.0000, 0.2106, 0.0000],\n",
       "          [0.4084, 0.0591, 0.3184,  ..., 0.0000, 0.2106, 0.0000],\n",
       "          ...,\n",
       "          [0.4084, 0.0591, 0.3184,  ..., 0.0000, 0.2106, 0.0000],\n",
       "          [0.4084, 0.0591, 0.3184,  ..., 0.0000, 0.2106, 0.0000],\n",
       "          [0.4084, 0.0591, 0.3184,  ..., 0.0000, 0.2106, 0.0000]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nero_model_outs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11588807680\n",
      "11588807680\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "if 'base_model2' in globals():\n",
    "    # base_model2.to('cpu')\n",
    "    del base_model2\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_grad',\n",
       " '_grad_fn',\n",
       " '_post_accumulate_grad_hooks',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'register_post_accumulate_grad_hook',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'retain_grad',\n",
       " 'retains_grad']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(nero_model.base_model.model.layers[0].self_attn.q_proj.lora_A.weight) if 'grad' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['requires_grad_', 'zero_grad']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(nero_model) if 'grad' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=8, bias=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nero_model.base_model.model.layers[0].self_attn.q_proj.lora_A.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.embed_tokens.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.0.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.0.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.0.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.0.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.0.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.0.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.0.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.0.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.0.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.0.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.0.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.0.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.0.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.0.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.0.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.0.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.0.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.0.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.1.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.1.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.1.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.1.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.1.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.1.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.1.mlp.gate_proj.base_layer.weight --> True torch.float16\n",
      "base_model.model.layers.1.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.1.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.1.mlp.up_proj.base_layer.weight --> True torch.float16\n",
      "base_model.model.layers.1.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.1.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.1.mlp.down_proj.base_layer.weight --> True torch.float16\n",
      "base_model.model.layers.1.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.1.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.1.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.1.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.1.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.2.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.2.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.2.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.2.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.2.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.2.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.2.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.2.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.2.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.2.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.2.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.2.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.2.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.2.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.2.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.2.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.2.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.2.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.3.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.3.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.3.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.3.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.3.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.3.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.3.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.3.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.3.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.3.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.3.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.3.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.3.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.3.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.3.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.3.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.3.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.3.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.4.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.4.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.4.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.4.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.4.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.4.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.4.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.4.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.4.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.4.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.4.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.4.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.4.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.4.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.4.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.4.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.4.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.4.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.5.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.5.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.5.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.5.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.5.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.5.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.5.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.5.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.5.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.5.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.5.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.5.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.5.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.5.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.5.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.5.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.5.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.5.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.6.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.6.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.6.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.6.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.6.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.6.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.6.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.6.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.6.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.6.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.6.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.6.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.6.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.6.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.6.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.6.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.6.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.6.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.7.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.7.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.7.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.7.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.7.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.7.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.7.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.7.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.7.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.7.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.7.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.7.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.7.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.7.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.7.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.7.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.7.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.7.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.8.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.8.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.8.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.8.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.8.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.8.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.8.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.8.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.8.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.8.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.8.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.8.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.8.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.8.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.8.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.8.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.8.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.8.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.9.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.9.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.9.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.9.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.9.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.9.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.9.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.9.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.9.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.9.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.9.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.9.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.9.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.9.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.9.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.9.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.9.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.9.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.10.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.10.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.10.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.10.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.10.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.10.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.10.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.10.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.10.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.10.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.10.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.10.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.10.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.10.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.10.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.10.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.10.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.10.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.11.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.11.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.11.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.11.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.11.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.11.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.11.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.11.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.11.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.11.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.11.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.11.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.11.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.11.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.11.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.11.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.11.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.11.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.12.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.12.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.12.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.12.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.12.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.12.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.12.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.12.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.12.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.12.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.12.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.12.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.12.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.12.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.12.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.12.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.12.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.12.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.13.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.13.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.13.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.13.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.13.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.13.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.13.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.13.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.13.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.13.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.13.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.13.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.13.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.13.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.13.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.13.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.13.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.13.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.14.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.14.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.14.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.14.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.14.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.14.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.14.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.14.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.14.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.14.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.14.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.14.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.14.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.14.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.14.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.14.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.14.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.14.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.15.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.15.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.15.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.15.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.15.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.15.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.15.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.15.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.15.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.15.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.15.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.15.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.15.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.15.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.15.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.15.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.15.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.15.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.16.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.16.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.16.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.16.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.16.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.16.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.16.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.16.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.16.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.16.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.16.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.16.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.16.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.16.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.16.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.16.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.16.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.16.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.17.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.17.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.17.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.17.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.17.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.17.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.17.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.17.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.17.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.17.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.17.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.17.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.17.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.17.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.17.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.17.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.17.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.17.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.18.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.18.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.18.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.18.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.18.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.18.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.18.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.18.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.18.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.18.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.18.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.18.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.18.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.18.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.18.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.18.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.18.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.18.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.19.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.19.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.19.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.19.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.19.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.19.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.19.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.19.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.19.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.19.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.19.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.19.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.19.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.19.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.19.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.19.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.19.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.19.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.20.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.20.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.20.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.20.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.20.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.20.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.20.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.20.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.20.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.20.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.20.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.20.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.20.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.20.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.20.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.20.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.20.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.20.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.21.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.21.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.21.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.21.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.21.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.21.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.21.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.21.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.21.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.21.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.21.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.21.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.21.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.21.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.21.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.21.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.21.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.21.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.22.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.22.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.22.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.22.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.22.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.22.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.22.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.22.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.22.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.22.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.22.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.22.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.22.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.22.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.22.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.22.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.22.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.22.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.23.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.23.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.23.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.23.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.23.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.23.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.23.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.23.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.23.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.23.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.23.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.23.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.23.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.23.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.23.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.23.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.23.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.23.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.24.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.24.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.24.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.24.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.24.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.24.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.24.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.24.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.24.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.24.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.24.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.24.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.24.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.24.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.24.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.24.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.24.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.24.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.25.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.25.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.25.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.25.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.25.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.25.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.25.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.25.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.25.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.25.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.25.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.25.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.25.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.25.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.25.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.25.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.25.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.25.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.26.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.26.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.26.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.26.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.26.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.26.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.26.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.26.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.26.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.26.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.26.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.26.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.26.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.26.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.26.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.26.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.26.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.26.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.27.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.27.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.27.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.27.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.27.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.27.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.27.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.27.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.27.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.27.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.27.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.27.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.27.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.27.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.27.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.27.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.27.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.27.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.28.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.28.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.28.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.28.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.28.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.28.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.28.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.28.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.28.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.28.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.28.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.28.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.28.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.28.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.28.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.28.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.28.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.28.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.29.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.29.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.29.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.29.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.29.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.29.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.29.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.29.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.29.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.29.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.29.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.29.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.29.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.29.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.29.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.29.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.29.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.29.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.30.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.30.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.30.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.30.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.30.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.30.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.30.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.30.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.30.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.30.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.30.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.30.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.30.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.30.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.30.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.30.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.30.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.30.post_attention_layernorm.weight --> True torch.float16\n",
      "\n",
      "base_model.model.layers.31.self_attn.q_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.q_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.q_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.q_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.q_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.31.self_attn.k_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.31.self_attn.k_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.k_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.k_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.k_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.k_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.k_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.31.self_attn.v_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.v_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.v_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.v_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.v_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.31.self_attn.o_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.31.self_attn.o_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.o_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.o_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.o_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.o_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.self_attn.o_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.31.mlp.gate_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.31.mlp.gate_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.gate_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.gate_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.gate_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.31.mlp.gate_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.gate_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.31.mlp.up_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.31.mlp.up_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.up_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.up_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.up_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.31.mlp.up_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.up_proj.nero_B.bias --> True torch.float32\n",
      "\n",
      "base_model.model.layers.31.mlp.down_proj.base_layer.weight --> False torch.uint8\n",
      "base_model.model.layers.31.mlp.down_proj.lora_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.down_proj.lora_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.down_proj.nero_A.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.down_proj.nero_A.bias --> True torch.float32\n",
      "base_model.model.layers.31.mlp.down_proj.nero_B.weight --> True torch.float32\n",
      "base_model.model.layers.31.mlp.down_proj.nero_B.bias --> True torch.float32\n",
      "base_model.model.layers.31.input_layernorm.weight --> True torch.float16\n",
      "base_model.model.layers.31.post_attention_layernorm.weight --> True torch.float16\n",
      "base_model.model.norm.weight --> True torch.float16\n",
      "base_model.lm_head.weight --> True torch.float16\n"
     ]
    }
   ],
   "source": [
    "for n, p in nero_model.named_parameters():\n",
    "    if 'base_layer.weight' in n: \n",
    "        print()\n",
    "    print(n, \"-->\", p.requires_grad, p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6283761152\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# base_model1.to('cpu')\n",
    "# lora_model.to('cpu')\n",
    "# del base_model1\n",
    "# del nero_model\n",
    "\n",
    "print(torch.cuda.memory_allocated())\n",
    "\n",
    "if 'nero_model' in globals():\n",
    "    nero_model.to('cpu')\n",
    "    del nero_model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "nero_model.base_model.model.layers[0].self_attn.q_proj.nero_A: Parameter containing:\n",
      "tensor([[-4.4971e-01,  3.1203e-01,  5.2366e-01,  ...,  2.6862e-01,\n",
      "          2.4798e-01,  6.7461e-04],\n",
      "        [-4.1525e-02,  4.5143e-01, -5.7140e-01,  ..., -6.3062e-02,\n",
      "         -1.2641e-01,  2.3381e-01],\n",
      "        [-6.1668e-02, -2.4167e-01, -8.9052e-02,  ..., -1.3829e-01,\n",
      "         -1.7905e-01,  1.7789e-02],\n",
      "        ...,\n",
      "        [ 1.9156e-02, -1.1599e+00,  3.7303e-01,  ...,  4.3789e-01,\n",
      "          7.5028e-02, -4.8528e-01],\n",
      "        [ 1.6513e-01,  7.7233e-01,  5.8815e-01,  ...,  2.0706e-01,\n",
      "         -2.5718e-01, -5.1134e-01],\n",
      "        [ 2.2792e-01, -2.5864e-01, -2.1519e-01,  ...,  2.5236e-01,\n",
      "         -1.1390e-01, -8.2920e-01]], device='cuda:0', requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, 4096).to('cuda')\n",
    "print(x.requires_grad)\n",
    "print('nero_A:', nero_model.base_model.model.layers[0].self_attn.q_proj.nero_A.weight)\n",
    "y = nero_model.base_model.model.layers[0].self_attn.q_proj.nero_A(x)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'check_lora_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-13-613842266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Check LoRA parameters (unloaded):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheck_lora_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnero_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlora_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_configs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'L2T1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lora_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adapter_model.safetensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'check_lora_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "lora_path = os.path.join(model_configs['L2T1']['lora_dir'], 'adapter_model.safetensors')\n",
    "nero_model.load_lora_weights(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_text(nero_model, tokenizer, prompt=\"Preheat the oven to 350 degrees and place the cookie dough\", skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-40.7062,  -6.6492,  -3.7754,  13.8341,  20.6145,  53.7147, -21.1631,\n",
       "         -40.2303],\n",
       "        [ -6.0338,  12.2223,   1.5039,  18.9884,  22.6390, -32.5255,  -3.8959,\n",
       "          38.7502],\n",
       "        [ 17.4199,  18.8145, -19.4615, -15.9465, -22.6114, -15.0024,  29.2715,\n",
       "         -41.9063],\n",
       "        [-18.6150, -13.4174,  -6.4494,  22.2234,   8.2466, -11.7426,   2.5520,\n",
       "         -19.9913],\n",
       "        [ -9.3683,   8.1962,  16.1248, -23.4653,  25.8517, -39.0575, -30.0681,\n",
       "           4.5514],\n",
       "        [ 38.9216, -17.0592, -25.0972,  12.2915,  -6.2447,   2.4214, -18.8855,\n",
       "          52.9850],\n",
       "        [  2.4511,  -6.4056, -43.2899, -56.0964, -24.5220, -17.7764,  32.7902,\n",
       "         -22.3956],\n",
       "        [ 12.9756,   8.9490,   7.6858,  -6.9334,  -5.3790,  11.6487,  26.5811,\n",
       "          -5.7308]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nero_model.base_model.model.layers[0].self_attn.q_proj.nero_A(torch.randn(8, 4096).to('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.eval()\n",
    "# lora_model.gradient_checkpointing_enable() # Fix error: 'LlamaDecoderLayer' object has no attribute '_gradient_checkpointing_func'\n",
    "device = next(lora_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "lora_model_outs = lora_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  (nero_A): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=1024, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=14336, bias=False)\n",
      "  (nero_A): Linear(in_features=14336, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=14336, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n",
      "[NeroLayer] Forward pass executed for NeroLayer(\n",
      "  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (dropout): Identity()\n",
      "  (lora_A): Linear(in_features=14336, out_features=8, bias=False)\n",
      "  (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  (nero_A): Linear(in_features=4096, out_features=8, bias=True)\n",
      "  (nero_B): Linear(in_features=8, out_features=4096, bias=True)\n",
      ")\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "nero_out.requires_grad: False\n",
      "nero_out.grad_fn: None\n"
     ]
    }
   ],
   "source": [
    "nero_model.train()\n",
    "nero_model.gradient_checkpointing_enable() # Fix error: 'LlamaDecoderLayer' object has no attribute '_gradient_checkpointing_func'\n",
    "device = next(nero_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "nero_model_outs = nero_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4004, 0.0000, 0.1152,  ..., 0.0000, 0.0000, 0.4817],\n",
       "         [0.4004, 0.0000, 0.1152,  ..., 0.0000, 0.0000, 0.4817],\n",
       "         [0.4004, 0.0000, 0.1152,  ..., 0.0000, 0.0000, 0.4817],\n",
       "         ...,\n",
       "         [0.4004, 0.0000, 0.1152,  ..., 0.0000, 0.0000, 0.4817],\n",
       "         [0.4004, 0.0000, 0.1152,  ..., 0.0000, 0.0000, 0.4817],\n",
       "         [0.4004, 0.0000, 0.1152,  ..., 0.0000, 0.0000, 0.4817]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nero_model_outs[1]['layers__DOT__0__DOT__self_attn__DOT__q_proj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0049, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "def loss_func_v1(nero_outs, lora_outs):\n",
    "    assert nero_outs.keys() == lora_outs.keys() # TODO: Print warning message\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for layer_name in lora_outs.keys():\n",
    "        # Normalized MSE loss\n",
    "        mse_loss = F.mse_loss(nero_outs[layer_name], lora_outs[layer_name], reduction='sum') / torch.sum(nero_outs[layer_name] ** 2)\n",
    "        total_loss += mse_loss\n",
    "\n",
    "    return total_loss / len(lora_outs)  # Averaging loss across layers\n",
    "\n",
    "loss = loss_func_v1(nero_model_outs[1], lora_model_outs[1])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-15-2859123600.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(pred_outs, gt_outs, lambda_reg, lora_A_list, lora_B_list):\n",
    "    total_loss = 0.0\n",
    "    num_layers = len(gt_outs)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        # Normalized MSE loss\n",
    "        mse_loss = F.mse_loss(pred_outs[i], gt_outs[i], reduction='sum') / torch.sum(pred_outs[i] ** 2)\n",
    "        \n",
    "        # L2 regularization for LoRA matrices\n",
    "        reg_loss = lambda_reg * (torch.norm(lora_A_list[i], p=2) ** 2 + torch.norm(lora_B_list[i], p=2) ** 2)\n",
    "\n",
    "        total_loss += mse_loss + reg_loss\n",
    "\n",
    "    return total_loss / num_layers  # Averaging loss across layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
