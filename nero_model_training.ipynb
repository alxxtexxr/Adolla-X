{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(repo_id, checkpoint):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, 2000, 25) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    if checkpoint:\n",
    "        return os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, skip_special_tokens=True):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'].to(device), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=skip_special_tokens)[0])\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id,\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang, _ = lora_repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    split = f'train[:{(train_size+test_size)}]'\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(data_id, data_dir=data_dir, split=split) # TODO: Limit dataset size first\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "- L1T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650/adapter_model.safetensors\n",
      "- L2T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650/adapter_model.safetensors\n",
      "\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "device = 'cuda'\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 1024\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_configs = {\n",
    "    'L1T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "        'lora_dir': None,\n",
    "        'lora_path': None,\n",
    "        'lora_config': None,\n",
    "    },\n",
    "    'L2T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "        'lora_dir': None,\n",
    "        'lora_path': None,\n",
    "        'lora_config': None,\n",
    "    },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    lora_dir = download_hf_model(config['hf_lora_id'], config['checkpoint'])\n",
    "    model_configs[key]['lora_dir'] = lora_dir\n",
    "    model_configs[key]['lora_path'] = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "    model_configs[key]['lora_config'] = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "for key, config in model_configs.items():\n",
    "    print(f\"- {key}:\")\n",
    "    for config_name, config_value in config.items():\n",
    "        if config_name == 'lora_config':\n",
    "            continue\n",
    "        print(f\"{'-':>3} {config_name:<10}: {config_value}\")\n",
    "print()\n",
    "\n",
    "assert model_configs['L1T1']['lora_config'].base_model_name_or_path == model_configs['L2T1']['lora_config'].base_model_name_or_path, \"Base models must be the same\"\n",
    "base_model_name = model_configs['L1T1']['lora_config'].base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/bnb.py\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 return_lora_output=False, debug=False):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_lora_output = return_lora_output\n",
    "        self.debug = debug\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"================================================================\")\n",
    "            print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "            print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        if requires_conversion:\n",
    "            lora_out = lora_out.to(base_out.dtype)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "            print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        output = base_out + lora_out\n",
    "\n",
    "        if self.return_lora_output:\n",
    "            return output, lora_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_weights(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class LoraModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, \n",
    "                 return_lora_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        self.return_lora_outputs = return_lora_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "    \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                lora_layer = LoraLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    return_lora_output=self.return_lora_outputs,\n",
    "                    debug=self.debug,\n",
    "                )\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = lora_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "\n",
    "    def freeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for lora_layer in self.lora_layers.values():\n",
    "            for param in lora_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_weights(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for lora_layer_name, lora_layer in self.lora_layers.items():\n",
    "            lora_layer_name = lora_layer_name.replace('__DOT__', '.')\n",
    "            lora_layer_name = prefix + lora_layer_name\n",
    "            if f'{lora_layer_name}.lora_A.weight' in state_dict and f'{lora_layer_name}.lora_B.weight' in state_dict:\n",
    "                lora_layer.load_lora_weights(state_dict, lora_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_lora_outputs:\n",
    "            lora_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, lora_out = _out\n",
    "                    lora_outs[layer_name] = lora_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.lora_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, lora_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_lora_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "lora_model = LoraModel(\n",
    "    base_lora_model, \n",
    "    model_configs['L2T1']['lora_config'],\n",
    "    return_lora_outputs=True,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : -0.0003347942838445306\n",
      "- Min     : -1.636489748954773\n",
      "- Max     : 1.305248498916626\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 2.2152138626552187e-05\n",
      "- Min     : -0.06327299773693085\n",
      "- Max     : 0.0625513345003128\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(lora_model)\n",
    "print()\n",
    "\n",
    "lora_model.load_lora_weights(model_configs['L2T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.freeze_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.train()\n",
    "device = next(lora_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "lora_model_outs = lora_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0036, -0.0022,  0.0020,  ..., -0.0032, -0.0023, -0.0026],\n",
       "         [-0.0068, -0.0042,  0.0053,  ..., -0.0093, -0.0069, -0.0077],\n",
       "         [-0.0037, -0.0036,  0.0024,  ..., -0.0055, -0.0041, -0.0046],\n",
       "         ...,\n",
       "         [-0.0059, -0.0045,  0.0033,  ..., -0.0072, -0.0053, -0.0059],\n",
       "         [-0.0064, -0.0063,  0.0004,  ..., -0.0070, -0.0050, -0.0058],\n",
       "         [-0.0051, -0.0008,  0.0057,  ..., -0.0058, -0.0042, -0.0048]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model_outs[1]['layers__DOT__0__DOT__self_attn__DOT__q_proj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nero Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 # For debugging \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # For debugging\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.nero_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"================================================================\")\n",
    "            print(self.module_name)\n",
    "            print(\"================================================================\")\n",
    "            print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "            print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "            print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # nero_out = F.relu(self.nero_B(self.nero_A(self.dropout(lora_out))) * self.scaling)\n",
    "        nero_dropout_out = self.dropout(lora_out)\n",
    "        nero_A_out = self.nero_A(nero_dropout_out)\n",
    "        nero_B_out = self.nero_B(nero_A_out)\n",
    "        nero_scaling_out = nero_B_out * self.scaling\n",
    "        nero_out = F.relu(nero_scaling_out)\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"nero_out.requires_grad:\", nero_out.requires_grad)\n",
    "            print(\"nero_out.grad_fn:\", nero_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "            nero_out_has_nan = torch.isnan(nero_out).any()\n",
    "            if nero_out_has_nan:\n",
    "                print(\"!!! NERO OUT HAS NAN !!!\")\n",
    "                print(\"nero_out:\")\n",
    "                print(nero_out)\n",
    "                print()\n",
    "                print(\"nero_scaling_out:\")\n",
    "                print(nero_scaling_out)\n",
    "                print()\n",
    "                print(\"nero_B_out:\")\n",
    "                print(nero_B_out)\n",
    "                print()\n",
    "                print(\"nero_A_out:\")\n",
    "                print(nero_A_out)\n",
    "                print()\n",
    "                print(\"nero_dropout_out:\")\n",
    "                print(nero_dropout_out)\n",
    "                print()\n",
    "                print(\"lora_out:\")\n",
    "                print(lora_out)\n",
    "                print()\n",
    "\n",
    "        # Add `base_out` with gradients-detached `nero_out`, \n",
    "        # so that `base_out` does not carry gradients\n",
    "        nero_out_detached = nero_out.detach()\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"nero_out_detached.requires_grad:\", nero_out_detached.requires_grad)\n",
    "            print(\"nero_out_detached.grad_fn:\", nero_out_detached.grad_fn)\n",
    "            print()\n",
    "\n",
    "        output = base_out + nero_out_detached\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"output.requires_grad:\", output.requires_grad)\n",
    "            print(\"output.grad_fn:\", output.grad_fn)\n",
    "            print()\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_weights(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_weights(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_weights(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    model_configs['L1T1']['lora_config'], \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=True,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 0.0021499674767255783\n",
      "- Min     : -1.397733211517334\n",
      "- Max     : 1.4835515022277832\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "nero_model.load_lora_weights(model_configs['L1T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nero_model.freeze_all_except_nero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nero_model.train()\n",
    "device = next(nero_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "nero_model_outs = nero_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0389, 0.1897, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0389, 0.1897, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0389, 0.1897, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0389, 0.1897, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0389, 0.1897, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0389, 0.1897, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nero_model_outs[1]['layers__DOT__0__DOT__self_attn__DOT__q_proj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_hf_dataset_from_lora(model_configs['L2T1']['hf_lora_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer.eos_token\n",
    "tokenizer.pad_token = eos_token\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example['text'])\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    tokenize_fn, \n",
    "    batched=True, \n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "# Concatenate all tokens into one long stream, then split into blocks\n",
    "block_size = 16\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = []\n",
    "    for input_ids in examples['input_ids']:\n",
    "        concatenated += input_ids\n",
    "\n",
    "    total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "    input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "    attention_mask = [[1] * block_size for _ in input_ids]\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "dataset_grouped = dataset_tokenized.map(\n",
    "    group_texts, \n",
    "    batched=True, \n",
    "    batch_size=1000,\n",
    "    remove_columns=dataset_tokenized.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    dataset_grouped, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _debugged = False\n",
    "# _layer_name = None\n",
    "# _nero_out = None\n",
    "# _lora_out = None\n",
    "# _mse_loss_unnormed = None\n",
    "# _nero_out_sum = None\n",
    "\n",
    "# def loss_fn_v1(nero_outs, lora_outs):\n",
    "#     assert nero_outs.keys() == lora_outs.keys() # TODO: Print warning message\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name]\n",
    "\n",
    "#         # Normalized MSE loss\n",
    "#         # mse_loss = F.mse_loss(nero_out, lora_out, reduction='sum') / torch.sum(nero_outs[layer_name] ** 2)\n",
    "#         mse_loss_unnormed = F.mse_loss(nero_out, lora_out, reduction='sum')\n",
    "#         nero_out_sum = torch.sum(nero_outs[layer_name] ** 2)\n",
    "#         mse_loss = mse_loss_unnormed / nero_out_sum\n",
    "\n",
    "#         print(\"================================================================\")\n",
    "#         print(layer_name)\n",
    "#         print(\"================================================================\")\n",
    "#         print(\"mse_loss:\", mse_loss)\n",
    "#         print(\"nero_out_sum:\", nero_out_sum)\n",
    "#         print(\"mse_loss_unnormed:\", mse_loss_unnormed)\n",
    "#         print()\n",
    "\n",
    "#         global _debugged\n",
    "#         if (torch.isinf(nero_out_sum) or torch.isnan(mse_loss_unnormed)) and not _debugged:\n",
    "#             global _layer_name\n",
    "#             global _nero_out\n",
    "#             global _lora_out\n",
    "#             global _mse_loss_unnormed\n",
    "#             global _nero_out_sum\n",
    "#             _layer_name = layer_name\n",
    "#             _nero_out = nero_out\n",
    "#             _lora_out = lora_out\n",
    "#             _mse_loss_unnormed = mse_loss_unnormed\n",
    "#             _nero_out_sum = nero_out_sum\n",
    "#             _debugged = True\n",
    "\n",
    "#         if torch.isnan(mse_loss):\n",
    "#             nero_out_has_nan = torch.isnan(nero_out).any()\n",
    "#             lora_out_has_nan = torch.isnan(lora_out).any()\n",
    "\n",
    "#             print(\"nero_out_has_nan:\", nero_out_has_nan)\n",
    "#             print(\"lora_out_has_nan:\", lora_out_has_nan)\n",
    "#             print()\n",
    "\n",
    "#             # if nero_out_has_nan:\n",
    "#             print(\"nero_out:\")\n",
    "#             print(nero_out)\n",
    "#             print()\n",
    "            \n",
    "#             # if lora_out_has_nan:\n",
    "#             print(\"lora_out:\")\n",
    "#             print(lora_out)\n",
    "#             print()\n",
    "\n",
    "#         total_loss += mse_loss\n",
    "\n",
    "#     return total_loss / len(lora_outs)  # Averaging loss across layers\n",
    "\n",
    "# # loss = loss_func_v1(nero_model_outs[1], lora_model_outs[1])\n",
    "# # print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn_v2(nero_outs, lora_outs):\n",
    "#     assert nero_outs.keys() == lora_outs.keys()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         print(\"================================================================\")\n",
    "#         print(layer_name)\n",
    "#         print(\"================================================================\")\n",
    "\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name]\n",
    "\n",
    "#         diff = nero_out - lora_out\n",
    "#         print(\"mean(abs(diff)):\", diff.abs().mean())\n",
    "#         print(\"mean((diff)^2):\", (diff ** 2).mean())\n",
    "#         print()\n",
    "        \n",
    "#         print(\"max abs(diff):\", diff.abs().max().item())\n",
    "#         print(\"mean abs(diff):\", diff.abs().mean().item())\n",
    "#         print(\"std abs(diff):\", diff.abs().std().item())\n",
    "#         print()\n",
    "\n",
    "#         # Use mean MSE to prevent overflow and keep scale uniform\n",
    "#         mse_loss = F.mse_loss(nero_out.float(), lora_out.float(), reduction='mean')  # Compute in float32\n",
    "\n",
    "#         print(\"mse_loss:\", mse_loss)\n",
    "#         print()\n",
    "\n",
    "#         total_loss += mse_loss\n",
    "\n",
    "#     return total_loss / len(lora_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_debugged = False\n",
    "_nero_out = None\n",
    "_lora_out = None\n",
    "\n",
    "def loss_fn_v3(nero_outs, lora_outs, debug=False):\n",
    "    assert nero_outs.keys() == lora_outs.keys()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for layer_name in lora_outs.keys():\n",
    "        nero_out = nero_outs[layer_name]\n",
    "        lora_out = lora_outs[layer_name]\n",
    "\n",
    "        # Use mean MAE to prevent overflow and keep scale uniform\n",
    "        # mae_loss = F.l1_loss(nero_out.float(), lora_out.float(), reduction='mean')  \n",
    "        mae_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()))\n",
    "\n",
    "        if debug:\n",
    "            print(\"================================================================\")\n",
    "            print(layer_name)\n",
    "            print(\"================================================================\")\n",
    "            \n",
    "            global _debugged\n",
    "            if not _debugged:\n",
    "                global _nero_out\n",
    "                global _lora_out\n",
    "                _nero_out = nero_out\n",
    "                _lora_out = lora_out\n",
    "                _debugged = True\n",
    "\n",
    "            diff = nero_out - lora_out\n",
    "            print(\"mean(abs(diff)):\", diff.abs().mean())\n",
    "            print(\"mean((diff)^2):\", (diff ** 2).mean())\n",
    "            print()\n",
    "            \n",
    "            print(\"max abs(diff):\", diff.abs().max().item())\n",
    "            print(\"mean abs(diff):\", diff.abs().mean().item())\n",
    "            print(\"std abs(diff):\", diff.abs().std().item())\n",
    "            print()\n",
    "\n",
    "            print(\"mae_loss:\", mae_loss)\n",
    "            print()\n",
    "\n",
    "        total_loss += mae_loss\n",
    "\n",
    "    return total_loss / len(lora_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250731_171146-b52o4znr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/b52o4znr' target=\"_blank\">zesty-firebrand-10</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/b52o4znr' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/b52o4znr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1, step: 1/514310, loss: 0.18166251480579376\n",
      "epoch: 1/1, step: 2/514310, loss: 0.18192507326602936\n",
      "epoch: 1/1, step: 3/514310, loss: 0.18143393099308014\n",
      "epoch: 1/1, step: 4/514310, loss: 0.18144918978214264\n",
      "epoch: 1/1, step: 5/514310, loss: 0.18147920072078705\n",
      "epoch: 1/1, step: 6/514310, loss: 0.1812676042318344\n",
      "epoch: 1/1, step: 7/514310, loss: 0.18099839985370636\n",
      "epoch: 1/1, step: 8/514310, loss: 0.1809649020433426\n",
      "epoch: 1/1, step: 9/514310, loss: 0.1810023933649063\n",
      "epoch: 1/1, step: 10/514310, loss: 0.18096528947353363\n",
      "epoch: 1/1, step: 11/514310, loss: 0.1813088208436966\n",
      "epoch: 1/1, step: 12/514310, loss: 0.18130142986774445\n",
      "epoch: 1/1, step: 13/514310, loss: 0.18066956102848053\n",
      "epoch: 1/1, step: 14/514310, loss: 0.1812947541475296\n",
      "epoch: 1/1, step: 15/514310, loss: 0.18087956309318542\n",
      "epoch: 1/1, step: 16/514310, loss: 0.18116238713264465\n",
      "epoch: 1/1, step: 17/514310, loss: 0.18086124956607819\n",
      "epoch: 1/1, step: 18/514310, loss: 0.1808139532804489\n",
      "epoch: 1/1, step: 19/514310, loss: 0.18074247241020203\n",
      "epoch: 1/1, step: 20/514310, loss: 0.1811441034078598\n",
      "epoch: 1/1, step: 21/514310, loss: 0.1806108057498932\n",
      "epoch: 1/1, step: 22/514310, loss: 0.18056844174861908\n",
      "epoch: 1/1, step: 23/514310, loss: 0.180726557970047\n",
      "epoch: 1/1, step: 24/514310, loss: 0.18048687279224396\n",
      "epoch: 1/1, step: 25/514310, loss: 0.18038024008274078\n",
      "epoch: 1/1, step: 26/514310, loss: 0.18044282495975494\n",
      "epoch: 1/1, step: 27/514310, loss: 0.18044455349445343\n",
      "epoch: 1/1, step: 28/514310, loss: 0.1801348775625229\n",
      "epoch: 1/1, step: 29/514310, loss: 0.18013472855091095\n",
      "epoch: 1/1, step: 30/514310, loss: 0.17993028461933136\n",
      "epoch: 1/1, step: 31/514310, loss: 0.18011990189552307\n",
      "epoch: 1/1, step: 32/514310, loss: 0.17988905310630798\n",
      "epoch: 1/1, step: 33/514310, loss: 0.18065236508846283\n",
      "epoch: 1/1, step: 34/514310, loss: 0.18056899309158325\n",
      "epoch: 1/1, step: 35/514310, loss: 0.17996105551719666\n",
      "epoch: 1/1, step: 36/514310, loss: 0.1804087609052658\n",
      "epoch: 1/1, step: 37/514310, loss: 0.17970265448093414\n",
      "epoch: 1/1, step: 38/514310, loss: 0.1805385798215866\n",
      "epoch: 1/1, step: 39/514310, loss: 0.17974993586540222\n",
      "epoch: 1/1, step: 40/514310, loss: 0.17977836728096008\n",
      "epoch: 1/1, step: 41/514310, loss: 0.17931869626045227\n",
      "epoch: 1/1, step: 42/514310, loss: 0.17964261770248413\n",
      "epoch: 1/1, step: 43/514310, loss: 0.1796988546848297\n",
      "epoch: 1/1, step: 44/514310, loss: 0.17965464293956757\n",
      "epoch: 1/1, step: 45/514310, loss: 0.17979444563388824\n",
      "epoch: 1/1, step: 46/514310, loss: 0.17953629791736603\n",
      "epoch: 1/1, step: 47/514310, loss: 0.17913129925727844\n",
      "epoch: 1/1, step: 48/514310, loss: 0.1792183518409729\n",
      "epoch: 1/1, step: 49/514310, loss: 0.17938175797462463\n",
      "epoch: 1/1, step: 50/514310, loss: 0.17892664670944214\n",
      "epoch: 1/1, step: 51/514310, loss: 0.1793241798877716\n",
      "epoch: 1/1, step: 52/514310, loss: 0.17926791310310364\n",
      "epoch: 1/1, step: 53/514310, loss: 0.17886725068092346\n",
      "epoch: 1/1, step: 54/514310, loss: 0.1800607144832611\n",
      "epoch: 1/1, step: 55/514310, loss: 0.1787821650505066\n",
      "epoch: 1/1, step: 56/514310, loss: 0.17865632474422455\n",
      "epoch: 1/1, step: 57/514310, loss: 0.1784435361623764\n",
      "epoch: 1/1, step: 58/514310, loss: 0.17849582433700562\n",
      "epoch: 1/1, step: 59/514310, loss: 0.17879578471183777\n",
      "epoch: 1/1, step: 60/514310, loss: 0.1785365492105484\n",
      "epoch: 1/1, step: 61/514310, loss: 0.17861001193523407\n",
      "epoch: 1/1, step: 62/514310, loss: 0.17878882586956024\n",
      "epoch: 1/1, step: 63/514310, loss: 0.17806389927864075\n",
      "epoch: 1/1, step: 64/514310, loss: 0.178224116563797\n",
      "epoch: 1/1, step: 65/514310, loss: 0.17845045030117035\n",
      "epoch: 1/1, step: 66/514310, loss: 0.17876186966896057\n",
      "epoch: 1/1, step: 67/514310, loss: 0.1777581125497818\n",
      "epoch: 1/1, step: 68/514310, loss: 0.17762726545333862\n",
      "epoch: 1/1, step: 69/514310, loss: 0.1779264509677887\n",
      "epoch: 1/1, step: 70/514310, loss: 0.1781845986843109\n",
      "epoch: 1/1, step: 71/514310, loss: 0.17815729975700378\n",
      "epoch: 1/1, step: 72/514310, loss: 0.17757271230220795\n",
      "epoch: 1/1, step: 73/514310, loss: 0.17744775116443634\n",
      "epoch: 1/1, step: 74/514310, loss: 0.17709670960903168\n",
      "epoch: 1/1, step: 75/514310, loss: 0.17752383649349213\n",
      "epoch: 1/1, step: 76/514310, loss: 0.17792043089866638\n",
      "epoch: 1/1, step: 77/514310, loss: 0.17750464379787445\n",
      "epoch: 1/1, step: 78/514310, loss: 0.1771596521139145\n",
      "epoch: 1/1, step: 79/514310, loss: 0.1771014779806137\n",
      "epoch: 1/1, step: 80/514310, loss: 0.1766888052225113\n",
      "epoch: 1/1, step: 81/514310, loss: 0.17673064768314362\n",
      "epoch: 1/1, step: 82/514310, loss: 0.1769513338804245\n",
      "epoch: 1/1, step: 83/514310, loss: 0.1769302785396576\n",
      "epoch: 1/1, step: 84/514310, loss: 0.17662113904953003\n",
      "epoch: 1/1, step: 85/514310, loss: 0.1770746409893036\n",
      "epoch: 1/1, step: 86/514310, loss: 0.1769385188817978\n",
      "epoch: 1/1, step: 87/514310, loss: 0.176401749253273\n",
      "epoch: 1/1, step: 88/514310, loss: 0.17656837403774261\n",
      "epoch: 1/1, step: 89/514310, loss: 0.17696408927440643\n",
      "epoch: 1/1, step: 90/514310, loss: 0.17581209540367126\n",
      "epoch: 1/1, step: 91/514310, loss: 0.17603544890880585\n",
      "epoch: 1/1, step: 92/514310, loss: 0.17600472271442413\n",
      "epoch: 1/1, step: 93/514310, loss: 0.17589609324932098\n",
      "epoch: 1/1, step: 94/514310, loss: 0.17561450600624084\n",
      "epoch: 1/1, step: 95/514310, loss: 0.17554821074008942\n",
      "epoch: 1/1, step: 96/514310, loss: 0.17539681494235992\n",
      "epoch: 1/1, step: 97/514310, loss: 0.17564712464809418\n",
      "epoch: 1/1, step: 98/514310, loss: 0.17592428624629974\n",
      "epoch: 1/1, step: 99/514310, loss: 0.1751195788383484\n",
      "epoch: 1/1, step: 100/514310, loss: 0.17562906444072723\n",
      "epoch: 1/1, step: 101/514310, loss: 0.175664484500885\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>loss</td><td>â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–†â–†â–…â–…â–†â–…â–…â–…â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–</td></tr><tr><td>step</td><td>â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.17566</td></tr><tr><td>step</td><td>101</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-firebrand-10</strong> at: <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/b52o4znr' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/b52o4znr</a><br> View project at: <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250731_171146-b52o4znr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "wandb_run = wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    # config={\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'epochs': 10,\n",
    "    # },\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        _, nero_outs = nero_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, lora_outs = lora_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = loss_fn_v3(nero_outs, lora_outs, debug=False)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'step': step + 1,\n",
    "            'loss': loss.item(),\n",
    "        })\n",
    "        print(f\"epoch: {epoch + 1}/{num_epochs}, step: {step + 1}/{len(train_loader)}, loss: {loss.item()}\")\n",
    "\n",
    "        if step == 100:\n",
    "            break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
