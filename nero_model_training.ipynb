{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(repo_id, checkpoint):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, 2000, 25) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    if checkpoint:\n",
    "        return os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, skip_special_tokens=True):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'].to(device), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=skip_special_tokens)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "{'L1T1': {'checkpoint': 650,\n",
      "          'lora_dir': 'L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650',\n",
      "          'lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650'},\n",
      " 'L2T1': {'checkpoint': 650,\n",
      "          'lora_dir': 'L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650',\n",
      "          'lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629'}}\n",
      "\n",
      "LoRA configuration:\n",
      "{'_custom_modules': None,\n",
      " 'alpha_pattern': {},\n",
      " 'auto_mapping': None,\n",
      " 'base_model_name_or_path': 'unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit',\n",
      " 'bias': 'none',\n",
      " 'corda_config': None,\n",
      " 'eva_config': None,\n",
      " 'exclude_modules': None,\n",
      " 'fan_in_fan_out': False,\n",
      " 'inference_mode': True,\n",
      " 'init_lora_weights': True,\n",
      " 'layer_replication': None,\n",
      " 'layers_pattern': None,\n",
      " 'layers_to_transform': None,\n",
      " 'loftq_config': {},\n",
      " 'lora_alpha': 16,\n",
      " 'lora_bias': False,\n",
      " 'lora_dropout': 0,\n",
      " 'megatron_config': None,\n",
      " 'megatron_core': 'megatron.core',\n",
      " 'modules_to_save': None,\n",
      " 'peft_type': <PeftType.LORA: 'LORA'>,\n",
      " 'qalora_group_size': 16,\n",
      " 'r': 8,\n",
      " 'rank_pattern': {},\n",
      " 'revision': None,\n",
      " 'runtime_config': LoraRuntimeConfig(ephemeral_gpu_offload=False),\n",
      " 'target_modules': {'down_proj',\n",
      "                    'gate_proj',\n",
      "                    'k_proj',\n",
      "                    'o_proj',\n",
      "                    'q_proj',\n",
      "                    'up_proj',\n",
      "                    'v_proj'},\n",
      " 'task_type': 'CAUSAL_LM',\n",
      " 'trainable_token_indices': None,\n",
      " 'use_dora': False,\n",
      " 'use_qalora': False,\n",
      " 'use_rslora': False}\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "device = 'cuda'\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 1024\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_configs = {\n",
    "    'L1T1': {\n",
    "        'lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "    'L2T1': {\n",
    "        'lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    model_configs[key]['lora_dir'] = download_hf_model(config['lora_id'], config['checkpoint'])\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "pprint(model_configs)\n",
    "print()\n",
    "\n",
    "lora_config = LoraConfig.from_pretrained(model_configs['L2T1']['lora_dir'])\n",
    "print(\"LoRA configuration:\")\n",
    "pprint(lora_config.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/bnb.py\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha, dropout, lora_bias, use_rslora, return_lora_output=False):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_lora_output = return_lora_output\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        print(\"================================================================\")\n",
    "        print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "        print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "        print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        if requires_conversion:\n",
    "            lora_out = lora_out.to(base_out.dtype)\n",
    "        \n",
    "        print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "        print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "        print()\n",
    "\n",
    "        output = base_out + lora_out\n",
    "\n",
    "        if self.return_lora_output:\n",
    "            return output, lora_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_weights(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class LoraModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, return_lora_outputs=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        self.return_lora_outputs = return_lora_outputs\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "    \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                lora_layer = LoraLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    return_lora_output=self.return_lora_outputs,\n",
    "                )\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = lora_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "\n",
    "    def freeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for lora_layer in self.lora_layers.values():\n",
    "            for param in lora_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_weights(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for lora_layer_name, lora_layer in self.lora_layers.items():\n",
    "            lora_layer_name = lora_layer_name.replace('__DOT__', '.')\n",
    "            lora_layer_name = prefix + lora_layer_name\n",
    "            if f'{lora_layer_name}.lora_A.weight' in state_dict and f'{lora_layer_name}.lora_B.weight' in state_dict:\n",
    "                lora_layer.load_lora_weights(state_dict, lora_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_lora_outputs:\n",
    "            lora_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, lora_out = _out\n",
    "                    lora_outs[layer_name] = lora_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.lora_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, lora_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_model_1 = AutoModelForCausalLM.from_pretrained(lora_config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_config.base_model_name_or_path)\n",
    "lora_model = LoraModel(base_model_1, lora_config, return_lora_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 0.0014135234523564577\n",
      "- Min     : -1.449187994003296\n",
      "- Max     : 1.4309760332107544\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(lora_model)\n",
    "print()\n",
    "\n",
    "lora_path = os.path.join(model_configs['L1T1']['lora_dir'], 'adapter_model.safetensors')\n",
    "lora_model.load_lora_weights(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.freeze_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_model.train()\n",
    "device = next(lora_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "lora_model_outs = lora_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.3689e-04,  9.8288e-05,  1.9665e-03,  ..., -2.6093e-03,\n",
       "          -2.6684e-03, -2.7065e-03],\n",
       "         [ 8.0585e-04, -6.7253e-03, -6.6681e-03,  ..., -3.8910e-02,\n",
       "          -3.5675e-02, -3.6743e-02],\n",
       "         [-1.2369e-03, -7.9651e-03, -1.1253e-02,  ..., -2.9968e-02,\n",
       "          -2.6855e-02, -2.7893e-02],\n",
       "         ...,\n",
       "         [ 1.1005e-03, -6.9275e-03, -4.7417e-03,  ..., -3.6438e-02,\n",
       "          -3.3875e-02, -3.4790e-02],\n",
       "         [-4.1902e-05, -1.1978e-02, -1.4435e-02,  ..., -5.2795e-02,\n",
       "          -4.8126e-02, -4.9835e-02],\n",
       "         [ 5.3215e-04, -9.1553e-03, -1.2909e-02,  ..., -4.5715e-02,\n",
       "          -4.1351e-02, -4.2847e-02]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model_outs[1]['layers__DOT__0__DOT__self_attn__DOT__q_proj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nero Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.nero_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        print(\"================================================================\")\n",
    "        print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "        print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "        print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "        print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "        print()\n",
    "\n",
    "        nero_out = F.relu(self.nero_B(self.nero_A(self.dropout(lora_out))) * self.scaling)\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "        self.last_nero_out = nero_out\n",
    "\n",
    "        print(\"nero_out.requires_grad:\", nero_out.requires_grad)\n",
    "        print(\"nero_out.grad_fn:\", nero_out.grad_fn)\n",
    "        print()\n",
    "\n",
    "        output = base_out + nero_out\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_weights(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        # self.lora_A.weight.requires_grad = False\n",
    "        # self.lora_B.weight.requires_grad = False\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "            # self.lora_A.bias.requires_grad = False\n",
    "            # self.lora_B.bias.requires_grad = False\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_weights(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_weights(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_model_2 = AutoModelForCausalLM.from_pretrained(lora_config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_config.base_model_name_or_path)\n",
    "nero_model = NeroModel(\n",
    "    base_model_2, \n",
    "    lora_config, \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 0.0011637862771749496\n",
      "- Min     : -1.6798632144927979\n",
      "- Max     : 1.491278052330017\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 2.2152138626552187e-05\n",
      "- Min     : -0.06327299773693085\n",
      "- Max     : 0.0625513345003128\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "lora_path = os.path.join(model_configs['L2T1']['lora_dir'], 'adapter_model.safetensors')\n",
    "nero_model.load_lora_weights(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nero_model.freeze_all_except_nero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e6fb0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e6e60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: False\n",
      "base_out.grad_fn: None\n",
      "\n",
      "lora_out.requires_grad: False\n",
      "lora_out.grad_fn: None\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e6fb0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e6e60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e6fb0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e6e60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <UnsafeViewBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e40a0>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e4130>\n",
      "\n",
      "================================================================\n",
      "base_out.requires_grad: True\n",
      "base_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n",
      "lora_out.requires_grad: True\n",
      "lora_out.grad_fn: <MulBackward0 object at 0x794b183e4130>\n",
      "\n",
      "nero_out.requires_grad: True\n",
      "nero_out.grad_fn: <ToCopyBackward0 object at 0x794b183e7d60>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nero_model.train()\n",
    "device = next(nero_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "nero_model_outs = nero_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3853, 0.0000, 0.5693,  ..., 0.0213, 0.0000, 0.0000],\n",
       "         [0.3853, 0.0000, 0.5693,  ..., 0.0213, 0.0000, 0.0000],\n",
       "         [0.3853, 0.0000, 0.5693,  ..., 0.0213, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3853, 0.0000, 0.5693,  ..., 0.0213, 0.0000, 0.0000],\n",
       "         [0.3853, 0.0000, 0.5693,  ..., 0.0213, 0.0000, 0.0000],\n",
       "         [0.3853, 0.0000, 0.5693,  ..., 0.0213, 0.0000, 0.0000]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nero_model_outs[1]['layers__DOT__0__DOT__self_attn__DOT__q_proj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0049, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def loss_func_v1(nero_outs, lora_outs):\n",
    "    assert nero_outs.keys() == lora_outs.keys() # TODO: Print warning message\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for layer_name in lora_outs.keys():\n",
    "        # Normalized MSE loss\n",
    "        mse_loss = F.mse_loss(nero_outs[layer_name], lora_outs[layer_name], reduction='sum') / torch.sum(nero_outs[layer_name] ** 2)\n",
    "        total_loss += mse_loss\n",
    "\n",
    "    return total_loss / len(lora_outs)  # Averaging loss across layers\n",
    "\n",
    "loss = loss_func_v1(nero_model_outs[1], lora_model_outs[1])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(pred_outs, gt_outs, lambda_reg, lora_A_list, lora_B_list):\n",
    "    total_loss = 0.0\n",
    "    num_layers = len(gt_outs)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        # Normalized MSE loss\n",
    "        mse_loss = F.mse_loss(pred_outs[i], gt_outs[i], reduction='sum') / torch.sum(pred_outs[i] ** 2)\n",
    "        \n",
    "        # L2 regularization for LoRA matrices\n",
    "        reg_loss = lambda_reg * (torch.norm(lora_A_list[i], p=2) ** 2 + torch.norm(lora_B_list[i], p=2) ** 2)\n",
    "\n",
    "        total_loss += mse_loss + reg_loss\n",
    "\n",
    "    return total_loss / num_layers  # Averaging loss across layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
