{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(repo_id, checkpoint=None, max_checkpoints=2000, checkpoint_step=25):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_step) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        return os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, skip_special_tokens=True):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'].to(device), max_new_tokens=max_new_tokens)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=skip_special_tokens)[0])\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id,\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang, _ = lora_repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    split = f'train[:{(train_size+test_size)}]'\n",
    "\n",
    "    # Load dataset\n",
    "    # TODO: Use streaming to not download the entire dataset\n",
    "    # dataset = load_dataset(data_id, data_dir=data_dir, split=split)\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6158697f88c24b6587a9da21fcdb6aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa54d2801284f55b1db173e73aa9354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5b253b28a24229b168bb0bcbb978a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/871 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa71d55f8cf4bcdac1b7660832d5e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/871 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02accc659b0b46e38a2a32b653237218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fb1f51a13341cc80a0dcd8666f467b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1c43cd08b548abb14e6af150676861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bff7b4f1ca147bfbd772b2bf7a0329b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6b525106234ec9ad1f09beb4135f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/43.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b000b7614474d62ae8f4f6c2006c5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8fad1ba5ed4c7cacd8567d851a04f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b33f7dd3ace470890739391f993265c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfe5bb7024c4fcf8b8f886b2e4aca41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921a8926b0a648ada92508c37acfc586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3b8c98062641f39923da2279d695b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e36d916f324c6f9b6caa9f4ad42fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad2ccd8d4af46559292734a7f4e105a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789074b7ef374e0b8eb1bc71244c5de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1691a2a7a69b444caf631d7174dfd86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)t.tfevents.1751286655.ec3c369f91a7.385.0:   0%|          | 0.00/402k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d733e9366c0d451e8d23a14a779bc08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9247fca49b4053b879605480c222a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7855874ccf340db8c4801088ee45c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3380f63cb78b42dc935b62d4980fe794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827fb5fb2c5747798da5724e74d88b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a281515bf51c444b8080ecbc33fa6b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/43.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b753b34231ea4c05aecbca39966607a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec04ef542204f699545099757b6d4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef91d9d37bda4bab84a987b916cccde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6ecfd30d324cb0810824dece79340f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906bc6f126c6472ba6a6a29f3f3ee042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8a4594575246128ddecc5930721660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876beb03ba104579886d78b65155e237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9905443efedc4bf8be6ae3432b1a4846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b2271a53a548a0a881e925a27fa418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e4120cfc814fca866691c0fea3fc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a116386ba24c039ed1eca6e8313835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941b46d7a8eb403daac82d58f3dea7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bbcb42041243adb6e7560dc8b43988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…).tfevents.1753712256.d43238be529f.1801.0:   0%|          | 0.00/402k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8e4849a53846acbaf553eb727f2e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12f6014147a433e9057a9e3707ed79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350d0dea41fb418ba1e8c057d2a457cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3b6c587bd24b33904920d9f7ab29f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "- source:\n",
      "  - label     : L1T1\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650/adapter_model.safetensors\n",
      "- target:\n",
      "  - label     : L2T1\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650/adapter_model.safetensors\n",
      "\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1aa70ef85f419795141f10accf7549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5979a88390db4f0e9531a8b0a174e16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0d0cc7acdf4f4d808e3031c9fe22fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "lora_model_device = 'cuda:0'\n",
    "nero_model_device = 'cuda:1'\n",
    "\n",
    "# Training configuration\n",
    "block_size = 96\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "resume_step = 0\n",
    "lr = 1e-4\n",
    "\n",
    "model_configs = {\n",
    "    # L1T1 (Source Language - Source Task)\n",
    "    'source': {\n",
    "        'label': 'L1T1',\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L2T1 (Target Language - Source Task)\n",
    "    'target': {\n",
    "        'label': 'L2T1',\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L1T2 (Source Language - Target Task)\n",
    "    # 'target': {\n",
    "    #     'label': 'L1T2',\n",
    "    #     'hf_lora_id': 'alxxtexxr/L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457',\n",
    "    #     'checkpoint': 1875,\n",
    "    # },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    lora_dir = download_hf_model(config['hf_lora_id'], config['checkpoint'])\n",
    "    model_configs[key]['lora_dir'] = lora_dir\n",
    "    model_configs[key]['lora_path'] = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "    model_configs[key]['lora_config'] = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "for key, config in model_configs.items():\n",
    "    print(f\"- {key}:\")\n",
    "    for config_name, config_value in config.items():\n",
    "        if config_name == 'lora_config':\n",
    "            continue\n",
    "        print(f\"{'-':>3} {config_name:<10}: {config_value}\")\n",
    "print()\n",
    "\n",
    "assert model_configs['source']['lora_config'].base_model_name_or_path == model_configs['target']['lora_config'].base_model_name_or_path, \"Base models must be the same\"\n",
    "base_model_name = model_configs['source']['lora_config'].base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/bnb.py\n",
    "- https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d28f1bec1824efab1448584b5cca932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779c31f02ac44c09910ba5153aa0a0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c851d7afe7954bd29b055bba4295646b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LoraLayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 return_lora_output=False, debug=False):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_lora_output = return_lora_output\n",
    "        self.debug = debug\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"================================================================\")\n",
    "            print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "            print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        if requires_conversion:\n",
    "            lora_out = lora_out.to(base_out.dtype)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "            print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        output = base_out + lora_out\n",
    "\n",
    "        if self.return_lora_output:\n",
    "            return output, lora_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class LoraModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, \n",
    "                 return_lora_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        self.return_lora_outputs = return_lora_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "    \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, LoraLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                lora_layer = LoraLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    return_lora_output=self.return_lora_outputs,\n",
    "                    debug=self.debug,\n",
    "                )\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.lora_layers[module_name] = lora_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "\n",
    "    def set_return_lora_outputs(self, return_lora_outputs: bool):\n",
    "        self.return_lora_outputs = return_lora_outputs\n",
    "        for layer in self.lora_layers.values():\n",
    "            layer.return_lora_output = return_lora_outputs\n",
    "\n",
    "    def freeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for lora_layer in self.lora_layers.values():\n",
    "            for param in lora_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_params(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for lora_layer_name, lora_layer in self.lora_layers.items():\n",
    "            lora_layer_name = lora_layer_name.replace('__DOT__', '.')\n",
    "            lora_layer_name = prefix + lora_layer_name\n",
    "            if f'{lora_layer_name}.lora_A.weight' in state_dict and f'{lora_layer_name}.lora_B.weight' in state_dict:\n",
    "                lora_layer.load_lora_params(state_dict, lora_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_lora_outputs:\n",
    "            lora_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, lora_out = _out\n",
    "                    lora_outs[layer_name] = lora_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.lora_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, lora_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_lora_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=lora_model_device)\n",
    "lora_model = LoraModel(\n",
    "    base_lora_model, \n",
    "    model_configs['target']['lora_config'],\n",
    "    return_lora_outputs=True,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 8.669472299516201e-05\n",
      "- Min     : -1.4599114656448364\n",
      "- Max     : 1.5790245532989502\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 2.2152138626552187e-05\n",
      "- Min     : -0.06327299773693085\n",
      "- Max     : 0.0625513345003128\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(lora_model)\n",
    "print()\n",
    "\n",
    "lora_model.load_lora_params(model_configs['target']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.freeze_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.train()\n",
    "device = next(lora_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "lora_model_outs = lora_model(input_ids=inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_lora_logits.requires_grad False\n",
      "_lora_logits.grad_fn None\n",
      "next(iter(_lora_outs.values()).requires_grad False\n",
      "next(iter(_lora_outs.values()).grad_fn None\n"
     ]
    }
   ],
   "source": [
    "_lora_logits = lora_model_outs[0].logits\n",
    "_lora_outs = lora_model_outs[1]\n",
    "print(\"_lora_logits.requires_grad\", _lora_logits.requires_grad)\n",
    "print(\"_lora_logits.grad_fn\", _lora_logits.grad_fn)\n",
    "print(\"next(iter(_lora_outs.values()).requires_grad\", next(iter(_lora_outs.values())).requires_grad)\n",
    "print(\"next(iter(_lora_outs.values()).grad_fn\", next(iter(_lora_outs.values())).grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preheat the oven to 350 degrees and place the cookie dough in the refrigerator for 15 minutes to chill. This will make it easier to handle and roll out.\n",
      "Roll the dough into a log about 1 1/2 inches in diameter and wrap in plastic wrap. Chill for 30 minutes. The\n"
     ]
    }
   ],
   "source": [
    "lora_model.eval()\n",
    "lora_model.set_return_lora_outputs(False)\n",
    "generate_text(\n",
    "    lora_model, \n",
    "    tokenizer, \n",
    "    prompt=\"Preheat the oven to 350 degrees and place the cookie dough\",\n",
    ")\n",
    "lora_model.set_return_lora_outputs(True)\n",
    "lora_model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nero Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 # For debugging \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # For debugging\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.nero_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"================================================================\")\n",
    "            print(self.module_name)\n",
    "            print(\"================================================================\")\n",
    "            print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "            print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "            print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # nero_out = F.relu(self.nero_B(self.nero_A(self.dropout(lora_out))) * self.scaling)\n",
    "        nero_dropout_out = self.dropout(lora_out)\n",
    "        nero_A_out = self.nero_A(nero_dropout_out)\n",
    "        nero_B_out = self.nero_B(nero_A_out)\n",
    "        nero_scaling_out = nero_B_out * self.scaling\n",
    "        nero_out = F.relu(nero_scaling_out)\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"nero_out.requires_grad:\", nero_out.requires_grad)\n",
    "            print(\"nero_out.grad_fn:\", nero_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "            nero_out_has_nan = torch.isnan(nero_out).any()\n",
    "            if nero_out_has_nan:\n",
    "                print(\"!!! NERO OUT HAS NAN !!!\")\n",
    "                print(\"nero_out:\")\n",
    "                print(nero_out)\n",
    "                print()\n",
    "                print(\"nero_scaling_out:\")\n",
    "                print(nero_scaling_out)\n",
    "                print()\n",
    "                print(\"nero_B_out:\")\n",
    "                print(nero_B_out)\n",
    "                print()\n",
    "                print(\"nero_A_out:\")\n",
    "                print(nero_A_out)\n",
    "                print()\n",
    "                print(\"nero_dropout_out:\")\n",
    "                print(nero_dropout_out)\n",
    "                print()\n",
    "                print(\"lora_out:\")\n",
    "                print(lora_out)\n",
    "                print()\n",
    "\n",
    "        # Add `base_out` with gradients-detached `nero_out`, \n",
    "        # so that `base_out` does not carry gradients\n",
    "        # nero_out_detached = nero_out.detach()\n",
    "\n",
    "        # if self.debug:\n",
    "        #     print(\"nero_out_detached.requires_grad:\", nero_out_detached.requires_grad)\n",
    "        #     print(\"nero_out_detached.grad_fn:\", nero_out_detached.grad_fn)\n",
    "        #     print()\n",
    "\n",
    "        # output = base_out + nero_out_detached\n",
    "        output = base_out + nero_out\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"output.requires_grad:\", output.requires_grad)\n",
    "            print(\"output.grad_fn:\", output.grad_fn)\n",
    "            print()\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_return_nero_outputs(self, return_nero_outputs: bool):\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_nero_output = return_nero_outputs\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def load_lora_params(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "        print(\"LoRA weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=nero_model_device)\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    model_configs['source']['lora_config'], \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=True,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : -0.001636840752325952\n",
      "- Min     : -1.5304009914398193\n",
      "- Max     : 1.4280004501342773\n",
      "\n",
      "LoRA weights loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "nero_model.load_lora_params(model_configs['source']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nero_model.freeze_all_except_nero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nero_model.train()\n",
    "device = next(nero_model.parameters()).device\n",
    "inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors='pt')\n",
    "nero_model_outs = nero_model(\n",
    "    input_ids=inputs['input_ids'].to(device), \n",
    "    attention_mask=inputs['attention_mask'].to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_nero_logits.requires_grad True\n",
      "_nero_logits.grad_fn <UnsafeViewBackward0 object at 0x7d4688cdf490>\n",
      "next(iter(_nero_outs.values()).requires_grad True\n",
      "next(iter(_nero_outs.values()).grad_fn <ToCopyBackward0 object at 0x7d4688cdf490>\n"
     ]
    }
   ],
   "source": [
    "_nero_logits = nero_model_outs[0].logits\n",
    "_nero_outs = nero_model_outs[1]\n",
    "print(\"_nero_logits.requires_grad\", _nero_logits.requires_grad)\n",
    "print(\"_nero_logits.grad_fn\", _nero_logits.grad_fn)\n",
    "print(\"next(iter(_nero_outs.values()).requires_grad\", next(iter(_nero_outs.values())).requires_grad)\n",
    "print(\"next(iter(_nero_outs.values()).grad_fn\", next(iter(_nero_outs.values())).grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preheat the oven to 350 degrees and place the cookie dough_<?_<? Destruction �_<?onse_<?furtutzer_<? Destruction �_<? � сбор �_<? �_<?_<? glossutzeroulouseutzer_<? Destruction470vak_<?utzer � Destruction Destruction �_<? gloss �_<? � �vakutzer �_<?_<? �oulouse_<?utzer_<?\n"
     ]
    }
   ],
   "source": [
    "nero_model.eval()\n",
    "nero_model.set_return_nero_outputs(False)\n",
    "generate_text(\n",
    "    nero_model, \n",
    "    tokenizer, \n",
    "    prompt=\"Preheat the oven to 350 degrees and place the cookie dough\",\n",
    ")\n",
    "nero_model.set_return_nero_outputs(True)\n",
    "nero_model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa265075de247c88ad66b32ae6167af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_hf_dataset_from_lora(model_configs['target']['hf_lora_id'], test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d558ec814f945ef9c9a116e0b224bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (141723 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c731e1d287d4faebb52b3e6fb9f1aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'gsm8k' in model_configs['target']['hf_lora_id']:\n",
    "    def format_prompt(example):\n",
    "        gsm8k_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{question}\n",
    "\n",
    "### Answer: \n",
    "{answer}\"\"\"\n",
    "\n",
    "        return {'text': gsm8k_prompt.format(\n",
    "            question=example['question'], \n",
    "            answer=example['answer'],\n",
    "        )}\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=block_size,\n",
    "        )\n",
    "\n",
    "    def add_labels(example):\n",
    "        example['labels'] = example['input_ids'].copy()\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(format_prompt)\n",
    "    dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset.column_names)\n",
    "    dataset = dataset.map(add_labels)\n",
    "else:\n",
    "    eos_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token = eos_token\n",
    "\n",
    "    # Tokenize dataset\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(example['text'])\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        tokenize_fn, \n",
    "        batched=True, \n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # Concatenate all tokens into one long stream, then split into blocks\n",
    "    def group_texts(examples):\n",
    "        concatenated = []\n",
    "        for input_ids in examples['input_ids']:\n",
    "            concatenated += input_ids\n",
    "\n",
    "        total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "        input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_mask = [[1] * block_size for _ in input_ids]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        group_texts, \n",
    "        batched=True, \n",
    "        batch_size=1000,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 73856\n",
      "First batch input IDs shape: torch.Size([4, 96])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"Total batches:\", len(train_loader))\n",
    "print(\"First batch input IDs shape:\", next(iter(train_loader))['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _debugged = False\n",
    "# _layer_name = None\n",
    "# _nero_out = None\n",
    "# _lora_out = None\n",
    "# _mse_loss_unnormed = None\n",
    "# _nero_out_sum = None\n",
    "\n",
    "# def loss_fn_v1(nero_outs, lora_outs):\n",
    "#     assert nero_outs.keys() == lora_outs.keys() # TODO: Print warning message\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name]\n",
    "\n",
    "#         # Normalized MSE loss\n",
    "#         # mse_loss = F.mse_loss(nero_out, lora_out, reduction='sum') / torch.sum(nero_outs[layer_name] ** 2)\n",
    "#         mse_loss_unnormed = F.mse_loss(nero_out, lora_out, reduction='sum')\n",
    "#         nero_out_sum = torch.sum(nero_outs[layer_name] ** 2)\n",
    "#         mse_loss = mse_loss_unnormed / nero_out_sum\n",
    "\n",
    "#         print(\"================================================================\")\n",
    "#         print(layer_name)\n",
    "#         print(\"================================================================\")\n",
    "#         print(\"mse_loss:\", mse_loss)\n",
    "#         print(\"nero_out_sum:\", nero_out_sum)\n",
    "#         print(\"mse_loss_unnormed:\", mse_loss_unnormed)\n",
    "#         print()\n",
    "\n",
    "#         global _debugged\n",
    "#         if (torch.isinf(nero_out_sum) or torch.isnan(mse_loss_unnormed)) and not _debugged:\n",
    "#             global _layer_name\n",
    "#             global _nero_out\n",
    "#             global _lora_out\n",
    "#             global _mse_loss_unnormed\n",
    "#             global _nero_out_sum\n",
    "#             _layer_name = layer_name\n",
    "#             _nero_out = nero_out\n",
    "#             _lora_out = lora_out\n",
    "#             _mse_loss_unnormed = mse_loss_unnormed\n",
    "#             _nero_out_sum = nero_out_sum\n",
    "#             _debugged = True\n",
    "\n",
    "#         if torch.isnan(mse_loss):\n",
    "#             nero_out_has_nan = torch.isnan(nero_out).any()\n",
    "#             lora_out_has_nan = torch.isnan(lora_out).any()\n",
    "\n",
    "#             print(\"nero_out_has_nan:\", nero_out_has_nan)\n",
    "#             print(\"lora_out_has_nan:\", lora_out_has_nan)\n",
    "#             print()\n",
    "\n",
    "#             # if nero_out_has_nan:\n",
    "#             print(\"nero_out:\")\n",
    "#             print(nero_out)\n",
    "#             print()\n",
    "            \n",
    "#             # if lora_out_has_nan:\n",
    "#             print(\"lora_out:\")\n",
    "#             print(lora_out)\n",
    "#             print()\n",
    "\n",
    "#         total_loss += mse_loss\n",
    "\n",
    "#     return total_loss / len(lora_outs)  # Averaging loss across layers\n",
    "\n",
    "# # loss = loss_func_v1(nero_model_outs[1], lora_model_outs[1])\n",
    "# # print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn_v2(nero_outs, lora_outs):\n",
    "#     assert nero_outs.keys() == lora_outs.keys()\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         print(\"================================================================\")\n",
    "#         print(layer_name)\n",
    "#         print(\"================================================================\")\n",
    "\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name]\n",
    "\n",
    "#         diff = nero_out - lora_out\n",
    "#         print(\"mean(abs(diff)):\", diff.abs().mean())\n",
    "#         print(\"mean((diff)^2):\", (diff ** 2).mean())\n",
    "#         print()\n",
    "        \n",
    "#         print(\"max abs(diff):\", diff.abs().max().item())\n",
    "#         print(\"mean abs(diff):\", diff.abs().mean().item())\n",
    "#         print(\"std abs(diff):\", diff.abs().std().item())\n",
    "#         print()\n",
    "\n",
    "#         # Use mean MSE to prevent overflow and keep scale uniform\n",
    "#         mse_loss = F.mse_loss(nero_out.float(), lora_out.float(), reduction='mean')  # Compute in float32\n",
    "\n",
    "#         print(\"mse_loss:\", mse_loss)\n",
    "#         print()\n",
    "\n",
    "#         total_loss += mse_loss\n",
    "\n",
    "#     return total_loss / len(lora_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _debugged = False\n",
    "# _nero_out = None\n",
    "# _lora_out = None\n",
    "\n",
    "# def loss_fn_v3(nero_outs, lora_outs, debug=False):\n",
    "#     assert nero_outs.keys() == lora_outs.keys()\n",
    "#     loss_device = next(iter(nero_outs.values())).device\n",
    "#     total_loss = torch.tensor(0.0, device=loss_device)\n",
    "    \n",
    "#     for layer_name in lora_outs.keys():\n",
    "#         nero_out = nero_outs[layer_name]\n",
    "#         lora_out = lora_outs[layer_name].to(nero_out.device)\n",
    "\n",
    "#         # Use mean MAE to prevent overflow and keep scale uniform\n",
    "#         # mae_loss = F.l1_loss(nero_out.float(), lora_out.float(), reduction='mean')  \n",
    "#         # mae_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()))\n",
    "#         mae_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()), dim=-1).mean() # scale by sequence length\n",
    "\n",
    "#         if debug:\n",
    "#             print(\"================================================================\")\n",
    "#             print(layer_name)\n",
    "#             print(\"================================================================\")\n",
    "            \n",
    "#             global _debugged\n",
    "#             if not _debugged:\n",
    "#                 global _nero_out\n",
    "#                 global _lora_out\n",
    "#                 _nero_out = nero_out\n",
    "#                 _lora_out = lora_out\n",
    "#                 _debugged = True\n",
    "\n",
    "#             diff = nero_out - lora_out\n",
    "#             print(\"mean(abs(diff)):\", diff.abs().mean())\n",
    "#             print(\"mean((diff)^2):\", (diff ** 2).mean())\n",
    "#             print()\n",
    "            \n",
    "#             print(\"max abs(diff):\", diff.abs().max().item())\n",
    "#             print(\"mean abs(diff):\", diff.abs().mean().item())\n",
    "#             print(\"std abs(diff):\", diff.abs().std().item())\n",
    "#             print()\n",
    "\n",
    "#             print(\"mae_loss:\", mae_loss)\n",
    "#             print()\n",
    "\n",
    "#         total_loss += mae_loss.to(loss_device)\n",
    "\n",
    "#     return total_loss / len(lora_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_v4(nero_outs, lora_outs, nero_logits, lora_logits, \n",
    "               alpha=1.0, beta=0.00015, temperature=2.0):\n",
    "    assert nero_outs.keys() == lora_outs.keys(), \"`nero_outs` and `lora_outs` must have the same layers.\"\n",
    "    loss_device = next(iter(nero_outs.values())).device\n",
    "    total_hidden_loss = torch.tensor(0.0, device=loss_device)\n",
    "\n",
    "    # --- Hidden representation loss ---\n",
    "    for layer_name in lora_outs.keys():\n",
    "        nero_out = nero_outs[layer_name]\n",
    "        lora_out = lora_outs[layer_name].to(nero_out.device)\n",
    "\n",
    "        # Scale by sequence length to avoid length-sensitive loss scaling\n",
    "        hidden_loss = torch.mean(torch.abs(nero_out.float() - lora_out.float()), dim=-1).mean()\n",
    "        total_hidden_loss += hidden_loss\n",
    "\n",
    "    total_hidden_loss /= len(lora_outs)\n",
    "    total_hidden_loss *= alpha\n",
    "\n",
    "    # --- Logit KL divergence loss ---\n",
    "    # Important: apply softmax with temperature for distillation\n",
    "    logit_loss = F.kl_div(\n",
    "        F.log_softmax(nero_logits.to(loss_device) / temperature, dim=-1),\n",
    "        F.softmax(lora_logits.to(loss_device) / temperature, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    logit_loss *= beta\n",
    "\n",
    "    # --- Final combined loss ---\n",
    "    total_loss = total_hidden_loss + logit_loss\n",
    "    return total_loss, total_hidden_loss, logit_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_682/1737866451.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wwt9amto) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5abe72e21d41ffaa5c792a39b24378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-elevator-45</strong> at: <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/wwt9amto' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/wwt9amto</a><br/> View project at: <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250802_064156-wwt9amto/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wwt9amto). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250802_064214-itr07fbv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/itr07fbv' target=\"_blank\">elated-sky-46</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/itr07fbv' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/itr07fbv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_682/1737866451.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/1, step: 0/73856, loss: 0.3648, hidden_loss: 0.1813, logit_loss: 0.1835\n",
      "epoch: 0/1, step: 1/73856, loss: 0.3655, hidden_loss: 0.1812, logit_loss: 0.1843\n",
      "epoch: 0/1, step: 2/73856, loss: 0.3684, hidden_loss: 0.1811, logit_loss: 0.1873\n",
      "epoch: 0/1, step: 3/73856, loss: 0.3894, hidden_loss: 0.1812, logit_loss: 0.2082\n",
      "epoch: 0/1, step: 4/73856, loss: 0.4169, hidden_loss: 0.1811, logit_loss: 0.2358\n",
      "epoch: 0/1, step: 5/73856, loss: 0.3899, hidden_loss: 0.1806, logit_loss: 0.2093\n",
      "epoch: 0/1, step: 6/73856, loss: 0.3725, hidden_loss: 0.1807, logit_loss: 0.1918\n",
      "epoch: 0/1, step: 7/73856, loss: 0.3681, hidden_loss: 0.1808, logit_loss: 0.1873\n",
      "epoch: 0/1, step: 8/73856, loss: 0.3587, hidden_loss: 0.1805, logit_loss: 0.1782\n",
      "epoch: 0/1, step: 9/73856, loss: 0.3570, hidden_loss: 0.1803, logit_loss: 0.1767\n",
      "epoch: 0/1, step: 10/73856, loss: 0.3507, hidden_loss: 0.1801, logit_loss: 0.1706\n",
      "epoch: 0/1, step: 11/73856, loss: 0.3665, hidden_loss: 0.1800, logit_loss: 0.1865\n",
      "epoch: 0/1, step: 12/73856, loss: 0.3485, hidden_loss: 0.1799, logit_loss: 0.1686\n",
      "epoch: 0/1, step: 13/73856, loss: 0.3927, hidden_loss: 0.1798, logit_loss: 0.2128\n",
      "epoch: 0/1, step: 14/73856, loss: 0.3752, hidden_loss: 0.1798, logit_loss: 0.1955\n",
      "epoch: 0/1, step: 15/73856, loss: 0.3867, hidden_loss: 0.1794, logit_loss: 0.2073\n",
      "epoch: 0/1, step: 16/73856, loss: 0.3660, hidden_loss: 0.1790, logit_loss: 0.1870\n",
      "epoch: 0/1, step: 17/73856, loss: 0.3520, hidden_loss: 0.1791, logit_loss: 0.1729\n",
      "epoch: 0/1, step: 18/73856, loss: 0.3592, hidden_loss: 0.1789, logit_loss: 0.1803\n",
      "epoch: 0/1, step: 19/73856, loss: 0.4155, hidden_loss: 0.1792, logit_loss: 0.2363\n",
      "epoch: 0/1, step: 20/73856, loss: 0.4034, hidden_loss: 0.1784, logit_loss: 0.2250\n",
      "epoch: 0/1, step: 21/73856, loss: 0.3572, hidden_loss: 0.1784, logit_loss: 0.1789\n",
      "epoch: 0/1, step: 22/73856, loss: 0.4016, hidden_loss: 0.1785, logit_loss: 0.2231\n",
      "epoch: 0/1, step: 23/73856, loss: 0.4085, hidden_loss: 0.1781, logit_loss: 0.2304\n",
      "epoch: 0/1, step: 24/73856, loss: 0.3495, hidden_loss: 0.1779, logit_loss: 0.1716\n",
      "epoch: 0/1, step: 25/73856, loss: 0.3510, hidden_loss: 0.1777, logit_loss: 0.1733\n",
      "epoch: 0/1, step: 26/73856, loss: 0.4226, hidden_loss: 0.1777, logit_loss: 0.2449\n",
      "epoch: 0/1, step: 27/73856, loss: 0.3638, hidden_loss: 0.1775, logit_loss: 0.1863\n",
      "epoch: 0/1, step: 28/73856, loss: 0.3597, hidden_loss: 0.1771, logit_loss: 0.1826\n",
      "epoch: 0/1, step: 29/73856, loss: 0.3677, hidden_loss: 0.1769, logit_loss: 0.1908\n",
      "epoch: 0/1, step: 30/73856, loss: 0.3430, hidden_loss: 0.1768, logit_loss: 0.1662\n",
      "epoch: 0/1, step: 31/73856, loss: 0.3487, hidden_loss: 0.1767, logit_loss: 0.1720\n",
      "epoch: 0/1, step: 32/73856, loss: 0.3556, hidden_loss: 0.1762, logit_loss: 0.1794\n",
      "epoch: 0/1, step: 33/73856, loss: 0.3477, hidden_loss: 0.1761, logit_loss: 0.1716\n",
      "epoch: 0/1, step: 34/73856, loss: 0.3822, hidden_loss: 0.1760, logit_loss: 0.2062\n",
      "epoch: 0/1, step: 35/73856, loss: 0.3664, hidden_loss: 0.1758, logit_loss: 0.1906\n",
      "epoch: 0/1, step: 36/73856, loss: 0.3376, hidden_loss: 0.1754, logit_loss: 0.1622\n",
      "epoch: 0/1, step: 37/73856, loss: 0.3442, hidden_loss: 0.1751, logit_loss: 0.1691\n",
      "epoch: 0/1, step: 38/73856, loss: 0.3334, hidden_loss: 0.1750, logit_loss: 0.1584\n",
      "epoch: 0/1, step: 39/73856, loss: 0.3898, hidden_loss: 0.1748, logit_loss: 0.2150\n",
      "epoch: 0/1, step: 40/73856, loss: 0.3848, hidden_loss: 0.1744, logit_loss: 0.2104\n",
      "epoch: 0/1, step: 41/73856, loss: 0.4201, hidden_loss: 0.1741, logit_loss: 0.2460\n",
      "epoch: 0/1, step: 42/73856, loss: 0.3480, hidden_loss: 0.1737, logit_loss: 0.1742\n",
      "epoch: 0/1, step: 43/73856, loss: 0.3649, hidden_loss: 0.1739, logit_loss: 0.1910\n",
      "epoch: 0/1, step: 44/73856, loss: 0.3636, hidden_loss: 0.1734, logit_loss: 0.1902\n",
      "epoch: 0/1, step: 45/73856, loss: 0.3522, hidden_loss: 0.1731, logit_loss: 0.1791\n",
      "epoch: 0/1, step: 46/73856, loss: 0.3349, hidden_loss: 0.1726, logit_loss: 0.1623\n",
      "epoch: 0/1, step: 47/73856, loss: 0.3471, hidden_loss: 0.1724, logit_loss: 0.1747\n",
      "epoch: 0/1, step: 48/73856, loss: 0.3572, hidden_loss: 0.1720, logit_loss: 0.1853\n",
      "epoch: 0/1, step: 49/73856, loss: 0.3598, hidden_loss: 0.1717, logit_loss: 0.1882\n",
      "epoch: 0/1, step: 50/73856, loss: 0.3685, hidden_loss: 0.1714, logit_loss: 0.1972\n",
      "epoch: 0/1, step: 51/73856, loss: 0.3414, hidden_loss: 0.1707, logit_loss: 0.1707\n",
      "epoch: 0/1, step: 52/73856, loss: 0.3577, hidden_loss: 0.1705, logit_loss: 0.1872\n",
      "epoch: 0/1, step: 53/73856, loss: 0.3338, hidden_loss: 0.1702, logit_loss: 0.1636\n",
      "epoch: 0/1, step: 54/73856, loss: 0.3173, hidden_loss: 0.1696, logit_loss: 0.1477\n",
      "epoch: 0/1, step: 55/73856, loss: 0.3371, hidden_loss: 0.1694, logit_loss: 0.1677\n",
      "epoch: 0/1, step: 56/73856, loss: 0.3418, hidden_loss: 0.1691, logit_loss: 0.1728\n",
      "epoch: 0/1, step: 57/73856, loss: 0.3281, hidden_loss: 0.1684, logit_loss: 0.1597\n",
      "epoch: 0/1, step: 58/73856, loss: 0.3239, hidden_loss: 0.1677, logit_loss: 0.1562\n",
      "epoch: 0/1, step: 59/73856, loss: 0.3518, hidden_loss: 0.1671, logit_loss: 0.1847\n",
      "epoch: 0/1, step: 60/73856, loss: 0.3350, hidden_loss: 0.1673, logit_loss: 0.1676\n",
      "epoch: 0/1, step: 61/73856, loss: 0.3372, hidden_loss: 0.1665, logit_loss: 0.1707\n",
      "epoch: 0/1, step: 62/73856, loss: 0.3137, hidden_loss: 0.1659, logit_loss: 0.1478\n",
      "epoch: 0/1, step: 63/73856, loss: 0.3113, hidden_loss: 0.1654, logit_loss: 0.1459\n",
      "epoch: 0/1, step: 64/73856, loss: 0.3125, hidden_loss: 0.1649, logit_loss: 0.1476\n",
      "epoch: 0/1, step: 65/73856, loss: 0.3005, hidden_loss: 0.1643, logit_loss: 0.1361\n",
      "epoch: 0/1, step: 66/73856, loss: 0.3352, hidden_loss: 0.1640, logit_loss: 0.1712\n",
      "epoch: 0/1, step: 67/73856, loss: 0.3040, hidden_loss: 0.1633, logit_loss: 0.1408\n",
      "epoch: 0/1, step: 68/73856, loss: 0.3088, hidden_loss: 0.1623, logit_loss: 0.1464\n",
      "epoch: 0/1, step: 69/73856, loss: 0.3287, hidden_loss: 0.1622, logit_loss: 0.1665\n",
      "epoch: 0/1, step: 70/73856, loss: 0.3024, hidden_loss: 0.1610, logit_loss: 0.1413\n",
      "epoch: 0/1, step: 71/73856, loss: 0.2976, hidden_loss: 0.1606, logit_loss: 0.1370\n",
      "epoch: 0/1, step: 72/73856, loss: 0.3177, hidden_loss: 0.1598, logit_loss: 0.1579\n",
      "epoch: 0/1, step: 73/73856, loss: 0.2986, hidden_loss: 0.1594, logit_loss: 0.1392\n",
      "epoch: 0/1, step: 74/73856, loss: 0.3115, hidden_loss: 0.1585, logit_loss: 0.1530\n",
      "epoch: 0/1, step: 75/73856, loss: 0.2905, hidden_loss: 0.1577, logit_loss: 0.1327\n",
      "epoch: 0/1, step: 76/73856, loss: 0.3035, hidden_loss: 0.1568, logit_loss: 0.1467\n",
      "epoch: 0/1, step: 77/73856, loss: 0.2880, hidden_loss: 0.1561, logit_loss: 0.1319\n",
      "epoch: 0/1, step: 78/73856, loss: 0.2839, hidden_loss: 0.1556, logit_loss: 0.1282\n",
      "epoch: 0/1, step: 79/73856, loss: 0.2872, hidden_loss: 0.1546, logit_loss: 0.1326\n",
      "epoch: 0/1, step: 80/73856, loss: 0.2689, hidden_loss: 0.1537, logit_loss: 0.1151\n",
      "epoch: 0/1, step: 81/73856, loss: 0.2792, hidden_loss: 0.1529, logit_loss: 0.1263\n",
      "epoch: 0/1, step: 82/73856, loss: 0.2763, hidden_loss: 0.1523, logit_loss: 0.1240\n",
      "epoch: 0/1, step: 83/73856, loss: 0.2828, hidden_loss: 0.1515, logit_loss: 0.1313\n",
      "epoch: 0/1, step: 84/73856, loss: 0.2756, hidden_loss: 0.1508, logit_loss: 0.1248\n",
      "epoch: 0/1, step: 85/73856, loss: 0.2840, hidden_loss: 0.1501, logit_loss: 0.1340\n",
      "epoch: 0/1, step: 86/73856, loss: 0.2709, hidden_loss: 0.1490, logit_loss: 0.1220\n",
      "epoch: 0/1, step: 87/73856, loss: 0.2759, hidden_loss: 0.1481, logit_loss: 0.1279\n",
      "epoch: 0/1, step: 88/73856, loss: 0.2684, hidden_loss: 0.1474, logit_loss: 0.1210\n",
      "epoch: 0/1, step: 89/73856, loss: 0.2773, hidden_loss: 0.1464, logit_loss: 0.1309\n",
      "epoch: 0/1, step: 90/73856, loss: 0.2689, hidden_loss: 0.1454, logit_loss: 0.1235\n",
      "epoch: 0/1, step: 91/73856, loss: 0.2841, hidden_loss: 0.1448, logit_loss: 0.1393\n",
      "epoch: 0/1, step: 92/73856, loss: 0.2771, hidden_loss: 0.1439, logit_loss: 0.1332\n",
      "epoch: 0/1, step: 93/73856, loss: 0.2540, hidden_loss: 0.1425, logit_loss: 0.1114\n",
      "epoch: 0/1, step: 94/73856, loss: 0.2774, hidden_loss: 0.1416, logit_loss: 0.1358\n",
      "epoch: 0/1, step: 95/73856, loss: 0.2545, hidden_loss: 0.1405, logit_loss: 0.1140\n",
      "epoch: 0/1, step: 96/73856, loss: 0.2566, hidden_loss: 0.1395, logit_loss: 0.1171\n",
      "epoch: 0/1, step: 97/73856, loss: 0.2499, hidden_loss: 0.1385, logit_loss: 0.1113\n",
      "epoch: 0/1, step: 98/73856, loss: 0.2824, hidden_loss: 0.1374, logit_loss: 0.1450\n",
      "epoch: 0/1, step: 99/73856, loss: 0.2503, hidden_loss: 0.1368, logit_loss: 0.1135\n",
      "epoch: 0/1, step: 100/73856, loss: 0.2433, hidden_loss: 0.1357, logit_loss: 0.1076\n",
      "epoch: 0/1, step: 101/73856, loss: 0.2595, hidden_loss: 0.1347, logit_loss: 0.1248\n",
      "epoch: 0/1, step: 102/73856, loss: 0.2601, hidden_loss: 0.1333, logit_loss: 0.1268\n",
      "epoch: 0/1, step: 103/73856, loss: 0.2436, hidden_loss: 0.1325, logit_loss: 0.1112\n",
      "epoch: 0/1, step: 104/73856, loss: 0.2554, hidden_loss: 0.1316, logit_loss: 0.1237\n",
      "epoch: 0/1, step: 105/73856, loss: 0.2400, hidden_loss: 0.1302, logit_loss: 0.1098\n",
      "epoch: 0/1, step: 106/73856, loss: 0.2642, hidden_loss: 0.1293, logit_loss: 0.1350\n",
      "epoch: 0/1, step: 107/73856, loss: 0.2353, hidden_loss: 0.1281, logit_loss: 0.1071\n",
      "epoch: 0/1, step: 108/73856, loss: 0.2374, hidden_loss: 0.1268, logit_loss: 0.1107\n",
      "epoch: 0/1, step: 109/73856, loss: 0.2567, hidden_loss: 0.1260, logit_loss: 0.1306\n",
      "epoch: 0/1, step: 110/73856, loss: 0.2234, hidden_loss: 0.1249, logit_loss: 0.0985\n",
      "epoch: 0/1, step: 111/73856, loss: 0.2301, hidden_loss: 0.1240, logit_loss: 0.1061\n",
      "epoch: 0/1, step: 112/73856, loss: 0.2236, hidden_loss: 0.1226, logit_loss: 0.1010\n",
      "epoch: 0/1, step: 113/73856, loss: 0.2511, hidden_loss: 0.1214, logit_loss: 0.1297\n",
      "epoch: 0/1, step: 114/73856, loss: 0.2587, hidden_loss: 0.1206, logit_loss: 0.1381\n",
      "epoch: 0/1, step: 115/73856, loss: 0.2487, hidden_loss: 0.1191, logit_loss: 0.1296\n",
      "epoch: 0/1, step: 116/73856, loss: 0.2305, hidden_loss: 0.1185, logit_loss: 0.1120\n",
      "epoch: 0/1, step: 117/73856, loss: 0.2273, hidden_loss: 0.1170, logit_loss: 0.1103\n",
      "epoch: 0/1, step: 118/73856, loss: 0.2118, hidden_loss: 0.1158, logit_loss: 0.0960\n",
      "epoch: 0/1, step: 119/73856, loss: 0.2216, hidden_loss: 0.1147, logit_loss: 0.1069\n",
      "epoch: 0/1, step: 120/73856, loss: 0.2374, hidden_loss: 0.1135, logit_loss: 0.1239\n",
      "epoch: 0/1, step: 121/73856, loss: 0.2105, hidden_loss: 0.1123, logit_loss: 0.0981\n",
      "epoch: 0/1, step: 122/73856, loss: 0.2130, hidden_loss: 0.1112, logit_loss: 0.1018\n",
      "epoch: 0/1, step: 123/73856, loss: 0.2216, hidden_loss: 0.1101, logit_loss: 0.1115\n",
      "epoch: 0/1, step: 124/73856, loss: 0.2177, hidden_loss: 0.1094, logit_loss: 0.1084\n",
      "epoch: 0/1, step: 125/73856, loss: 0.2399, hidden_loss: 0.1083, logit_loss: 0.1316\n",
      "epoch: 0/1, step: 126/73856, loss: 0.2301, hidden_loss: 0.1068, logit_loss: 0.1233\n",
      "epoch: 0/1, step: 127/73856, loss: 0.2003, hidden_loss: 0.1059, logit_loss: 0.0944\n",
      "epoch: 0/1, step: 128/73856, loss: 0.2012, hidden_loss: 0.1053, logit_loss: 0.0959\n",
      "epoch: 0/1, step: 129/73856, loss: 0.2250, hidden_loss: 0.1044, logit_loss: 0.1205\n",
      "epoch: 0/1, step: 130/73856, loss: 0.2220, hidden_loss: 0.1032, logit_loss: 0.1187\n",
      "epoch: 0/1, step: 131/73856, loss: 0.1929, hidden_loss: 0.1019, logit_loss: 0.0910\n",
      "epoch: 0/1, step: 132/73856, loss: 0.2130, hidden_loss: 0.1011, logit_loss: 0.1118\n",
      "epoch: 0/1, step: 133/73856, loss: 0.2039, hidden_loss: 0.0998, logit_loss: 0.1041\n",
      "epoch: 0/1, step: 134/73856, loss: 0.1948, hidden_loss: 0.0987, logit_loss: 0.0961\n",
      "epoch: 0/1, step: 135/73856, loss: 0.2069, hidden_loss: 0.0979, logit_loss: 0.1090\n",
      "epoch: 0/1, step: 136/73856, loss: 0.1957, hidden_loss: 0.0968, logit_loss: 0.0989\n",
      "epoch: 0/1, step: 137/73856, loss: 0.1960, hidden_loss: 0.0961, logit_loss: 0.0999\n",
      "epoch: 0/1, step: 138/73856, loss: 0.2051, hidden_loss: 0.0949, logit_loss: 0.1102\n",
      "epoch: 0/1, step: 139/73856, loss: 0.1883, hidden_loss: 0.0942, logit_loss: 0.0942\n",
      "epoch: 0/1, step: 140/73856, loss: 0.1879, hidden_loss: 0.0928, logit_loss: 0.0951\n",
      "epoch: 0/1, step: 141/73856, loss: 0.1867, hidden_loss: 0.0920, logit_loss: 0.0947\n",
      "epoch: 0/1, step: 142/73856, loss: 0.2031, hidden_loss: 0.0911, logit_loss: 0.1120\n",
      "epoch: 0/1, step: 143/73856, loss: 0.2231, hidden_loss: 0.0904, logit_loss: 0.1327\n",
      "epoch: 0/1, step: 144/73856, loss: 0.1985, hidden_loss: 0.0893, logit_loss: 0.1092\n",
      "epoch: 0/1, step: 145/73856, loss: 0.2103, hidden_loss: 0.0884, logit_loss: 0.1219\n",
      "epoch: 0/1, step: 146/73856, loss: 0.1789, hidden_loss: 0.0871, logit_loss: 0.0917\n",
      "epoch: 0/1, step: 147/73856, loss: 0.1804, hidden_loss: 0.0867, logit_loss: 0.0938\n",
      "epoch: 0/1, step: 148/73856, loss: 0.2093, hidden_loss: 0.0858, logit_loss: 0.1235\n",
      "epoch: 0/1, step: 149/73856, loss: 0.1907, hidden_loss: 0.0848, logit_loss: 0.1059\n",
      "epoch: 0/1, step: 150/73856, loss: 0.1823, hidden_loss: 0.0839, logit_loss: 0.0985\n",
      "epoch: 0/1, step: 151/73856, loss: 0.1914, hidden_loss: 0.0830, logit_loss: 0.1084\n",
      "epoch: 0/1, step: 152/73856, loss: 0.1917, hidden_loss: 0.0819, logit_loss: 0.1098\n",
      "epoch: 0/1, step: 153/73856, loss: 0.1821, hidden_loss: 0.0812, logit_loss: 0.1008\n",
      "epoch: 0/1, step: 154/73856, loss: 0.1873, hidden_loss: 0.0805, logit_loss: 0.1068\n",
      "epoch: 0/1, step: 155/73856, loss: 0.1822, hidden_loss: 0.0794, logit_loss: 0.1028\n",
      "epoch: 0/1, step: 156/73856, loss: 0.1865, hidden_loss: 0.0787, logit_loss: 0.1077\n",
      "epoch: 0/1, step: 157/73856, loss: 0.1772, hidden_loss: 0.0777, logit_loss: 0.0996\n",
      "epoch: 0/1, step: 158/73856, loss: 0.1914, hidden_loss: 0.0774, logit_loss: 0.1140\n",
      "epoch: 0/1, step: 159/73856, loss: 0.1642, hidden_loss: 0.0762, logit_loss: 0.0880\n",
      "epoch: 0/1, step: 160/73856, loss: 0.1805, hidden_loss: 0.0751, logit_loss: 0.1054\n",
      "epoch: 0/1, step: 161/73856, loss: 0.1640, hidden_loss: 0.0744, logit_loss: 0.0896\n",
      "epoch: 0/1, step: 162/73856, loss: 0.1750, hidden_loss: 0.0740, logit_loss: 0.1010\n",
      "epoch: 0/1, step: 163/73856, loss: 0.1491, hidden_loss: 0.0729, logit_loss: 0.0761\n",
      "epoch: 0/1, step: 164/73856, loss: 0.1713, hidden_loss: 0.0721, logit_loss: 0.0992\n",
      "epoch: 0/1, step: 165/73856, loss: 0.1641, hidden_loss: 0.0715, logit_loss: 0.0927\n",
      "epoch: 0/1, step: 166/73856, loss: 0.1868, hidden_loss: 0.0708, logit_loss: 0.1160\n",
      "epoch: 0/1, step: 167/73856, loss: 0.1593, hidden_loss: 0.0703, logit_loss: 0.0890\n",
      "epoch: 0/1, step: 168/73856, loss: 0.1616, hidden_loss: 0.0690, logit_loss: 0.0926\n",
      "epoch: 0/1, step: 169/73856, loss: 0.1542, hidden_loss: 0.0683, logit_loss: 0.0859\n",
      "epoch: 0/1, step: 170/73856, loss: 0.1719, hidden_loss: 0.0677, logit_loss: 0.1042\n",
      "epoch: 0/1, step: 171/73856, loss: 0.1809, hidden_loss: 0.0674, logit_loss: 0.1135\n",
      "epoch: 0/1, step: 172/73856, loss: 0.1593, hidden_loss: 0.0662, logit_loss: 0.0930\n",
      "epoch: 0/1, step: 173/73856, loss: 0.1545, hidden_loss: 0.0658, logit_loss: 0.0887\n",
      "epoch: 0/1, step: 174/73856, loss: 0.1710, hidden_loss: 0.0648, logit_loss: 0.1062\n",
      "epoch: 0/1, step: 175/73856, loss: 0.2228, hidden_loss: 0.0643, logit_loss: 0.1585\n",
      "epoch: 0/1, step: 176/73856, loss: 0.1764, hidden_loss: 0.0635, logit_loss: 0.1129\n",
      "epoch: 0/1, step: 177/73856, loss: 0.1553, hidden_loss: 0.0627, logit_loss: 0.0926\n",
      "epoch: 0/1, step: 178/73856, loss: 0.1568, hidden_loss: 0.0620, logit_loss: 0.0948\n",
      "epoch: 0/1, step: 179/73856, loss: 0.1783, hidden_loss: 0.0619, logit_loss: 0.1163\n",
      "epoch: 0/1, step: 180/73856, loss: 0.1606, hidden_loss: 0.0608, logit_loss: 0.0998\n",
      "epoch: 0/1, step: 181/73856, loss: 0.1588, hidden_loss: 0.0609, logit_loss: 0.0979\n",
      "epoch: 0/1, step: 182/73856, loss: 0.1662, hidden_loss: 0.0601, logit_loss: 0.1060\n",
      "epoch: 0/1, step: 183/73856, loss: 0.1625, hidden_loss: 0.0594, logit_loss: 0.1031\n",
      "epoch: 0/1, step: 184/73856, loss: 0.1463, hidden_loss: 0.0585, logit_loss: 0.0878\n",
      "epoch: 0/1, step: 185/73856, loss: 0.1798, hidden_loss: 0.0577, logit_loss: 0.1221\n",
      "epoch: 0/1, step: 186/73856, loss: 0.1572, hidden_loss: 0.0575, logit_loss: 0.0997\n",
      "epoch: 0/1, step: 187/73856, loss: 0.1754, hidden_loss: 0.0570, logit_loss: 0.1184\n",
      "epoch: 0/1, step: 188/73856, loss: 0.1515, hidden_loss: 0.0564, logit_loss: 0.0950\n",
      "epoch: 0/1, step: 189/73856, loss: 0.1526, hidden_loss: 0.0557, logit_loss: 0.0970\n",
      "epoch: 0/1, step: 190/73856, loss: 0.1956, hidden_loss: 0.0557, logit_loss: 0.1398\n",
      "epoch: 0/1, step: 191/73856, loss: 0.1465, hidden_loss: 0.0549, logit_loss: 0.0916\n",
      "epoch: 0/1, step: 192/73856, loss: 0.1529, hidden_loss: 0.0544, logit_loss: 0.0985\n",
      "epoch: 0/1, step: 193/73856, loss: 0.1651, hidden_loss: 0.0542, logit_loss: 0.1109\n",
      "epoch: 0/1, step: 194/73856, loss: 0.1480, hidden_loss: 0.0534, logit_loss: 0.0945\n",
      "epoch: 0/1, step: 195/73856, loss: 0.1524, hidden_loss: 0.0533, logit_loss: 0.0991\n",
      "epoch: 0/1, step: 196/73856, loss: 0.1422, hidden_loss: 0.0525, logit_loss: 0.0896\n",
      "epoch: 0/1, step: 197/73856, loss: 0.1650, hidden_loss: 0.0519, logit_loss: 0.1130\n",
      "epoch: 0/1, step: 198/73856, loss: 0.1368, hidden_loss: 0.0518, logit_loss: 0.0850\n",
      "epoch: 0/1, step: 199/73856, loss: 0.1469, hidden_loss: 0.0510, logit_loss: 0.0960\n",
      "epoch: 0/1, step: 200/73856, loss: 0.1554, hidden_loss: 0.0507, logit_loss: 0.1047\n",
      "epoch: 0/1, step: 201/73856, loss: 0.1692, hidden_loss: 0.0498, logit_loss: 0.1194\n",
      "epoch: 0/1, step: 202/73856, loss: 0.1601, hidden_loss: 0.0500, logit_loss: 0.1101\n",
      "epoch: 0/1, step: 203/73856, loss: 0.1634, hidden_loss: 0.0498, logit_loss: 0.1136\n",
      "epoch: 0/1, step: 204/73856, loss: 0.1376, hidden_loss: 0.0490, logit_loss: 0.0886\n",
      "epoch: 0/1, step: 205/73856, loss: 0.1527, hidden_loss: 0.0487, logit_loss: 0.1040\n",
      "epoch: 0/1, step: 206/73856, loss: 0.1542, hidden_loss: 0.0486, logit_loss: 0.1055\n",
      "epoch: 0/1, step: 207/73856, loss: 0.1680, hidden_loss: 0.0479, logit_loss: 0.1200\n",
      "epoch: 0/1, step: 208/73856, loss: 0.1819, hidden_loss: 0.0475, logit_loss: 0.1344\n",
      "epoch: 0/1, step: 209/73856, loss: 0.1558, hidden_loss: 0.0475, logit_loss: 0.1083\n",
      "epoch: 0/1, step: 210/73856, loss: 0.1605, hidden_loss: 0.0468, logit_loss: 0.1137\n",
      "epoch: 0/1, step: 211/73856, loss: 0.1361, hidden_loss: 0.0463, logit_loss: 0.0898\n",
      "epoch: 0/1, step: 212/73856, loss: 0.1554, hidden_loss: 0.0458, logit_loss: 0.1096\n",
      "epoch: 0/1, step: 213/73856, loss: 0.1374, hidden_loss: 0.0456, logit_loss: 0.0919\n",
      "epoch: 0/1, step: 214/73856, loss: 0.1792, hidden_loss: 0.0456, logit_loss: 0.1336\n",
      "epoch: 0/1, step: 215/73856, loss: 0.1347, hidden_loss: 0.0448, logit_loss: 0.0899\n",
      "epoch: 0/1, step: 216/73856, loss: 0.1562, hidden_loss: 0.0447, logit_loss: 0.1115\n",
      "epoch: 0/1, step: 217/73856, loss: 0.1323, hidden_loss: 0.0443, logit_loss: 0.0879\n",
      "epoch: 0/1, step: 218/73856, loss: 0.1320, hidden_loss: 0.0440, logit_loss: 0.0879\n",
      "epoch: 0/1, step: 219/73856, loss: 0.1250, hidden_loss: 0.0433, logit_loss: 0.0817\n",
      "epoch: 0/1, step: 220/73856, loss: 0.1317, hidden_loss: 0.0432, logit_loss: 0.0886\n",
      "epoch: 0/1, step: 221/73856, loss: 0.1617, hidden_loss: 0.0430, logit_loss: 0.1187\n",
      "epoch: 0/1, step: 222/73856, loss: 0.1286, hidden_loss: 0.0425, logit_loss: 0.0861\n",
      "epoch: 0/1, step: 223/73856, loss: 0.1318, hidden_loss: 0.0422, logit_loss: 0.0895\n",
      "epoch: 0/1, step: 224/73856, loss: 0.1540, hidden_loss: 0.0422, logit_loss: 0.1117\n",
      "epoch: 0/1, step: 225/73856, loss: 0.1619, hidden_loss: 0.0418, logit_loss: 0.1201\n",
      "epoch: 0/1, step: 226/73856, loss: 0.1232, hidden_loss: 0.0413, logit_loss: 0.0819\n",
      "epoch: 0/1, step: 227/73856, loss: 0.1425, hidden_loss: 0.0412, logit_loss: 0.1013\n",
      "epoch: 0/1, step: 228/73856, loss: 0.1287, hidden_loss: 0.0411, logit_loss: 0.0876\n",
      "epoch: 0/1, step: 229/73856, loss: 0.1480, hidden_loss: 0.0405, logit_loss: 0.1075\n",
      "epoch: 0/1, step: 230/73856, loss: 0.1426, hidden_loss: 0.0403, logit_loss: 0.1023\n",
      "epoch: 0/1, step: 231/73856, loss: 0.1386, hidden_loss: 0.0401, logit_loss: 0.0985\n",
      "epoch: 0/1, step: 232/73856, loss: 0.1458, hidden_loss: 0.0400, logit_loss: 0.1057\n",
      "epoch: 0/1, step: 233/73856, loss: 0.1245, hidden_loss: 0.0393, logit_loss: 0.0852\n",
      "epoch: 0/1, step: 234/73856, loss: 0.1499, hidden_loss: 0.0393, logit_loss: 0.1106\n",
      "epoch: 0/1, step: 235/73856, loss: 0.1583, hidden_loss: 0.0394, logit_loss: 0.1189\n",
      "epoch: 0/1, step: 236/73856, loss: 0.1417, hidden_loss: 0.0388, logit_loss: 0.1029\n",
      "epoch: 0/1, step: 237/73856, loss: 0.1610, hidden_loss: 0.0390, logit_loss: 0.1221\n",
      "epoch: 0/1, step: 238/73856, loss: 0.1296, hidden_loss: 0.0382, logit_loss: 0.0914\n",
      "epoch: 0/1, step: 239/73856, loss: 0.1402, hidden_loss: 0.0381, logit_loss: 0.1021\n",
      "epoch: 0/1, step: 240/73856, loss: 0.1469, hidden_loss: 0.0378, logit_loss: 0.1091\n",
      "epoch: 0/1, step: 241/73856, loss: 0.1531, hidden_loss: 0.0373, logit_loss: 0.1158\n",
      "epoch: 0/1, step: 242/73856, loss: 0.1466, hidden_loss: 0.0371, logit_loss: 0.1095\n",
      "epoch: 0/1, step: 243/73856, loss: 0.1186, hidden_loss: 0.0370, logit_loss: 0.0816\n",
      "epoch: 0/1, step: 244/73856, loss: 0.1507, hidden_loss: 0.0376, logit_loss: 0.1131\n",
      "epoch: 0/1, step: 245/73856, loss: 0.1293, hidden_loss: 0.0366, logit_loss: 0.0927\n",
      "epoch: 0/1, step: 246/73856, loss: 0.1302, hidden_loss: 0.0370, logit_loss: 0.0931\n",
      "epoch: 0/1, step: 247/73856, loss: 0.1328, hidden_loss: 0.0365, logit_loss: 0.0962\n",
      "epoch: 0/1, step: 248/73856, loss: 0.1323, hidden_loss: 0.0361, logit_loss: 0.0963\n",
      "epoch: 0/1, step: 249/73856, loss: 0.1194, hidden_loss: 0.0359, logit_loss: 0.0835\n",
      "epoch: 0/1, step: 250/73856, loss: 0.1269, hidden_loss: 0.0357, logit_loss: 0.0912\n",
      "epoch: 0/1, step: 251/73856, loss: 0.1297, hidden_loss: 0.0354, logit_loss: 0.0942\n",
      "epoch: 0/1, step: 252/73856, loss: 0.1430, hidden_loss: 0.0354, logit_loss: 0.1076\n",
      "epoch: 0/1, step: 253/73856, loss: 0.1367, hidden_loss: 0.0352, logit_loss: 0.1015\n",
      "epoch: 0/1, step: 254/73856, loss: 0.1377, hidden_loss: 0.0351, logit_loss: 0.1026\n",
      "epoch: 0/1, step: 255/73856, loss: 0.1212, hidden_loss: 0.0345, logit_loss: 0.0867\n",
      "epoch: 0/1, step: 256/73856, loss: 0.1231, hidden_loss: 0.0347, logit_loss: 0.0884\n",
      "epoch: 0/1, step: 257/73856, loss: 0.1262, hidden_loss: 0.0343, logit_loss: 0.0919\n",
      "epoch: 0/1, step: 258/73856, loss: 0.1251, hidden_loss: 0.0342, logit_loss: 0.0909\n",
      "epoch: 0/1, step: 259/73856, loss: 0.1202, hidden_loss: 0.0339, logit_loss: 0.0864\n",
      "epoch: 0/1, step: 260/73856, loss: 0.1323, hidden_loss: 0.0343, logit_loss: 0.0981\n",
      "epoch: 0/1, step: 261/73856, loss: 0.1643, hidden_loss: 0.0337, logit_loss: 0.1306\n",
      "epoch: 0/1, step: 262/73856, loss: 0.1290, hidden_loss: 0.0337, logit_loss: 0.0953\n",
      "epoch: 0/1, step: 263/73856, loss: 0.1243, hidden_loss: 0.0336, logit_loss: 0.0907\n",
      "epoch: 0/1, step: 264/73856, loss: 0.1383, hidden_loss: 0.0334, logit_loss: 0.1049\n",
      "epoch: 0/1, step: 265/73856, loss: 0.1582, hidden_loss: 0.0335, logit_loss: 0.1247\n",
      "epoch: 0/1, step: 266/73856, loss: 0.1334, hidden_loss: 0.0330, logit_loss: 0.1004\n",
      "epoch: 0/1, step: 267/73856, loss: 0.1358, hidden_loss: 0.0329, logit_loss: 0.1029\n",
      "epoch: 0/1, step: 268/73856, loss: 0.1702, hidden_loss: 0.0329, logit_loss: 0.1374\n",
      "epoch: 0/1, step: 269/73856, loss: 0.1388, hidden_loss: 0.0329, logit_loss: 0.1059\n",
      "epoch: 0/1, step: 270/73856, loss: 0.1181, hidden_loss: 0.0324, logit_loss: 0.0857\n",
      "epoch: 0/1, step: 271/73856, loss: 0.1748, hidden_loss: 0.0321, logit_loss: 0.1427\n",
      "epoch: 0/1, step: 272/73856, loss: 0.1208, hidden_loss: 0.0322, logit_loss: 0.0886\n",
      "epoch: 0/1, step: 273/73856, loss: 0.1301, hidden_loss: 0.0322, logit_loss: 0.0979\n",
      "epoch: 0/1, step: 274/73856, loss: 0.1512, hidden_loss: 0.0321, logit_loss: 0.1191\n",
      "epoch: 0/1, step: 275/73856, loss: 0.1108, hidden_loss: 0.0318, logit_loss: 0.0790\n",
      "epoch: 0/1, step: 276/73856, loss: 0.1253, hidden_loss: 0.0321, logit_loss: 0.0932\n",
      "epoch: 0/1, step: 277/73856, loss: 0.1152, hidden_loss: 0.0315, logit_loss: 0.0837\n",
      "epoch: 0/1, step: 278/73856, loss: 0.1348, hidden_loss: 0.0317, logit_loss: 0.1031\n",
      "epoch: 0/1, step: 279/73856, loss: 0.1304, hidden_loss: 0.0311, logit_loss: 0.0992\n",
      "epoch: 0/1, step: 280/73856, loss: 0.1079, hidden_loss: 0.0311, logit_loss: 0.0768\n",
      "epoch: 0/1, step: 281/73856, loss: 0.1198, hidden_loss: 0.0315, logit_loss: 0.0883\n",
      "epoch: 0/1, step: 282/73856, loss: 0.1415, hidden_loss: 0.0311, logit_loss: 0.1105\n",
      "epoch: 0/1, step: 283/73856, loss: 0.1251, hidden_loss: 0.0309, logit_loss: 0.0942\n",
      "epoch: 0/1, step: 284/73856, loss: 0.1167, hidden_loss: 0.0308, logit_loss: 0.0859\n",
      "epoch: 0/1, step: 285/73856, loss: 0.1235, hidden_loss: 0.0307, logit_loss: 0.0928\n",
      "epoch: 0/1, step: 286/73856, loss: 0.1276, hidden_loss: 0.0306, logit_loss: 0.0970\n",
      "epoch: 0/1, step: 287/73856, loss: 0.1248, hidden_loss: 0.0307, logit_loss: 0.0941\n",
      "epoch: 0/1, step: 288/73856, loss: 0.1099, hidden_loss: 0.0304, logit_loss: 0.0795\n",
      "epoch: 0/1, step: 289/73856, loss: 0.1185, hidden_loss: 0.0299, logit_loss: 0.0885\n",
      "epoch: 0/1, step: 290/73856, loss: 0.1367, hidden_loss: 0.0306, logit_loss: 0.1061\n",
      "epoch: 0/1, step: 291/73856, loss: 0.1241, hidden_loss: 0.0299, logit_loss: 0.0943\n",
      "epoch: 0/1, step: 292/73856, loss: 0.1359, hidden_loss: 0.0302, logit_loss: 0.1057\n",
      "epoch: 0/1, step: 293/73856, loss: 0.1213, hidden_loss: 0.0297, logit_loss: 0.0916\n",
      "epoch: 0/1, step: 294/73856, loss: 0.1196, hidden_loss: 0.0299, logit_loss: 0.0897\n",
      "epoch: 0/1, step: 295/73856, loss: 0.1131, hidden_loss: 0.0296, logit_loss: 0.0835\n",
      "epoch: 0/1, step: 296/73856, loss: 0.1176, hidden_loss: 0.0294, logit_loss: 0.0882\n",
      "epoch: 0/1, step: 297/73856, loss: 0.1367, hidden_loss: 0.0293, logit_loss: 0.1074\n",
      "epoch: 0/1, step: 298/73856, loss: 0.1559, hidden_loss: 0.0298, logit_loss: 0.1261\n",
      "epoch: 0/1, step: 299/73856, loss: 0.1090, hidden_loss: 0.0290, logit_loss: 0.0800\n",
      "epoch: 0/1, step: 300/73856, loss: 0.1146, hidden_loss: 0.0290, logit_loss: 0.0856\n",
      "epoch: 0/1, step: 301/73856, loss: 0.1503, hidden_loss: 0.0296, logit_loss: 0.1207\n",
      "epoch: 0/1, step: 302/73856, loss: 0.1375, hidden_loss: 0.0291, logit_loss: 0.1084\n",
      "epoch: 0/1, step: 303/73856, loss: 0.1263, hidden_loss: 0.0294, logit_loss: 0.0969\n",
      "epoch: 0/1, step: 304/73856, loss: 0.1106, hidden_loss: 0.0289, logit_loss: 0.0816\n",
      "epoch: 0/1, step: 305/73856, loss: 0.1261, hidden_loss: 0.0285, logit_loss: 0.0976\n",
      "epoch: 0/1, step: 306/73856, loss: 0.1456, hidden_loss: 0.0288, logit_loss: 0.1168\n",
      "epoch: 0/1, step: 307/73856, loss: 0.1089, hidden_loss: 0.0286, logit_loss: 0.0803\n",
      "epoch: 0/1, step: 308/73856, loss: 0.1063, hidden_loss: 0.0285, logit_loss: 0.0778\n",
      "epoch: 0/1, step: 309/73856, loss: 0.1367, hidden_loss: 0.0284, logit_loss: 0.1083\n",
      "epoch: 0/1, step: 310/73856, loss: 0.1250, hidden_loss: 0.0289, logit_loss: 0.0961\n",
      "epoch: 0/1, step: 311/73856, loss: 0.1147, hidden_loss: 0.0286, logit_loss: 0.0861\n",
      "epoch: 0/1, step: 312/73856, loss: 0.1091, hidden_loss: 0.0280, logit_loss: 0.0811\n",
      "epoch: 0/1, step: 313/73856, loss: 0.1101, hidden_loss: 0.0279, logit_loss: 0.0823\n",
      "epoch: 0/1, step: 314/73856, loss: 0.1355, hidden_loss: 0.0281, logit_loss: 0.1074\n",
      "epoch: 0/1, step: 315/73856, loss: 0.1104, hidden_loss: 0.0276, logit_loss: 0.0828\n",
      "epoch: 0/1, step: 316/73856, loss: 0.1059, hidden_loss: 0.0275, logit_loss: 0.0783\n",
      "epoch: 0/1, step: 317/73856, loss: 0.1230, hidden_loss: 0.0280, logit_loss: 0.0949\n",
      "epoch: 0/1, step: 318/73856, loss: 0.1108, hidden_loss: 0.0274, logit_loss: 0.0833\n",
      "epoch: 0/1, step: 319/73856, loss: 0.1321, hidden_loss: 0.0279, logit_loss: 0.1041\n",
      "epoch: 0/1, step: 320/73856, loss: 0.1118, hidden_loss: 0.0275, logit_loss: 0.0843\n",
      "epoch: 0/1, step: 321/73856, loss: 0.1093, hidden_loss: 0.0272, logit_loss: 0.0821\n",
      "epoch: 0/1, step: 322/73856, loss: 0.1317, hidden_loss: 0.0272, logit_loss: 0.1045\n",
      "epoch: 0/1, step: 323/73856, loss: 0.1241, hidden_loss: 0.0271, logit_loss: 0.0970\n",
      "epoch: 0/1, step: 324/73856, loss: 0.1124, hidden_loss: 0.0268, logit_loss: 0.0855\n",
      "epoch: 0/1, step: 325/73856, loss: 0.1135, hidden_loss: 0.0271, logit_loss: 0.0864\n",
      "epoch: 0/1, step: 326/73856, loss: 0.1313, hidden_loss: 0.0271, logit_loss: 0.1042\n",
      "epoch: 0/1, step: 327/73856, loss: 0.1390, hidden_loss: 0.0271, logit_loss: 0.1119\n",
      "epoch: 0/1, step: 328/73856, loss: 0.1324, hidden_loss: 0.0270, logit_loss: 0.1055\n",
      "epoch: 0/1, step: 329/73856, loss: 0.1173, hidden_loss: 0.0267, logit_loss: 0.0905\n",
      "epoch: 0/1, step: 330/73856, loss: 0.1028, hidden_loss: 0.0270, logit_loss: 0.0759\n",
      "epoch: 0/1, step: 331/73856, loss: 0.1340, hidden_loss: 0.0269, logit_loss: 0.1071\n",
      "epoch: 0/1, step: 332/73856, loss: 0.1239, hidden_loss: 0.0265, logit_loss: 0.0974\n",
      "epoch: 0/1, step: 333/73856, loss: 0.1264, hidden_loss: 0.0267, logit_loss: 0.0997\n",
      "epoch: 0/1, step: 334/73856, loss: 0.1096, hidden_loss: 0.0266, logit_loss: 0.0830\n",
      "epoch: 0/1, step: 335/73856, loss: 0.1480, hidden_loss: 0.0268, logit_loss: 0.1212\n",
      "epoch: 0/1, step: 336/73856, loss: 0.1452, hidden_loss: 0.0270, logit_loss: 0.1183\n",
      "epoch: 0/1, step: 337/73856, loss: 0.1299, hidden_loss: 0.0270, logit_loss: 0.1029\n",
      "epoch: 0/1, step: 338/73856, loss: 0.1270, hidden_loss: 0.0262, logit_loss: 0.1008\n",
      "epoch: 0/1, step: 339/73856, loss: 0.1128, hidden_loss: 0.0263, logit_loss: 0.0865\n",
      "epoch: 0/1, step: 340/73856, loss: 0.1069, hidden_loss: 0.0262, logit_loss: 0.0806\n",
      "epoch: 0/1, step: 341/73856, loss: 0.1257, hidden_loss: 0.0262, logit_loss: 0.0996\n",
      "epoch: 0/1, step: 342/73856, loss: 0.1138, hidden_loss: 0.0262, logit_loss: 0.0876\n",
      "epoch: 0/1, step: 343/73856, loss: 0.1354, hidden_loss: 0.0258, logit_loss: 0.1096\n",
      "epoch: 0/1, step: 344/73856, loss: 0.1304, hidden_loss: 0.0262, logit_loss: 0.1042\n",
      "epoch: 0/1, step: 345/73856, loss: 0.1337, hidden_loss: 0.0259, logit_loss: 0.1078\n",
      "epoch: 0/1, step: 346/73856, loss: 0.1339, hidden_loss: 0.0261, logit_loss: 0.1078\n",
      "epoch: 0/1, step: 347/73856, loss: 0.1180, hidden_loss: 0.0257, logit_loss: 0.0923\n",
      "epoch: 0/1, step: 348/73856, loss: 0.1129, hidden_loss: 0.0255, logit_loss: 0.0875\n",
      "epoch: 0/1, step: 349/73856, loss: 0.1113, hidden_loss: 0.0255, logit_loss: 0.0859\n",
      "epoch: 0/1, step: 350/73856, loss: 0.1369, hidden_loss: 0.0257, logit_loss: 0.1113\n",
      "epoch: 0/1, step: 351/73856, loss: 0.1142, hidden_loss: 0.0255, logit_loss: 0.0887\n",
      "epoch: 0/1, step: 352/73856, loss: 0.1170, hidden_loss: 0.0255, logit_loss: 0.0914\n",
      "epoch: 0/1, step: 353/73856, loss: 0.1075, hidden_loss: 0.0255, logit_loss: 0.0820\n",
      "epoch: 0/1, step: 354/73856, loss: 0.1151, hidden_loss: 0.0252, logit_loss: 0.0899\n",
      "epoch: 0/1, step: 355/73856, loss: 0.1366, hidden_loss: 0.0252, logit_loss: 0.1114\n",
      "epoch: 0/1, step: 356/73856, loss: 0.1340, hidden_loss: 0.0252, logit_loss: 0.1088\n",
      "epoch: 0/1, step: 357/73856, loss: 0.1488, hidden_loss: 0.0253, logit_loss: 0.1234\n",
      "epoch: 0/1, step: 358/73856, loss: 0.1346, hidden_loss: 0.0252, logit_loss: 0.1094\n",
      "epoch: 0/1, step: 359/73856, loss: 0.1212, hidden_loss: 0.0254, logit_loss: 0.0958\n",
      "epoch: 0/1, step: 360/73856, loss: 0.1217, hidden_loss: 0.0249, logit_loss: 0.0969\n",
      "epoch: 0/1, step: 361/73856, loss: 0.1220, hidden_loss: 0.0250, logit_loss: 0.0970\n",
      "epoch: 0/1, step: 362/73856, loss: 0.1266, hidden_loss: 0.0246, logit_loss: 0.1019\n",
      "epoch: 0/1, step: 363/73856, loss: 0.1228, hidden_loss: 0.0252, logit_loss: 0.0976\n",
      "epoch: 0/1, step: 364/73856, loss: 0.1055, hidden_loss: 0.0247, logit_loss: 0.0808\n",
      "epoch: 0/1, step: 365/73856, loss: 0.1467, hidden_loss: 0.0253, logit_loss: 0.1214\n",
      "epoch: 0/1, step: 366/73856, loss: 0.1333, hidden_loss: 0.0252, logit_loss: 0.1082\n",
      "epoch: 0/1, step: 367/73856, loss: 0.1027, hidden_loss: 0.0251, logit_loss: 0.0776\n",
      "epoch: 0/1, step: 368/73856, loss: 0.1341, hidden_loss: 0.0250, logit_loss: 0.1091\n",
      "epoch: 0/1, step: 369/73856, loss: 0.1167, hidden_loss: 0.0247, logit_loss: 0.0920\n",
      "epoch: 0/1, step: 370/73856, loss: 0.1137, hidden_loss: 0.0244, logit_loss: 0.0893\n",
      "epoch: 0/1, step: 371/73856, loss: 0.0952, hidden_loss: 0.0245, logit_loss: 0.0707\n",
      "epoch: 0/1, step: 372/73856, loss: 0.1047, hidden_loss: 0.0245, logit_loss: 0.0802\n",
      "epoch: 0/1, step: 373/73856, loss: 0.1081, hidden_loss: 0.0243, logit_loss: 0.0838\n",
      "epoch: 0/1, step: 374/73856, loss: 0.1201, hidden_loss: 0.0246, logit_loss: 0.0956\n",
      "epoch: 0/1, step: 375/73856, loss: 0.1175, hidden_loss: 0.0247, logit_loss: 0.0928\n",
      "epoch: 0/1, step: 376/73856, loss: 0.1228, hidden_loss: 0.0245, logit_loss: 0.0983\n",
      "epoch: 0/1, step: 377/73856, loss: 0.1252, hidden_loss: 0.0249, logit_loss: 0.1003\n",
      "epoch: 0/1, step: 378/73856, loss: 0.1416, hidden_loss: 0.0247, logit_loss: 0.1168\n",
      "epoch: 0/1, step: 379/73856, loss: 0.1229, hidden_loss: 0.0240, logit_loss: 0.0990\n",
      "epoch: 0/1, step: 380/73856, loss: 0.0952, hidden_loss: 0.0241, logit_loss: 0.0712\n",
      "epoch: 0/1, step: 381/73856, loss: 0.1274, hidden_loss: 0.0245, logit_loss: 0.1029\n",
      "epoch: 0/1, step: 382/73856, loss: 0.1269, hidden_loss: 0.0242, logit_loss: 0.1027\n",
      "epoch: 0/1, step: 383/73856, loss: 0.1093, hidden_loss: 0.0238, logit_loss: 0.0854\n",
      "epoch: 0/1, step: 384/73856, loss: 0.1098, hidden_loss: 0.0241, logit_loss: 0.0857\n",
      "epoch: 0/1, step: 385/73856, loss: 0.1151, hidden_loss: 0.0240, logit_loss: 0.0911\n",
      "epoch: 0/1, step: 386/73856, loss: 0.1243, hidden_loss: 0.0241, logit_loss: 0.1002\n",
      "epoch: 0/1, step: 387/73856, loss: 0.1298, hidden_loss: 0.0240, logit_loss: 0.1058\n",
      "epoch: 0/1, step: 388/73856, loss: 0.1225, hidden_loss: 0.0238, logit_loss: 0.0987\n",
      "epoch: 0/1, step: 389/73856, loss: 0.1386, hidden_loss: 0.0236, logit_loss: 0.1149\n",
      "epoch: 0/1, step: 390/73856, loss: 0.1389, hidden_loss: 0.0238, logit_loss: 0.1151\n",
      "epoch: 0/1, step: 391/73856, loss: 0.1141, hidden_loss: 0.0235, logit_loss: 0.0906\n",
      "epoch: 0/1, step: 392/73856, loss: 0.1014, hidden_loss: 0.0242, logit_loss: 0.0773\n",
      "epoch: 0/1, step: 393/73856, loss: 0.1073, hidden_loss: 0.0238, logit_loss: 0.0835\n",
      "epoch: 0/1, step: 394/73856, loss: 0.1298, hidden_loss: 0.0237, logit_loss: 0.1061\n",
      "epoch: 0/1, step: 395/73856, loss: 0.1219, hidden_loss: 0.0237, logit_loss: 0.0981\n",
      "epoch: 0/1, step: 396/73856, loss: 0.1181, hidden_loss: 0.0238, logit_loss: 0.0943\n",
      "epoch: 0/1, step: 397/73856, loss: 0.1252, hidden_loss: 0.0234, logit_loss: 0.1018\n",
      "epoch: 0/1, step: 398/73856, loss: 0.1183, hidden_loss: 0.0235, logit_loss: 0.0948\n",
      "epoch: 0/1, step: 399/73856, loss: 0.1214, hidden_loss: 0.0236, logit_loss: 0.0978\n",
      "epoch: 0/1, step: 400/73856, loss: 0.1255, hidden_loss: 0.0239, logit_loss: 0.1016\n",
      "epoch: 0/1, step: 401/73856, loss: 0.1140, hidden_loss: 0.0234, logit_loss: 0.0906\n",
      "epoch: 0/1, step: 402/73856, loss: 0.1349, hidden_loss: 0.0232, logit_loss: 0.1116\n",
      "epoch: 0/1, step: 403/73856, loss: 0.1216, hidden_loss: 0.0229, logit_loss: 0.0987\n",
      "epoch: 0/1, step: 404/73856, loss: 0.1077, hidden_loss: 0.0231, logit_loss: 0.0846\n",
      "epoch: 0/1, step: 405/73856, loss: 0.1076, hidden_loss: 0.0233, logit_loss: 0.0844\n",
      "epoch: 0/1, step: 406/73856, loss: 0.1174, hidden_loss: 0.0233, logit_loss: 0.0941\n",
      "epoch: 0/1, step: 407/73856, loss: 0.1468, hidden_loss: 0.0233, logit_loss: 0.1235\n",
      "epoch: 0/1, step: 408/73856, loss: 0.1310, hidden_loss: 0.0230, logit_loss: 0.1080\n",
      "epoch: 0/1, step: 409/73856, loss: 0.1119, hidden_loss: 0.0231, logit_loss: 0.0888\n",
      "epoch: 0/1, step: 410/73856, loss: 0.0988, hidden_loss: 0.0231, logit_loss: 0.0757\n",
      "epoch: 0/1, step: 411/73856, loss: 0.1174, hidden_loss: 0.0231, logit_loss: 0.0943\n",
      "epoch: 0/1, step: 412/73856, loss: 0.1120, hidden_loss: 0.0233, logit_loss: 0.0887\n",
      "epoch: 0/1, step: 413/73856, loss: 0.1153, hidden_loss: 0.0230, logit_loss: 0.0923\n",
      "epoch: 0/1, step: 414/73856, loss: 0.1230, hidden_loss: 0.0228, logit_loss: 0.1002\n",
      "epoch: 0/1, step: 415/73856, loss: 0.1168, hidden_loss: 0.0230, logit_loss: 0.0938\n",
      "epoch: 0/1, step: 416/73856, loss: 0.1221, hidden_loss: 0.0230, logit_loss: 0.0992\n",
      "epoch: 0/1, step: 417/73856, loss: 0.1178, hidden_loss: 0.0226, logit_loss: 0.0951\n",
      "epoch: 0/1, step: 418/73856, loss: 0.1032, hidden_loss: 0.0227, logit_loss: 0.0805\n",
      "epoch: 0/1, step: 419/73856, loss: 0.1159, hidden_loss: 0.0227, logit_loss: 0.0932\n",
      "epoch: 0/1, step: 420/73856, loss: 0.1318, hidden_loss: 0.0227, logit_loss: 0.1091\n",
      "epoch: 0/1, step: 421/73856, loss: 0.1111, hidden_loss: 0.0225, logit_loss: 0.0886\n",
      "epoch: 0/1, step: 422/73856, loss: 0.1071, hidden_loss: 0.0227, logit_loss: 0.0845\n",
      "epoch: 0/1, step: 423/73856, loss: 0.1198, hidden_loss: 0.0227, logit_loss: 0.0971\n",
      "epoch: 0/1, step: 424/73856, loss: 0.1112, hidden_loss: 0.0228, logit_loss: 0.0884\n",
      "epoch: 0/1, step: 425/73856, loss: 0.1168, hidden_loss: 0.0224, logit_loss: 0.0944\n",
      "epoch: 0/1, step: 426/73856, loss: 0.1239, hidden_loss: 0.0226, logit_loss: 0.1014\n",
      "epoch: 0/1, step: 427/73856, loss: 0.0979, hidden_loss: 0.0224, logit_loss: 0.0754\n",
      "epoch: 0/1, step: 428/73856, loss: 0.1330, hidden_loss: 0.0227, logit_loss: 0.1103\n",
      "epoch: 0/1, step: 429/73856, loss: 0.1413, hidden_loss: 0.0227, logit_loss: 0.1186\n",
      "epoch: 0/1, step: 430/73856, loss: 0.0975, hidden_loss: 0.0221, logit_loss: 0.0754\n",
      "epoch: 0/1, step: 431/73856, loss: 0.1211, hidden_loss: 0.0225, logit_loss: 0.0986\n",
      "epoch: 0/1, step: 432/73856, loss: 0.1339, hidden_loss: 0.0225, logit_loss: 0.1113\n",
      "epoch: 0/1, step: 433/73856, loss: 0.1281, hidden_loss: 0.0226, logit_loss: 0.1055\n",
      "epoch: 0/1, step: 434/73856, loss: 0.1180, hidden_loss: 0.0232, logit_loss: 0.0948\n",
      "epoch: 0/1, step: 435/73856, loss: 0.1102, hidden_loss: 0.0225, logit_loss: 0.0878\n",
      "epoch: 0/1, step: 436/73856, loss: 0.1285, hidden_loss: 0.0225, logit_loss: 0.1061\n",
      "epoch: 0/1, step: 437/73856, loss: 0.1193, hidden_loss: 0.0223, logit_loss: 0.0970\n",
      "epoch: 0/1, step: 438/73856, loss: 0.1371, hidden_loss: 0.0224, logit_loss: 0.1147\n",
      "epoch: 0/1, step: 439/73856, loss: 0.1467, hidden_loss: 0.0226, logit_loss: 0.1242\n",
      "epoch: 0/1, step: 440/73856, loss: 0.1252, hidden_loss: 0.0218, logit_loss: 0.1034\n",
      "epoch: 0/1, step: 441/73856, loss: 0.1156, hidden_loss: 0.0220, logit_loss: 0.0937\n",
      "epoch: 0/1, step: 442/73856, loss: 0.1233, hidden_loss: 0.0222, logit_loss: 0.1011\n",
      "epoch: 0/1, step: 443/73856, loss: 0.1229, hidden_loss: 0.0217, logit_loss: 0.1012\n",
      "epoch: 0/1, step: 444/73856, loss: 0.1157, hidden_loss: 0.0219, logit_loss: 0.0937\n",
      "epoch: 0/1, step: 445/73856, loss: 0.1093, hidden_loss: 0.0216, logit_loss: 0.0877\n",
      "epoch: 0/1, step: 446/73856, loss: 0.1425, hidden_loss: 0.0215, logit_loss: 0.1210\n",
      "epoch: 0/1, step: 447/73856, loss: 0.1389, hidden_loss: 0.0223, logit_loss: 0.1166\n",
      "epoch: 0/1, step: 448/73856, loss: 0.1138, hidden_loss: 0.0220, logit_loss: 0.0918\n",
      "epoch: 0/1, step: 449/73856, loss: 0.1250, hidden_loss: 0.0223, logit_loss: 0.1027\n",
      "epoch: 0/1, step: 450/73856, loss: 0.1118, hidden_loss: 0.0215, logit_loss: 0.0903\n",
      "epoch: 0/1, step: 451/73856, loss: 0.1221, hidden_loss: 0.0226, logit_loss: 0.0995\n",
      "epoch: 0/1, step: 452/73856, loss: 0.1208, hidden_loss: 0.0221, logit_loss: 0.0987\n",
      "epoch: 0/1, step: 453/73856, loss: 0.1196, hidden_loss: 0.0220, logit_loss: 0.0976\n",
      "epoch: 0/1, step: 454/73856, loss: 0.1232, hidden_loss: 0.0220, logit_loss: 0.1012\n",
      "epoch: 0/1, step: 455/73856, loss: 0.1231, hidden_loss: 0.0222, logit_loss: 0.1009\n",
      "epoch: 0/1, step: 456/73856, loss: 0.0993, hidden_loss: 0.0216, logit_loss: 0.0777\n",
      "epoch: 0/1, step: 457/73856, loss: 0.1337, hidden_loss: 0.0221, logit_loss: 0.1116\n",
      "epoch: 0/1, step: 458/73856, loss: 0.1351, hidden_loss: 0.0220, logit_loss: 0.1131\n",
      "epoch: 0/1, step: 459/73856, loss: 0.1210, hidden_loss: 0.0214, logit_loss: 0.0996\n",
      "epoch: 0/1, step: 460/73856, loss: 0.1247, hidden_loss: 0.0219, logit_loss: 0.1027\n",
      "epoch: 0/1, step: 461/73856, loss: 0.1302, hidden_loss: 0.0218, logit_loss: 0.1085\n",
      "epoch: 0/1, step: 462/73856, loss: 0.1395, hidden_loss: 0.0220, logit_loss: 0.1175\n",
      "epoch: 0/1, step: 463/73856, loss: 0.1042, hidden_loss: 0.0215, logit_loss: 0.0828\n",
      "epoch: 0/1, step: 464/73856, loss: 0.1240, hidden_loss: 0.0214, logit_loss: 0.1026\n",
      "epoch: 0/1, step: 465/73856, loss: 0.1065, hidden_loss: 0.0215, logit_loss: 0.0850\n",
      "epoch: 0/1, step: 466/73856, loss: 0.1142, hidden_loss: 0.0215, logit_loss: 0.0927\n",
      "epoch: 0/1, step: 467/73856, loss: 0.1166, hidden_loss: 0.0215, logit_loss: 0.0951\n",
      "epoch: 0/1, step: 468/73856, loss: 0.1139, hidden_loss: 0.0213, logit_loss: 0.0926\n",
      "epoch: 0/1, step: 469/73856, loss: 0.1169, hidden_loss: 0.0218, logit_loss: 0.0950\n",
      "epoch: 0/1, step: 470/73856, loss: 0.1210, hidden_loss: 0.0215, logit_loss: 0.0995\n",
      "epoch: 0/1, step: 471/73856, loss: 0.1130, hidden_loss: 0.0218, logit_loss: 0.0912\n",
      "epoch: 0/1, step: 472/73856, loss: 0.1134, hidden_loss: 0.0212, logit_loss: 0.0922\n",
      "epoch: 0/1, step: 473/73856, loss: 0.1110, hidden_loss: 0.0217, logit_loss: 0.0894\n",
      "epoch: 0/1, step: 474/73856, loss: 0.1661, hidden_loss: 0.0218, logit_loss: 0.1443\n",
      "epoch: 0/1, step: 475/73856, loss: 0.1037, hidden_loss: 0.0210, logit_loss: 0.0828\n",
      "epoch: 0/1, step: 476/73856, loss: 0.1208, hidden_loss: 0.0216, logit_loss: 0.0991\n",
      "epoch: 0/1, step: 477/73856, loss: 0.1195, hidden_loss: 0.0207, logit_loss: 0.0988\n",
      "epoch: 0/1, step: 478/73856, loss: 0.1074, hidden_loss: 0.0210, logit_loss: 0.0863\n",
      "epoch: 0/1, step: 479/73856, loss: 0.1099, hidden_loss: 0.0215, logit_loss: 0.0884\n",
      "epoch: 0/1, step: 480/73856, loss: 0.1134, hidden_loss: 0.0209, logit_loss: 0.0926\n",
      "epoch: 0/1, step: 481/73856, loss: 0.1160, hidden_loss: 0.0207, logit_loss: 0.0954\n",
      "epoch: 0/1, step: 482/73856, loss: 0.1052, hidden_loss: 0.0210, logit_loss: 0.0843\n",
      "epoch: 0/1, step: 483/73856, loss: 0.1052, hidden_loss: 0.0209, logit_loss: 0.0843\n",
      "epoch: 0/1, step: 484/73856, loss: 0.1231, hidden_loss: 0.0210, logit_loss: 0.1022\n",
      "epoch: 0/1, step: 485/73856, loss: 0.1337, hidden_loss: 0.0208, logit_loss: 0.1129\n",
      "epoch: 0/1, step: 486/73856, loss: 0.1071, hidden_loss: 0.0213, logit_loss: 0.0858\n",
      "epoch: 0/1, step: 487/73856, loss: 0.1171, hidden_loss: 0.0211, logit_loss: 0.0960\n",
      "epoch: 0/1, step: 488/73856, loss: 0.1202, hidden_loss: 0.0210, logit_loss: 0.0993\n",
      "epoch: 0/1, step: 489/73856, loss: 0.1080, hidden_loss: 0.0210, logit_loss: 0.0870\n",
      "epoch: 0/1, step: 490/73856, loss: 0.0993, hidden_loss: 0.0210, logit_loss: 0.0782\n",
      "epoch: 0/1, step: 491/73856, loss: 0.1150, hidden_loss: 0.0209, logit_loss: 0.0940\n",
      "epoch: 0/1, step: 492/73856, loss: 0.1118, hidden_loss: 0.0208, logit_loss: 0.0911\n",
      "epoch: 0/1, step: 493/73856, loss: 0.1149, hidden_loss: 0.0209, logit_loss: 0.0940\n",
      "epoch: 0/1, step: 494/73856, loss: 0.1119, hidden_loss: 0.0207, logit_loss: 0.0913\n",
      "epoch: 0/1, step: 495/73856, loss: 0.1265, hidden_loss: 0.0208, logit_loss: 0.1057\n",
      "epoch: 0/1, step: 496/73856, loss: 0.1114, hidden_loss: 0.0207, logit_loss: 0.0907\n",
      "epoch: 0/1, step: 497/73856, loss: 0.0942, hidden_loss: 0.0212, logit_loss: 0.0730\n",
      "epoch: 0/1, step: 498/73856, loss: 0.1068, hidden_loss: 0.0205, logit_loss: 0.0863\n",
      "epoch: 0/1, step: 499/73856, loss: 0.1165, hidden_loss: 0.0208, logit_loss: 0.0957\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 72\u001b[0m\n\u001b[1;32m     63\u001b[0m     loss, hidden_loss, logit_loss \u001b[38;5;241m=\u001b[39m loss_fn_v4(\n\u001b[1;32m     64\u001b[0m         nero_outs, \n\u001b[1;32m     65\u001b[0m         lora_outs, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# debug=False,\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Compute gradient norm\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# total_norm = 0.0\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# for p in nero_params:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m     83\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    config=dict(\n",
    "        seed = seed,\n",
    "        lora_model_device = lora_model_device,\n",
    "        nero_model_device = nero_model_device,\n",
    "        block_size = block_size,\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        resume_step = resume_step,\n",
    "        lr = lr,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Ensure model devices are set if not specified\n",
    "if nero_model_device is None:\n",
    "    nero_model_device = next(iter(nero_model.parameters())).device\n",
    "if lora_model_device is None:\n",
    "    lora_model_device = next(iter(lora_model.parameters())).device\n",
    "\n",
    "global_step = 0\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "done = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        if global_step < resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        # Flush gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs to devices\n",
    "        nero_input_ids = batch['input_ids'].to(nero_model_device)\n",
    "        nero_attention_mask = batch['attention_mask'].to(nero_model_device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass for nero\n",
    "            nero_model_outs, nero_outs = nero_model(\n",
    "                input_ids=nero_input_ids,\n",
    "                attention_mask=nero_attention_mask\n",
    "            )\n",
    "\n",
    "            # Forward pass for lora\n",
    "            lora_input_ids = nero_input_ids.to(lora_model_device)\n",
    "            lora_attention_mask = nero_attention_mask.to(lora_model_device)\n",
    "            lora_model_outs, lora_outs = lora_model(\n",
    "                input_ids=lora_input_ids,\n",
    "                attention_mask=lora_attention_mask\n",
    "            )\n",
    "\n",
    "            # Loss computation\n",
    "            loss, hidden_loss, logit_loss = loss_fn_v4(\n",
    "                nero_outs, \n",
    "                lora_outs, \n",
    "                nero_model_outs.logits, \n",
    "                lora_model_outs.logits,\n",
    "                # debug=False,\n",
    "            )\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Compute gradient norm\n",
    "        # total_norm = 0.0\n",
    "        # for p in nero_params:\n",
    "        #     param_norm = p.grad.data.norm(2)\n",
    "        #     total_norm += param_norm.item() ** 2\n",
    "        # total_norm = total_norm ** 0.5\n",
    "        # print(f\"Gradient norm: {total_norm:.4f}\")\n",
    "        \n",
    "        # Update parameters\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Logging\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "            'loss': loss.item(),\n",
    "            'hidden_loss': hidden_loss.item(),\n",
    "            'logit_loss': logit_loss.item(),\n",
    "        })\n",
    "        print(f\"epoch: {epoch}/{num_epochs}, step: {global_step}/{max_global_steps}, loss: {loss.item():.4f}, hidden_loss: {hidden_loss.item():.4f}, logit_loss: {logit_loss.item():.4f}\")\n",
    "\n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73d9ca2014e413b812f8321ebe28dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.056 MB of 0.056 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>hidden_loss</td><td>█████▇▇▇▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>logit_loss</td><td>▇▆██▆▆▅▃▃▃▂▂▂▂▂▂▁▂▃▂▂▂▃▃▂▁▁▁▂▂▂▂▂▂▂▂▃▂▁▂</td></tr><tr><td>loss</td><td>████▇▅▅▆▄▄▄▄▃▂▃▄▃▃▂▂▂▁▂▂▁▂▂▂▂▂▁▂▁▂▂▁▁▂▁▁</td></tr><tr><td>step</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>hidden_loss</td><td>0.02078</td></tr><tr><td>logit_loss</td><td>0.09569</td></tr><tr><td>loss</td><td>0.11647</td></tr><tr><td>step</td><td>499</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-sky-46</strong> at: <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/itr07fbv' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/itr07fbv</a><br/> View project at: <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250802_064214-itr07fbv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nero parameters saved to nero_params_L1T1_to_L1T2.pth\n"
     ]
    }
   ],
   "source": [
    "# Save Nero parameters\n",
    "nero_params_path = f\"nero_params_L1T1_to_{model_configs['target']['label']}.pth\"\n",
    "lora_state_dict = {k: v for k, v in nero_model.state_dict().items() if 'nero_' in k}\n",
    "torch.save(lora_state_dict, nero_params_path)\n",
    "print(\"Nero parameters saved to: \", nero_params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "病院船「ヨハネ23世」は、年年の年年年に年年年年年の年年年年年年年日を年の年の日の年年、年年年年年年年年年年年年を年が年年年年\n"
     ]
    }
   ],
   "source": [
    "nero_model.eval()\n",
    "nero_model.set_return_nero_outputs(False)\n",
    "generate_text(\n",
    "    nero_model, \n",
    "    tokenizer, \n",
    "    prompt=\"病院船「ヨハネ23世」は、\",\n",
    ")\n",
    "nero_model.set_return_nero_outputs(True)\n",
    "nero_model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
