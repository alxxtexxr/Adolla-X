{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "!fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from typing import Optional, Literal, Union, List\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download, create_repo, upload_folder\n",
    "from safetensors.torch import load_file, save_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(\n",
    "        repo_id: str, \n",
    "        checkpoint: Optional[int], \n",
    "        max_checkpoints: str = 10_000, \n",
    "        checkpoint_steps: str = 25,\n",
    "    ):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_steps) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dir = os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir, checkpoint_dir\n",
    "\n",
    "def check_loss_and_grad_norm(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    on_loss_computed,\n",
    "    on_grad_norm_computed,\n",
    "    prompt=\"Paris is the capital of\",\n",
    "):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "    print(\"Loss:\", outputs.loss)\n",
    "    on_loss_computed()\n",
    "\n",
    "    # Backward pass\n",
    "    if outputs.loss.grad_fn is None:\n",
    "        print(\"Gradient norm:\", None)\n",
    "        return\n",
    "\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "    on_grad_norm_computed()\n",
    "\n",
    "def check_parameter(n, p):\n",
    "    print(f\"- {'name':<8}:\", n)\n",
    "    print(f\"- {'device':<8}:\", p.device)\n",
    "    print(f\"- {'dtype':<8}:\", p.dtype)\n",
    "    print(f\"- {'mean':<8}:\", p.mean().item())\n",
    "    print(f\"- {'min':<8}:\", p.min().item())\n",
    "    print(f\"- {'max':<8}:\", p.max().item())\n",
    "\n",
    "def check_lora_parameters(model, prefix=None):\n",
    "    prefix = 'lora.' + prefix if prefix != None else 'lora'\n",
    "    for n, p in model.named_parameters():\n",
    "        if prefix in n:\n",
    "            check_parameter(n, p)\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def get_task_and_lang_from_repo_id(repo_id: str):\n",
    "    task, lang, _ = repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "    return task, lang\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id: str,\n",
    "    train_size: int = 5000,\n",
    "    test_size: int = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang = get_task_and_lang_from_repo_id(lora_repo_id)\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    \n",
    "    # Load dataset using streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_grad_norm(params):\n",
    "    grad_norm = 0.0\n",
    "    for p in params:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def compute_named_grad_norm(named_params):\n",
    "    grad_norm = 0.0\n",
    "    for n, p in named_params.items():\n",
    "        if p.grad is not None:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            print(\"{n} p_grad_norm:\", p_grad_norm)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "        else:\n",
    "            print(f\"[WARN] No gradient for {n}\")\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def format_float(v):\n",
    "    if abs(v) < 0.0001 or abs(v) >= 10000:\n",
    "        return f\"{v:.4e}\"\n",
    "    else:\n",
    "        return f\"{v:.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e84102a32f452b8f35c572c5ad4daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf40f9c249f4f6a9a1f31430c65073c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "- L1T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650/adapter_model.safetensors\n",
      "- L2T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650/adapter_model.safetensors\n",
      "\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "device = 'auto'\n",
    "\n",
    "# Data configuration\n",
    "hf_data_id = 'alxxtexxr/Nero-XLT-Dataset'\n",
    "hf_data_dir = 'gsm8k_en_5K_1K_1K_512'\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 1.0\n",
    "lr = 2e-4\n",
    "warmup_ratio = 0.1\n",
    "# num_warmup_steps = 100\n",
    "checkpoint_steps = 10\n",
    "push_to_hf = False\n",
    "generate_steps = 10\n",
    "L1T2_sample_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question:\n",
    "James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "L2T2_sample_prompt = \"\"\"### 命令:\n",
    "次の数学の問題を段階的に解いてください。\n",
    "\n",
    "### 質問:\n",
    "ジェームズは週に2回、2人の異なる友人に3ページの手紙を書きます。彼は年間何ページ書きますか？\n",
    "\n",
    "### 答え:\n",
    "\"\"\"\n",
    "distance_fn = 'mahalanobis'\n",
    "\n",
    "# Model configurations\n",
    "model_configs = {\n",
    "    # L1T1 (Source Language - Source Task)\n",
    "    'L1T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L2T1 (Target Language - Source Task)\n",
    "    'L2T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L1T2 (Source Language - Target Task)\n",
    "    # 'L1T2': {\n",
    "    #     'hf_lora_id': 'alxxtexxr/L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457',\n",
    "    #     'checkpoint': 1875,\n",
    "    # },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    _, lora_dir = download_hf_model(config['hf_lora_id'], config['checkpoint'])\n",
    "    model_configs[key]['lora_dir'] = lora_dir\n",
    "    model_configs[key]['lora_path'] = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "    model_configs[key]['lora_config'] = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "for key, config in model_configs.items():\n",
    "    print(f\"- {key}:\")\n",
    "    for config_name, config_value in config.items():\n",
    "        if config_name == 'lora_config':\n",
    "            continue\n",
    "        print(f\"{'-':>3} {config_name:<10}: {config_value}\")\n",
    "print()\n",
    "\n",
    "assert (\n",
    "    model_configs['L1T1']['lora_config'].base_model_name_or_path == \n",
    "    model_configs['L2T1']['lora_config'].base_model_name_or_path\n",
    "), \"Base models must be the same\"\n",
    "base_model_name = model_configs['L1T1']['lora_config'].base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating Nero directory: L3.1-8B-gsm8k-en-5K-1K-1K-512-Nero-mahalanobis-v20250822145500\n",
      "[INFO] Nero directory created!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face configuration\n",
    "hf_nero_id = None\n",
    "resume_step = 0\n",
    "\n",
    "if hf_nero_id is not None and resume_step > 0:\n",
    "    print(f\"[INFO] Downloading Nero checkpoint at step {resume_step} from Hugging Face repository:\", hf_nero_id)\n",
    "    nero_dir, _ = download_hf_model(hf_nero_id, resume_step)\n",
    "    print(f\"[INFO] Nero checkpoint downloaded successfully!\")\n",
    "else:\n",
    "    hf_username = 'alxxtexxr'\n",
    "    nero_dir = f'L3.1-8B-{hf_data_dir.replace(\"_\", \"-\")}-Nero-{distance_fn}-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    print(f\"[INFO] Creating Nero directory:\", nero_dir)\n",
    "    hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "    os.makedirs(nero_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Nero directory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer,\n",
    "                 \n",
    "                 # LoRA parameters\n",
    "                 L1T1_lora_params, \n",
    "                 L2T1_lora_params,\n",
    "\n",
    "                 # Nero parameters\n",
    "                 nero_params,\n",
    "                 train_L2T2_nero = False,\n",
    "                 eval_L2T2_nero = False,\n",
    "                 return_L2T2_nero_loss = False,\n",
    "                 \n",
    "                 # Distance function parameters  \n",
    "                 distance_fn='euclidean',\n",
    "                 distance_fn_pairwise=True,\n",
    "                 \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.train_L2T2_nero = train_L2T2_nero\n",
    "        self.eval_L2T2_nero = eval_L2T2_nero\n",
    "        self.return_L2T2_nero_loss = return_L2T2_nero_loss\n",
    "        \n",
    "        self.distance_fn = distance_fn\n",
    "        self.distance_fn_pairwise = distance_fn_pairwise\n",
    "\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "\n",
    "        # Initialize LoRA and Nero layers\n",
    "        self.lora = nn.ModuleDict({\n",
    "            'L1T1': self._init_lora_layer(in_features, out_features, **L1T1_lora_params, device=self.device),\n",
    "            'L2T1': self._init_lora_layer(in_features, out_features, **L2T1_lora_params, device=self.device),\n",
    "        })\n",
    "        self.nero = nn.ModuleDict({\n",
    "            'L1T2': self._init_nero_layer(out_features, **nero_params, device=self.device),\n",
    "            'L2T2': self._init_nero_layer(out_features, **nero_params, device=self.device),\n",
    "        })\n",
    "        \n",
    "    def _init_lora_layer(self, in_features, out_features, rank, alpha, dropout=0.0, bias=True, use_rslora=False, device=None):\n",
    "        scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        layer = nn.ModuleDict({\n",
    "            'A': nn.Linear(in_features, rank, bias=bias, device=device),\n",
    "            'B': nn.Linear(rank, out_features, bias=bias, device=device),\n",
    "            'dropout': dropout_layer\n",
    "        })\n",
    "        layer.scaling = scaling\n",
    "        layer.bias_flag = bias\n",
    "        nn.init.normal_(layer['A'].weight, 0.0, 1 / math.sqrt(rank))\n",
    "        nn.init.zeros_(layer['B'].weight)\n",
    "        return layer\n",
    "\n",
    "    def _init_nero_layer(self, features, rank, bias=True, device=None):\n",
    "        layer = nn.ModuleDict({\n",
    "            'A': nn.Linear(features, rank, bias=bias, device=device),\n",
    "            'B': nn.Linear(rank, features, bias=bias, device=device),\n",
    "        })\n",
    "        layer.bias_flag = bias\n",
    "        nn.init.normal_(layer['A'].weight, 0.0, 1 / math.sqrt(rank))\n",
    "        nn.init.zeros_(layer['B'].weight)\n",
    "        return layer\n",
    "    \n",
    "    def _distance_fn(self, a, b, huber_delta=1.0, eps=1e-8,\n",
    "                    soft_radius_a=0.1, soft_radius_b=0.1, soft_num_samples=5, chunk_size=64):\n",
    "        def huber_loss(diff, delta):\n",
    "            mask = diff.abs() < delta\n",
    "            return torch.where(mask, 0.5 * diff**2, delta * (diff.abs() - 0.5 * delta))\n",
    "        \n",
    "        def sinkhorn_distance(x, y, epsilon=0.01, n_iter=50):\n",
    "            \"\"\"\n",
    "            Differentiable Sinkhorn distance between point clouds x and y.\n",
    "\n",
    "            Args:\n",
    "                x: (batch, n, d)\n",
    "                y: (batch, m, d)\n",
    "                epsilon: entropic regularization coefficient\n",
    "                n_iter: number of Sinkhorn iterations\n",
    "            Returns:\n",
    "                cost: scalar Sinkhorn distance\n",
    "            \"\"\"\n",
    "            # Cost matrix\n",
    "            C = torch.cdist(x, y, p=2) ** 2  # (batch, n, m)\n",
    "\n",
    "            # Uniform marginals\n",
    "            n, m = x.shape[1], y.shape[1]\n",
    "            mu = torch.full((x.size(0), n), 1.0 / n, device=x.device, dtype=x.dtype)\n",
    "            nu = torch.full((x.size(0), m), 1.0 / m, device=y.device, dtype=y.dtype)\n",
    "\n",
    "            # Dual potentials\n",
    "            u = torch.zeros_like(mu)\n",
    "            v = torch.zeros_like(nu)\n",
    "\n",
    "            # Iterative updates in log-domain\n",
    "            for _ in range(n_iter):\n",
    "                u = epsilon * (\n",
    "                    torch.log(mu + 1e-8) -\n",
    "                    torch.logsumexp((-C + v.unsqueeze(1)) / epsilon, dim=2)\n",
    "                ) + u\n",
    "\n",
    "                v = epsilon * (\n",
    "                    torch.log(nu + 1e-8) -\n",
    "                    torch.logsumexp((-C + u.unsqueeze(2)) / epsilon, dim=1)\n",
    "                ) + v\n",
    "\n",
    "            # Transport plan in log-space\n",
    "            log_pi = (-C + u.unsqueeze(2) + v.unsqueeze(1)) / epsilon\n",
    "            pi = torch.exp(log_pi - log_pi.max())  # subtract max for stability\n",
    "            pi = pi / (pi.sum(dim=(1, 2), keepdim=True) + 1e-8)  # normalize\n",
    "\n",
    "            # Sinkhorn cost\n",
    "            cost = (pi * C).sum(dim=(1, 2))\n",
    "            return cost.mean()\n",
    "        \n",
    "        if self.distance_fn == \"sinkhorn\":\n",
    "            return sinkhorn_distance(a, b)\n",
    "        \n",
    "        if self.distance_fn == \"mahalanobis\":\n",
    "            # flatten batch/sequence to (N, D)\n",
    "            a_flat = a.reshape(-1, a.shape[-1])\n",
    "            b_flat = b.reshape(-1, b.shape[-1])\n",
    "\n",
    "            # center by mean of concatenated features\n",
    "            x = torch.cat([a_flat, b_flat], dim=0)\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            x_centered = x - mean\n",
    "\n",
    "            # covariance along feature dimension with regularization\n",
    "            cov = (x_centered.T @ x_centered) / (x_centered.shape[0] - 1)\n",
    "            cov += torch.eye(cov.shape[0], device=cov.device, dtype=cov.dtype) * eps\n",
    "\n",
    "            cov_inv = torch.linalg.inv(cov)\n",
    "\n",
    "            if self.distance_fn_pairwise:\n",
    "                # pairwise distances: (N, M)\n",
    "                diff = a_flat.unsqueeze(1) - b_flat.unsqueeze(0)  # (N, M, D)\n",
    "                dist = torch.einsum('nmd,dd,nmd->nm', diff, cov_inv, diff)\n",
    "                return torch.sqrt(dist + eps)\n",
    "            else:\n",
    "                # non-pairwise element-wise distance: (N,)\n",
    "                diff = a_flat - b_flat\n",
    "                dist = torch.einsum('nd,dd,nd->n', diff, cov_inv, diff)\n",
    "                return torch.sqrt(dist + eps)\n",
    "\n",
    "        if self.distance_fn == 'soft_euclidean':\n",
    "            if self.distance_fn_pairwise:\n",
    "                # Ensure we always have (N, D) and (M, D) for chunking\n",
    "                if a.dim() == 3:\n",
    "                    # Already expanded to (N, M, D) → collapse back\n",
    "                    N, M, D = a.shape\n",
    "                    a_base = a[:, 0, :]  # take first along M axis\n",
    "                    b_base = b[0, :, :]  # take first along N axis\n",
    "                else:\n",
    "                    N, D = a.shape\n",
    "                    M = b.shape[0]\n",
    "                    a_base, b_base = a, b\n",
    "\n",
    "                dist_list = []\n",
    "                for i in range(0, N, chunk_size):\n",
    "                    a_chunk = a_base[i:i+chunk_size]  # (chunk, D)\n",
    "                    a_exp = a_chunk.unsqueeze(1).expand(-1, M, D)\n",
    "                    b_exp = b_base.unsqueeze(0).expand(a_chunk.shape[0], -1, D)\n",
    "\n",
    "                    a_pert = a_exp.unsqueeze(2) + torch.randn(\n",
    "                        a_exp.shape[0], M, soft_num_samples, D, device=a.device\n",
    "                    ) * soft_radius_a\n",
    "                    b_pert = b_exp.unsqueeze(2) + torch.randn(\n",
    "                        a_exp.shape[0], M, soft_num_samples, D, device=b.device\n",
    "                    ) * soft_radius_b\n",
    "\n",
    "                    diff = a_pert.unsqueeze(3) - b_pert.unsqueeze(2)  # (chunk, M, num_a, num_b, D)\n",
    "                    dist = torch.sqrt((diff**2).sum(dim=-1) + eps)\n",
    "                    min_dist = dist.min(dim=2)[0].min(dim=-1)[0]  # (chunk, M)\n",
    "                    dist_list.append(min_dist)\n",
    "                return torch.cat(dist_list, dim=0)\n",
    "\n",
    "        # Normal distances\n",
    "        if self.distance_fn_pairwise:\n",
    "            a_exp = a.unsqueeze(1)\n",
    "            b_exp = b.unsqueeze(0)\n",
    "            diff = a_exp - b_exp\n",
    "        else:\n",
    "            diff = a - b\n",
    "\n",
    "        if self.distance_fn == 'euclidean':\n",
    "            return torch.sqrt((diff**2).sum(dim=-1) + eps)\n",
    "        elif self.distance_fn == 'squared_euclidean':\n",
    "            return (diff**2).sum(dim=-1)\n",
    "        elif self.distance_fn == 'huber':\n",
    "            return huber_loss(diff, huber_delta).sum(dim=-1)\n",
    "        elif self.distance_fn == 'cosine':\n",
    "            a_norm = F.normalize(a_exp if self.distance_fn_pairwise else a, dim=-1, eps=eps)\n",
    "            b_norm = F.normalize(b_exp if self.distance_fn_pairwise else b, dim=-1, eps=eps)\n",
    "            return 1 - (a_norm * b_norm).sum(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distance function: {self.distance_fn}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ================================================================\n",
    "        # BASE LAYER\n",
    "        # ================================================================\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            assert self.lora.L1T1.A.weight.dtype == self.lora.L2T1.A.weight.dtype\n",
    "            x = x.to(self.lora.L1T1.A.weight.dtype)\n",
    "\n",
    "        if not self.eval_L2T2_nero:\n",
    "            # ================================================================\n",
    "            # L1T1 LoRA LAYER\n",
    "            # ================================================================\n",
    "            # L1T1_lora_out = self.lora.L1T1.B(self.lora.L1T1.A(self.lora.L1T1.dropout(x))) #* self.lora.L1T1.scaling\n",
    "            L1T1_lora_dropout_out = self.lora.L1T1.dropout(x)\n",
    "            L1T1_lora_A_out = self.lora.L1T1.A(L1T1_lora_dropout_out)\n",
    "            L1T1_lora_B_out = self.lora.L1T1.B(L1T1_lora_A_out)\n",
    "            # L1T1_lora_out = L1T1_lora_B_out * self.lora.L1T1.scaling\n",
    "            L1T1_lora_out = L1T1_lora_B_out\n",
    "\n",
    "            # ================================================================\n",
    "            # L1T2 Nero LAYER\n",
    "            # ================================================================\n",
    "            # L1T2_nero_out = self.nero.L1T2.B(F.relu(self.nero.L1T2.A(L1T1_lora_out)))\n",
    "            L1T2_nero_A_out = self.nero.L1T2.A(L1T1_lora_out)\n",
    "            L1T2_nero_relu_out = F.relu(L1T2_nero_A_out)\n",
    "            L1T2_nero_B_out = self.nero.L1T2.B(L1T2_nero_relu_out)\n",
    "            L1T2_nero_out = L1T2_nero_B_out * self.lora.L1T1.scaling\n",
    "            if requires_conversion:\n",
    "                L1T2_nero_out = L1T2_nero_out.to(base_out.dtype)\n",
    "            output = base_out + L1T2_nero_out\n",
    "\n",
    "        if self.eval_L2T2_nero or self.train_L2T2_nero:\n",
    "            # ================================================================\n",
    "            # L2T1 LoRA LAYER\n",
    "            # ================================================================\n",
    "            # L2T1_lora_out = self.lora.L2T1.B(self.lora.L2T1.A(self.lora.L2T1.dropout(x))) #* self.lora.L2T1.scaling\n",
    "            L2T1_lora_dropout_out = self.lora.L2T1.dropout(x)\n",
    "            L2T1_lora_A_out = self.lora.L2T1.A(L2T1_lora_dropout_out)\n",
    "            L2T1_lora_B_out = self.lora.L2T1.B(L2T1_lora_A_out)\n",
    "            # L2T1_lora_out = L2T1_lora_B_out * self.lora.L2T1.scaling\n",
    "            L2T1_lora_out = L2T1_lora_B_out\n",
    "\n",
    "            # ================================================================\n",
    "            # L2T2 Nero LAYER\n",
    "            # ================================================================\n",
    "            # L2T2_nero_out = self.nero.L2T2.B(F.relu(self.nero.L2T2.A(L2T1_lora_out)))\n",
    "            L2T2_nero_A_out = self.nero.L2T2.A(L2T1_lora_out)\n",
    "            L2T2_nero_relu_out = F.relu(L2T2_nero_A_out)\n",
    "            L2T2_nero_B_out = self.nero.L2T2.B(L2T2_nero_relu_out)\n",
    "            L2T2_nero_out = L2T2_nero_B_out * self.lora.L2T1.scaling\n",
    "            if requires_conversion:\n",
    "                L2T2_nero_out = L2T2_nero_out.to(base_out.dtype)\n",
    "            \n",
    "            if self.train_L2T2_nero and not self.eval_L2T2_nero:\n",
    "                # ================================================================\n",
    "                # DISTANCE-BASED LOSS\n",
    "                # ================================================================\n",
    "                # Cross-language distances for the same task: \n",
    "                #   Task 1: distance(L1T1, L2T1) \n",
    "                #   Task 2: distance(L1T2, L2T2)\n",
    "                LxT1_distance = self._distance_fn(L1T1_lora_out.float(), L2T1_lora_out.float())\n",
    "                LxT2_distance = self._distance_fn(L1T2_nero_out.float(), L2T2_nero_out.float()) # requires grad\n",
    "\n",
    "                # Cross-task distances for the same language: \n",
    "                #   Language 1: distance(L1T1, L1T2)\n",
    "                #   Language 2: distance(L2T1, L2T2)\n",
    "                L1Tx_distance = self._distance_fn(L1T1_lora_out.float(), L1T2_nero_out.float())\n",
    "                L2Tx_distance = self._distance_fn(L2T1_lora_out.float(), L2T2_nero_out.float()) # requires grad\n",
    "\n",
    "                # Distance constraints (enforced via MSE loss):\n",
    "                #   Cross-language distances should match across tasks:\n",
    "                #       distance(L1T1, L2T1) ≈ distance(L1T2, L2T2)\n",
    "                #   Cross-task distances should match across languages:\n",
    "                #       distance(L1T1, L1T2) ≈ distance(L2T1, L2T2)\n",
    "                L2T2_nero_loss = F.mse_loss(LxT2_distance, LxT1_distance) + F.mse_loss(L2Tx_distance, L1Tx_distance)\n",
    "\n",
    "            # ================================================================\n",
    "            # OUTPUTS\n",
    "            # ================================================================\n",
    "            # If `output` is None due to being in evaluate L2T2 Nero mode, \n",
    "            # compute and return the L2T2 Nero-injected output\n",
    "            # if output is None:\n",
    "            if self.eval_L2T2_nero and not self.train_L2T2_nero:\n",
    "                output = base_out + L2T2_nero_out\n",
    "                return output\n",
    "            \n",
    "            # If enabled, return both the L1T2 Nero–injected output and the L2T2 Nero loss\n",
    "            if self.return_L2T2_nero_loss:\n",
    "                return output, L2T2_nero_loss\n",
    "\n",
    "        # Return only the L1T2 Nero-injected output\n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, mode: Literal['L1T1', 'L2T1'], state_dict, prefix: str):\n",
    "        self.lora[mode].A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora[mode].B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora[mode].bias_flag:\n",
    "            self.lora[mode].A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora[mode].B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "\n",
    "    def load_nero_params(self, mode: Literal['L1T2', 'L2T2'], state_dict, prefix: str):\n",
    "        self.nero[mode].A.weight.data = state_dict[f'{prefix}.nero.{mode}.A.weight'].to(self.device)\n",
    "        self.nero[mode].B.weight.data = state_dict[f'{prefix}.nero.{mode}.B.weight'].to(self.device)\n",
    "        if self.nero[mode].bias_flag:\n",
    "            self.nero[mode].A.bias.data = state_dict[f'{prefix}.nero.{mode}.A.bias'].to(self.device)\n",
    "            self.nero[mode].B.bias.data = state_dict[f'{prefix}.nero.{mode}.B.bias'].to(self.device)\n",
    "    \n",
    "    def reinitialize_nero_params(self, modes: Union[List[Literal['L1T2', 'L2T2']], Literal['L1T2', 'L2T2', 'all']] = 'all'):\n",
    "        # Handle different type of modes\n",
    "        if modes == 'all':\n",
    "            target_modes = ['L1T2', 'L2T2']\n",
    "        elif isinstance(modes, str):\n",
    "            target_modes = [modes]\n",
    "        else:\n",
    "            target_modes = modes\n",
    "        \n",
    "        # Validate modes\n",
    "        valid_modes = {'L1T2', 'L2T2'}\n",
    "        for mode in target_modes:\n",
    "            if mode not in valid_modes:\n",
    "                raise ValueError(f\"Invalid mode '{mode}'. Must be one of {valid_modes}.\")\n",
    "        \n",
    "        # Reinitialize specified Nero layers\n",
    "        for mode in target_modes:\n",
    "            if mode in self.nero:\n",
    "                nero_layer = self.nero[mode]\n",
    "                \n",
    "                # Get the rank from the A layer's output features\n",
    "                rank = nero_layer['A'].out_features\n",
    "                \n",
    "                # Reinitialize A layer weights: normal distribution with std = 1/sqrt(rank)\n",
    "                nn.init.normal_(nero_layer['A'].weight, mean=0.0, std=1.0 / math.sqrt(rank))\n",
    "                \n",
    "                # Reinitialize B layer weights: zeros\n",
    "                nn.init.zeros_(nero_layer['B'].weight)\n",
    "                \n",
    "                # Reinitialize biases if they exist\n",
    "                if nero_layer.bias_flag:\n",
    "                    if nero_layer['A'].bias is not None:\n",
    "                        nn.init.zeros_(nero_layer['A'].bias)\n",
    "                    if nero_layer['B'].bias is not None:\n",
    "                        nn.init.zeros_(nero_layer['B'].bias)\n",
    "                            \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 base_model: nn.Module, \n",
    "                 L1T1_lora_config: LoraConfig, \n",
    "                 L2T1_lora_config: LoraConfig, \n",
    "                 train_L2T2_nero=True,\n",
    "                 eval_L2T2_nero=False,\n",
    "                 return_L2T2_nero_loss=False,\n",
    "                 distance_fn='euclidean',\n",
    "                 distance_fn_pairwise=True,\n",
    "                 debug: bool=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        # assert train_L2T2_nero != eval_L2T2_nero\n",
    "        self.train_L2T2_nero = train_L2T2_nero\n",
    "        self.eval_L2T2_nero = eval_L2T2_nero\n",
    "        self.return_L2T2_nero_loss = return_L2T2_nero_loss\n",
    "        \n",
    "        self.distance_fn = distance_fn\n",
    "        self.distance_fn_pairwise = distance_fn_pairwise\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self._wrap_target_layers(L1T1_lora_config, L2T1_lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, L1T1_lora_config, L2T1_lora_config):\n",
    "        assert L1T1_lora_config.target_modules == L2T1_lora_config.target_modules, \"[ERROR] L1T1 and L2T1 LoRA configurations must have the same target modules.\"\n",
    "\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in L1T1_lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    base_layer=module,\n",
    "                    train_L2T2_nero=self.train_L2T2_nero,\n",
    "                    eval_L2T2_nero=self.eval_L2T2_nero,\n",
    "                    return_L2T2_nero_loss=self.return_L2T2_nero_loss,\n",
    "\n",
    "                    # L1T1 LoRA parameters\n",
    "                    L1T1_lora_params={\n",
    "                        'rank': L1T1_lora_config.r, \n",
    "                        'alpha': L1T1_lora_config.lora_alpha, \n",
    "                        'dropout': L1T1_lora_config.lora_dropout,\n",
    "                        'bias': L1T1_lora_config.lora_bias,\n",
    "                        'use_rslora': L1T1_lora_config.use_rslora,\n",
    "                    },\n",
    "                \n",
    "                    # L2T1 LoRA parameters\n",
    "                    L2T1_lora_params={\n",
    "                        'rank': L2T1_lora_config.r, \n",
    "                        'alpha': L2T1_lora_config.lora_alpha, \n",
    "                        'dropout': L2T1_lora_config.lora_dropout,\n",
    "                        'bias': L2T1_lora_config.lora_bias,\n",
    "                        'use_rslora': L2T1_lora_config.use_rslora,\n",
    "                    },\n",
    "\n",
    "                    # Nero parameters (for temporary, use L1T1 LoRA parameters)\n",
    "                    nero_params={\n",
    "                        'rank': L1T1_lora_config.r, \n",
    "                        'bias': L1T1_lora_config.lora_bias,\n",
    "                    },\n",
    "\n",
    "                    # Distance function parameters\n",
    "                    distance_fn=self.distance_fn,\n",
    "                    distance_fn_pairwise=self.distance_fn_pairwise,\n",
    "\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_train_L2T2_nero(self, train_L2T2_nero: bool=True, verbose: bool=False):\n",
    "        self.train_L2T2_nero = train_L2T2_nero\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.train_L2T2_nero = train_L2T2_nero\n",
    "        \n",
    "        if train_L2T2_nero:\n",
    "            self.set_return_L2T2_nero_loss(True)\n",
    "            self.freeze_all_except_L2T2_nero()\n",
    "        else:\n",
    "            self.set_return_L2T2_nero_loss(False)\n",
    "            self.freeze_all_except_L1T2_nero()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Train L2T2 Nero set to '{train_L2T2_nero}'.\")\n",
    "\n",
    "    def set_eval_L2T2_nero(self, eval_L2T2_nero: bool=True, verbose: bool=False):\n",
    "        self.eval_L2T2_nero = eval_L2T2_nero\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.eval_L2T2_nero = eval_L2T2_nero\n",
    "        \n",
    "        if eval_L2T2_nero:\n",
    "            self.set_return_L2T2_nero_loss(False)\n",
    "            self.freeze_all()\n",
    "        else:\n",
    "            self.set_return_L2T2_nero_loss(True)\n",
    "            self.freeze_all_except_L2T2_nero()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Evaluate L2T2 Nero set to '{eval_L2T2_nero}'.\")\n",
    "    \n",
    "    def set_return_L2T2_nero_loss(self, return_L2T2_nero_loss: bool=True, verbose: bool=False):\n",
    "        self.return_L2T2_nero_loss = return_L2T2_nero_loss\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_L2T2_nero_loss = return_L2T2_nero_loss\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Return L2T2 Nero loss set to '{return_L2T2_nero_loss}'.\")\n",
    "    \n",
    "    def set_distance_fn(self, distance_fn: str, pairwise: bool, verbose: bool=False):\n",
    "        self.distance_fn = distance_fn\n",
    "        self.distance_fn_pairwise = pairwise\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.distance_fn = distance_fn\n",
    "            layer.distance_fn_pairwise = pairwise\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Distance function set to '{distance_fn}' with pairwise to {pairwise}.\")\n",
    "    \n",
    "    def freeze_all(self, verbose: bool=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen!\")\n",
    "\n",
    "    def freeze_all_except_L1T2_nero(self, verbose=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero.L1T2.A' in param_name or 'nero.L1T2.B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen except L1T2 Nero layers!\")\n",
    "    \n",
    "    def freeze_all_except_L2T2_nero(self, verbose=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero.L2T2.A' in param_name or 'nero.L2T2.B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen except L2T2 Nero layers!\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        print(\"[INFO] All layers are unfrozen!\")\n",
    "    \n",
    "    def load_lora_params(self, mode: Literal['L1T1', 'L2T1'], lora_path: str):\n",
    "        if not os.path.exists(lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] LoRA file not found:\", lora_path)\n",
    "        \n",
    "        if lora_path.endswith('.safetensors'):\n",
    "            state_dict = load_file(lora_path)\n",
    "        else:\n",
    "            state_dict = torch.load(lora_path, map_location='cpu')\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(mode, state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(f\"[INFO] {mode} LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def load_nero_params(self, mode: Literal['L1T2', 'L2T2'], nero_path):\n",
    "        if not os.path.exists(nero_path):\n",
    "            raise FileNotFoundError(\"[ERROR] Nero file not found:\", nero_path)\n",
    "        \n",
    "        if nero_path.endswith('.safetensors'):\n",
    "            state_dict = load_file(nero_path) # assuming .safetensors\n",
    "        else:\n",
    "            state_dict = torch.load(nero_path, map_location='cpu')\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.nero.{mode}.A.weight' in state_dict and f'{nero_layer_name}.nero.{mode}.B.weight' in state_dict:\n",
    "                nero_layer.load_nero_params(mode, state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(f\"[INFO] {mode} Nero parameters loaded successfully!\")\n",
    "    \n",
    "    def reinitialize_nero_params(self, modes: Union[List[Literal['L1T2', 'L2T2']], Literal['L1T2', 'L2T2', 'all']] = 'all', verbose: bool = False):\n",
    "        # Handle different type of modes\n",
    "        if modes == 'all':\n",
    "            target_modes = ['L1T2', 'L2T2']\n",
    "        elif isinstance(modes, str):\n",
    "            target_modes = [modes]\n",
    "        else:\n",
    "            target_modes = modes\n",
    "        \n",
    "        # Validate modes\n",
    "        valid_modes = {'L1T2', 'L2T2'}\n",
    "        for mode in target_modes:\n",
    "            if mode not in valid_modes:\n",
    "                raise ValueError(f\"Invalid mode '{mode}'. Must be one of {valid_modes}\")\n",
    "        \n",
    "        # Reinitialize parameters for all Nero layers\n",
    "        reinitialized_count = 0\n",
    "        for layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer.reinitialize_nero_params(modes)\n",
    "            reinitialized_count += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Reinitialized Nero parameters for {reinitialized_count} layers, modes: {target_modes}\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self.train_L2T2_nero:\n",
    "            L2T2_nero_losses = []\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                assert isinstance(_out, tuple) and len(_out) == 2\n",
    "                layer_out, L2T2_nero_layer_loss = _out\n",
    "                L2T2_nero_losses.append(L2T2_nero_layer_loss)\n",
    "                return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract hidden_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                outputs = self.base_model(*args, **kwargs)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "                    \n",
    "            # Move all `L2T2_nero_losses` to the same device\n",
    "            L2T2_nero_losses = [t.to(L2T2_nero_losses[0].device) for t in L2T2_nero_losses]\n",
    "            \n",
    "            # Average `L2T2_nero_losses`\n",
    "            L2T2_nero_loss = torch.stack(L2T2_nero_losses).mean()\n",
    "            \n",
    "            if self.return_L2T2_nero_loss:\n",
    "                return outputs, L2T2_nero_loss\n",
    "            else:\n",
    "                return outputs\n",
    "        \n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    device_map=device,\n",
    ")\n",
    "model = NeroModel(\n",
    "    base_model, \n",
    "    L1T1_lora_config=model_configs['L1T1']['lora_config'], \n",
    "    L2T1_lora_config=model_configs['L2T1']['lora_config'], \n",
    "    train_L2T2_nero=False,\n",
    "    return_L2T2_nero_loss=False,\n",
    "    distance_fn='euclidean',\n",
    "    distance_fn_pairwise=True,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded L1T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L1T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 0.002013110090047121\n",
      "- min     : -1.5293457508087158\n",
      "- max     : 1.3558658361434937\n",
      "\n",
      "[INFO] L1T1 LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded L1T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L1T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 6.287686119321734e-05\n",
      "- min     : -0.04176201671361923\n",
      "- max     : 0.04242725297808647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded L1T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L1T1')\n",
    "print()\n",
    "\n",
    "model.load_lora_params('L1T1', model_configs['L1T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L1T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L1T1')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded L2T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L2T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : -0.0005470038158819079\n",
      "- min     : -1.456594705581665\n",
      "- max     : 1.5319485664367676\n",
      "\n",
      "[INFO] L2T1 LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded L2T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.lora.L2T1.A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 2.2152138626552187e-05\n",
      "- min     : -0.06327299773693085\n",
      "- max     : 0.0625513345003128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded L2T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L2T1')\n",
    "print()\n",
    "\n",
    "model.load_lora_params('L2T1', model_configs['L2T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L2T1 LoRA parameters:\")\n",
    "check_lora_parameters(model, prefix='L2T1')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.freeze_all_except_L1T2_nero()\n",
    "print()\n",
    "\n",
    "# check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gradient checkpointing enabled!\n",
      "\n",
      "Loss: tensor(3.4257, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 11.072871021705916\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"[INFO] Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(hf_data_id, data_dir=hf_data_dir)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total batches: 1250\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"[INFO] Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask):\n",
      "(torch.Size([4, 512]), torch.Size([4, 512]))\n",
      "\n",
      "First batch text:\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "Randy had $3,000. Sm ...\n",
      "\n",
      "Loss: tensor(3.4257, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 11.072871021705916\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.func transforms don't yet support saved tensor hooks. Please open an issue with your use case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# prompts = [\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     \"The capital city of Japan is\", \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     \"The capital city of Indonesia is\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     padding=True,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs, L2T2_nero_outs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Test with first batch\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Test with sample prompts\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# input_ids=inputs['input_ids'].to(device),\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# attention_mask=inputs['attention_mask'].to(device),\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# labels=inputs['input_ids'].to(device),\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Disable cache to not conflict with gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(L2T2_nero_outs)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pairwise euclidean: 0.2222\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# non-pairwise euclidean: 0.2167\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# pairwise sinkhorn: 0.0681\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 626\u001b[0m, in \u001b[0;36mNeroModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m     hooks\u001b[38;5;241m.\u001b[39mappend(hook)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Remove hooks after forward pass, ensuring it's done even if an error occurs\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:959\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 959\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    961\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:460\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    442\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    443\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:1083\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 module\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m make_capture_wrapper(module, original_forward, key, specs\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   1081\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m-> 1083\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:377\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 377\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    387\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/masking_utils.py:805\u001b[0m, in \u001b[0;36mcreate_causal_mask\u001b[0;34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, or_mask_function, and_mask_function)\u001b[0m\n\u001b[1;32m    802\u001b[0m     allow_is_causal_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# We now create the mask\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_factory_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_is_causal_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_is_causal_skip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# additional kwarg for sdpa\u001b[39;49;00m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Additional kwarg for eager\u001b[39;49;00m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the config as well, in case someone wants to easily have their own mask_interface\u001b[39;49;00m\n\u001b[1;32m    815\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m causal_mask\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/masking_utils.py:441\u001b[0m, in \u001b[0;36msdpa_mask_older_torch\u001b[0;34m(batch_size, cache_position, kv_length, kv_offset, mask_function, attention_mask, local_size, allow_is_causal_skip, allow_torch_fix, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m kv_arange \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m kv_offset\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# This creates the 4D mask easily. Note that we do not include vmap over the batch_idx dimension as well,\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# as vmap cannot handle slicing a tensor from scalar tensor (it internally calls `.item()` which vmap does not allow\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# However, in more recent version of Pytorch, a trick was introduced to handle it - which is the reason we have\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# `sdpa_mask_recent_torch`, as it allows more general `mask_function`\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_vmap_for_bhqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbh_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_arange\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m causal_mask[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/apis.py:201\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:376\u001b[0m, in \u001b[0;36mdisable_saved_tensors_hooks\u001b[0;34m(error_message)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m     maybe_prev_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    374\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_autograd\u001b[38;5;241m.\u001b[39m_saved_tensors_hooks_get_disabled_error_message()\n\u001b[1;32m    375\u001b[0m     )\n\u001b[0;32m--> 376\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saved_tensors_hooks_disable\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror_message\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# See NOTE: [disabled_error_message invariant]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.func transforms don't yet support saved tensor hooks. Please open an issue with your use case."
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "# Set model to learn L2T2 Nero\n",
    "model.set_train_L2T2_nero(True)\n",
    "model.set_distance_fn(distance_fn, pairwise=True)\n",
    "model.train()\n",
    "\n",
    "# Forward pass\n",
    "device = next(model.parameters()).device\n",
    "# prompts = [\n",
    "#     \"The capital city of Japan is\", \n",
    "#     \"The capital city of Indonesia is\",\n",
    "# ]\n",
    "# inputs = tokenizer(\n",
    "#     prompts,\n",
    "#     return_tensors='pt',\n",
    "#     padding=True,\n",
    "# )\n",
    "outputs, L2T2_nero_outs = model(\n",
    "    # Test with first batch\n",
    "    input_ids=first_batch['input_ids'].to(device),\n",
    "    attention_mask=first_batch['attention_mask'].to(device),\n",
    "    labels=first_batch['input_ids'].to(device),\n",
    "\n",
    "    # Test with sample prompts\n",
    "    # input_ids=inputs['input_ids'].to(device),\n",
    "    # attention_mask=inputs['attention_mask'].to(device),\n",
    "    # labels=inputs['input_ids'].to(device),\n",
    "    \n",
    "    use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    ")\n",
    "\n",
    "print(L2T2_nero_outs)\n",
    "\n",
    "# pairwise euclidean: 0.2222\n",
    "# non-pairwise euclidean: 0.2167\n",
    "\n",
    "# pairwise cosine: 0.0022\n",
    "# non-pairwise cosine: 0.0024\n",
    "\n",
    "# pairwise squared euclidean: 33.0331\n",
    "# non-pairwise squared euclidean: 33.1182\n",
    "\n",
    "# pairwise huber: 6.0343\n",
    "# non-pairwise huber: 6.2125\n",
    "\n",
    "# pairwise cosine huber: 0.0640\n",
    "# non-pairwise cosine huber: 0.0663\n",
    "\n",
    "# pairwise sinkhorn: 0.0681"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Generated: ### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "70\n",
      "\n",
      "### Explanation: \n",
      "The square root of 196 is 14. The product of 14 and 15 is 210. Thus, the square root of 196 multiplied by 15 is 210.\n",
      "\n",
      "### Solution: \n",
      "The square root of 196 is 14. The product of 14 and 15 is 210. Thus, the square root of 196 multiplied by 15 is 210.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.set_train_L2T2_nero(False)\n",
    "model.set_eval_L2T2_nero(False)\n",
    "generated = generate_text(model, tokenizer, prompt=L1T2_sample_prompt, device=device)\n",
    "print(\"================================\")\n",
    "print(\"CHECK GENERATED TEXT\")\n",
    "print(\"================================\")\n",
    "print(f\"{'Generated':<9}:\", generated)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Generated: ### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "96\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.set_train_L2T2_nero(False)\n",
    "model.set_eval_L2T2_nero(True)\n",
    "generated = generate_text(model, tokenizer, prompt=L2T2_sample_prompt, device=device)\n",
    "print(\"================================\")\n",
    "print(\"CHECK GENERATED TEXT\")\n",
    "print(\"================================\")\n",
    "print(f\"{'Generated':<9}:\", generated)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1283/1621092812.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  L2T2_scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_1283/1621092812.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  L1T2_scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd2677c103241f8be670410a9715928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111314930000036, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250821_045556-0usj91pf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/0usj91pf' target=\"_blank\">eager-wave-175</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/0usj91pf' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/0usj91pf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the distance function\n",
    "model.set_distance_fn(distance_fn, pairwise=True)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# If `device` is not specified or set to 'auto', use the model's device\n",
    "# if device is None or device == 'auto':\n",
    "device = next(iter(model.parameters())).device\n",
    "\n",
    "# Set up optimizer and gradient scaler\n",
    "# for L2T2 Nero\n",
    "model.set_train_L2T2_nero(True)\n",
    "L2T2_nero_params = [p for n, p in model.named_parameters() if p.requires_grad]\n",
    "L2T2_optimizer = torch.optim.Adam(L2T2_nero_params, lr=lr, foreach=True)\n",
    "L2T2_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# for L1T2 Nero\n",
    "model.set_train_L2T2_nero(False)\n",
    "L1T2_nero_params = [p for n, p in model.named_parameters() if p.requires_grad]\n",
    "L1T2_optimizer = torch.optim.Adam(L1T2_nero_params, lr=lr, foreach=True)\n",
    "L1T2_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Set up LR scheduler\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "if warmup_ratio > 0:\n",
    "    # If `warmup_ratio` > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup # type: ignore\n",
    "    max_optimizer_steps = (max_global_steps // grad_accumulation_steps) // 2 # divide by 2 because we train in 2 modes\n",
    "    num_warmup_steps = int(warmup_ratio * max_optimizer_steps)\n",
    "    L1T2_scheduler = get_cosine_schedule_with_warmup(\n",
    "        L1T2_optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "    L2T2_scheduler = get_cosine_schedule_with_warmup(\n",
    "        L2T2_optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "else:\n",
    "    # If `warmup_ratio` is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "    L1T2_scheduler = LambdaLR(L1T2_optimizer, lr_lambda=lambda step: 1.0)\n",
    "    L2T2_scheduler = LambdaLR(L2T2_optimizer, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True, # End previous run and start a new one\n",
    "    config=dict(\n",
    "        # Project configuration\n",
    "        seed = seed,\n",
    "        device = device,\n",
    "\n",
    "        # Data configuration\n",
    "        hf_data_id = hf_data_id,\n",
    "        hf_data_dir = hf_data_dir,\n",
    "\n",
    "        # Training configuration\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "        lr = lr,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        checkpoint_steps = checkpoint_steps,\n",
    "        distance_fn = distance_fn,\n",
    "        resume_step = resume_step,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "global_step = resume_step\n",
    "start_epoch = 0\n",
    "\n",
    "def load_trainer_params(mode, model, optimizer, scheduler, scaler, checkpoint_dir, device):\n",
    "    # Load Nero parameters\n",
    "    nero_path = os.path.join(checkpoint_dir, f'{mode}_nero.safetensors')\n",
    "    model.load_nero_params(mode, nero_path)\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_dir, f'{mode}_optimizer.pt')\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "    \n",
    "    # Move optimizer state to the correct device\n",
    "    for param in optimizer.state:\n",
    "        param_device = param.device\n",
    "        param_dtype = param.dtype\n",
    "        for key, value in optimizer.state[param].items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                optimizer.state[param][key] = value.to(device=param_device, dtype=param_dtype)\n",
    "\n",
    "    # Load scheduler state\n",
    "    scheduler_path = os.path.join(checkpoint_dir, f'{mode}_scheduler.pt')\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
    "\n",
    "    # Load scaler state\n",
    "    scaler_path = os.path.join(checkpoint_dir, f'{mode}_scaler.pt')\n",
    "    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "\n",
    "if resume_step > 0:\n",
    "    checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{resume_step}')\n",
    "    print(f\"[INFO] Resuming training from checkpoint directory:\", checkpoint_dir)\n",
    "\n",
    "    # Load trainer parameters\n",
    "    load_trainer_params('L1T2', model, L1T2_optimizer, L1T2_scheduler, L1T2_scaler, checkpoint_dir, device)\n",
    "    load_trainer_params('L2T2', model, L2T2_optimizer, L2T2_scheduler, L2T2_scaler, checkpoint_dir, device)\n",
    "\n",
    "    # Load trainer state\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, 'trainer_state.json')\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        start_epoch = log_history[-1]['epoch'] if log_history else 0\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch} and step {resume_step}.\")\n",
    "\n",
    "    # Load RNG state for reproducibility\n",
    "    rng_path = os.path.join(checkpoint_dir, 'rng_state.pth')\n",
    "    if os.path.exists(rng_path):\n",
    "        rng_state = torch.load(rng_path)\n",
    "        random.setstate(rng_state['python'])\n",
    "        np.random.set_state(rng_state['numpy'])\n",
    "        torch.set_rng_state(rng_state['cpu'])\n",
    "        if torch.cuda.is_available() and rng_state['cuda']:\n",
    "            torch.cuda.set_rng_state_all(rng_state['cuda'])\n",
    "    \n",
    "    if resume_step % grad_accumulation_steps != 0:\n",
    "        print(\"[WARN] Resuming mid-gradient accumulation cycle. Make sure this is intended.\")\n",
    "else:\n",
    "    if push_to_hf:\n",
    "        # If it's new training, create Hugging Face repository\n",
    "        print(f\"[INFO] Creating Hugging Face repository:\", hf_nero_id) # print the link instead\n",
    "        create_repo(repo_id=hf_nero_id, repo_type='model', exist_ok=True)\n",
    "        print(f\"[INFO] Hugging Face repository created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1283/1180113685.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(4.0281, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 1, L1T2/loss: 8.0561, L1T2/lr: 6.4516e-06, L1T2/grad_norm: 6.1106, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 2, L2T2/loss: 0.0000e+00, L2T2/lr: 0.0000e+00\n",
      "mode: L2T2, epoch: 0, step: 3, L2T2/loss: 0.0000e+00, L2T2/lr: 6.4516e-06, L2T2/grad_norm: 0.0000e+00, L2T2/grad_norm_clipped: 0.0000e+00\n",
      "loss: tensor(3.8251, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 4, L1T2/loss: 7.6501, L1T2/lr: 6.4516e-06\n",
      "loss: tensor(3.8412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 5, L1T2/loss: 7.6824, L1T2/lr: 1.2903e-05, L1T2/grad_norm: nan, L1T2/grad_norm_clipped: nan\n",
      "mode: L2T2, epoch: 0, step: 6, L2T2/loss: 0.0000e+00, L2T2/lr: 6.4516e-06\n",
      "mode: L2T2, epoch: 0, step: 7, L2T2/loss: 0.0000e+00, L2T2/lr: 1.2903e-05, L2T2/grad_norm: 0.0000e+00, L2T2/grad_norm_clipped: 0.0000e+00\n",
      "loss: tensor(3.7391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 8, L1T2/loss: 7.4782, L1T2/lr: 1.2903e-05\n",
      "loss: tensor(4.1697, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 9, L1T2/loss: 8.3394, L1T2/lr: 1.9355e-05, L1T2/grad_norm: 5.5452, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 10, L2T2/loss: -1.4076e-06, L2T2/lr: 1.2903e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "42\n",
      "\n",
      "### Explanation:\n",
      "To solve this problem, we need to use the distributive property of multiplication over addition. The distributive property states that for any two numbers a, b, and c, we have: a(b+c) = ab + ac. In this case, we have:\n",
      "\n",
      "42(15) = 42(15 + 0) = 42(15) + 42(0) = 42(15) + 0 = 42(15) =\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "答え: 96\n",
      "\n",
      "\n",
      "mode: L2T2, epoch: 0, step: 11, L2T2/loss: -2.5710e-07, L2T2/lr: 1.9355e-05, L2T2/grad_norm: 8.5942e-06, L2T2/grad_norm_clipped: 8.5942e-06\n",
      "loss: tensor(4.0048, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 12, L1T2/loss: 8.0097, L1T2/lr: 1.9355e-05\n",
      "loss: tensor(3.6485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 13, L1T2/loss: 7.2970, L1T2/lr: 2.5806e-05, L1T2/grad_norm: 6.3276, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 14, L2T2/loss: -6.0251e-05, L2T2/lr: 1.9355e-05\n",
      "mode: L2T2, epoch: 0, step: 15, L2T2/loss: -5.4243e-05, L2T2/lr: 2.5806e-05, L2T2/grad_norm: 1.9467e-05, L2T2/grad_norm_clipped: 1.9467e-05\n",
      "loss: tensor(3.4336, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 16, L1T2/loss: 6.8671, L1T2/lr: 2.5806e-05\n",
      "loss: tensor(3.2397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 17, L1T2/loss: 6.4795, L1T2/lr: 3.2258e-05, L1T2/grad_norm: 5.4903, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 18, L2T2/loss: -0.0001, L2T2/lr: 2.5806e-05\n",
      "mode: L2T2, epoch: 0, step: 19, L2T2/loss: -0.0001, L2T2/lr: 3.2258e-05, L2T2/grad_norm: 3.0944e-05, L2T2/grad_norm_clipped: 3.0944e-05\n",
      "loss: tensor(3.6774, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 20, L1T2/loss: 7.3549, L1T2/lr: 3.2258e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "The square root of 196 multiplied by 15 is 28 times 15, which is 420.\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 = 12 - 56 = -44\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 = 12 - 56 = -44\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 = 12 - 56 = -44\n",
      "\n",
      "### 質問: \n",
      "12-7\n",
      "\n",
      "loss: tensor(3.2695, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 21, L1T2/loss: 6.5389, L1T2/lr: 3.8710e-05, L1T2/grad_norm: 5.1977, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 22, L2T2/loss: -0.0001, L2T2/lr: 3.2258e-05\n",
      "mode: L2T2, epoch: 0, step: 23, L2T2/loss: -0.0001, L2T2/lr: 3.8710e-05, L2T2/grad_norm: 4.4084e-05, L2T2/grad_norm_clipped: 4.4084e-05\n",
      "loss: tensor(3.8749, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 24, L1T2/loss: 7.7497, L1T2/lr: 3.8710e-05\n",
      "loss: tensor(4.0103, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 25, L1T2/loss: 8.0206, L1T2/lr: 4.5161e-05, L1T2/grad_norm: 4.8479, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 26, L2T2/loss: -0.0001, L2T2/lr: 3.8710e-05\n",
      "mode: L2T2, epoch: 0, step: 27, L2T2/loss: -0.0001, L2T2/lr: 4.5161e-05, L2T2/grad_norm: 6.0817e-05, L2T2/grad_norm_clipped: 6.0817e-05\n",
      "loss: tensor(3.5678, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 28, L1T2/loss: 7.1357, L1T2/lr: 4.5161e-05\n",
      "loss: tensor(3.4878, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 29, L1T2/loss: 6.9757, L1T2/lr: 5.1613e-05, L1T2/grad_norm: 4.2217, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 30, L2T2/loss: -9.7165e-05, L2T2/lr: 4.5161e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 = 12-56 = -44\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 = 12-56 = -44\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 = 12-56 = -44\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の\n",
      "\n",
      "mode: L2T2, epoch: 0, step: 31, L2T2/loss: -9.9388e-05, L2T2/lr: 5.1613e-05, L2T2/grad_norm: 7.6925e-05, L2T2/grad_norm_clipped: 7.6925e-05\n",
      "loss: tensor(3.6405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 32, L1T2/loss: 7.2810, L1T2/lr: 5.1613e-05\n",
      "loss: tensor(3.7002, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 33, L1T2/loss: 7.4003, L1T2/lr: 5.8065e-05, L1T2/grad_norm: 3.6004, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 34, L2T2/loss: -9.4703e-05, L2T2/lr: 5.1613e-05\n",
      "mode: L2T2, epoch: 0, step: 35, L2T2/loss: -8.9400e-05, L2T2/lr: 5.8065e-05, L2T2/grad_norm: 9.5470e-05, L2T2/grad_norm_clipped: 9.5470e-05\n",
      "loss: tensor(3.7493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 36, L1T2/loss: 7.4985, L1T2/lr: 5.8065e-05\n",
      "loss: tensor(3.5076, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 37, L1T2/loss: 7.0151, L1T2/lr: 6.4516e-05, L1T2/grad_norm: 2.7403, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 38, L2T2/loss: -8.1372e-05, L2T2/lr: 5.8065e-05\n",
      "mode: L2T2, epoch: 0, step: 39, L2T2/loss: -7.6739e-05, L2T2/lr: 6.4516e-05, L2T2/grad_norm: 0.0001, L2T2/grad_norm_clipped: 0.0001\n",
      "loss: tensor(3.7498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 40, L1T2/loss: 7.4997, L1T2/lr: 6.4516e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "The square root of 196 multiplied by 15 is 35.\n",
      "\n",
      "### Step by Step Solution: \n",
      "Let's solve this problem step by step. \n",
      "\n",
      "1. First, let's calculate the square root of 196. To do this, we can use the following formula:\n",
      "2. Square root of x = x^(1/2)\n",
      "3. Therefore, square root of 196 = 196^(1/2) = 14\n",
      "4. Next, let's multiply 14 by\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "0\n",
      "\n",
      "\n",
      "loss: tensor(3.1880, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 41, L1T2/loss: 6.3761, L1T2/lr: 7.0968e-05, L1T2/grad_norm: 2.4770, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 42, L2T2/loss: -6.4994e-05, L2T2/lr: 6.4516e-05\n",
      "mode: L2T2, epoch: 0, step: 43, L2T2/loss: -5.9176e-05, L2T2/lr: 7.0968e-05, L2T2/grad_norm: 0.0001, L2T2/grad_norm_clipped: 0.0001\n",
      "loss: tensor(2.6933, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 44, L1T2/loss: 5.3867, L1T2/lr: 7.0968e-05\n",
      "loss: tensor(3.5663, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 45, L1T2/loss: 7.1327, L1T2/lr: 7.7419e-05, L1T2/grad_norm: 2.2090, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 46, L2T2/loss: -4.9510e-05, L2T2/lr: 7.0968e-05\n",
      "mode: L2T2, epoch: 0, step: 47, L2T2/loss: -4.8261e-05, L2T2/lr: 7.7419e-05, L2T2/grad_norm: 0.0002, L2T2/grad_norm_clipped: 0.0002\n",
      "loss: tensor(3.3901, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 48, L1T2/loss: 6.7801, L1T2/lr: 7.7419e-05\n",
      "loss: tensor(3.3912, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 49, L1T2/loss: 6.7825, L1T2/lr: 8.3871e-05, L1T2/grad_norm: 2.0547, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 50, L2T2/loss: -3.3700e-05, L2T2/lr: 7.7419e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "The square root of 196 is 14. The square root of 196 multiplied by 15 is 210.\n",
      "\n",
      "### Explanation:\n",
      "The square root of 196 is 14. To find the square root of 196, we use the formula:\n",
      "√196 = 196^1/2\n",
      "= 14\n",
      "\n",
      "The square root of 196 multiplied by 15 is 210. To find the square root of 196 multiplied by 15, we use the formula:\n",
      "√196 *\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8の値は8です。\n",
      "\n",
      "\n",
      "mode: L2T2, epoch: 0, step: 51, L2T2/loss: -3.7586e-05, L2T2/lr: 8.3871e-05, L2T2/grad_norm: 0.0002, L2T2/grad_norm_clipped: 0.0002\n",
      "loss: tensor(3.2415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 52, L1T2/loss: 6.4829, L1T2/lr: 8.3871e-05\n",
      "loss: tensor(3.4852, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 53, L1T2/loss: 6.9705, L1T2/lr: 9.0323e-05, L1T2/grad_norm: 1.9139, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 54, L2T2/loss: -2.2245e-05, L2T2/lr: 8.3871e-05\n",
      "mode: L2T2, epoch: 0, step: 55, L2T2/loss: -1.8676e-05, L2T2/lr: 9.0323e-05, L2T2/grad_norm: 0.0002, L2T2/grad_norm_clipped: 0.0002\n",
      "loss: tensor(2.9960, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 56, L1T2/loss: 5.9920, L1T2/lr: 9.0323e-05\n",
      "loss: tensor(3.5420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 57, L1T2/loss: 7.0840, L1T2/lr: 9.6774e-05, L1T2/grad_norm: 1.6970, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 58, L2T2/loss: 7.5968e-06, L2T2/lr: 9.0323e-05\n",
      "mode: L2T2, epoch: 0, step: 59, L2T2/loss: 6.2832e-06, L2T2/lr: 9.6774e-05, L2T2/grad_norm: 0.0003, L2T2/grad_norm_clipped: 0.0003\n",
      "loss: tensor(2.7940, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 60, L1T2/loss: 5.5879, L1T2/lr: 9.6774e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "**Step 1:** 196 is a perfect square number, so the square root of 196 is 14.\n",
      "\n",
      "**Step 2:** 14 multiplied by 15 is 210.\n",
      "\n",
      "**Step 3:** The square root of 196 multiplied by 15 is 210.\n",
      "\n",
      "### Solution: \n",
      "The square root of 196 multiplied by 15 is 210.\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 の値は 0 です。\n",
      "\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 の値は 0 です。\n",
      "\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 の値は 0 です。\n",
      "\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の\n",
      "\n",
      "loss: tensor(3.5581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 61, L1T2/loss: 7.1163, L1T2/lr: 0.0001, L1T2/grad_norm: 1.5448, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 62, L2T2/loss: 3.5633e-05, L2T2/lr: 9.6774e-05\n",
      "mode: L2T2, epoch: 0, step: 63, L2T2/loss: 2.9770e-05, L2T2/lr: 0.0001, L2T2/grad_norm: 0.0003, L2T2/grad_norm_clipped: 0.0003\n",
      "loss: tensor(3.1895, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 64, L1T2/loss: 6.3789, L1T2/lr: 0.0001\n",
      "loss: tensor(3.3522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 65, L1T2/loss: 6.7044, L1T2/lr: 0.0001, L1T2/grad_norm: 1.6425, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 66, L2T2/loss: 6.0381e-05, L2T2/lr: 0.0001\n",
      "mode: L2T2, epoch: 0, step: 67, L2T2/loss: 5.9906e-05, L2T2/lr: 0.0001, L2T2/grad_norm: 0.0003, L2T2/grad_norm_clipped: 0.0003\n",
      "loss: tensor(3.1014, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 68, L1T2/loss: 6.2028, L1T2/lr: 0.0001\n",
      "loss: tensor(3.1993, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 69, L1T2/loss: 6.3986, L1T2/lr: 0.0001, L1T2/grad_norm: 1.4715, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 70, L2T2/loss: 0.0001, L2T2/lr: 0.0001\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "sqrt(196)*15 = 14*15 = 210\n",
      "\n",
      "### Solution: \n",
      "sqrt(196)*15 = sqrt(196^2/100)*15 = sqrt((196^2/100)*100)/sqrt(100) = sqrt(196^2)/sqrt(100)*sqrt(100)/sqrt(100) = sqrt(196^2)/10 = sqrt(196^2)/sqrt(100^2) = sqrt(196^2)/sqrt(100^2\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 は 12 - 7 * 8 という意味です。 \n",
      "12-7*8 の値は 8 です。 \n",
      "12-7*8 = 12 - 7 * 8 = 12 - 56 = -44\n",
      "\n",
      "\n",
      "mode: L2T2, epoch: 0, step: 71, L2T2/loss: 0.0001, L2T2/lr: 0.0001, L2T2/grad_norm: 0.0004, L2T2/grad_norm_clipped: 0.0004\n",
      "loss: tensor(3.0695, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 72, L1T2/loss: 6.1389, L1T2/lr: 0.0001\n",
      "loss: tensor(2.9050, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 73, L1T2/loss: 5.8100, L1T2/lr: 0.0001, L1T2/grad_norm: 1.3661, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 74, L2T2/loss: 0.0002, L2T2/lr: 0.0001\n",
      "mode: L2T2, epoch: 0, step: 75, L2T2/loss: 0.0002, L2T2/lr: 0.0001, L2T2/grad_norm: 0.0005, L2T2/grad_norm_clipped: 0.0005\n",
      "loss: tensor(3.3311, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 76, L1T2/loss: 6.6622, L1T2/lr: 0.0001\n",
      "loss: tensor(3.0224, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 77, L1T2/loss: 6.0448, L1T2/lr: 0.0001, L1T2/grad_norm: 1.7563, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 78, L2T2/loss: 0.0002, L2T2/lr: 0.0001\n",
      "mode: L2T2, epoch: 0, step: 79, L2T2/loss: 0.0002, L2T2/lr: 0.0001, L2T2/grad_norm: 0.0005, L2T2/grad_norm_clipped: 0.0005\n",
      "loss: tensor(2.9386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 80, L1T2/loss: 5.8772, L1T2/lr: 0.0001\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "First, we need to find the square root of 196. 196 is 14 squared. So the square root of 196 is 14. \n",
      "\n",
      "Next, we need to multiply 14 by 15. 14 x 15 = 210.\n",
      "\n",
      "Therefore, the square root of 196 multiplied by 15 is 210.\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "64\n",
      "\n",
      "\n",
      "loss: tensor(2.8503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 81, L1T2/loss: 5.7005, L1T2/lr: 0.0001, L1T2/grad_norm: 1.3118, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 82, L2T2/loss: 0.0003, L2T2/lr: 0.0001\n",
      "mode: L2T2, epoch: 0, step: 83, L2T2/loss: 0.0003, L2T2/lr: 0.0001, L2T2/grad_norm: 0.0006, L2T2/grad_norm_clipped: 0.0006\n",
      "loss: tensor(3.0317, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 84, L1T2/loss: 6.0634, L1T2/lr: 0.0001\n",
      "loss: tensor(2.7027, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 85, L1T2/loss: 5.4055, L1T2/lr: 0.0001, L1T2/grad_norm: 1.3261, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 86, L2T2/loss: 0.0004, L2T2/lr: 0.0001\n",
      "mode: L2T2, epoch: 0, step: 87, L2T2/loss: 0.0004, L2T2/lr: 0.0001, L2T2/grad_norm: 0.0007, L2T2/grad_norm_clipped: 0.0007\n",
      "loss: tensor(2.7034, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 88, L1T2/loss: 5.4068, L1T2/lr: 0.0001\n",
      "loss: tensor(2.6064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 89, L1T2/loss: 5.2127, L1T2/lr: 0.0001, L1T2/grad_norm: 1.0669, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 90, L2T2/loss: 0.0005, L2T2/lr: 0.0001\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "The square root of 196 is 14. The answer is 14 times 15 which is 210.\n",
      "\n",
      "### Answer: \n",
      "The square root of 196 is 14. The answer is 14 times 15 which is 210.\n",
      "\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "60\n",
      "\n",
      "### 質問: \n",
      "3+4*6/2 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "15\n",
      "\n",
      "### 質問: \n",
      "(3+4)*6/2 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12\n",
      "\n",
      "### 質問: \n",
      "(3+4)*6/2 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12\n",
      "\n",
      "### 質問: \n",
      "(3+4)*6/2 の\n",
      "\n",
      "mode: L2T2, epoch: 0, step: 91, L2T2/loss: 0.0005, L2T2/lr: 0.0001, L2T2/grad_norm: 0.0007, L2T2/grad_norm_clipped: 0.0007\n",
      "loss: tensor(2.8414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 92, L1T2/loss: 5.6829, L1T2/lr: 0.0001\n",
      "loss: tensor(2.8083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 93, L1T2/loss: 5.6166, L1T2/lr: 0.0002, L1T2/grad_norm: 1.1733, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 94, L2T2/loss: 0.0007, L2T2/lr: 0.0001\n",
      "mode: L2T2, epoch: 0, step: 95, L2T2/loss: 0.0007, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0009, L2T2/grad_norm_clipped: 0.0009\n",
      "loss: tensor(2.7939, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 96, L1T2/loss: 5.5877, L1T2/lr: 0.0002\n",
      "loss: tensor(2.6061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 97, L1T2/loss: 5.2122, L1T2/lr: 0.0002, L1T2/grad_norm: 0.9601, L1T2/grad_norm_clipped: 0.9601\n",
      "mode: L2T2, epoch: 0, step: 98, L2T2/loss: 0.0009, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 99, L2T2/loss: 0.0009, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0010, L2T2/grad_norm_clipped: 0.0010\n",
      "loss: tensor(2.5207, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 100, L1T2/loss: 5.0414, L1T2/lr: 0.0002\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "The square root of 196 is 14, and 14 multiplied by 15 is 210.\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "(12-7)*8\n",
      "12-7*8\n",
      "4*8\n",
      "32\n",
      "\n",
      "\n",
      "loss: tensor(2.7736, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 101, L1T2/loss: 5.5471, L1T2/lr: 0.0002, L1T2/grad_norm: 1.0419, L1T2/grad_norm_clipped: 1.0000\n",
      "mode: L2T2, epoch: 0, step: 102, L2T2/loss: 0.0012, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 103, L2T2/loss: 0.0012, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0012, L2T2/grad_norm_clipped: 0.0012\n",
      "loss: tensor(2.3487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 104, L1T2/loss: 4.6975, L1T2/lr: 0.0002\n",
      "loss: tensor(2.4369, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 105, L1T2/loss: 4.8737, L1T2/lr: 0.0002, L1T2/grad_norm: 0.8622, L1T2/grad_norm_clipped: 0.8622\n",
      "mode: L2T2, epoch: 0, step: 106, L2T2/loss: 0.0016, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 107, L2T2/loss: 0.0016, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0014, L2T2/grad_norm_clipped: 0.0014\n",
      "loss: tensor(2.3679, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 108, L1T2/loss: 4.7359, L1T2/lr: 0.0002\n",
      "loss: tensor(2.3932, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 109, L1T2/loss: 4.7864, L1T2/lr: 0.0002, L1T2/grad_norm: 0.7806, L1T2/grad_norm_clipped: 0.7806\n",
      "mode: L2T2, epoch: 0, step: 110, L2T2/loss: 0.0020, L2T2/lr: 0.0002\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "First, let's calculate the square root of 196.\n",
      "196 = 14 x 14\n",
      "The square root of 196 is 14.\n",
      "Next, let's multiply 14 by 15.\n",
      "14 x 15 = 210\n",
      "The square root of 196 multiplied by 15 is 210.\n",
      "\n",
      "### Explanation: \n",
      "To solve this problem, we first need to calculate the square root of 196. To do this, we can use the fact that the square root of a number\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "96\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "96\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "96\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "96\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "\n",
      "\n",
      "mode: L2T2, epoch: 0, step: 111, L2T2/loss: 0.0020, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0017, L2T2/grad_norm_clipped: 0.0017\n",
      "loss: tensor(2.4979, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 112, L1T2/loss: 4.9959, L1T2/lr: 0.0002\n",
      "loss: tensor(2.3408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 113, L1T2/loss: 4.6817, L1T2/lr: 0.0002, L1T2/grad_norm: 0.7146, L1T2/grad_norm_clipped: 0.7146\n",
      "mode: L2T2, epoch: 0, step: 114, L2T2/loss: 0.0025, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 115, L2T2/loss: 0.0026, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0022, L2T2/grad_norm_clipped: 0.0022\n",
      "loss: tensor(2.4696, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 116, L1T2/loss: 4.9392, L1T2/lr: 0.0002\n",
      "loss: tensor(2.3700, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 117, L1T2/loss: 4.7400, L1T2/lr: 0.0002, L1T2/grad_norm: 0.7010, L1T2/grad_norm_clipped: 0.7010\n",
      "mode: L2T2, epoch: 0, step: 118, L2T2/loss: 0.0031, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 119, L2T2/loss: 0.0032, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0029, L2T2/grad_norm_clipped: 0.0029\n",
      "loss: tensor(2.4861, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 120, L1T2/loss: 4.9722, L1T2/lr: 0.0002\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "#### Step 1:\n",
      "To find the square root of 196, we must find a number that when multiplied by itself results in 196. This number is 14.\n",
      "\n",
      "#### Step 2:\n",
      "To multiply 14 by 15, we must add 14 to itself 15 times. This results in 210.\n",
      "\n",
      "#### Step 3:\n",
      "The answer is 210.\n",
      "\n",
      "### Explanation: \n",
      "#### Step 1:\n",
      "To find the square root of 196, we must find a number that\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "```bash\n",
      "$ echo 12-7*8\n",
      "8\n",
      "```\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "```bash\n",
      "$ echo 12-7*8\n",
      "8\n",
      "```\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "```bash\n",
      "$ echo 12-7*8\n",
      "8\n",
      "```\n",
      "\n",
      "### 質問: \n",
      "\n",
      "\n",
      "loss: tensor(2.4484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 121, L1T2/loss: 4.8968, L1T2/lr: 0.0002, L1T2/grad_norm: 0.7822, L1T2/grad_norm_clipped: 0.7822\n",
      "mode: L2T2, epoch: 0, step: 122, L2T2/loss: 0.0042, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 123, L2T2/loss: 0.0039, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0035, L2T2/grad_norm_clipped: 0.0035\n",
      "loss: tensor(2.5352, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 124, L1T2/loss: 5.0705, L1T2/lr: 0.0002\n",
      "loss: tensor(2.0742, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 125, L1T2/loss: 4.1485, L1T2/lr: 0.0002, L1T2/grad_norm: 0.8223, L1T2/grad_norm_clipped: 0.8223\n",
      "mode: L2T2, epoch: 0, step: 126, L2T2/loss: 0.0053, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 127, L2T2/loss: 0.0053, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0044, L2T2/grad_norm_clipped: 0.0044\n",
      "loss: tensor(2.4809, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 128, L1T2/loss: 4.9619, L1T2/lr: 0.0002\n",
      "loss: tensor(2.5633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 129, L1T2/loss: 5.1267, L1T2/lr: 0.0002, L1T2/grad_norm: 0.6902, L1T2/grad_norm_clipped: 0.6902\n",
      "mode: L2T2, epoch: 0, step: 130, L2T2/loss: 0.0063, L2T2/lr: 0.0002\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "The square root of 196 is 14. The square root of 14 multiplied by 15 is 210.\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 の値は 8 です。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値は 96 ですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 の値は 96 ではありません。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値は 104 ですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8 の値は 104 ではありません。\n",
      "\n",
      "### 質問: \n",
      "12-7*8\n",
      "\n",
      "mode: L2T2, epoch: 0, step: 131, L2T2/loss: 0.0068, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0053, L2T2/grad_norm_clipped: 0.0053\n",
      "loss: tensor(2.2673, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 132, L1T2/loss: 4.5347, L1T2/lr: 0.0002\n",
      "loss: tensor(2.4102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 133, L1T2/loss: 4.8203, L1T2/lr: 0.0002, L1T2/grad_norm: 0.5690, L1T2/grad_norm_clipped: 0.5690\n",
      "mode: L2T2, epoch: 0, step: 134, L2T2/loss: 0.0082, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 135, L2T2/loss: 0.0077, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0064, L2T2/grad_norm_clipped: 0.0064\n",
      "loss: tensor(2.4605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 136, L1T2/loss: 4.9210, L1T2/lr: 0.0002\n",
      "loss: tensor(2.4272, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 137, L1T2/loss: 4.8544, L1T2/lr: 0.0002, L1T2/grad_norm: 0.5916, L1T2/grad_norm_clipped: 0.5916\n",
      "mode: L2T2, epoch: 0, step: 138, L2T2/loss: 0.0102, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 139, L2T2/loss: 0.0096, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0076, L2T2/grad_norm_clipped: 0.0076\n",
      "loss: tensor(2.0687, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 140, L1T2/loss: 4.1373, L1T2/lr: 0.0002\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L1)\n",
      "================================\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "What is the square root of 196 multiplied by 15?\n",
      "\n",
      "### Answer: \n",
      "#### 1. \n",
      "The square root of 196 is 14\n",
      "\n",
      "#### 2. \n",
      "14 x 15 = 210\n",
      "\n",
      "#### 3. \n",
      "The square root of 196 multiplied by 15 is 210\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT (L2)\n",
      "================================\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "12-7*8 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "12-7*8の計算は、まず12-7の計算をして、7を12から引く。次に、8を掛ける。計算は、12-7*8 = 40です。12-7*8の値は40です。\n",
      "\n",
      "### 命令:\n",
      "次の数学の問題を段階的に解いてください。\n",
      "\n",
      "### 質問: \n",
      "8-2/2+2 の値はいくつですか？\n",
      "\n",
      "### 回答: \n",
      "\n",
      "\n",
      "loss: tensor(2.2898, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 141, L1T2/loss: 4.5795, L1T2/lr: 0.0002, L1T2/grad_norm: 0.4802, L1T2/grad_norm_clipped: 0.4802\n",
      "mode: L2T2, epoch: 0, step: 142, L2T2/loss: 0.0116, L2T2/lr: 0.0002\n",
      "mode: L2T2, epoch: 0, step: 143, L2T2/loss: 0.0120, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0089, L2T2/grad_norm_clipped: 0.0089\n",
      "loss: tensor(2.2385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 144, L1T2/loss: 4.4770, L1T2/lr: 0.0002\n",
      "loss: tensor(2.3597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "mode: L1T2, epoch: 0, step: 145, L1T2/loss: 4.7195, L1T2/lr: 0.0002, L1T2/grad_norm: 0.4657, L1T2/grad_norm_clipped: 0.4657\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://miserably-adapted-crow.ngrok-free.app/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "train_L2T2_nero = False\n",
    "log_history = []\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of gradient accumulation cycle\n",
    "# This ensures there are no leftover gradients when resuming mid-cycle or after a previous cycle was interrupted\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    L1T2_optimizer.zero_grad(set_to_none=True)\n",
    "    L2T2_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Skip previously completed steps\n",
    "        if global_step <= resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        # Stop training if `max_global_steps` reached\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            model.set_eval_L2T2_nero(False)\n",
    "            model.set_train_L2T2_nero(train_L2T2_nero)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids,\n",
    "                use_cache=False, # Disable cache to avoid conflict with gradient checkpointing\n",
    "            )\n",
    "            \n",
    "            if train_L2T2_nero:\n",
    "                _, L2T2_nero_loss = outputs\n",
    "                _loss = L2T2_nero_loss\n",
    "            else:\n",
    "                _loss = outputs.loss\n",
    "                \n",
    "            loss = _loss / grad_accumulation_steps\n",
    "\n",
    "        log = {\n",
    "            'mode': 'L1T2' if not train_L2T2_nero else 'L2T2',\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "        }\n",
    "\n",
    "        # Backward pass\n",
    "        # with torch.autograd.set_detect_anomaly(True):\n",
    "        if train_L2T2_nero:\n",
    "            log['L2T2/loss'] = loss.item() * grad_accumulation_steps\n",
    "            model.set_return_L2T2_nero_loss(False)\n",
    "            L2T2_scaler.scale(loss).backward()\n",
    "        else:\n",
    "            print(\"loss:\", loss)\n",
    "            log['L1T2/loss'] = loss.item() * grad_accumulation_steps\n",
    "            L1T2_scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update parameters only at the end of gradient accumulation cycle\n",
    "        grad_norm_log = {}\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            if train_L2T2_nero:\n",
    "                # Unscale gradients before computing gradient norm and applying clipping\n",
    "                L2T2_scaler.unscale_(L2T2_optimizer)\n",
    "\n",
    "                # Compute gradient norm\n",
    "                grad_norm = compute_grad_norm(L2T2_nero_params)\n",
    "                grad_norm_log['L2T2/grad_norm'] = grad_norm\n",
    "\n",
    "                # Clip gradients\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(L2T2_nero_params, clip_grad_norm)\n",
    "                \n",
    "                # Compute clipped gradient norm\n",
    "                grad_norm_clipped = compute_grad_norm(L2T2_nero_params)\n",
    "                grad_norm_log['L2T2/grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "                # Update parameters\n",
    "                L2T2_scaler.step(L2T2_optimizer)\n",
    "                L2T2_scaler.update()\n",
    "                L2T2_scheduler.step()\n",
    "\n",
    "                # Zero gradients for the next gradient accumulation cycle\n",
    "                L2T2_optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                # Unscale gradients before computing gradient norm and applying clipping\n",
    "                L1T2_scaler.unscale_(L1T2_optimizer)\n",
    "                \n",
    "                # Compute gradient norm\n",
    "                grad_norm = compute_grad_norm(L1T2_nero_params)\n",
    "                grad_norm_log['L1T2/grad_norm'] = grad_norm\n",
    "\n",
    "                # Clip gradients\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(L1T2_nero_params, clip_grad_norm)\n",
    "                \n",
    "                # Compute clipped gradient norm\n",
    "                grad_norm_clipped = compute_grad_norm(L1T2_nero_params)\n",
    "                grad_norm_log['L1T2/grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "                # Update parameters\n",
    "                L1T2_scaler.step(L1T2_optimizer)\n",
    "                L1T2_scaler.update()\n",
    "                L1T2_scheduler.step()\n",
    "\n",
    "                # Zero gradients for the next gradient accumulation cycle\n",
    "                L1T2_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # After updating parameters, toggle training mode:\n",
    "            # if currently training L1T2, switch to L2T2\n",
    "            # if currently training L2T2, switch to L1T2\n",
    "            train_L2T2_nero = not train_L2T2_nero\n",
    "\n",
    "        # Logging\n",
    "        lr_log = (\n",
    "            {'L1T2/lr': L1T2_scheduler.get_last_lr()[0]} \n",
    "            if 'L1T2/loss' in log else \n",
    "            {'L2T2/lr': L2T2_scheduler.get_last_lr()[0]}\n",
    "        )\n",
    "        log = {\n",
    "            **log, \n",
    "            **lr_log, \n",
    "            **grad_norm_log,\n",
    "        }\n",
    "        log_history.append(log)\n",
    "        wandb.log(log)\n",
    "        print(\", \".join(\n",
    "            f\"{k}: {format_float(v)}\" if isinstance(v, float) else f\"{k}: {v}\"\n",
    "            for k, v in log.items()\n",
    "        ))\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step > 0 and global_step % checkpoint_steps == 0:\n",
    "            # Create checkpoint directory\n",
    "            checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save Nero parameters, along with optimizer, scheduler, and scaler states\n",
    "            L1T2_nero_state_dict = {n: p.detach().cpu() for n, p in model.named_parameters() if 'L1T2' in n}\n",
    "            save_file(L1T2_nero_state_dict, os.path.join(checkpoint_dir, 'L1T2_nero.safetensors'))\n",
    "            torch.save(L1T2_optimizer.state_dict(), os.path.join(checkpoint_dir, 'L1T2_optimizer.pt'))\n",
    "            torch.save(L1T2_scheduler.state_dict(), os.path.join(checkpoint_dir, 'L1T2_scheduler.pt'))\n",
    "            torch.save(L1T2_scaler.state_dict(), os.path.join(checkpoint_dir, 'L1T2_scaler.pt'))\n",
    "\n",
    "            L2T2_nero_state_dict = {n: p.detach().cpu() for n, p in model.named_parameters() if 'L2T2' in n}\n",
    "            save_file(L2T2_nero_state_dict, os.path.join(checkpoint_dir, 'L2T2_nero.safetensors'))\n",
    "            torch.save(L2T2_optimizer.state_dict(), os.path.join(checkpoint_dir, 'L2T2_optimizer.pt'))\n",
    "            torch.save(L2T2_scheduler.state_dict(), os.path.join(checkpoint_dir, 'L2T2_scheduler.pt'))\n",
    "            torch.save(L2T2_scaler.state_dict(), os.path.join(checkpoint_dir, 'L2T2_scaler.pt'))\n",
    "\n",
    "            # Save trainer state for resuming training\n",
    "            trainer_state = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'log_history': log_history,\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'w') as f:\n",
    "                json.dump(trainer_state, f, indent=2)\n",
    "\n",
    "            # Save RNG state for reproducibility\n",
    "            rng_state = {\n",
    "                'python': random.getstate(),\n",
    "                'numpy': np.random.get_state(),\n",
    "                'cpu': torch.get_rng_state(),\n",
    "                'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else [],\n",
    "            }\n",
    "            torch.save(rng_state, os.path.join(checkpoint_dir, 'rng_state.pth'))\n",
    "\n",
    "            # Upload checkpoint directory to Hugging Face repository\n",
    "            if push_to_hf:\n",
    "                upload_folder(\n",
    "                    folder_path=checkpoint_dir,\n",
    "                    repo_id=hf_nero_id,\n",
    "                    path_in_repo=f\"checkpoint-{global_step}\",\n",
    "                    commit_message=f\"Add checkpoint at step {global_step}\",\n",
    "                    repo_type='model',\n",
    "                )\n",
    "        \n",
    "        # Check generated text every `generate_steps`\n",
    "        if global_step > 0 and global_step % generate_steps == 0:\n",
    "            print()\n",
    "            \n",
    "            model.set_train_L2T2_nero(False)\n",
    "            model.set_eval_L2T2_nero(False)\n",
    "            generated = generate_text(model, tokenizer, L1T2_sample_prompt, device=device)\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT (L1)\")\n",
    "            print(\"================================\")\n",
    "            print(generated)\n",
    "            print()\n",
    "            \n",
    "            model.set_train_L2T2_nero(False)\n",
    "            model.set_eval_L2T2_nero(True)\n",
    "            generated = generate_text(model, tokenizer, L2T2_sample_prompt, device=device)\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT (L2)\")\n",
    "            print(\"================================\")\n",
    "            print(generated)\n",
    "            print()\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
