{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "!fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://miserably-adapted-crow.ngrok-free.app/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from typing import Optional\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import default_data_collator, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download, create_repo, upload_folder\n",
    "from safetensors.torch import load_file, save_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(\n",
    "        repo_id: str, \n",
    "        checkpoint: Optional[int], \n",
    "        max_checkpoints: str = 10_000, \n",
    "        checkpoint_steps: str = 25,\n",
    "    ):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_steps) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = None\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_dir = os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir, checkpoint_dir\n",
    "\n",
    "def check_loss_and_grad_norm(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt=\"Paris is the capital of\",\n",
    "):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "    print(\"Loss:\", outputs.loss)\n",
    "\n",
    "    # Backward pass\n",
    "    if outputs.loss.grad_fn is None:\n",
    "        print(\"Gradient norm:\", None)\n",
    "        return\n",
    "\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "\n",
    "def check_parameter(n, p):\n",
    "    print(f\"- {'name':<8}:\", n)\n",
    "    print(f\"- {'device':<8}:\", p.device)\n",
    "    print(f\"- {'dtype':<8}:\", p.dtype)\n",
    "    print(f\"- {'mean':<8}:\", p.mean().item())\n",
    "    print(f\"- {'min':<8}:\", p.min().item())\n",
    "    print(f\"- {'max':<8}:\", p.max().item())\n",
    "\n",
    "def check_lora_parameters(model, prefix=None):\n",
    "    prefix = prefix + '_lora' if prefix != None else 'lora'\n",
    "    for n, p in model.named_parameters():\n",
    "        if prefix in n:\n",
    "            check_parameter(n, p)\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=32, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def get_task_and_lang_from_repo_id(repo_id: str):\n",
    "    task, lang, _ = repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "    return task, lang\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id: str,\n",
    "    train_size: int = 5000,\n",
    "    test_size: int = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang = get_task_and_lang_from_repo_id(lora_repo_id)\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    \n",
    "    # Load dataset using streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def compute_grad_norm(params):\n",
    "    grad_norm = 0.0\n",
    "    for p in params:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def compute_named_grad_norm(named_params):\n",
    "    grad_norm = 0.0\n",
    "    for n, p in named_params.items():\n",
    "        if p.grad is not None:\n",
    "            p_grad_norm = p.grad.data.norm(2)\n",
    "            grad_norm += p_grad_norm.item() ** 2\n",
    "        else:\n",
    "            print(f\"[WARN] No gradient for {n}\")\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def format_float(v):\n",
    "    if abs(v) < 0.0001 or abs(v) >= 10000:\n",
    "        return f\"{v:.4e}\"\n",
    "    else:\n",
    "        return f\"{v:.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d01a07d7a7411c8faa72b02e887649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b7a0a17b234218abdf588b07db174a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c34e154dee44d8ad0df5d1db5a9108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations:\n",
      "- L1T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650/checkpoint-650/adapter_model.safetensors\n",
      "- L2T1:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629\n",
      "  - checkpoint: 650\n",
      "  - lora_dir  : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650\n",
      "  - lora_path : L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629/checkpoint-650/adapter_model.safetensors\n",
      "- L1T2:\n",
      "  - hf_lora_id: alxxtexxr/L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457\n",
      "  - checkpoint: 1875\n",
      "  - lora_dir  : L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457/checkpoint-1875\n",
      "  - lora_path : L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457/checkpoint-1875/adapter_model.safetensors\n",
      "\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "device = 'auto'\n",
    "\n",
    "# Data configuration\n",
    "hf_data_id = 'alxxtexxr/Nero-XLT-Dataset'\n",
    "hf_data_dir = 'gsm8k_en_5K_1K_1K_512'\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 3.0\n",
    "lr = 2e-4\n",
    "warmup_ratio = 0.03\n",
    "# num_warmup_steps = 100\n",
    "checkpoint_steps = 50\n",
    "generate_steps = 50\n",
    "sample_prompt = '日本は'\n",
    "\n",
    "# Model configurations\n",
    "model_configs = {\n",
    "    # L1T1 (Source Language - Source Task)\n",
    "    'L1T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L2T1 (Target Language - Source Task)\n",
    "    'L2T1': {\n",
    "        'hf_lora_id': 'alxxtexxr/L3.1-8B-wikipedia-ja-5K-LoRA-v20250728141629',\n",
    "        'checkpoint': 650,\n",
    "    },\n",
    "\n",
    "    # L1T2 (Source Language - Target Task)\n",
    "    # 'L1T2': {\n",
    "    #     'hf_lora_id': 'alxxtexxr/L3.1-8B-gsm8k-en-5K-LoRA-v20250701060457',\n",
    "    #     'checkpoint': 1875,\n",
    "    # },\n",
    "}\n",
    "\n",
    "for key, config in model_configs.items():\n",
    "    _, lora_dir = download_hf_model(config['hf_lora_id'], config['checkpoint'])\n",
    "    model_configs[key]['lora_dir'] = lora_dir\n",
    "    model_configs[key]['lora_path'] = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "    model_configs[key]['lora_config'] = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "print(\"Model configurations:\",)\n",
    "for key, config in model_configs.items():\n",
    "    print(f\"- {key}:\")\n",
    "    for config_name, config_value in config.items():\n",
    "        if config_name == 'lora_config':\n",
    "            continue\n",
    "        print(f\"{'-':>3} {config_name:<10}: {config_value}\")\n",
    "print()\n",
    "\n",
    "assert (\n",
    "    model_configs['L1T1']['lora_config'].base_model_name_or_path == \n",
    "    model_configs['L2T1']['lora_config'].base_model_name_or_path == \n",
    "    model_configs['L1T2']['lora_config'].base_model_name_or_path\n",
    "), \"Base models must be the same\"\n",
    "base_model_name = model_configs['L1T1']['lora_config'].base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating Nero directory: L3.1-8B-gsm8k-en-5K-1K-1K-512K-Nero-v20250814024441\n",
      "[INFO] Nero directory created!\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face configuration\n",
    "hf_nero_id = None\n",
    "resume_step = 0\n",
    "\n",
    "if hf_nero_id is not None and resume_step > 0:\n",
    "    print(f\"[INFO] Downloading Nero checkpoint at step {resume_step} from Hugging Face repository:\", hf_nero_id)\n",
    "    nero_dir, _ = download_hf_model(hf_nero_id, resume_step)\n",
    "    print(f\"[INFO] Nero checkpoint downloaded successfully!\")\n",
    "else:\n",
    "    hf_username = 'alxxtexxr'\n",
    "    nero_dir = f'L3.1-8B-{hf_data_dir.replace(\"_\", \"-\")}K-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "    print(f\"[INFO] Creating Nero directory:\", nero_dir)\n",
    "    hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "    os.makedirs(nero_dir, exist_ok=True)\n",
    "    print(f\"[INFO] Nero directory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer,\n",
    "                 \n",
    "                 # LoRA parameters\n",
    "                 L1T1_lora_rank, L1T1_lora_alpha, L1T1_lora_dropout, L1T1_lora_bias, L1T1_lora_use_rslora,\n",
    "                 L2T1_lora_rank, L2T1_lora_alpha, L2T1_lora_dropout, L2T1_lora_bias, L2T1_lora_use_rslora,\n",
    "\n",
    "                 # Nero parameters\n",
    "                 nero_rank, nero_alpha, nero_bias, # nero_dropout, nero_use_rslora,\n",
    "                 train_L2T2_nero = False,\n",
    "                 return_L2T2_nero_loss = False,\n",
    "                 \n",
    "                 # Distance function parameters  \n",
    "                 distance_fn='euclidean',\n",
    "                 distance_fn_pairwise=True,\n",
    "                 \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.train_L2T2_nero = train_L2T2_nero\n",
    "        self.return_L2T2_nero_loss = return_L2T2_nero_loss\n",
    "        \n",
    "        self.distance_fn = distance_fn\n",
    "        self.distance_fn_pairwise = distance_fn_pairwise\n",
    "\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "\n",
    "        # ================================================================================================================================\n",
    "        # L1T1 LoRA\n",
    "        # ================================================================================================================================\n",
    "        self.L1T1_lora_alpha = L1T1_lora_alpha\n",
    "        self.L1T1_lora_bias = L1T1_lora_bias\n",
    "        self.L1T1_lora_scaling = L1T1_lora_alpha / math.sqrt(L1T1_lora_rank) if L1T1_lora_use_rslora else L1T1_lora_alpha / L1T1_lora_rank\n",
    "        self.L1T1_lora_dropout = nn.Dropout(L1T1_lora_dropout) if L1T1_lora_dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # LoRA decomposition\n",
    "        self.L1T1_lora_A = nn.Linear(in_features, L1T1_lora_rank, bias=L1T1_lora_bias).to(self.device)\n",
    "        self.L1T1_lora_B = nn.Linear(L1T1_lora_rank, out_features, bias=L1T1_lora_bias).to(self.device)\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        L1T1_lora_std = 1 / torch.sqrt(torch.tensor(L1T1_lora_rank).float())\n",
    "        nn.init.normal_(self.L1T1_lora_A.weight, mean=0.0, std=L1T1_lora_std)\n",
    "        nn.init.zeros_(self.L1T1_lora_B.weight) \n",
    "\n",
    "        # ================================================================================================================================\n",
    "        # L2T1 LoRA\n",
    "        # ================================================================================================================================\n",
    "        self.L2T1_lora_alpha = L2T1_lora_alpha\n",
    "        self.L2T1_lora_bias = L2T1_lora_bias\n",
    "        self.L2T1_lora_scaling = L2T1_lora_alpha / math.sqrt(L2T1_lora_rank) if L2T1_lora_use_rslora else L2T1_lora_alpha / L2T1_lora_rank\n",
    "        self.L2T1_lora_dropout = nn.Dropout(L2T1_lora_dropout) if L2T1_lora_dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # LoRA decomposition\n",
    "        self.L2T1_lora_A = nn.Linear(in_features, L2T1_lora_rank, bias=L2T1_lora_bias).to(self.device)\n",
    "        self.L2T1_lora_B = nn.Linear(L2T1_lora_rank, out_features, bias=L2T1_lora_bias).to(self.device)\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        L2T1_lora_std = 1 / torch.sqrt(torch.tensor(L2T1_lora_rank).float())\n",
    "        nn.init.normal_(self.L2T1_lora_A.weight, mean=0.0, std=L2T1_lora_std)\n",
    "        nn.init.zeros_(self.L2T1_lora_B.weight) \n",
    "        \n",
    "        # ================================================================================================================================\n",
    "        # L1T2 Nero\n",
    "        # ================================================================================================================================\n",
    "        self.nero_alpha = nero_alpha\n",
    "        self.nero_bias = nero_bias\n",
    "        # self.nero_scaling = nero_alpha / math.sqrt(nero_rank) if nero_use_rslora else nero_alpha / nero_rank\n",
    "        # self.nero_dropout = nn.Dropout(nero_dropout) if nero_dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # Nero decomposition like LoRA\n",
    "        self.L1T2_nero_A = nn.Linear(out_features, nero_rank, bias=nero_bias).to(self.device)\n",
    "        self.L1T2_nero_B = nn.Linear(nero_rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices like LoRA: A ~ N(0, 1/rank), B initialized to 0\n",
    "        nero_std = 1 / torch.sqrt(torch.tensor(nero_rank).float())\n",
    "        nn.init.normal_(self.L1T2_nero_A.weight, mean=0.0, std=nero_std)\n",
    "        nn.init.zeros_(self.L1T2_nero_B.weight)\n",
    "\n",
    "        # ================================================================================================================================\n",
    "        # L1T2 Nero\n",
    "        # ================================================================================================================================\n",
    "        # Nero decomposition like LoRA\n",
    "        self.L2T2_nero_A = nn.Linear(out_features, nero_rank, bias=nero_bias).to(self.device)\n",
    "        self.L2T2_nero_B = nn.Linear(nero_rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices like LoRA: A ~ N(0, 1/rank), B initialized to 0\n",
    "        nn.init.normal_(self.L2T2_nero_A.weight, mean=0.0, std=nero_std)\n",
    "        nn.init.zeros_(self.L2T2_nero_B.weight)\n",
    "    \n",
    "    def _distance_fn(self, a, b, huber_delta=1.0, huber_weight=1.0, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Compute distances between two tensors a and b.\n",
    "\n",
    "        Args:\n",
    "            a: [batch_size_a, seq_len, hidden_dim]\n",
    "            b: [batch_size_b, seq_len, hidden_dim]\n",
    "            huber_delta: delta for Huber distance\n",
    "            eps: small constant for numerical stability\n",
    "\n",
    "        Returns:\n",
    "            Tensor of distances:\n",
    "            - If self.distance_fn_pairwise=False: [batch_size_a, seq_len] (elementwise)\n",
    "            - If self.distance_fn_pairwise=True:  [batch_size_a, batch_size_b, seq_len] (pairwise)\n",
    "        \"\"\"\n",
    "        def huber_loss(diff, delta):\n",
    "            abs_diff = diff.abs()\n",
    "            mask = abs_diff < delta\n",
    "            return torch.where(mask, 0.5 * diff**2, delta * (abs_diff - 0.5 * delta))\n",
    "\n",
    "        if self.distance_fn_pairwise:\n",
    "            # Expand for pairwise computation\n",
    "            a_exp = a.unsqueeze(1)  # [batch_size_a, 1, seq_len, hidden_dim]\n",
    "            b_exp = b.unsqueeze(0)  # [1, batch_size_b, seq_len, hidden_dim]\n",
    "            if self.distance_fn == 'cosine':\n",
    "                a_norm = F.normalize(a_exp, dim=-1, eps=eps)\n",
    "                b_norm = F.normalize(b_exp, dim=-1, eps=eps)\n",
    "                return 1 - (a_norm * b_norm).sum(dim=-1)\n",
    "            elif self.distance_fn in ['euclidean', 'squared_euclidean']:\n",
    "                diff = a_exp - b_exp\n",
    "                dist_sq = (diff**2).sum(dim=-1)\n",
    "                if self.distance_fn == 'euclidean':\n",
    "                    return torch.sqrt(dist_sq + eps)\n",
    "                else:\n",
    "                    return dist_sq\n",
    "            elif self.distance_fn == 'huber':\n",
    "                diff = a_exp - b_exp\n",
    "                return huber_loss(diff, huber_delta).sum(dim=-1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported distance function: {self.distance_fn}\")\n",
    "        else:\n",
    "            # Elementwise distances\n",
    "            if self.distance_fn == 'cosine':\n",
    "                a_norm = F.normalize(a, dim=-1, eps=eps)\n",
    "                b_norm = F.normalize(b, dim=-1, eps=eps)\n",
    "                return 1 - (a_norm * b_norm).sum(dim=-1)\n",
    "            elif self.distance_fn == 'euclidean':\n",
    "                return torch.norm(a - b, p=2, dim=-1)\n",
    "            elif self.distance_fn == 'squared_euclidean':\n",
    "                return ((a - b)**2).sum(dim=-1)\n",
    "            elif self.distance_fn == 'huber':\n",
    "                diff = a - b\n",
    "                return huber_loss(diff, huber_delta).sum(dim=-1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported distance function: {self.distance_fn}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"================================================================\")\n",
    "        # print(self.module_name)\n",
    "        # print(\"================================================================\")\n",
    "\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            assert self.L1T1_lora_A.weight.dtype == self.L2T1_lora_A.weight.dtype\n",
    "            x = x.to(self.L1T1_lora_A.weight.dtype)\n",
    "\n",
    "        # L1T1_lora_out = self.L1T1_lora_B(self.L1T1_lora_A(self.L1T1_lora_dropout(x))) #* self.L1T1_lora_scaling\n",
    "        L1T1_lora_dropout_out = self.L1T1_lora_dropout(x)\n",
    "        L1T1_lora_A_out = self.L1T1_lora_A(L1T1_lora_dropout_out)\n",
    "        L1T1_lora_B_out = self.L1T1_lora_B(L1T1_lora_A_out)\n",
    "        # L1T1_lora_out = L1T1_lora_B_out * self.L1T1_lora_scaling\n",
    "        L1T1_lora_out = L1T1_lora_B_out\n",
    "\n",
    "        # L1T2_nero_out = self.L1T2_nero_B(F.relu(self.L1T2_nero_A(L1T1_lora_out)))\n",
    "        L1T2_nero_A_out = self.L1T2_nero_A(L1T1_lora_out)\n",
    "        L1T2_nero_relu_out = F.relu(L1T2_nero_A_out)\n",
    "        L1T2_nero_B_out = self.L1T2_nero_B(L1T2_nero_relu_out)\n",
    "        L1T2_nero_out = L1T2_nero_B_out * self.L1T1_lora_scaling\n",
    "        if requires_conversion:\n",
    "            L1T2_nero_out = L1T2_nero_out.to(base_out.dtype)\n",
    "        output = base_out + L1T2_nero_out\n",
    "\n",
    "        if self.train_L2T2_nero:\n",
    "            # L2T1_lora_out = self.L2T1_lora_B(self.L2T1_lora_A(self.L2T1_lora_dropout(x))) #* self.L2T1_lora_scaling\n",
    "            L2T1_lora_dropout_out = self.L2T1_lora_dropout(x)\n",
    "            L2T1_lora_A_out = self.L2T1_lora_A(L2T1_lora_dropout_out)\n",
    "            L2T1_lora_B_out = self.L2T1_lora_B(L2T1_lora_A_out)\n",
    "            # L2T1_lora_out = L2T1_lora_B_out * self.L2T1_lora_scaling\n",
    "            L2T1_lora_out = L2T1_lora_B_out\n",
    "\n",
    "            # L2T2_nero_out = self.L2T2_nero_B(F.relu(self.L2T2_nero_A(L2T1_lora_out)))\n",
    "            L2T2_nero_A_out = self.L2T2_nero_A(L2T1_lora_out)\n",
    "            L2T2_nero_relu_out = F.relu(L2T2_nero_A_out)\n",
    "            L2T2_nero_B_out = self.L2T2_nero_B(L2T2_nero_relu_out)\n",
    "            L2T2_nero_out = L2T2_nero_B_out * self.L1T1_lora_scaling\n",
    "            if requires_conversion:\n",
    "                L2T2_nero_out = L2T2_nero_out.to(base_out.dtype)\n",
    "\n",
    "            # ================================================================\n",
    "            # LOSS\n",
    "            # ================================================================\n",
    "            # Language distances: distance(L1T1, L2T1), distance(L1T2, L2T2)\n",
    "            LxT1_distance = self._distance_fn(L1T1_lora_out.float(), L2T1_lora_out.float())\n",
    "            LxT2_distance = self._distance_fn(L1T2_nero_out.float(), L2T2_nero_out.float()) # requires grad\n",
    "\n",
    "            # Task distances: distance(L1T1, L1T2), distance(L2T1, L2T2)\n",
    "            L1Tx_distance = self._distance_fn(L1T1_lora_out.float(), L1T2_nero_out.float())\n",
    "            L2Tx_distance = self._distance_fn(L2T1_lora_out.float(), L2T2_nero_out.float()) # requires grad\n",
    "\n",
    "            # Distance constraints:\n",
    "            # distance(L1T1, L2T1) ~= distance(L1T2, L2T2)\n",
    "            # distance(L1T1, L1T2) ~= distance(L2T1, L2T2)\n",
    "            L2T2_nero_loss = F.mse_loss(LxT2_distance, LxT1_distance) + F.mse_loss(L2Tx_distance, L1Tx_distance)\n",
    "            \n",
    "            if self.return_L2T2_nero_loss:\n",
    "                return output, L2T2_nero_loss\n",
    "\n",
    "        return output\n",
    "\n",
    "    def load_L1T1_lora_params(self, state_dict, prefix):\n",
    "        self.L1T1_lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.L1T1_lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.L1T1_lora_bias:\n",
    "            self.L1T1_lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.L1T1_lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "    def load_L2T1_lora_params(self, state_dict, prefix):\n",
    "        self.L2T1_lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.L2T1_lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.L2T1_lora_bias:\n",
    "            self.L2T1_lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.L2T1_lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "\n",
    "    def load_L1T2_nero_params(self, state_dict, prefix):\n",
    "        self.L1T2_nero_A.weight.data = state_dict[f'{prefix}.L1T2_nero_A.weight'].to(self.device)\n",
    "        self.L1T2_nero_B.weight.data = state_dict[f'{prefix}.L1T2_nero_B.weight'].to(self.device)\n",
    "        if self.nero_bias:\n",
    "            self.L1T2_nero_A.bias.data = state_dict[f'{prefix}.L1T2_nero_A.bias'].to(self.device)\n",
    "            self.L1T2_nero_B.bias.data = state_dict[f'{prefix}.L1T2_nero_B.bias'].to(self.device)\n",
    "    \n",
    "    def load_L2T2_nero_params(self, state_dict, prefix):\n",
    "        self.L2T2_nero_A.weight.data = state_dict[f'{prefix}.L2T2_nero_A.weight'].to(self.device)\n",
    "        self.L2T2_nero_B.weight.data = state_dict[f'{prefix}.L2T2_nero_B.weight'].to(self.device)\n",
    "        if self.nero_bias:\n",
    "            self.L2T2_nero_A.bias.data = state_dict[f'{prefix}.L2T2_nero_A.bias'].to(self.device)\n",
    "            self.L2T2_nero_B.bias.data = state_dict[f'{prefix}.L2T2_nero_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 base_model: nn.Module, \n",
    "                 L1T1_lora_config: LoraConfig, \n",
    "                 L2T1_lora_config: LoraConfig, \n",
    "                 train_L2T2_nero=False,\n",
    "                 return_L2T2_nero_loss=False,\n",
    "                 distance_fn='euclidean',\n",
    "                 distance_fn_pairwise=True,\n",
    "                 debug: bool=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.train_L2T2_nero = train_L2T2_nero\n",
    "        self.return_L2T2_nero_loss = return_L2T2_nero_loss\n",
    "        \n",
    "        self.distance_fn = distance_fn\n",
    "        self.distance_fn_pairwise = distance_fn_pairwise\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self._wrap_target_layers(L1T1_lora_config, L2T1_lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, L1T1_lora_config, L2T1_lora_config):\n",
    "        assert L1T1_lora_config.target_modules == L2T1_lora_config.target_modules, \"[ERROR] L1T1 and L2T1 LoRA configurations must have the same target modules.\"\n",
    "\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in L1T1_lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    base_layer=module,\n",
    "                    train_L2T2_nero=self.train_L2T2_nero,\n",
    "                    return_L2T2_nero_loss=self.return_L2T2_nero_loss,\n",
    "\n",
    "                    # L1T1 LoRA parameters\n",
    "                    L1T1_lora_rank=L1T1_lora_config.r, \n",
    "                    L1T1_lora_alpha=L1T1_lora_config.lora_alpha, \n",
    "                    L1T1_lora_dropout=L1T1_lora_config.lora_dropout,\n",
    "                    L1T1_lora_bias=L1T1_lora_config.lora_bias,\n",
    "                    L1T1_lora_use_rslora=L1T1_lora_config.use_rslora,\n",
    "\n",
    "                    # L2T1 LoRA parameters\n",
    "                    L2T1_lora_rank=L2T1_lora_config.r, \n",
    "                    L2T1_lora_alpha=L2T1_lora_config.lora_alpha, \n",
    "                    L2T1_lora_dropout=L2T1_lora_config.lora_dropout,\n",
    "                    L2T1_lora_bias=L2T1_lora_config.lora_bias,\n",
    "                    L2T1_lora_use_rslora=L2T1_lora_config.use_rslora,\n",
    "\n",
    "                    # Nero parameters (for temporary, use L1T1 LoRA parameters)\n",
    "                    nero_rank=L1T1_lora_config.r, \n",
    "                    nero_alpha=L1T1_lora_config.lora_alpha, \n",
    "                    nero_bias=L1T1_lora_config.lora_bias,\n",
    "                    # nero_dropout=L1T1_lora_config.lora_dropout, \n",
    "\n",
    "                    # Distance function parameters\n",
    "                    distance_fn=self.distance_fn,\n",
    "                    distance_fn_pairwise=self.distance_fn_pairwise,\n",
    "\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_train_L2T2_nero(self, train_L2T2_nero: bool, verbose: bool=False):\n",
    "        self.train_L2T2_nero = train_L2T2_nero\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.train_L2T2_nero = train_L2T2_nero\n",
    "        \n",
    "        if train_L2T2_nero:\n",
    "            self.set_return_L2T2_nero_loss(True)\n",
    "            self.freeze_all_except_L2T2_nero()\n",
    "        else:\n",
    "            self.set_return_L2T2_nero_loss(False)\n",
    "            self.freeze_all_except_L1T2_nero()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Learn L2T2 Nero set to '{train_L2T2_nero}'.\")\n",
    "    \n",
    "    def set_return_L2T2_nero_loss(self, return_L2T2_nero_loss: bool, verbose: bool=False):\n",
    "        self.return_L2T2_nero_loss = return_L2T2_nero_loss\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_L2T2_nero_loss = return_L2T2_nero_loss\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Return L2T2 Nero loss set to '{return_L2T2_nero_loss}'.\")\n",
    "    \n",
    "    def set_distance_fn(self, distance_fn: str, pairwise: bool, verbose: bool=False):\n",
    "        self.distance_fn = distance_fn\n",
    "        self.distance_fn_pairwise = pairwise\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.distance_fn = distance_fn\n",
    "            layer.distance_fn_pairwise = pairwise\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[INFO] Distance function set to '{distance_fn}' with pairwise to {pairwise}.\")\n",
    "    \n",
    "    def freeze_all(self, verbose=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen!\")\n",
    "\n",
    "    def freeze_all_except_L1T2_nero(self, verbose=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'L1T2_nero_A' in param_name or 'L1T2_nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen except L1T2 Nero layers!\")\n",
    "    \n",
    "    def freeze_all_except_L2T2_nero(self, verbose=False):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'L2T2_nero_A' in param_name or 'L2T2_nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"[INFO] All layers are frozen except L2T2 Nero layers!\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        print(\"[INFO] All layers are unfrozen!\")\n",
    "    \n",
    "    def load_L1T1_lora_params(self, L1T1_lora_path):\n",
    "        if not os.path.exists(L1T1_lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] L1T1 LoRA file not found:\", L1T1_lora_path)\n",
    "        \n",
    "        if L1T1_lora_path.endswith('.bin'):\n",
    "            state_dict = torch.load(L1T1_lora_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(L1T1_lora_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_L1T1_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] L1T1 LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def load_L2T1_lora_params(self, L2T1_lora_path):\n",
    "        if not os.path.exists(L2T1_lora_path):\n",
    "            raise FileNotFoundError(\"[ERROR] L2T1 LoRA file not found:\", L2T1_lora_path)\n",
    "        \n",
    "        if L2T1_lora_path.endswith('.bin'):\n",
    "            state_dict = torch.load(L2T1_lora_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(L2T1_lora_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_L2T1_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] L2T1 LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def load_L1T2_nero_params(self, L1T2_nero_path):\n",
    "        if not os.path.exists(L1T2_nero_path):\n",
    "            raise FileNotFoundError(\"[ERROR] L1T2_Nero file not found:\", L1T2_nero_path)\n",
    "        \n",
    "        if L1T2_nero_path.endswith('.bin'):\n",
    "            state_dict = torch.load(L1T2_nero_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(L1T2_nero_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.L1T2_nero_A.weight' in state_dict and f'{nero_layer_name}.L1T2_nero_B.weight' in state_dict:\n",
    "                nero_layer.load_L1T2_nero_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] L1T2 Nero parameters loaded successfully!\")\n",
    "    \n",
    "    def load_L2T2_nero_params(self, L2T2_nero_path):\n",
    "        if not os.path.exists(L2T2_nero_path):\n",
    "            raise FileNotFoundError(\"[ERROR] L2T2_Nero file not found:\", L2T2_nero_path)\n",
    "        \n",
    "        if L2T2_nero_path.endswith('.bin'):\n",
    "            state_dict = torch.load(L2T2_nero_path, map_location='cpu')\n",
    "        else:\n",
    "            state_dict = load_file(L2T2_nero_path) # assuming .safetensors\n",
    "\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.L2T2_nero_A.weight' in state_dict and f'{nero_layer_name}.L2T2_nero_B.weight' in state_dict:\n",
    "                nero_layer.load_L2T2_nero_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"[INFO] L2T2 Nero parameters loaded successfully!\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self.train_L2T2_nero:\n",
    "            L2T2_nero_losses = []\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                assert isinstance(_out, tuple) and len(_out) == 2\n",
    "                layer_out, L2T2_nero_layer_loss = _out\n",
    "                L2T2_nero_losses.append(L2T2_nero_layer_loss)\n",
    "                return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract hidden_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                outputs = self.base_model(*args, **kwargs)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            L2T2_nero_loss = torch.stack(L2T2_nero_losses).mean()\n",
    "            \n",
    "            if self.return_L2T2_nero_loss:\n",
    "                return outputs, L2T2_nero_loss\n",
    "            else:\n",
    "                return outputs\n",
    "        \n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, \n",
    "    device_map=device,\n",
    ")\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    L1T1_lora_config=model_configs['L1T1']['lora_config'], \n",
    "    L2T1_lora_config=model_configs['L2T1']['lora_config'], \n",
    "    train_L2T2_nero=False,\n",
    "    return_L2T2_nero_loss=False,\n",
    "    distance_fn='euclidean',\n",
    "    distance_fn_pairwise=True,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nero_model.set_mode('teacher')\n",
    "\n",
    "# for param in nero_model.base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "        \n",
    "# for nero_layer in nero_model.nero_layers.values():\n",
    "#     for param_name, param in nero_layer.named_parameters():\n",
    "#         if 'teacher_lora_A' in param_name or 'teacher_lora_B' in param_name:\n",
    "#             param.requires_grad = True\n",
    "#         else:\n",
    "#             param.requires_grad = False\n",
    "\n",
    "# check_loss_and_grad_norm(nero_model, tokenizer, prompt=\"Despite the ominous clouds gathering on the horizon and the distant rumble of thunder echoing through the valley, the determined hikers,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded L1T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.L1T1_lora_A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 0.00037309923209249973\n",
      "- min     : -1.5596299171447754\n",
      "- max     : 1.5415360927581787\n",
      "\n",
      "[INFO] L1T1 LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded L1T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.L1T1_lora_A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 6.287686119321734e-05\n",
      "- min     : -0.04176201671361923\n",
      "- max     : 0.04242725297808647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded L1T1 LoRA parameters:\")\n",
    "check_lora_parameters(nero_model, prefix='L1T1')\n",
    "print()\n",
    "\n",
    "nero_model.load_L1T1_lora_params(model_configs['L1T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L1T1 LoRA parameters:\")\n",
    "check_lora_parameters(nero_model, prefix='L1T1')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check unloaded L2T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.L2T1_lora_A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : -0.002967829117551446\n",
      "- min     : -1.4062590599060059\n",
      "- max     : 1.7496132850646973\n",
      "\n",
      "[INFO] L2T1 LoRA parameters loaded successfully!\n",
      "\n",
      "Check loaded L2T1 LoRA parameters:\n",
      "- name    : base_model.model.layers.0.self_attn.q_proj.L2T1_lora_A.weight\n",
      "- device  : cuda:0\n",
      "- dtype   : torch.float32\n",
      "- mean    : 2.2152138626552187e-05\n",
      "- min     : -0.06327299773693085\n",
      "- max     : 0.0625513345003128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Check unloaded L2T1 LoRA parameters:\")\n",
    "check_lora_parameters(nero_model, prefix='L2T1')\n",
    "print()\n",
    "\n",
    "nero_model.load_L2T1_lora_params(model_configs['L2T1']['lora_path'])\n",
    "print()\n",
    "\n",
    "print(\"Check loaded L2T1 LoRA parameters:\")\n",
    "check_lora_parameters(nero_model, prefix='L2T1')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nero_model.freeze_all_except_L1T2_nero()\n",
    "print()\n",
    "\n",
    "# check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gradient checkpointing enabled!\n",
      "\n",
      "Loss: tensor(3.4235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Gradient norm: 5.486812315546735\n"
     ]
    }
   ],
   "source": [
    "nero_model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"[INFO] Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(hf_data_id, data_dir=hf_data_dir)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total batches: 1250\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "print(\"[INFO] Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask):\n",
      "(torch.Size([4, 512]), torch.Size([4, 512]))\n",
      "\n",
      "First batch text:\n",
      "### Instruction:\n",
      "Solve the following math problem step by step.\n",
      "\n",
      "### Question: \n",
      "Suraya picked 12 app ...\n",
      "\n",
      "Loss: tensor(3.4235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Gradient norm: 5.486812315546735\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2594, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "# Set model to learn L2T2 Nero\n",
    "nero_model.set_train_L2T2_nero(True)\n",
    "nero_model.set_distance_fn('euclidean', pairwise=True)\n",
    "nero_model.train()\n",
    "\n",
    "# Forward pass\n",
    "device = next(nero_model.parameters()).device\n",
    "# prompts = [\n",
    "#     \"The capital city of Japan is\", \n",
    "#     \"The capital city of Indonesia is\",\n",
    "# ]\n",
    "# inputs = tokenizer(\n",
    "#     prompts,\n",
    "#     return_tensors='pt',\n",
    "#     padding=True,\n",
    "# )\n",
    "outputs, L2T2_nero_outs = nero_model(\n",
    "    # Test with first batch\n",
    "    input_ids=first_batch['input_ids'].to(device),\n",
    "    attention_mask=first_batch['attention_mask'].to(device),\n",
    "    labels=first_batch['input_ids'].to(device),\n",
    "\n",
    "    # Test with sample prompts\n",
    "    # input_ids=inputs['input_ids'].to(device),\n",
    "    # attention_mask=inputs['attention_mask'].to(device),\n",
    "    # labels=inputs['input_ids'].to(device),\n",
    "    \n",
    "    use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    ")\n",
    "\n",
    "print(L2T2_nero_outs)\n",
    "\n",
    "# pairwise euclidean: 0.2222\n",
    "# non-pairwise euclidean: 0.2167\n",
    "\n",
    "# pairwise cosine: 0.0022\n",
    "# non-pairwise cosine: 0.0024\n",
    "\n",
    "# pairwise squared euclidean: 33.0331\n",
    "# non-pairwise squared euclidean: 33.1182\n",
    "\n",
    "# pairwise huber: 6.0343\n",
    "# non-pairwise huber: 6.2125\n",
    "\n",
    "# pairwise cosine huber: 0.0640\n",
    "# non-pairwise cosine huber: 0.0663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1039015169.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_L2T2 = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipython-input-1039015169.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250814_024519-w458nyvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/w458nyvr' target=\"_blank\">wise-wildflower-157</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/w458nyvr' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/w458nyvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the distance function\n",
    "nero_model.set_distance_fn('euclidean', pairwise=True)\n",
    "\n",
    "# Set the model to training mode\n",
    "nero_model.train()\n",
    "\n",
    "# If `device` is not specified or set to 'auto', use the model's device\n",
    "# if device is None or device == 'auto':\n",
    "device = next(iter(nero_model.parameters())).device\n",
    "\n",
    "# Set up optimizer and gradient scaler\n",
    "nero_model.set_train_L2T2_nero(True)\n",
    "# nero_params_L2T2 = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "nero_named_params_L2T2 = {n: p for n, p in nero_model.named_parameters() if p.requires_grad}\n",
    "optimizer_L2T2 = torch.optim.Adam(nero_named_params_L2T2.values(), lr=lr, foreach=False)\n",
    "scaler_L2T2 = torch.cuda.amp.GradScaler()\n",
    "\n",
    "nero_model.set_train_L2T2_nero(False)\n",
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr, foreach=False)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Set up LR scheduler\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "if warmup_ratio > 0:\n",
    "# if num_warmup_steps > 0:\n",
    "    # If `warmup_ratio` > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup # type: ignore\n",
    "    max_optimizer_steps = max_global_steps // grad_accumulation_steps\n",
    "    num_warmup_steps = int(warmup_ratio * max_optimizer_steps)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "    scheduler_L2T2 = get_cosine_schedule_with_warmup(\n",
    "        optimizer_L2T2,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=max_optimizer_steps,\n",
    "\n",
    "        # Cycle length is 2000 steps or less, but at least 1 cycle, and at most 10 cycles\n",
    "        num_cycles=min(max(1, max_optimizer_steps // 2000), 10), \n",
    "    )\n",
    "else:\n",
    "    # If `warmup_ratio` is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 1.0)\n",
    "    scheduler_L2T2 = LambdaLR(optimizer_L2T2, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True, # End previous run and start a new one\n",
    "    config=dict(\n",
    "        # Project configuration\n",
    "        seed = seed,\n",
    "        # target_lang=target_lang,\n",
    "        # target_task=target_task,\n",
    "        device = device,\n",
    "\n",
    "        # Data configuration\n",
    "        # train_size = train_size,\n",
    "        # test_size = test_size,\n",
    "        # max_seq_length = max_seq_length,\n",
    "\n",
    "        # Training configuration\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        resume_step = resume_step,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "        lr = lr,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        # num_warmup_steps = num_warmup_steps,\n",
    "        checkpoint_steps = checkpoint_steps,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "global_step = resume_step\n",
    "start_epoch = 0\n",
    "\n",
    "if resume_step > 0:\n",
    "    checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{resume_step}')\n",
    "    print(f\"[INFO] Resuming training from checkpoint directory:\", checkpoint_dir)\n",
    "\n",
    "    # Load Nero parameters\n",
    "    nero_path = os.path.join(checkpoint_dir, 'adapter_model.safetensors')\n",
    "    nero_model.load_nero_params(nero_path)\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_dir, 'optimizer.pt')\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "    \n",
    "    # Move optimizer state to the correct device\n",
    "    for param in optimizer.state:\n",
    "        param_device = param.device\n",
    "        param_dtype = param.dtype\n",
    "        for key, value in optimizer.state[param].items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                optimizer.state[param][key] = value.to(device=param_device, dtype=param_dtype)\n",
    "\n",
    "    # Load scheduler state\n",
    "    scheduler_path = os.path.join(checkpoint_dir, 'scheduler.pt')\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
    "\n",
    "    # Load scaler state\n",
    "    scaler_path = os.path.join(checkpoint_dir, 'scaler.pt')\n",
    "    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "\n",
    "    # Load trainer state\n",
    "    trainer_state_path = os.path.join(checkpoint_dir, 'trainer_state.json')\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        log_history = trainer_state.get('log_history', [])\n",
    "        start_epoch = log_history[-1]['epoch'] if log_history else 0\n",
    "        print(f\"[INFO] Resuming training from epoch {start_epoch} and step {resume_step}.\")\n",
    "\n",
    "    # Load RNG state for reproducibility\n",
    "    rng_path = os.path.join(checkpoint_dir, 'rng_state.pth')\n",
    "    if os.path.exists(rng_path):\n",
    "        rng_state = torch.load(rng_path)\n",
    "        random.setstate(rng_state['python'])\n",
    "        np.random.set_state(rng_state['numpy'])\n",
    "        torch.set_rng_state(rng_state['cpu'])\n",
    "        if torch.cuda.is_available() and rng_state['cuda']:\n",
    "            torch.cuda.set_rng_state_all(rng_state['cuda'])\n",
    "    \n",
    "    if resume_step % grad_accumulation_steps != 0:\n",
    "        print(\"[WARN] Resuming mid-gradient accumulation cycle. Make sure this is intended.\")\n",
    "else:\n",
    "    # If it's new training, create Hugging Face repository\n",
    "    print(f\"[INFO] Creating Hugging Face repository:\", hf_nero_id) # print the link instead\n",
    "    create_repo(repo_id=hf_nero_id, repo_type='model', exist_ok=True)\n",
    "    print(f\"[INFO] Hugging Face repository created successfully!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1452451506.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: L1T2, epoch: 0, step: 1, L1T2/loss: 8.1048, L1T2/lr: 3.3333e-05, L2T2/lr: 0.0000e+00, L1T2/grad_norm: 0.1440, L1T2/grad_norm_clipped: 0.1440\n",
      "target: L2T2, epoch: 0, step: 2, L2T2/loss: 0.2984, L1T2/lr: 3.3333e-05, L2T2/lr: 0.0000e+00\n",
      "target: L2T2, epoch: 0, step: 3, L2T2/loss: 0.2729, L1T2/lr: 3.3333e-05, L2T2/lr: 3.3333e-05, L2T2/grad_norm: 0.0244, L2T2/grad_norm_clipped: 0.0244\n",
      "target: L1T2, epoch: 0, step: 4, L1T2/loss: 8.4594, L1T2/lr: 3.3333e-05, L2T2/lr: 3.3333e-05\n",
      "target: L1T2, epoch: 0, step: 5, L1T2/loss: 8.3069, L1T2/lr: 6.6667e-05, L2T2/lr: 3.3333e-05, L1T2/grad_norm: 0.2683, L1T2/grad_norm_clipped: 0.2683\n",
      "target: L2T2, epoch: 0, step: 6, L2T2/loss: 0.2873, L1T2/lr: 6.6667e-05, L2T2/lr: 3.3333e-05\n",
      "target: L2T2, epoch: 0, step: 7, L2T2/loss: 0.2741, L1T2/lr: 6.6667e-05, L2T2/lr: 6.6667e-05, L2T2/grad_norm: 0.1044, L2T2/grad_norm_clipped: 0.1044\n",
      "target: L1T2, epoch: 0, step: 8, L1T2/loss: 8.6767, L1T2/lr: 6.6667e-05, L2T2/lr: 6.6667e-05\n",
      "target: L1T2, epoch: 0, step: 9, L1T2/loss: 8.6315, L1T2/lr: 0.0001, L2T2/lr: 6.6667e-05, L1T2/grad_norm: 0.2534, L1T2/grad_norm_clipped: 0.2534\n",
      "target: L2T2, epoch: 0, step: 10, L2T2/loss: 0.3053, L1T2/lr: 0.0001, L2T2/lr: 6.6667e-05\n",
      "target: L2T2, epoch: 0, step: 11, L2T2/loss: 0.4241, L1T2/lr: 0.0001, L2T2/lr: 0.0001, L2T2/grad_norm: 0.1710, L2T2/grad_norm_clipped: 0.1710\n",
      "target: L1T2, epoch: 0, step: 12, L1T2/loss: 7.0687, L1T2/lr: 0.0001, L2T2/lr: 0.0001\n",
      "target: L1T2, epoch: 0, step: 13, L1T2/loss: 8.8378, L1T2/lr: 0.0001, L2T2/lr: 0.0001, L1T2/grad_norm: 0.1678, L1T2/grad_norm_clipped: 0.1678\n",
      "target: L2T2, epoch: 0, step: 14, L2T2/loss: 0.3123, L1T2/lr: 0.0001, L2T2/lr: 0.0001\n",
      "target: L2T2, epoch: 0, step: 15, L2T2/loss: 0.3276, L1T2/lr: 0.0001, L2T2/lr: 0.0001, L2T2/grad_norm: 0.1506, L2T2/grad_norm_clipped: 0.1506\n",
      "target: L1T2, epoch: 0, step: 16, L1T2/loss: 7.7286, L1T2/lr: 0.0001, L2T2/lr: 0.0001\n",
      "target: L1T2, epoch: 0, step: 17, L1T2/loss: 7.4577, L1T2/lr: 0.0002, L2T2/lr: 0.0001, L1T2/grad_norm: 0.1530, L1T2/grad_norm_clipped: 0.1530\n",
      "target: L2T2, epoch: 0, step: 18, L2T2/loss: 0.2992, L1T2/lr: 0.0002, L2T2/lr: 0.0001\n",
      "target: L2T2, epoch: 0, step: 19, L2T2/loss: 0.2661, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L2T2/grad_norm: 0.1366, L2T2/grad_norm_clipped: 0.1366\n",
      "target: L1T2, epoch: 0, step: 20, L1T2/loss: 7.5526, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L1T2, epoch: 0, step: 21, L1T2/loss: 8.4748, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L1T2/grad_norm: 0.1462, L1T2/grad_norm_clipped: 0.1462\n",
      "target: L2T2, epoch: 0, step: 22, L2T2/loss: 0.2346, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L2T2, epoch: 0, step: 23, L2T2/loss: 0.2808, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L2T2/grad_norm: 0.1298, L2T2/grad_norm_clipped: 0.1298\n",
      "target: L1T2, epoch: 0, step: 24, L1T2/loss: 8.6870, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L1T2, epoch: 0, step: 25, L1T2/loss: 7.9648, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L1T2/grad_norm: 0.1376, L1T2/grad_norm_clipped: 0.1376\n",
      "target: L2T2, epoch: 0, step: 26, L2T2/loss: 0.2319, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L2T2, epoch: 0, step: 27, L2T2/loss: 0.2343, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L2T2/grad_norm: 0.1232, L2T2/grad_norm_clipped: 0.1232\n",
      "target: L1T2, epoch: 0, step: 28, L1T2/loss: 8.3116, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L1T2, epoch: 0, step: 29, L1T2/loss: 7.7381, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L1T2/grad_norm: 0.1348, L1T2/grad_norm_clipped: 0.1348\n",
      "target: L2T2, epoch: 0, step: 30, L2T2/loss: 0.1937, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L2T2, epoch: 0, step: 31, L2T2/loss: 0.2111, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L2T2/grad_norm: 0.1134, L2T2/grad_norm_clipped: 0.1134\n",
      "target: L1T2, epoch: 0, step: 32, L1T2/loss: 9.3790, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L1T2, epoch: 0, step: 33, L1T2/loss: 7.8636, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L1T2/grad_norm: 0.1287, L1T2/grad_norm_clipped: 0.1287\n",
      "target: L2T2, epoch: 0, step: 34, L2T2/loss: 0.1333, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L2T2, epoch: 0, step: 35, L2T2/loss: 0.1857, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0950, L2T2/grad_norm_clipped: 0.0950\n",
      "target: L1T2, epoch: 0, step: 36, L1T2/loss: 7.3495, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L1T2, epoch: 0, step: 37, L1T2/loss: 8.4639, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L1T2/grad_norm: 0.1315, L1T2/grad_norm_clipped: 0.1315\n",
      "target: L2T2, epoch: 0, step: 38, L2T2/loss: 0.1702, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L2T2, epoch: 0, step: 39, L2T2/loss: 0.1540, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0991, L2T2/grad_norm_clipped: 0.0991\n",
      "target: L1T2, epoch: 0, step: 40, L1T2/loss: 8.0918, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L1T2, epoch: 0, step: 41, L1T2/loss: 8.3505, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L1T2/grad_norm: 0.1299, L1T2/grad_norm_clipped: 0.1299\n",
      "target: L2T2, epoch: 0, step: 42, L2T2/loss: 0.1027, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L2T2, epoch: 0, step: 43, L2T2/loss: 0.1395, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0767, L2T2/grad_norm_clipped: 0.0767\n",
      "target: L1T2, epoch: 0, step: 44, L1T2/loss: 8.4914, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L1T2, epoch: 0, step: 45, L1T2/loss: 8.0890, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L1T2/grad_norm: 0.1308, L1T2/grad_norm_clipped: 0.1308\n",
      "target: L2T2, epoch: 0, step: 46, L2T2/loss: 0.1034, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L2T2, epoch: 0, step: 47, L2T2/loss: 0.0994, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L2T2/grad_norm: 0.0680, L2T2/grad_norm_clipped: 0.0680\n",
      "target: L1T2, epoch: 0, step: 48, L1T2/loss: 8.5945, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n",
      "target: L1T2, epoch: 0, step: 49, L1T2/loss: 7.6351, L1T2/lr: 0.0002, L2T2/lr: 0.0002, L1T2/grad_norm: 0.1324, L1T2/grad_norm_clipped: 0.1324\n",
      "target: L2T2, epoch: 0, step: 50, L2T2/loss: 0.1078, L1T2/lr: 0.0002, L2T2/lr: 0.0002\n"
     ]
    },
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-689d5078-08ffb899152d178717201155;d6d99ca5-2642-4bbd-ae06-dd8aa9062d6e)\n\nRepository Not Found for url: https://huggingface.co/api/models/alxxtexxr/L3.1-8B-gsm8k-en-5K-1K-1K-512K-Nero-v20250814024441/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/api/models/alxxtexxr/L3.1-8B-gsm8k-en-5K-1K-1K-512K-Nero-v20250814024441/preupload/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1452451506.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# Upload checkpoint directory to Hugging Face repository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             upload_folder(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0mfolder_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_nero_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \u001b[0;31m# Otherwise, call the function normally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0m_inner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_future_compatible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mupload_folder\u001b[0;34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001b[0m\n\u001b[1;32m   4980\u001b[0m         \u001b[0mcommit_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommit_message\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Upload folder using huggingface_hub\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4982\u001b[0;31m         commit_info = self.create_commit(\n\u001b[0m\u001b[1;32m   4983\u001b[0m             \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4984\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \u001b[0;31m# Otherwise, call the function normally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0m_inner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_future_compatible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4237\u001b[0m         \u001b[0m_warn_on_overwriting_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4239\u001b[0;31m         self.preupload_lfs_files(\n\u001b[0m\u001b[1;32m   4240\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4241\u001b[0m             \u001b[0madditions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mpreupload_lfs_files\u001b[0;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[1;32m   4463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madditions_no_upload_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4465\u001b[0;31m                 _fetch_upload_modes(\n\u001b[0m\u001b[1;32m   4466\u001b[0m                     \u001b[0madditions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditions_no_upload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4467\u001b[0m                     \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_commit_api.py\u001b[0m in \u001b[0;36m_fetch_upload_modes\u001b[0;34m(additions, repo_type, repo_id, headers, revision, endpoint, create_pr, gitignore_content)\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"create_pr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcreate_pr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         )\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0mpreupload_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_preupload_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mupload_modes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"uploadMode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreupload_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"files\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;34m\" https://huggingface.co/docs/huggingface_hub/authentication\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             )\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-689d5078-08ffb899152d178717201155;d6d99ca5-2642-4bbd-ae06-dd8aa9062d6e)\n\nRepository Not Found for url: https://huggingface.co/api/models/alxxtexxr/L3.1-8B-gsm8k-en-5K-1K-1K-512K-Nero-v20250814024441/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case."
     ]
    }
   ],
   "source": [
    "train_L2T2_nero = False\n",
    "log_history = []\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of gradient accumulation cycle\n",
    "# This ensures there are no leftover gradients when resuming mid-cycle or after a previous cycle was interrupted\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    optimizer_L2T2.zero_grad(set_to_none=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Skip previously completed steps\n",
    "        if global_step <= resume_step:\n",
    "            global_step += 1\n",
    "            continue\n",
    "\n",
    "        # Stop training if `max_global_steps` reached\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            nero_model.set_train_L2T2_nero(train_L2T2_nero)\n",
    "            \n",
    "            outputs = nero_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids,\n",
    "                use_cache=False, # Disable cache to avoid conflict with gradient checkpointing\n",
    "            )\n",
    "            \n",
    "            if train_L2T2_nero:\n",
    "                _, L2T2_nero_loss = outputs\n",
    "                _loss = L2T2_nero_loss\n",
    "            else:\n",
    "                _loss = outputs.loss\n",
    "                \n",
    "            loss = _loss / grad_accumulation_steps\n",
    "\n",
    "        log = {\n",
    "            'mode': 'L1T2' if not train_L2T2_nero else 'L2T2',\n",
    "            'epoch': epoch,\n",
    "            'step': global_step,\n",
    "        }\n",
    "\n",
    "        # Backward pass\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            if train_L2T2_nero:\n",
    "                log['L2T2/loss'] = loss.item() * grad_accumulation_steps\n",
    "                nero_model.set_return_L2T2_nero_loss(False)\n",
    "                scaler_L2T2.scale(loss).backward()\n",
    "            else:\n",
    "                log['L1T2/loss'] = loss.item() * grad_accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update parameters only at the end of gradient accumulation cycle\n",
    "        grad_norm_log = {}\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            if train_L2T2_nero:\n",
    "                # Unscale gradients before computing gradient norm and applying clipping\n",
    "                scaler_L2T2.unscale_(optimizer_L2T2)\n",
    "\n",
    "                # Compute gradient norm\n",
    "                grad_norm = compute_named_grad_norm(nero_named_params_L2T2)\n",
    "                grad_norm_log['L2T2/grad_norm'] = grad_norm\n",
    "\n",
    "                # Clip gradients\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(nero_named_params_L2T2.values(), clip_grad_norm)\n",
    "                \n",
    "                # Compute clipped gradient norm\n",
    "                grad_norm_clipped = compute_named_grad_norm(nero_named_params_L2T2)\n",
    "                grad_norm_log['L2T2/grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "                # Update parameters\n",
    "                scaler_L2T2.step(optimizer_L2T2)\n",
    "                scaler_L2T2.update()\n",
    "                scheduler_L2T2.step()\n",
    "\n",
    "                # Zero gradients for the next gradient accumulation cycle\n",
    "                optimizer_L2T2.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                # Unscale gradients before computing gradient norm and applying clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                \n",
    "                # Compute gradient norm\n",
    "                grad_norm = compute_grad_norm(nero_params)\n",
    "                grad_norm_log['L1T2/grad_norm'] = grad_norm\n",
    "\n",
    "                # Clip gradients\n",
    "                if clip_grad_norm is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(nero_params, clip_grad_norm)\n",
    "                \n",
    "                # Compute clipped gradient norm\n",
    "                grad_norm_clipped = compute_grad_norm(nero_params)\n",
    "                grad_norm_log['L1T2/grad_norm_clipped'] = grad_norm_clipped\n",
    "\n",
    "                # Update parameters\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "\n",
    "                # Zero gradients for the next gradient accumulation cycle\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # After updating parameters, toggle training mode:\n",
    "            # if currently training L1T2, switch to L2T2\n",
    "            # if currently training L2T2, switch to L1T2\n",
    "            train_L2T2_nero = not train_L2T2_nero\n",
    "\n",
    "        # Logging\n",
    "        lr_log = (\n",
    "            {'L1T2/lr': scheduler.get_last_lr()[0]} \n",
    "            if not train_L2T2_nero else \n",
    "            {'L2T2/lr': scheduler_L2T2.get_last_lr()[0]}\n",
    "        )\n",
    "        log = {\n",
    "            **log, \n",
    "            **lr_log, \n",
    "            **grad_norm_log,\n",
    "        }\n",
    "        log_history.append(log)\n",
    "        wandb.log(log)\n",
    "        print(\", \".join(\n",
    "            f\"{k}: {format_float(v)}\" if isinstance(v, float) else f\"{k}: {v}\"\n",
    "            for k, v in log.items()\n",
    "        ))\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step > 0 and global_step % checkpoint_steps == 0:\n",
    "            # Create checkpoint directory\n",
    "            checkpoint_dir = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save Nero parameters, along with optimizer, scheduler, and scaler states\n",
    "            nero_state_dict = {n: p.detach().cpu() for n, p in nero_model.named_parameters() if 'L1T2' in n}\n",
    "            save_file(nero_state_dict, os.path.join(checkpoint_dir, 'adapter_model_L1T2.safetensors'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, 'optimizer_L1T2.pt'))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(checkpoint_dir, 'scheduler_L1T2.pt'))\n",
    "            torch.save(scaler.state_dict(), os.path.join(checkpoint_dir, 'scaler_L1T2.pt'))\n",
    "\n",
    "            nero_state_dict_L2T2 = {n: p.detach().cpu() for n, p in nero_model.named_parameters() if 'L2T2' in n}\n",
    "            save_file(nero_state_dict_L2T2, os.path.join(checkpoint_dir, 'adapter_model_L2T2.safetensors'))\n",
    "            torch.save(optimizer_L2T2.state_dict(), os.path.join(checkpoint_dir, 'optimizer_L2T2.pt'))\n",
    "            torch.save(scheduler_L2T2.state_dict(), os.path.join(checkpoint_dir, 'scheduler_L2T2.pt'))\n",
    "            torch.save(scaler_L2T2.state_dict(), os.path.join(checkpoint_dir, 'scaler_L2T2.pt'))\n",
    "\n",
    "            # Save trainer state for resuming training\n",
    "            trainer_state = {\n",
    "                'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "                'log_history': log_history,\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'w') as f:\n",
    "                json.dump(trainer_state, f, indent=2)\n",
    "\n",
    "            # Save RNG state for reproducibility\n",
    "            rng_state = {\n",
    "                'python': random.getstate(),\n",
    "                'numpy': np.random.get_state(),\n",
    "                'cpu': torch.get_rng_state(),\n",
    "                'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else [],\n",
    "            }\n",
    "            torch.save(rng_state, os.path.join(checkpoint_dir, 'rng_state.pth'))\n",
    "\n",
    "            # Upload checkpoint directory to Hugging Face repository\n",
    "            upload_folder(\n",
    "                folder_path=checkpoint_dir,\n",
    "                repo_id=hf_nero_id,\n",
    "                path_in_repo=f\"checkpoint-{global_step}\",\n",
    "                commit_message=f\"Add checkpoint at step {global_step}\",\n",
    "                repo_type='model',\n",
    "            )\n",
    "        \n",
    "        # Check generated text every `generate_steps`\n",
    "        if global_step > 0 and global_step % generate_steps == 0:\n",
    "            nero_model.set_return_hidden_outputs(False)\n",
    "            nero_model.set_mode('student')\n",
    "            generated = generate_text(nero_model, tokenizer, sample_prompt, device=device)\n",
    "            print()\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT\")\n",
    "            print(\"================================\")\n",
    "            print(f\"{'Prompt':<9}:\", sample_prompt)\n",
    "            print(f\"{'Generated':<9}:\", generated)\n",
    "            print()\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>grad_norm</td><td>▁</td></tr><tr><td>grad_norm_clipped</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>grad_norm</td><td>0.14058</td></tr><tr><td>grad_norm_clipped</td><td>0.14058</td></tr><tr><td>loss</td><td>7.88927</td></tr><tr><td>lr</td><td>3e-05</td></tr><tr><td>mode</td><td>L1T2</td></tr><tr><td>step</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-salad-140</strong> at: <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/nkhsruua' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/nkhsruua</a><br> View project at: <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250814_000604-nkhsruua/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
