{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import UnslothTrainer, UnslothTrainingArguments, is_bf16_supported\n",
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    default_data_collator, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(repo_id, checkpoint=None, max_checkpoints=2000, checkpoint_step=25):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_step) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        return os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir\n",
    "\n",
    "def check_loss_and_grad_norm(model, tokenizer, prompt=\"Paris is the capital of\"):\n",
    "    # Set model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    print(\"Loss:\", outputs.loss)\n",
    "\n",
    "    # Backward pass\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for p in [p for n, p in model.named_parameters() if p.requires_grad]:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=32, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id,\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang, _ = lora_repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    split = f'train[:{(train_size+test_size)}]'\n",
    "\n",
    "    # Load dataset\n",
    "    # TODO: Use streaming to not download the entire dataset\n",
    "    # dataset = load_dataset(data_id, data_dir=data_dir, split=split)\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "seed = 69\n",
    "target_lang = 'ja' # 'en' | 'id' | 'es'\n",
    "target_task = 'wikipedia' # 'wikipedia' | 'gsm8k'\n",
    "device = 'auto' # 'cpu' | 'cuda' | 'auto'\n",
    "\n",
    "# Data configuration\n",
    "train_size = 5000\n",
    "test_size = 0\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 0.5\n",
    "\n",
    "num_epochs = 1\n",
    "max_global_steps = None\n",
    "resume_step = 0\n",
    "\n",
    "warmup_ratio = 0.05\n",
    "lr = 2e-4\n",
    "\n",
    "sample_prompt = 'Êó•Êú¨ÂõΩ„ÅØ„ÄÅ'\n",
    "\n",
    "# Model configurations\n",
    "hf_lora_id = 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650'\n",
    "checkpoint = 650\n",
    "\n",
    "lora_dir = download_hf_model(hf_lora_id, checkpoint)\n",
    "lora_path = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "lora_config = LoraConfig.from_pretrained(lora_dir)\n",
    "\n",
    "base_model_name = lora_config.base_model_name_or_path\n",
    "print(f\"Base model name: {base_model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo\n",
    "\n",
    "checkpoint_steps = 100\n",
    "\n",
    "hf_username = 'alxxtexxr'\n",
    "nero_dir = f'L3.1-8B-{target_task}-{target_lang}-{train_size//1000}K-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "os.makedirs(nero_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(nero_dir, '.git')):\n",
    "    create_repo(hf_nero_id, exist_ok=True)\n",
    "    repo = Repository(local_dir=nero_dir, clone_from=hf_nero_id)\n",
    "else:\n",
    "    repo = Repository(local_dir=nero_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 # For debugging \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # For debugging\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.nero_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"================================================================\")\n",
    "            print(self.module_name)\n",
    "            print(\"================================================================\")\n",
    "            print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "            print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "            print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # nero_out = F.relu(self.nero_B(self.nero_A(self.dropout(lora_out))) * self.scaling)\n",
    "        nero_dropout_out = self.dropout(lora_out)\n",
    "        nero_A_out = self.nero_A(nero_dropout_out)\n",
    "        nero_B_out = self.nero_B(nero_A_out)\n",
    "        nero_scaling_out = nero_B_out * self.scaling\n",
    "        nero_out = F.relu(nero_scaling_out)\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"nero_out.requires_grad:\", nero_out.requires_grad)\n",
    "            print(\"nero_out.grad_fn:\", nero_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "            nero_out_has_nan = torch.isnan(nero_out).any()\n",
    "            if nero_out_has_nan:\n",
    "                print(\"!!! NERO OUT HAS NAN !!!\")\n",
    "                print(\"nero_out:\")\n",
    "                print(nero_out)\n",
    "                print()\n",
    "                print(\"nero_scaling_out:\")\n",
    "                print(nero_scaling_out)\n",
    "                print()\n",
    "                print(\"nero_B_out:\")\n",
    "                print(nero_B_out)\n",
    "                print()\n",
    "                print(\"nero_A_out:\")\n",
    "                print(nero_A_out)\n",
    "                print()\n",
    "                print(\"nero_dropout_out:\")\n",
    "                print(nero_dropout_out)\n",
    "                print()\n",
    "                print(\"lora_out:\")\n",
    "                print(lora_out)\n",
    "                print()\n",
    "\n",
    "        # Add `base_out` with gradients-detached `nero_out`, \n",
    "        # so that `base_out` does not carry gradients\n",
    "        # nero_out_detached = nero_out.detach()\n",
    "\n",
    "        # if self.debug:\n",
    "        #     print(\"nero_out_detached.requires_grad:\", nero_out_detached.requires_grad)\n",
    "        #     print(\"nero_out_detached.grad_fn:\", nero_out_detached.grad_fn)\n",
    "        #     print()\n",
    "\n",
    "        # output = base_out + nero_out_detached\n",
    "        output = base_out + nero_out\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"output.requires_grad:\", output.requires_grad)\n",
    "            print(\"output.grad_fn:\", output.grad_fn)\n",
    "            print()\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_return_nero_outputs(self, return_nero_outputs: bool):\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_nero_output = return_nero_outputs\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        print(\"All layers are frozen except Nero layers!\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        print(\"All layers are unfrozen!\")\n",
    "    \n",
    "    def load_lora_params(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(*args, **kwargs)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device)\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    lora_config, \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=False,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers are frozen except Nero layers!\n",
      "Loss: tensor(14.4288, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 1.595538939960756\n"
     ]
    }
   ],
   "source": [
    "nero_model.freeze_all_except_nero()\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 0.003367212601006031\n",
      "- Min     : -1.3833860158920288\n",
      "- Max     : 1.708866834640503\n",
      "\n",
      "LoRA parameters loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n",
      "\n",
      "Loss: tensor(14.4288, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 1.8612670598700995\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "nero_model.load_lora_params(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient checkpointing enabled!\n",
      "\n",
      "Loss: tensor(14.4288, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 1.8612670598700995\n"
     ]
    }
   ],
   "source": [
    "nero_model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_class : None\n",
      "bos_token_id : 128000\n",
      "pad_token_id : 128004\n",
      "eos_token_id : 128001\n",
      "sep_token_id : None\n",
      "decoder_start_token_id : None\n",
      "forced_bos_token_id : None\n",
      "forced_eos_token_id : None\n",
      "suppress_tokens : None\n",
      "begin_suppress_tokens : None\n"
     ]
    }
   ],
   "source": [
    "for k, v in nero_model.config.__dict__.items():\n",
    "    if 'token' in k:\n",
    "        print(k, \":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nero_model.eval()\n",
    "# generate_text(\n",
    "#     nero_model, \n",
    "#     tokenizer, \n",
    "#     prompt=\"Paris is the capital of\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_hf_dataset(\n",
    "    target_lang, \n",
    "    target_task, \n",
    "    train_size=train_size, \n",
    "    test_size=test_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cbe075a8bc4d108e12d598cadbabbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2006b7309c44829f2311b7a347783f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dataset_map_config(remove_columns=None):\n",
    "    return dict(\n",
    "        batched=True, \n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "\n",
    "if target_task == 'gsm8k':\n",
    "    eos_token = tokenizer.eos_token\n",
    "    \n",
    "    def format_gsm8k_prompt(example):\n",
    "        gsm8k_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{question}\n",
    "\n",
    "### Answer: \n",
    "{answer}\"\"\" + eos_token\n",
    "\n",
    "        return {'text': gsm8k_prompt.format(\n",
    "            question=example['question'], \n",
    "            answer=example['answer'],\n",
    "        )}\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_seq_length,\n",
    "        )\n",
    "\n",
    "    def add_labels(example):\n",
    "        example['labels'] = example['input_ids'].copy()\n",
    "        return example\n",
    "\n",
    "    dataset_formatted = dataset.map(\n",
    "        format_gsm8k_prompt, \n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_tokenized = dataset_formatted.map(\n",
    "        tokenize_fn, \n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_final = dataset_tokenized.map(\n",
    "        add_labels,\n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "else:\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(example['text'])\n",
    "\n",
    "    # Concatenate all tokens into one long stream, then split into blocks\n",
    "    block_size = max_seq_length\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated = []\n",
    "        for input_ids in examples['input_ids']:\n",
    "            concatenated += input_ids\n",
    "\n",
    "        total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "        input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_mask = [[1] * block_size for _ in input_ids]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "    dataset_tokenized = dataset.map(\n",
    "        tokenize_fn,\n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_final = dataset_tokenized.map(\n",
    "        group_texts, \n",
    "        **get_dataset_map_config(remove_columns=dataset_tokenized.column_names),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 6923\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset_final, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "print(\"Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask, labels):\n",
      "(torch.Size([4, 1024]), torch.Size([4, 1024]), torch.Size([4, 1024]))\n",
      "\n",
      "First batch text:\n",
      "„ÇΩ„Ç§„Éâ„Ç®„Éï„Çß„Éâ„É™„É≥„ÄÅÂ°©ÈÖ∏„Éï„Çß„Éã„É¨„Éï„É™„É≥„Å™„Å©Ôºâ„ÇÑÊäó„Ç≥„É™„É≥Ââ§Ôºà„Éô„É©„Éâ„É≥„ÉäÁ∑è„Ç¢„É´„Ç´„É≠„Ç§„Éâ„ÄÅ„ÉÄ„ÉÑ„É©„Ç®„Ç≠„Çπ„Å™„Å©Ôºâ„Å®„ÅÑ„Å£„ÅüËñ¨„ÅåÈÖçÂêà„Åï„Çå„Å¶„ÅÑ„Çã„Åå„ÄÅÁöÆËÜö„ÅÆ„Åã„ÇÜ„Åø„Å™„Å©„ÅÆÈ£≤„ÅøËñ¨„Å´„ÅØ„ÄÅ„Åª„Å®„Çì„Å©Á¨¨‰∏Ä‰∏ñ‰ª£Êäó„Éí„Çπ„Çø„Éü„É≥Ëñ¨„Å†„Åë„Å®„ÅÑ ...\n",
      "\n",
      "Loss: tensor(13.9235, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 0.46805604645141224\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask, labels):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "    first_batch['labels'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer, prompt=first_batch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris is the capital of_<?_<?_<?470470 Shore ÔøΩ ÔøΩ ÔøΩ470 ÔøΩ ÔøΩ470470 ÔøΩ470_<? ÔøΩ470 Destructionutzer_<?470furtonse470470_<? Destruction_<? ÔøΩ470'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(nero_model, tokenizer=tokenizer, prompt=\"Paris is the capital of\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1373/78758964.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067efe8211064dd9a4eb101fd75eda4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111318193333217, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250803_033055-tjwcjam4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/tjwcjam4' target=\"_blank\">lively-darkness-70</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/tjwcjam4' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/tjwcjam4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1373/78758964.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ ÔøΩonse470 ÔøΩ ÔøΩ Shoreutzeronse470470 ÔøΩ470 Destruction ÔøΩ470470 gloss470_<?470469470onse470673ÔøΩ673470_<? ÔøΩfurt_<?\n",
      "\n",
      "epoch: 0/1, step: 1/6923, loss: 13.826272964477539, grad_norm: 0.43860121483564196, lr: 5.780346820809248e-07\n",
      "epoch: 0/1, step: 3/6923, loss: 13.90209674835205, grad_norm: 0.4719550617953492, lr: 1.1560693641618497e-06\n",
      "epoch: 0/1, step: 5/6923, loss: 13.89864444732666, grad_norm: 0.47841054724501925, lr: 1.7341040462427746e-06\n",
      "epoch: 0/1, step: 7/6923, loss: 13.826224327087402, grad_norm: 0.4230777352932786, lr: 2.3121387283236993e-06\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅutzer ÔøΩ_<?_<? Destruction ÔøΩonse470emouth470onse_<? ÔøΩonseemouth Destruction673470utzer ÔøΩ470ÔøΩ ÔøΩ_<? ÔøΩutzer ÔøΩÂß´_<?onse_<? Destruction\n",
      "\n",
      "epoch: 0/1, step: 9/6923, loss: 13.999789237976074, grad_norm: 0.45872058397934123, lr: 2.8901734104046244e-06\n",
      "epoch: 0/1, step: 11/6923, loss: 13.87027359008789, grad_norm: 0.4380367950843459, lr: 3.468208092485549e-06\n",
      "epoch: 0/1, step: 13/6923, loss: 13.794644355773926, grad_norm: 0.4451357949522682, lr: 4.046242774566474e-06\n",
      "epoch: 0/1, step: 15/6923, loss: 13.938424110412598, grad_norm: 0.4649252680944785, lr: 4.624277456647399e-06\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ470 Sole gloss470 ÔøΩÔøΩ ÔøΩonse Destruction470_<?470 ÔøΩ glossutzer —Å–±–æ—Ävak Shore_<?onse ÔøΩutzer —Å–±–æ—Äoulouse673470 ÔøΩ470470 Shoreprivught\n",
      "\n",
      "epoch: 0/1, step: 17/6923, loss: 13.835043907165527, grad_norm: 0.497327880066605, lr: 5.202312138728324e-06\n",
      "epoch: 0/1, step: 19/6923, loss: 13.883379936218262, grad_norm: 0.4441328853575391, lr: 5.780346820809249e-06\n",
      "epoch: 0/1, step: 21/6923, loss: 13.808504104614258, grad_norm: 0.46266692134843834, lr: 6.358381502890173e-06\n",
      "epoch: 0/1, step: 23/6923, loss: 13.824165344238281, grad_norm: 0.4708479837517832, lr: 6.936416184971098e-06\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ Sole469 sna470470470 ÔøΩ_<?470utzeronse_<?470 DestructionÔøΩ ÔøΩ_<?470_<?470furt gloss ÔøΩ470 Shoreoulouse470470_<?_<? Shore Destruction\n",
      "\n",
      "epoch: 0/1, step: 25/6923, loss: 13.939615249633789, grad_norm: 0.49665138791661695, lr: 7.514450867052024e-06\n",
      "epoch: 0/1, step: 27/6923, loss: 13.828964233398438, grad_norm: 0.43211483207396867, lr: 8.092485549132949e-06\n",
      "epoch: 0/1, step: 29/6923, loss: 13.864217758178711, grad_norm: 0.458797365179201, lr: 8.670520231213873e-06\n",
      "epoch: 0/1, step: 31/6923, loss: 13.913424491882324, grad_norm: 0.5224489086959965, lr: 9.248554913294797e-06\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ470_<?_<?onse_<?470utzer ÔøΩ470emouth470_<?oulouse ÔøΩ470470_<?_<?utzerutzer Destruction470ÔøΩ_<?_<?470onse_<? ÔøΩ ÔøΩonse_<?\n",
      "\n",
      "epoch: 0/1, step: 33/6923, loss: 13.862387657165527, grad_norm: 0.4272794555028918, lr: 9.826589595375723e-06\n",
      "epoch: 0/1, step: 35/6923, loss: 13.901519775390625, grad_norm: 0.472008205177754, lr: 1.0404624277456647e-05\n",
      "epoch: 0/1, step: 37/6923, loss: 13.786839485168457, grad_norm: 0.4508512445403244, lr: 1.0982658959537573e-05\n",
      "epoch: 0/1, step: 39/6923, loss: 13.740276336669922, grad_norm: 0.4218261659088714, lr: 1.1560693641618498e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ_<?_<?_<?emouthutzer ÔøΩ Shore470onse470 Sole470utzerutzer Destructionutzer ÔøΩutzer Shore_<?Âß´470 Supervught470 glossonse_<?470470_<? ÔøΩ\n",
      "\n",
      "epoch: 0/1, step: 41/6923, loss: 13.79101848602295, grad_norm: 0.46023346402842685, lr: 1.2138728323699422e-05\n",
      "epoch: 0/1, step: 43/6923, loss: 13.825986862182617, grad_norm: 0.4396299860582547, lr: 1.2716763005780346e-05\n",
      "epoch: 0/1, step: 45/6923, loss: 13.855631828308105, grad_norm: 0.49661903103152627, lr: 1.329479768786127e-05\n",
      "epoch: 0/1, step: 47/6923, loss: 13.752291679382324, grad_norm: 0.45370027795101486, lr: 1.3872832369942197e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ_<? ÔøΩ gloss —Å–±–æ—Ä —Å–±–æ—Ä470_<?_<? ÔøΩonse470_<?utzeroulouse ÔøΩ ÔøΩ470_<?470 ÔøΩught470470onseoulouse ÔøΩ_<? ÔøΩ Destructiononse sna Destruction\n",
      "\n",
      "epoch: 0/1, step: 49/6923, loss: 13.771974563598633, grad_norm: 0.47511787546537676, lr: 1.4450867052023123e-05\n",
      "epoch: 0/1, step: 51/6923, loss: 13.74585247039795, grad_norm: 0.48038609878445143, lr: 1.5028901734104049e-05\n",
      "epoch: 0/1, step: 53/6923, loss: 13.810050010681152, grad_norm: 0.4815765286861631, lr: 1.5606936416184973e-05\n",
      "epoch: 0/1, step: 55/6923, loss: 13.856400489807129, grad_norm: 0.44708636294527593, lr: 1.6184971098265897e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ Shore_<?furt470470_<? Destruction Destruction_<?emouth_<? ÔøΩfurt470 ÔøΩ470utzer ÔøΩ_<?470_<?utzer470470_<?469_<?onse Shore_<?470onse\n",
      "\n",
      "epoch: 0/1, step: 57/6923, loss: 13.872519493103027, grad_norm: 0.4491466480265435, lr: 1.676300578034682e-05\n",
      "epoch: 0/1, step: 59/6923, loss: 13.769885063171387, grad_norm: 0.44992199743008754, lr: 1.7341040462427746e-05\n",
      "epoch: 0/1, step: 61/6923, loss: 13.813604354858398, grad_norm: 0.46421770118778055, lr: 1.791907514450867e-05\n",
      "epoch: 0/1, step: 63/6923, loss: 13.747045516967773, grad_norm: 0.4449768965679778, lr: 1.8497109826589594e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ470470_<?470470 ÔøΩ673_<?470470470 ÔøΩutzer470_<?_<?_<?priv Superv_<? Superv ÔøΩ_<?vak_<? f√∏ ÔøΩ470_<?470onse470\n",
      "\n",
      "epoch: 0/1, step: 65/6923, loss: 13.759828567504883, grad_norm: 0.4870786899717583, lr: 1.907514450867052e-05\n",
      "epoch: 0/1, step: 67/6923, loss: 13.850215911865234, grad_norm: 0.46464323277167857, lr: 1.9653179190751446e-05\n",
      "epoch: 0/1, step: 69/6923, loss: 13.910351753234863, grad_norm: 0.5192774368497008, lr: 2.023121387283237e-05\n",
      "epoch: 0/1, step: 71/6923, loss: 13.778595924377441, grad_norm: 0.42085883929574847, lr: 2.0809248554913295e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ ÔøΩ470470470470_<?onseemouth470470 Destructiononse DestructionÔøΩ ÔøΩ ÔøΩÂß´470470 ÔøΩ470 Destruction470 —Å–±–æ—Ä Superv ÔøΩ ÔøΩ469470 Destruction_<? Shore\n",
      "\n",
      "epoch: 0/1, step: 73/6923, loss: 13.67696475982666, grad_norm: 0.5062038479403546, lr: 2.1387283236994223e-05\n",
      "epoch: 0/1, step: 75/6923, loss: 13.88582992553711, grad_norm: 0.532224962575536, lr: 2.1965317919075147e-05\n",
      "epoch: 0/1, step: 77/6923, loss: 13.731040000915527, grad_norm: 0.4985383395458039, lr: 2.254335260115607e-05\n",
      "epoch: 0/1, step: 79/6923, loss: 13.72586441040039, grad_norm: 0.4283921161607934, lr: 2.3121387283236996e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ470furt Shore ÔøΩ_<?furtonse470onsepriv470470469470470onseonse470470470 ÔøΩ470469470470470470470470Âß´_<? Shore\n",
      "\n",
      "epoch: 0/1, step: 81/6923, loss: 13.715118408203125, grad_norm: 0.459439580112249, lr: 2.369942196531792e-05\n",
      "epoch: 0/1, step: 83/6923, loss: 13.742403984069824, grad_norm: 0.47316163816992773, lr: 2.4277456647398844e-05\n",
      "epoch: 0/1, step: 85/6923, loss: 13.572978019714355, grad_norm: 0.42671384182109673, lr: 2.485549132947977e-05\n",
      "epoch: 0/1, step: 87/6923, loss: 13.567320823669434, grad_norm: 0.4472220526108587, lr: 2.5433526011560693e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ ÔøΩ470470emouth_<?ught Destruction673_<? Supervonseoulouse673 ApplicationRecord_<?_<?emouthonse ÔøΩ Shoreonse_<?469470_<?_<?_<?470 Ricaonse470onse\n",
      "\n",
      "epoch: 0/1, step: 89/6923, loss: 13.632171630859375, grad_norm: 0.4139978721232577, lr: 2.6011560693641617e-05\n",
      "epoch: 0/1, step: 91/6923, loss: 13.664680480957031, grad_norm: 0.4334724137715241, lr: 2.658959537572254e-05\n",
      "epoch: 0/1, step: 93/6923, loss: 13.646482467651367, grad_norm: 0.4269726540116502, lr: 2.7167630057803466e-05\n",
      "epoch: 0/1, step: 95/6923, loss: 13.652364730834961, grad_norm: 0.49105660786859506, lr: 2.7745664739884393e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ470470_<?priv Shore ÔøΩoulouse Destruction ÔøΩ ÔøΩ Sole_<? Ricaonse470 snaÔøΩ ÔøΩ_<?ÔøΩ470vak f√∏ Shore ÔøΩ673470ughtonse469 ÔøΩ470\n",
      "\n",
      "epoch: 0/1, step: 97/6923, loss: 13.641216278076172, grad_norm: 0.476841934393507, lr: 2.832369942196532e-05\n",
      "epoch: 0/1, step: 99/6923, loss: 13.555696487426758, grad_norm: 0.4185094346367544, lr: 2.8901734104046245e-05\n",
      "epoch: 0/1, step: 101/6923, loss: 13.556522369384766, grad_norm: 0.428732964821393, lr: 2.947976878612717e-05\n",
      "epoch: 0/1, step: 103/6923, loss: 13.601245880126953, grad_norm: 0.43996708598455603, lr: 3.0057803468208097e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ470 ÔøΩ470furtvakonse470_<?470469onseught ÔøΩ ÔøΩ f√∏470469ught_<?Âß´onse470470 f√∏ught ÔøΩ_<? ÔøΩ Shoreutzer ÔøΩ Destruction\n",
      "\n",
      "epoch: 0/1, step: 105/6923, loss: 13.541949272155762, grad_norm: 0.49552907550571546, lr: 3.063583815028902e-05\n",
      "epoch: 0/1, step: 107/6923, loss: 13.62288761138916, grad_norm: 0.4656420326899882, lr: 3.1213872832369946e-05\n",
      "epoch: 0/1, step: 109/6923, loss: 13.653884887695312, grad_norm: 0.42119871843892465, lr: 3.179190751445087e-05\n",
      "epoch: 0/1, step: 111/6923, loss: 13.622177124023438, grad_norm: 0.5171984950036175, lr: 3.2369942196531794e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ ÔøΩutzer469_<?470ught —Å–±–æ—Ä_<?_<?privutzer Superv ÔøΩonseonse —Å–±–æ—Ä ÔøΩ469utzer ÔøΩonse Destruction_<?673utzer gloss470 ÔøΩonse673470 Rica\n",
      "\n",
      "epoch: 0/1, step: 113/6923, loss: 13.56907844543457, grad_norm: 0.4441397922782901, lr: 3.294797687861272e-05\n",
      "epoch: 0/1, step: 115/6923, loss: 13.597082138061523, grad_norm: 0.4867376498676213, lr: 3.352601156069364e-05\n",
      "epoch: 0/1, step: 117/6923, loss: 13.482660293579102, grad_norm: 0.5306272670807703, lr: 3.410404624277457e-05\n",
      "epoch: 0/1, step: 119/6923, loss: 13.373187065124512, grad_norm: 0.4547726504223382, lr: 3.468208092485549e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ470 Rica_<?_<?470 Destructionpriv470 Destruction ÔøΩ469 Sole Shore ÔøΩ470 Sole_<? Shore470_<? ÔøΩught470 ÔøΩ_<? ÔøΩught470 gloss ÔøΩ Destruction470\n",
      "\n",
      "epoch: 0/1, step: 121/6923, loss: 13.487855911254883, grad_norm: 0.4557521667295076, lr: 3.5260115606936416e-05\n",
      "epoch: 0/1, step: 123/6923, loss: 13.496175765991211, grad_norm: 0.4032837935123056, lr: 3.583815028901734e-05\n",
      "epoch: 0/1, step: 125/6923, loss: 13.45585823059082, grad_norm: 0.44819924906522923, lr: 3.6416184971098265e-05\n",
      "epoch: 0/1, step: 127/6923, loss: 13.366800308227539, grad_norm: 0.458943218985375, lr: 3.699421965317919e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ470 Sole673 ÔøΩ470 cubonse ÔøΩ Shore Rica Shore f√∏470470 Destruction ÔøΩ gloss470470469470ÔøΩ ÔøΩ gloss ÔøΩught Sole470onse ÔøΩ Soleonse\n",
      "\n",
      "epoch: 0/1, step: 129/6923, loss: 13.412419319152832, grad_norm: 0.4465152989282533, lr: 3.757225433526011e-05\n",
      "epoch: 0/1, step: 131/6923, loss: 13.08640193939209, grad_norm: 0.49179609650752304, lr: 3.815028901734104e-05\n",
      "epoch: 0/1, step: 133/6923, loss: 13.296109199523926, grad_norm: 0.45034587469659143, lr: 3.872832369942196e-05\n",
      "epoch: 0/1, step: 135/6923, loss: 13.333605766296387, grad_norm: 0.5183360709868071, lr: 3.930635838150289e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ_<? Rica470 Shore instruction_<?470onse_<?470Broken673 ÔøΩÔøΩonse f√∏ Sole Destruction_<? Rica_<?470 Shore Soleonse470470470utzer ÔøΩ gloss673\n",
      "\n",
      "epoch: 0/1, step: 137/6923, loss: 13.28933048248291, grad_norm: 0.47774192785755876, lr: 3.988439306358382e-05\n",
      "epoch: 0/1, step: 139/6923, loss: 13.387091636657715, grad_norm: 0.470440365076546, lr: 4.046242774566474e-05\n",
      "epoch: 0/1, step: 141/6923, loss: 13.30284309387207, grad_norm: 0.43369066025472736, lr: 4.1040462427745666e-05\n",
      "epoch: 0/1, step: 143/6923, loss: 13.180869102478027, grad_norm: 0.4540798197013257, lr: 4.161849710982659e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ_<?_<?469 —Å–±–æ—Ä470onse470 ÔøΩ„Äë470„Äë470470673470470_<?469 f√∏„Äë_<? Shoreutzer ÔøΩ ÔøΩfurt_<? Rica470470priv Destruction\n",
      "\n",
      "epoch: 0/1, step: 145/6923, loss: 13.23878288269043, grad_norm: 0.5099239459584983, lr: 4.2196531791907514e-05\n",
      "epoch: 0/1, step: 147/6923, loss: 13.205448150634766, grad_norm: 0.49258414603979006, lr: 4.2774566473988445e-05\n",
      "epoch: 0/1, step: 149/6923, loss: 13.121028900146484, grad_norm: 0.4788002204046401, lr: 4.335260115606937e-05\n",
      "epoch: 0/1, step: 151/6923, loss: 13.092462539672852, grad_norm: 0.5548496711790082, lr: 4.3930635838150294e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ_<? ÔøΩ ÔøΩonse sna_<?onse470470 ÔøΩ sna_<?ÔøΩ_<? glossught470 Destruction gloss cub470 gloss470349470 Ricautzer ÔøΩ470ughtpriv_<?\n",
      "\n",
      "epoch: 0/1, step: 153/6923, loss: 13.023714065551758, grad_norm: 0.48449344706337977, lr: 4.450867052023122e-05\n",
      "epoch: 0/1, step: 155/6923, loss: 13.150964736938477, grad_norm: 0.4375223288708637, lr: 4.508670520231214e-05\n",
      "epoch: 0/1, step: 157/6923, loss: 12.990538597106934, grad_norm: 0.5204460054698377, lr: 4.566473988439307e-05\n",
      "epoch: 0/1, step: 159/6923, loss: 12.948806762695312, grad_norm: 0.48406253448772046, lr: 4.624277456647399e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ_<?470 ÔøΩ470 Superv470onse ÔøΩ glossecess ApplicationRecordught470onse ÔøΩ470_<? Superv470 ÔøΩ ApplicationRecord Rica673470 gloss470 instruction —Å–±–æ—Ä Shore sna_<?470\n",
      "\n",
      "epoch: 0/1, step: 161/6923, loss: 13.063302040100098, grad_norm: 0.49524990530602325, lr: 4.6820809248554915e-05\n",
      "epoch: 0/1, step: 163/6923, loss: 12.965156555175781, grad_norm: 0.457730534782095, lr: 4.739884393063584e-05\n",
      "epoch: 0/1, step: 165/6923, loss: 12.809782028198242, grad_norm: 0.513142922463145, lr: 4.7976878612716764e-05\n",
      "epoch: 0/1, step: 167/6923, loss: 12.733969688415527, grad_norm: 0.5232169766034451, lr: 4.855491329479769e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ673 Shore470_<?470470470470_<?ught gloss_<?470673 Destruction Shore RicaÂß´ught Shore ÔøΩ_<? Shore ÔøΩ instruction ÔøΩ470470 ÔøΩ ÔøΩ673utzer\n",
      "\n",
      "epoch: 0/1, step: 169/6923, loss: 12.8378324508667, grad_norm: 0.4804051604248199, lr: 4.913294797687861e-05\n",
      "epoch: 0/1, step: 171/6923, loss: 12.680384635925293, grad_norm: 0.5916678815424287, lr: 4.971098265895954e-05\n",
      "epoch: 0/1, step: 173/6923, loss: 12.724939346313477, grad_norm: 0.5447962811067611, lr: 5.028901734104047e-05\n",
      "epoch: 0/1, step: 175/6923, loss: 12.820030212402344, grad_norm: 0.4889652264327601, lr: 5.0867052023121385e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ Shore Shoreecessonse ÔøΩpriv470 Sole Sole„Äë_<?470 sna ÔøΩ gloss ÔøΩ_<? Shore Shore_<?470ÔøΩ f√∏470 prosecutionught ÔøΩonseÔøΩ ÔøΩ Shore ÔøΩ\n",
      "\n",
      "epoch: 0/1, step: 177/6923, loss: 12.761188507080078, grad_norm: 0.47860665804506985, lr: 5.1445086705202317e-05\n",
      "epoch: 0/1, step: 179/6923, loss: 12.704569816589355, grad_norm: 0.5052633475101052, lr: 5.2023121387283234e-05\n",
      "epoch: 0/1, step: 181/6923, loss: 12.702839851379395, grad_norm: 0.5038302138520921, lr: 5.2601156069364165e-05\n",
      "epoch: 0/1, step: 183/6923, loss: 12.490092277526855, grad_norm: 0.5022961306038405, lr: 5.317919075144508e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅvak sna ÔøΩ prosecution gloss6734701 gloss gloss SupervÔøΩ gloss ÔøΩ470470onseonse470„Äë_<?_<?_<? Superv Destruction ÔøΩ f√∏ Destruction470 Sole470„Äë\n",
      "\n",
      "epoch: 0/1, step: 185/6923, loss: 12.603179931640625, grad_norm: 0.4793917292574457, lr: 5.3757225433526014e-05\n",
      "epoch: 0/1, step: 187/6923, loss: 12.689988136291504, grad_norm: 0.4972912379715873, lr: 5.433526011560693e-05\n",
      "epoch: 0/1, step: 189/6923, loss: 12.54214096069336, grad_norm: 0.5570611972198852, lr: 5.491329479768786e-05\n",
      "epoch: 0/1, step: 191/6923, loss: 12.522122383117676, grad_norm: 0.5504734843026403, lr: 5.5491329479768787e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ gloss Sole ÔøΩ_<?vak Shore ApplicationRecord ÔøΩ ApplicationRecord1 f√∏ÔøΩ ÔøΩBroken470 ÔøΩ ShoreÔøΩ470 sna ÔøΩÔøΩ_<? Shore673 ÔøΩ Destruction673 cub ÔøΩ Satoshi Shore\n",
      "\n",
      "epoch: 0/1, step: 193/6923, loss: 12.467042922973633, grad_norm: 0.5257642350246109, lr: 5.606936416184971e-05\n",
      "epoch: 0/1, step: 195/6923, loss: 12.144336700439453, grad_norm: 0.5742503030524401, lr: 5.664739884393064e-05\n",
      "epoch: 0/1, step: 197/6923, loss: 12.297635078430176, grad_norm: 0.5655686639392024, lr: 5.722543352601156e-05\n",
      "epoch: 0/1, step: 199/6923, loss: 12.251151084899902, grad_norm: 0.5798697265957222, lr: 5.780346820809249e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ11priv Shore_<?_<?ÔøΩ„Ç¢vak Destruction Shore ÔøΩ f√∏ gloss673 ShoreÂàá Shore Destruction DestructionÔøΩ cub Destruction fpriv1 cub11„Ç¢7 Shore\n",
      "\n",
      "epoch: 0/1, step: 201/6923, loss: 12.3114652633667, grad_norm: 0.5726229151876346, lr: 5.8381502890173415e-05\n",
      "epoch: 0/1, step: 203/6923, loss: 12.099308967590332, grad_norm: 0.569842802937372, lr: 5.895953757225434e-05\n",
      "epoch: 0/1, step: 205/6923, loss: 11.856000900268555, grad_norm: 0.5781365388917019, lr: 5.9537572254335263e-05\n",
      "epoch: 0/1, step: 207/6923, loss: 11.476505279541016, grad_norm: 0.747360077583863, lr: 6.0115606936416195e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅonse_<? ÔøΩ Superv could673 gloss„Äë470_<? „Ç¢ Shore<1ÔøΩ470ÔøΩ11 1 Superv ÔøΩ —Å–±–æ—Ä470 —Å–±–æ—Ä ÔøΩ f cub f√∏utzer\n",
      "\n",
      "epoch: 0/1, step: 209/6923, loss: 11.673490524291992, grad_norm: 0.7277365535014936, lr: 6.069364161849711e-05\n",
      "epoch: 0/1, step: 211/6923, loss: 11.602152824401855, grad_norm: 0.6359104662138689, lr: 6.127167630057804e-05\n",
      "epoch: 0/1, step: 213/6923, loss: 11.847206115722656, grad_norm: 0.6555744329999857, lr: 6.184971098265896e-05\n",
      "epoch: 0/1, step: 215/6923, loss: 11.809331893920898, grad_norm: 0.6396643323887007, lr: 6.242774566473989e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ11 f√∏11„Éª1„Ç¢673 1 Shore„Ç¢ÔøΩ„Äë„Ç§1„Ç¢1 ÔøΩ16731/ÔøΩ„Å´470„Äë„Äë/1 \n",
      "\n",
      "epoch: 0/1, step: 217/6923, loss: 11.41784954071045, grad_norm: 0.7003985962895584, lr: 6.300578034682081e-05\n",
      "epoch: 0/1, step: 225/6923, loss: 11.444259643554688, grad_norm: 0.8046111214807806, lr: 6.53179190751445e-05\n",
      "epoch: 0/1, step: 227/6923, loss: 11.69931411743164, grad_norm: 0.837933587733941, lr: 6.589595375722544e-05\n",
      "epoch: 0/1, step: 229/6923, loss: 10.793088912963867, grad_norm: 0.689664481007034, lr: 6.647398843930635e-05\n",
      "epoch: 0/1, step: 231/6923, loss: 11.371674537658691, grad_norm: 0.7447406127527052, lr: 6.705202312138729e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„Ç¢„Å´„Ç¢1„ÄÅ„Ç¢„Ç§„Ç¢„Éª „Ç¢„Å´1„Äë„Å´„Éª1„ÄÅ„Ç¢„Éà„Ç¢„Ç¢„Å´„Éª„Éª„Ç¢„Åø„Å´„ÄÅ„Å´„ÄÅ1\n",
      "\n",
      "epoch: 0/1, step: 233/6923, loss: 11.13610553741455, grad_norm: 0.8399049264547198, lr: 6.763005780346822e-05\n",
      "epoch: 0/1, step: 235/6923, loss: 11.280020713806152, grad_norm: 0.740996686312557, lr: 6.820809248554913e-05\n",
      "epoch: 0/1, step: 237/6923, loss: 10.76995849609375, grad_norm: 0.6324667510675333, lr: 6.878612716763007e-05\n",
      "epoch: 0/1, step: 239/6923, loss: 11.08592414855957, grad_norm: 0.7011424184611428, lr: 6.936416184971098e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„Éª„Éª„ÄÅ„Ç¢„Éª„Çí„Å´„ÄÅ„ÄÅ„Éª„Éª„Éª„ÄÅ„ÄÅ„Ç¢„ÄÅ„ÄÅ„Éª„ÄÅ„ÄÅ„Éª„ÄÅ„Ç§„Å´„Å´„Å´„Åå„ÄÅ„Éª„ÄÅ„Çí„Å´\n",
      "\n",
      "epoch: 0/1, step: 241/6923, loss: 10.476279258728027, grad_norm: 0.5571897071405232, lr: 6.994219653179191e-05\n",
      "epoch: 0/1, step: 243/6923, loss: 10.522531509399414, grad_norm: 0.6301929073146333, lr: 7.052023121387283e-05\n",
      "epoch: 0/1, step: 245/6923, loss: 10.710328102111816, grad_norm: 0.6322278641363274, lr: 7.109826589595376e-05\n",
      "epoch: 0/1, step: 247/6923, loss: 10.750255584716797, grad_norm: 0.6723454942704981, lr: 7.167630057803468e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„ÄÅ„Éª„ÄÅ„Å´„ÄÅ„Å´„Ç¢„Å´„ÄÅ„Å´„ÄÅ„ÄÅ„ÄÅ„Éª„Å´„Ç¢„Åå„ÄÅ„ÄÅ„ÄÅ„ÄÅ„ÄÅ„Éª„ÄÅ„ÄÅ„Éª„Ç¢„ÄÅ„ÅÆ„ÄÅ„Åå„Å´\n",
      "\n",
      "epoch: 0/1, step: 249/6923, loss: 10.745842933654785, grad_norm: 0.8458298785519361, lr: 7.225433526011561e-05\n",
      "epoch: 0/1, step: 251/6923, loss: 10.429646492004395, grad_norm: 0.7787886467406532, lr: 7.283236994219653e-05\n",
      "epoch: 0/1, step: 253/6923, loss: 10.586320877075195, grad_norm: 0.854500203820892, lr: 7.341040462427746e-05\n",
      "epoch: 0/1, step: 255/6923, loss: 10.725835800170898, grad_norm: 0.837273407682688, lr: 7.398843930635838e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„Éª„ÄÅ„ÄÅ„ÄÅ„ÄÅ„ÄÅ„ÄÅ„ÄÅ„ÄÅ„Éª„ÄÅ„ÄÅ„ÄÅ„ÄÅ„ÄÅ„ÄÅ„ÄÅ„Å´„Ç¢„ÄÅ„ÄÅ„Åå„Ç¢„Éª„ÄÅ„ÄÅ„Å´„Åå„Éª„Å´„ÅÆ„Ç¢\n",
      "\n",
      "epoch: 0/1, step: 257/6923, loss: 10.780648231506348, grad_norm: 0.971938884923046, lr: 7.456647398843931e-05\n",
      "epoch: 0/1, step: 259/6923, loss: 10.295234680175781, grad_norm: 0.7308190672492177, lr: 7.514450867052023e-05\n",
      "epoch: 0/1, step: 261/6923, loss: 10.659699440002441, grad_norm: 0.780117714640956, lr: 7.572254335260116e-05\n",
      "epoch: 0/1, step: 263/6923, loss: 10.498305320739746, grad_norm: 0.7159036184002197, lr: 7.630057803468207e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„Åå„ÄÅ„Ç¢„Ç¢„Å´„ÄÅ„ÅåÂ∏Ç„Å®„ÄÅ„Å´„ÅÆ„ÄÅ„Å´„ÄÅ„Ç¢„ÄÅ„Éª„ÄÅ„Ç§„ÅÆ„ÄÅ„ÄÅ„Åå„ÅÆ„ÄÅ„Éà„ÄÅ„ÄÅ„Åå„ÅÆ„ÄÅ\n",
      "\n",
      "epoch: 0/1, step: 265/6923, loss: 10.510498046875, grad_norm: 0.5963424044916839, lr: 7.6878612716763e-05\n",
      "epoch: 0/1, step: 267/6923, loss: 10.48258113861084, grad_norm: 0.6243697383430719, lr: 7.745664739884392e-05\n",
      "epoch: 0/1, step: 269/6923, loss: 10.361193656921387, grad_norm: 0.5606448070440423, lr: 7.803468208092485e-05\n",
      "epoch: 0/1, step: 271/6923, loss: 10.238481521606445, grad_norm: 0.552881419297146, lr: 7.861271676300579e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„Å™„ÄÅ„ÄÅ„ÅÆ„ÅÆ„ÅÆ„ÅÆ„Åå„Åß„ÅÆ„Éº„ÄÅ„ÅÆ„Ç®„ÉàÊï∞„Éà„ÄÅ„ÄÅ„Éª„Éº„ÅÆ„Åå„ÄÅ„Åå„Åå„Éª„Å´Â∏Ç„Çí„Çí„ÅÆ\n",
      "\n",
      "epoch: 0/1, step: 273/6923, loss: 10.252020835876465, grad_norm: 0.6338457879441667, lr: 7.91907514450867e-05\n",
      "epoch: 0/1, step: 275/6923, loss: 9.825996398925781, grad_norm: 0.8183230709862918, lr: 7.976878612716763e-05\n",
      "epoch: 0/1, step: 277/6923, loss: 10.301292419433594, grad_norm: 0.5197304926784166, lr: 8.034682080924855e-05\n",
      "epoch: 0/1, step: 279/6923, loss: 10.388989448547363, grad_norm: 0.5813054060349211, lr: 8.092485549132948e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅÂ∏Ç„ÅÆ„Åå„Å´„Åó„ÅÆ„Çí„ÄÅ„Å™„ÅÆ„Å´„Åå„ÅÑ„Å´„Åß„Åå„Åå„ÅÆ„ÅÆÂπ¥„ÄÅÂ§™„ÅøÂπ¥„Ç¢„Éª„Åß„ÅÆ„Çí„Åå„Éà„Å™\n",
      "\n",
      "epoch: 0/1, step: 281/6923, loss: 9.913298606872559, grad_norm: 0.5559018591239155, lr: 8.15028901734104e-05\n",
      "epoch: 0/1, step: 283/6923, loss: 10.264389991760254, grad_norm: 0.6847160164337348, lr: 8.208092485549133e-05\n",
      "epoch: 0/1, step: 285/6923, loss: 9.946136474609375, grad_norm: 0.5964704607165413, lr: 8.265895953757226e-05\n",
      "epoch: 0/1, step: 287/6923, loss: 9.8699369430542, grad_norm: 0.554822750272899, lr: 8.323699421965318e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„ÅØ„ÅÆ„ÅÆ„ÅÆ„Çí„ÄÅ„ÅÆ„Éª„ÅÆ„ÅåÂπ¥„ÅÆ„Ç¢Âπ¥„ÅÆ„Å™„Å´Ôºà„ÄÅ„Çí„ÅÆ„Çí„ÅÆ„ÅÆ„Åå„ÅÆ„ÅÆ„ÅÆ„ÅÆ„ÅÆ„Çí„Åå\n",
      "\n",
      "epoch: 0/1, step: 289/6923, loss: 10.223572731018066, grad_norm: 0.5945588200165829, lr: 8.381502890173411e-05\n",
      "epoch: 0/1, step: 291/6923, loss: 10.300542831420898, grad_norm: 0.6885817118320674, lr: 8.439306358381503e-05\n",
      "epoch: 0/1, step: 293/6923, loss: 9.97362995147705, grad_norm: 1.0455265761904327, lr: 8.497109826589596e-05\n",
      "epoch: 0/1, step: 295/6923, loss: 9.969159126281738, grad_norm: 0.754746570883597, lr: 8.554913294797689e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„ÄÅÂπ¥„ÅÆ„Å´„ÅÆ„ÄÅÂπ¥Êï∞„É¨„ÅÆ„ÅÆ„Çí„ÅÆ„Çí„Å®„ÅÆ„Çí„ÅÆ„ÅÆÂπ¥„Åß„ÄÅ„ÅÆ„Å´Âπ¥„ÅÆ„ÅÆ„ÅÆ„Å´„Åå„ÄÅ„ÅÆ\n",
      "\n",
      "epoch: 0/1, step: 297/6923, loss: 9.994148254394531, grad_norm: 0.6404664796444913, lr: 8.612716763005781e-05\n",
      "epoch: 0/1, step: 299/6923, loss: 9.437685012817383, grad_norm: 0.8448121036344522, lr: 8.670520231213874e-05\n",
      "epoch: 0/1, step: 301/6923, loss: 9.771946907043457, grad_norm: 0.7734842809408906, lr: 8.728323699421966e-05\n",
      "epoch: 0/1, step: 303/6923, loss: 9.446237564086914, grad_norm: 0.7885824552788651, lr: 8.786127167630059e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„Å® „ÅÆ7„ÅÆÔºàÔºà„ÅÆ„Åß„ÅÆ„ÅØ„ÄåÂπ¥„Åß„Çí„ÄÅ„ÇíÔºà„ÄÅ„Åó„ÅÆ„Åó„ÅÆÂπ¥„ÅÆ„ÅÆ„ÅÆ„ÅÆÔºàÂπ¥„Åå„ÄÅ\n",
      "\n",
      "epoch: 0/1, step: 305/6923, loss: 9.513167381286621, grad_norm: 0.6768406476187581, lr: 8.84393063583815e-05\n",
      "epoch: 0/1, step: 307/6923, loss: 9.242049217224121, grad_norm: 0.6245434060030671, lr: 8.901734104046244e-05\n",
      "epoch: 0/1, step: 309/6923, loss: 9.76780891418457, grad_norm: 1.2241926485798946, lr: 8.959537572254337e-05\n",
      "epoch: 0/1, step: 311/6923, loss: 9.06343936920166, grad_norm: 0.6813064089628762, lr: 9.017341040462428e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„ÅØ„Äå„ÅßÂπ¥„É≥Ôºà„ÅÆ„ÅÆÔºà„ÅÆ„Åó„ÄÅ  „Åå ‰∫∫„ÅßÂπ¥„ÅÆÔºà„Åå„ÄÅ„ÇπÔºà„Åô„Çã„Çí„ÅÆ„Çí„Äå„ÅÆ1\n",
      "\n",
      "epoch: 0/1, step: 313/6923, loss: 9.559611320495605, grad_norm: 0.9802060895836565, lr: 9.075144508670522e-05\n",
      "epoch: 0/1, step: 315/6923, loss: 9.791906356811523, grad_norm: 1.3542804159763993, lr: 9.132947976878613e-05\n",
      "epoch: 0/1, step: 317/6923, loss: 9.216526985168457, grad_norm: 0.7957719339347584, lr: 9.190751445086706e-05\n",
      "epoch: 0/1, step: 319/6923, loss: 9.224589347839355, grad_norm: 0.9301032928785096, lr: 9.248554913294798e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„Çí„ÅÆ„ÄÅ„ÅÆ „Å´„ÅØÔºà„Äå„ÄÅ„Åô„Çã„É¨„ÅÆ„Çí Âπ¥Ôºà„Ç≥  „Çπ„ÄÅ „Äå„ÄÅ„ÄÅ„ÇπÔºà‰∫∫„Äå„ÄÅÂπ¥\n",
      "\n",
      "epoch: 0/1, step: 321/6923, loss: 9.12857723236084, grad_norm: 0.8000527541954864, lr: 9.306358381502891e-05\n",
      "epoch: 0/1, step: 323/6923, loss: 9.298274040222168, grad_norm: 0.9189379867256712, lr: 9.364161849710983e-05\n",
      "epoch: 0/1, step: 325/6923, loss: 9.502694129943848, grad_norm: 0.9073123605653349, lr: 9.421965317919076e-05\n",
      "epoch: 0/1, step: 327/6923, loss: 9.566665649414062, grad_norm: 0.9481404318666393, lr: 9.479768786127168e-05\n",
      "\n",
      "================================\n",
      "CHECK GENERATED TEXT\n",
      "================================\n",
      "Prompt   : Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ\n",
      "Generated: Êù±‰∫¨ÈÉΩ„ÅØ„ÄÅ„ÅØ„ÄÅÔºà„Å´Ôºà„Çí„ÇπÔºâ„Çπ„ÅØ„Åß„Åô„ÇãÔºà„ÅÆ„ÅØÂπ¥„ÅÆÁâ©Âπ¥„Å®„Åó„Åü„É≥„ÅóÂπ¥„É≥Âπ¥„É¨„Åó„ÅüÔºà„Äå„Åô„Çã„Åó„Åü\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://miserably-adapted-crow.ngrok-free.app/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# Ensure model devices are set if not specified\n",
    "if device is None or device == 'auto':\n",
    "    device = next(iter(nero_model.parameters())).device\n",
    "\n",
    "# Setup optimizer and scaler\n",
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Compute total steps and warm-up steps\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "\n",
    "# Setup scheduler\n",
    "if warmup_ratio > 0:\n",
    "    # If warmup_ratio > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=max_global_steps,\n",
    "    )\n",
    "else:\n",
    "    # If warmup_ratio is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True,\n",
    "    config=dict(\n",
    "        seed = seed,\n",
    "        target_lang=target_lang,\n",
    "        target_task=target_task,\n",
    "        device = device,\n",
    "        train_size = train_size,\n",
    "        test_size = test_size,\n",
    "        max_seq_length = max_seq_length,\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        resume_step = resume_step,\n",
    "        lr = lr,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Set model to training mode\n",
    "nero_model.train()\n",
    "\n",
    "# Training loop\n",
    "global_step = 0\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of accumulation cycle\n",
    "# Ensure no leftover gradients if resuming mid-cycle or if a previous cycle was interrupted.\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        if global_step < resume_step:\n",
    "            global_step += 1\n",
    "            continue        \n",
    "\n",
    "        # Move inputs to devices\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            outputs = nero_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                use_cache=False, # disable cache to not conflict with gradient checkpointing\n",
    "            )\n",
    "\n",
    "            # Compute loss\n",
    "            loss = outputs.loss / grad_accumulation_steps\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Step optimizer and log only at end of accumulation cycle\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            # Unscale gradients before computing gradient norm and clipping gradients\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # Compute gradient norm\n",
    "            grad_norm = 0.0\n",
    "            for p in nero_params:\n",
    "                p_grad_norm = p.grad.data.norm(2)\n",
    "                grad_norm += p_grad_norm.item() ** 2\n",
    "            grad_norm = grad_norm ** 0.5\n",
    "\n",
    "            # Clp gradients\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(nero_params, clip_grad_norm)\n",
    "\n",
    "            # Optimizer step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Zero gradients for next accumulation cycle\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Logging\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'step': global_step,\n",
    "                'lr': scheduler.get_last_lr()[0],\n",
    "                'loss': loss.item() * grad_accumulation_steps,\n",
    "                'grad_norm': grad_norm,\n",
    "            })\n",
    "            print(f\"epoch: {epoch}/{num_epochs}, \"\n",
    "                f\"step: {global_step}/{max_global_steps}, \"\n",
    "                f\"loss: {loss.item() * grad_accumulation_steps}, \"\n",
    "                f\"grad_norm: {grad_norm}, \"\n",
    "                f\"lr: {scheduler.get_last_lr()[0]}\")\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step % checkpoint_steps == 0:\n",
    "            checkpoint_path = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "            torch.save(nero_model.state_dict(), os.path.join(checkpoint_path, 'pytorch_model.bin'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(checkpoint_path, 'optimizer.pt'))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(checkpoint_path, 'scheduler.pt'))\n",
    "            torch.save(scaler.state_dict(), os.path.join(checkpoint_path, 'scaler.pt'))\n",
    "\n",
    "            # Save metadata for resuming\n",
    "            metadata = {\n",
    "                'step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'lr': scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            torch.save(metadata, os.path.join(checkpoint_path, 'training_state.pt'))\n",
    "\n",
    "            # Commit and push to Hugging Face Hub\n",
    "            repo.git_add(pattern=f\"checkpoint-{global_step}\")\n",
    "            repo.git_commit(f\"Add checkpoint at step {global_step}\")\n",
    "            repo.git_push()\n",
    "            \n",
    "        # if global_step % 8 == 0:\n",
    "        #     generated = generate_text(nero_model, tokenizer, sample_prompt, device=device)\n",
    "        #     print()\n",
    "        #     print(\"================================\")\n",
    "        #     print(\"CHECK GENERATED TEXT\")\n",
    "        #     print(\"================================\")\n",
    "        #     print(f\"{'Prompt':<9}:\", sample_prompt)\n",
    "        #     print(f\"{'Generated':<9}:\", generated)\n",
    "        #     print()\n",
    "\n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = f'L3.1-8B-{target_task}-{target_lang}-{train_size//1000}K-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "# hub_model_id = f'alxxtexxr/{project_name}'\n",
    "\n",
    "# print(\"Project name:\", project_name)\n",
    "# print(\"Hugging Face model ID:\", hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Nero parameters\n",
    "# nero_params_path = f\"nero_params_L1T1_to_{model_configs['target']['label']}.pth\"\n",
    "# lora_state_dict = {k: v for k, v in nero_model.state_dict().items() if 'nero_' in k}\n",
    "# torch.save(lora_state_dict, nero_params_path)\n",
    "# print(\"Nero parameters saved to: \", nero_params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nero_model.eval()\n",
    "# nero_model.set_return_nero_outputs(False)\n",
    "# generate_text(\n",
    "#     nero_model, \n",
    "#     tokenizer, \n",
    "#     prompt=\"Êµ∑„ÅØ\",\n",
    "# )\n",
    "# nero_model.set_return_nero_outputs(True)\n",
    "# nero_model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
