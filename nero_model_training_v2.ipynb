{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all processess on GPU\n",
    "# !fuser -v /dev/nvidia* -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "%pip install trl==0.19.1 # Fix error: ImportError: cannot import name 'ConstantLengthDataset' from 'trl.trainer.utils'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import functools\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    default_data_collator, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hf_model(repo_id, checkpoint=None, max_checkpoints=2000, checkpoint_step=25):\n",
    "    local_dir = repo_id.split('/')[-1]\n",
    "    ignore_checkpoints = None\n",
    "    \n",
    "    if checkpoint is not None:\n",
    "        ignore_checkpoints = [f'checkpoint-{i}/*' for i in range(0, max_checkpoints, checkpoint_step) if i != checkpoint]\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=repo_id,\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=ignore_checkpoints,\n",
    "    )\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        return os.path.join(local_dir, f'checkpoint-{checkpoint}')\n",
    "    return local_dir\n",
    "\n",
    "def check_loss_and_grad_norm(model, tokenizer, prompt=\"Paris is the capital of\"):\n",
    "    # Set model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Zero gradients manually\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    # Forward pass\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors='pt',\n",
    "    ).to(next(model.parameters()).device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "        use_cache=False, # Disable cache to not conflict with gradient checkpointing\n",
    "    )\n",
    "    print(\"Loss:\", outputs.loss)\n",
    "\n",
    "    # Backward pass\n",
    "    outputs.loss.backward()\n",
    "\n",
    "    # Compute gradient norm\n",
    "    grad_norm = 0.0\n",
    "    for p in [p for n, p in model.named_parameters() if p.requires_grad]:\n",
    "        p_grad_norm = p.grad.data.norm(2)\n",
    "        grad_norm += p_grad_norm.item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "\n",
    "    print(\"Gradient norm:\", grad_norm)\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_lora_parameters(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora' in n:\n",
    "            print(f\"- {'Name':<8}:\", n)\n",
    "            print(f\"- {'Mean':<8}:\", p.mean().item())\n",
    "            print(f\"- {'Min':<8}:\", p.min().item())\n",
    "            print(f\"- {'Max':<8}:\", p.max().item())\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=32, device=None, skip_special_tokens=True):\n",
    "    device = device or next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    model.train()\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "def load_hf_dataset_from_lora(\n",
    "    lora_repo_id,\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Get task and language\n",
    "    task, lang, _ = lora_repo_id.split('B-')[-1].split('K-')[0].split('-')\n",
    "\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "    split = f'train[:{(train_size+test_size)}]'\n",
    "\n",
    "    # Load dataset\n",
    "    # TODO: Use streaming to not download the entire dataset\n",
    "    # dataset = load_dataset(data_id, data_dir=data_dir, split=split)\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split='train', streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_hf_dataset(\n",
    "    lang, \n",
    "    task,\n",
    "    split='train',\n",
    "    train_size = 5000,\n",
    "    test_size = 1000,\n",
    "):\n",
    "    # Set up Hugging Face configuration\n",
    "    data_id_map = {\n",
    "        'wikipedia': 'wikimedia/wikipedia',\n",
    "        'gsm8k': 'openai/gsm8k',\n",
    "    }\n",
    "    data_id = data_id_map[task]\n",
    "    data_dir = f'20231101.{lang}' if task == 'wikipedia' else 'main'\n",
    "\n",
    "    # Use streaming\n",
    "    dataset_stream = load_dataset(data_id, data_dir=data_dir, split=split, streaming=True)\n",
    "\n",
    "    # Manually take train_size + test_size samples\n",
    "    total_size = train_size + test_size\n",
    "    sliced_data = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        if i >= total_size:\n",
    "            break\n",
    "        sliced_data.append(example)\n",
    "\n",
    "    # Convert to regular in-memory dataset\n",
    "    dataset = Dataset.from_list(sliced_data)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name: L3.1-8B-wikipedia-ja-5K-Nero-v20250803164454\n",
      "Base model name: unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo\n",
    "\n",
    "# Project configuration\n",
    "seed = 69\n",
    "target_lang = 'ja' # 'en' | 'id' | 'es'\n",
    "target_task = 'wikipedia' # 'wikipedia' | 'gsm8k'\n",
    "device = 'auto' # 'cpu' | 'cuda' | 'auto'\n",
    "\n",
    "# Data configuration\n",
    "train_size = 5000\n",
    "test_size = 0 # Temporary\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Training configuration\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "max_global_steps = 10\n",
    "resume_step = 0\n",
    "grad_accumulation_steps = 2\n",
    "clip_grad_norm = 0.5\n",
    "lr = 2e-4\n",
    "warmup_ratio = 0.05\n",
    "checkpoint_steps = 10\n",
    "generate_steps = 10\n",
    "sample_prompt = 'Êó•Êú¨ÂõΩ„ÅØ„ÄÅ'\n",
    "\n",
    "# Model configurations\n",
    "hf_lora_id = 'alxxtexxr/L3.1-8B-wikipedia-en-5K-LoRA-v20250630122650'\n",
    "checkpoint = 650\n",
    "\n",
    "lora_dir = download_hf_model(hf_lora_id, checkpoint)\n",
    "lora_path = os.path.join(lora_dir, 'adapter_model.safetensors')\n",
    "lora_config = LoraConfig.from_pretrained(lora_dir)\n",
    "base_model_name = lora_config.base_model_name_or_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "project_name = f'L3.1-8B-{target_task}-{target_lang}-{train_size//1000}K-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "nero_dir = project_name\n",
    "\n",
    "print(f\"Project name: {project_name}\")\n",
    "print(f\"Base model name: {base_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face configuration\n",
    "hf_username = 'alxxtexxr'\n",
    "hf_nero_id = f'{hf_username}/{nero_dir}'\n",
    "os.makedirs(nero_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(nero_dir, '.git')):\n",
    "    create_repo(hf_nero_id, exist_ok=True)\n",
    "    repo = Repository(local_dir=nero_dir, clone_from=hf_nero_id)\n",
    "else:\n",
    "    repo = Repository(local_dir=nero_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d91c05d40a47dbbb2fd614672939c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65e012a02d546be8560338fcab5528e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75e19477a734c578bd91874ad26146d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NeroLayer(nn.Module):\n",
    "    def __init__(self, base_layer, \n",
    "                 # LoRA parameters\n",
    "                 rank, alpha, dropout, lora_bias, use_rslora, \n",
    "                 # Nero parameters\n",
    "                 nero_bias=False, \n",
    "                 return_nero_output=False,\n",
    "                 # For debugging \n",
    "                 debug=False,\n",
    "                 module_name=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.device = base_layer.weight.device\n",
    "        self.alpha = alpha\n",
    "        self.lora_bias = lora_bias\n",
    "        self.scaling = alpha / math.sqrt(rank) if use_rslora else alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.return_nero_output = return_nero_output\n",
    "\n",
    "        # For debugging\n",
    "        self.debug = debug\n",
    "        self.module_name = module_name\n",
    "\n",
    "        # Extract input and output features from the base layer\n",
    "        in_features = getattr(base_layer, 'in_features', None)\n",
    "        out_features = getattr(base_layer, 'out_features', None)\n",
    "\n",
    "        if in_features is None or out_features is None:\n",
    "            raise ValueError(f\"Cannot determine in_features or out_features from {base_layer}.\")\n",
    "        \n",
    "        # LoRA decomposition: A (down-projection) and B (up-projection)\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=lora_bias).to(self.device)  # Projects down\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=lora_bias).to(self.device) # Projects up\n",
    "\n",
    "        # Initialize LoRA matrices: A ~ N(0, 1/rank), B initialized to 0\n",
    "        std = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # Nero decomposition: additional transformation applied to LoRA output\n",
    "        self.nero_A = nn.Linear(out_features, rank, bias=nero_bias).to(self.device)\n",
    "        self.nero_B = nn.Linear(rank, out_features, bias=nero_bias).to(self.device)\n",
    "\n",
    "        # Initialize Nero matrices similarly\n",
    "        nn.init.normal_(self.nero_A.weight, mean=0.0, std=std)\n",
    "        nn.init.zeros_(self.nero_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward through base layer\n",
    "        base_out = self.base_layer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"================================================================\")\n",
    "            print(self.module_name)\n",
    "            print(\"================================================================\")\n",
    "            print(\"base_out.requires_grad:\", base_out.requires_grad)\n",
    "            print(\"base_out.grad_fn:\", base_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # LoRA transformation\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            x = x.to(self.lora_A.weight.dtype)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        # if requires_conversion:\n",
    "        #     lora_out = lora_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"lora_out.requires_grad:\", lora_out.requires_grad)\n",
    "            print(\"lora_out.grad_fn:\", lora_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "        # nero_out = F.relu(self.nero_B(self.nero_A(self.dropout(lora_out))) * self.scaling)\n",
    "        nero_dropout_out = self.dropout(lora_out)\n",
    "        nero_A_out = self.nero_A(nero_dropout_out)\n",
    "        nero_B_out = self.nero_B(nero_A_out)\n",
    "        nero_scaling_out = nero_B_out * self.scaling\n",
    "        nero_out = F.relu(nero_scaling_out)\n",
    "        if requires_conversion:\n",
    "            nero_out = nero_out.to(base_out.dtype)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"nero_out.requires_grad:\", nero_out.requires_grad)\n",
    "            print(\"nero_out.grad_fn:\", nero_out.grad_fn)\n",
    "            print()\n",
    "\n",
    "            nero_out_has_nan = torch.isnan(nero_out).any()\n",
    "            if nero_out_has_nan:\n",
    "                print(\"!!! NERO OUT HAS NAN !!!\")\n",
    "                print(\"nero_out:\")\n",
    "                print(nero_out)\n",
    "                print()\n",
    "                print(\"nero_scaling_out:\")\n",
    "                print(nero_scaling_out)\n",
    "                print()\n",
    "                print(\"nero_B_out:\")\n",
    "                print(nero_B_out)\n",
    "                print()\n",
    "                print(\"nero_A_out:\")\n",
    "                print(nero_A_out)\n",
    "                print()\n",
    "                print(\"nero_dropout_out:\")\n",
    "                print(nero_dropout_out)\n",
    "                print()\n",
    "                print(\"lora_out:\")\n",
    "                print(lora_out)\n",
    "                print()\n",
    "\n",
    "        # Add `base_out` with gradients-detached `nero_out`, \n",
    "        # so that `base_out` does not carry gradients\n",
    "        # nero_out_detached = nero_out.detach()\n",
    "\n",
    "        # if self.debug:\n",
    "        #     print(\"nero_out_detached.requires_grad:\", nero_out_detached.requires_grad)\n",
    "        #     print(\"nero_out_detached.grad_fn:\", nero_out_detached.grad_fn)\n",
    "        #     print()\n",
    "\n",
    "        # output = base_out + nero_out_detached\n",
    "        output = base_out + nero_out\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"output.requires_grad:\", output.requires_grad)\n",
    "            print(\"output.grad_fn:\", output.grad_fn)\n",
    "            print()\n",
    "\n",
    "        if self.return_nero_output:\n",
    "            return output, nero_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def load_lora_params(self, state_dict, prefix):\n",
    "        self.lora_A.weight.data = state_dict[f'{prefix}.lora_A.weight'].to(self.device)\n",
    "        self.lora_B.weight.data = state_dict[f'{prefix}.lora_B.weight'].to(self.device)\n",
    "        if self.lora_bias:\n",
    "            self.lora_A.bias.data = state_dict[f'{prefix}.lora_A.bias'].to(self.device)\n",
    "            self.lora_B.bias.data = state_dict[f'{prefix}.lora_B.bias'].to(self.device)\n",
    "    \n",
    "class NeroModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, lora_config: LoraConfig, nero_bias: bool=False, \n",
    "                 return_nero_outputs: bool=False, debug: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nero_bias = nero_bias\n",
    "        self.nero_layers = nn.ModuleDict()\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        self.debug = debug\n",
    "\n",
    "        # Wrap target layers with NeroLayer\n",
    "        self._wrap_target_layers(lora_config)\n",
    "        \n",
    "    def _wrap_target_layers(self, lora_config):\n",
    "        for module_name, module in self.base_model.named_modules():\n",
    "            if isinstance(module, NeroLayer):\n",
    "                # Convert module name format and store reference\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = module\n",
    "                continue\n",
    "\n",
    "            if any(module_name.endswith(target_module) for target_module in lora_config.target_modules) and isinstance(module, nn.Linear):    \n",
    "                parent_module, child_name = self._get_parent_module(module_name)\n",
    "                nero_layer = NeroLayer(\n",
    "                    module, \n",
    "                    lora_config.r, \n",
    "                    lora_config.lora_alpha, \n",
    "                    lora_config.lora_dropout, \n",
    "                    lora_config.lora_bias, \n",
    "                    lora_config.use_rslora,\n",
    "                    nero_bias=self.nero_bias,\n",
    "                    return_nero_output=self.return_nero_outputs,\n",
    "                    debug=self.debug,\n",
    "                    module_name=module_name,\n",
    "                )\n",
    "                setattr(parent_module, child_name, nero_layer)\n",
    "\n",
    "                # Store LoRA layers for weight loading\n",
    "                module_name = module_name.rsplit('model.', 1)[-1]\n",
    "                module_name = module_name.replace('.', '__DOT__')\n",
    "                self.nero_layers[module_name] = nero_layer\n",
    "    \n",
    "    def _get_parent_module(self, module_name):\n",
    "        parts = module_name.split('.')\n",
    "        parent_module = self.base_model\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        return parent_module, parts[-1]\n",
    "    \n",
    "    def set_return_nero_outputs(self, return_nero_outputs: bool):\n",
    "        self.return_nero_outputs = return_nero_outputs\n",
    "        for layer in self.nero_layers.values():\n",
    "            layer.return_nero_output = return_nero_outputs\n",
    "\n",
    "    def freeze_all_except_nero(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param_name, param in nero_layer.named_parameters():\n",
    "                if 'nero_A' in param_name or 'nero_B' in param_name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        print(\"All layers are frozen except Nero layers!\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for nero_layer in self.nero_layers.values():\n",
    "            for param in nero_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        print(\"All layers are unfrozen!\")\n",
    "    \n",
    "    def load_lora_params(self, lora_path):\n",
    "        state_dict = load_file(lora_path)\n",
    "        prefix = list(state_dict.keys())[0].rsplit('model.', 1)[0] + 'model.'\n",
    "\n",
    "        for nero_layer_name, nero_layer in self.nero_layers.items():\n",
    "            nero_layer_name = nero_layer_name.replace('__DOT__', '.')\n",
    "            nero_layer_name = prefix + nero_layer_name\n",
    "            if f'{nero_layer_name}.lora_A.weight' in state_dict and f'{nero_layer_name}.lora_B.weight' in state_dict:\n",
    "                nero_layer.load_lora_params(state_dict, nero_layer_name)\n",
    "            else:\n",
    "                # TODO: Print warning message\n",
    "                pass\n",
    "\n",
    "        print(\"LoRA parameters loaded successfully!\")\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        if self.return_nero_outputs:\n",
    "            nero_outs = {}\n",
    "            \n",
    "            def _hook_fn(layer_name, module, _in, _out):\n",
    "                if isinstance(_out, tuple) and len(_out) == 2:\n",
    "                    layer_out, nero_out = _out\n",
    "                    nero_outs[layer_name] = nero_out # Store nero_out separately\n",
    "                    return layer_out # Return only layer_out to avoid breaking model flow\n",
    "\n",
    "            # Register hooks to extract nero_out during forward pass\n",
    "            hooks = []\n",
    "            for layer_name, layer in self.nero_layers.items():\n",
    "                hook = layer.register_forward_hook(functools.partial(_hook_fn, layer_name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "            try:\n",
    "                output = self.base_model(*args, **kwargs)\n",
    "            finally:\n",
    "                # Remove hooks after forward pass, ensuring it's done even if an error occurs\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "            return output, nero_outs\n",
    "        \n",
    "        return self.base_model(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name) # Try getting attribute from self\n",
    "        except AttributeError:\n",
    "            return getattr(self.base_model, name) # Fallback to base_model\n",
    "\n",
    "base_nero_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device)\n",
    "nero_model = NeroModel(\n",
    "    base_nero_model, \n",
    "    lora_config, \n",
    "    nero_bias=True, \n",
    "    return_nero_outputs=False,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers are frozen except Nero layers!\n",
      "Loss: tensor(14.5990, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 1.5991796752380212\n"
     ]
    }
   ],
   "source": [
    "nero_model.freeze_all_except_nero()\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check LoRA parameters (unloaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 0.0016585660632699728\n",
      "- Min     : -1.574367880821228\n",
      "- Max     : 1.5179626941680908\n",
      "\n",
      "LoRA parameters loaded successfully!\n",
      "\n",
      "Check LoRA parameters (loaded):\n",
      "- Name    : base_model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "- Mean    : 6.287686119321734e-05\n",
      "- Min     : -0.04176201671361923\n",
      "- Max     : 0.04242725297808647\n",
      "\n",
      "Loss: tensor(14.5990, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 1.8391875321443354\n"
     ]
    }
   ],
   "source": [
    "print(\"Check LoRA parameters (unloaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "nero_model.load_lora_params(lora_path)\n",
    "print()\n",
    "\n",
    "print(\"Check LoRA parameters (loaded):\")\n",
    "check_lora_parameters(nero_model)\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient checkpointing enabled!\n",
      "\n",
      "Loss: tensor(14.5990, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 1.8391875321443354\n"
     ]
    }
   ],
   "source": [
    "nero_model.gradient_checkpointing_enable({'use_reentrant': False})\n",
    "print(\"Gradient checkpointing enabled!\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_class : None\n",
      "bos_token_id : 128000\n",
      "pad_token_id : 128004\n",
      "eos_token_id : 128001\n",
      "sep_token_id : None\n",
      "decoder_start_token_id : None\n",
      "forced_bos_token_id : None\n",
      "forced_eos_token_id : None\n",
      "suppress_tokens : None\n",
      "begin_suppress_tokens : None\n"
     ]
    }
   ],
   "source": [
    "for k, v in nero_model.config.__dict__.items():\n",
    "    if 'token' in k:\n",
    "        print(k, \":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nero_model.eval()\n",
    "# generate_text(\n",
    "#     nero_model, \n",
    "#     tokenizer, \n",
    "#     prompt=\"Paris is the capital of\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f86a5e98289473db3226dea70f07053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_hf_dataset(\n",
    "    target_lang, \n",
    "    target_task, \n",
    "    train_size=train_size, \n",
    "    test_size=test_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31203a562bae454bace910ad7d3ac61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb90113dda7d46998a921d9dde53e1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dataset_map_config(remove_columns=None):\n",
    "    return dict(\n",
    "        batched=True, \n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "\n",
    "if target_task == 'gsm8k':\n",
    "    eos_token = tokenizer.eos_token\n",
    "    \n",
    "    def format_gsm8k_prompt(example):\n",
    "        gsm8k_prompt = \"\"\"### Instruction:\n",
    "Solve the following math problem step by step.\n",
    "\n",
    "### Question: \n",
    "{question}\n",
    "\n",
    "### Answer: \n",
    "{answer}\"\"\" + eos_token\n",
    "\n",
    "        return {'text': gsm8k_prompt.format(\n",
    "            question=example['question'], \n",
    "            answer=example['answer'],\n",
    "        )}\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_seq_length,\n",
    "        )\n",
    "\n",
    "    def add_labels(example):\n",
    "        example['labels'] = example['input_ids'].copy()\n",
    "        return example\n",
    "\n",
    "    dataset_formatted = dataset.map(\n",
    "        format_gsm8k_prompt, \n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_tokenized = dataset_formatted.map(\n",
    "        tokenize_fn, \n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_final = dataset_tokenized.map(\n",
    "        add_labels,\n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "else:\n",
    "    def tokenize_fn(example):\n",
    "        return tokenizer(example['text'])\n",
    "\n",
    "    # Concatenate all tokens into one long stream, then split into blocks\n",
    "    block_size = max_seq_length\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated = []\n",
    "        for input_ids in examples['input_ids']:\n",
    "            concatenated += input_ids\n",
    "\n",
    "        total_length = len(concatenated) // block_size * block_size\n",
    "\n",
    "        input_ids = [concatenated[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        attention_mask = [[1] * block_size for _ in input_ids]\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "    dataset_tokenized = dataset.map(\n",
    "        tokenize_fn,\n",
    "        **get_dataset_map_config(remove_columns=dataset.column_names),\n",
    "    )\n",
    "    dataset_final = dataset_tokenized.map(\n",
    "        group_texts, \n",
    "        **get_dataset_map_config(remove_columns=dataset_tokenized.column_names),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 6923\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset_final, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "print(\"Total batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch data shape (input_ids, attention_mask, labels):\n",
      "(torch.Size([4, 1024]), torch.Size([4, 1024]), torch.Size([4, 1024]))\n",
      "\n",
      "First batch text:\n",
      "NSII CX/UXÔºàCRT‰∏Ä‰ΩìÂûã386SX TOWNSÔºâ„ÄÄ1992Âπ¥Â∫¶„Ç∞„ÉÉ„Éâ„Éá„Ç∂„Ç§„É≥Ë≥ûÂèóË≥û\n",
      " 1992Âπ¥2Êúà - FM TOWNSII UX40\n",
      " 1992Âπ¥11Êúà4Êó• - FM TOWNSII H ...\n",
      "\n",
      "Loss: tensor(13.9625, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "Gradient norm: 0.71104503952095\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"First batch data shape (input_ids, attention_mask, labels):\")\n",
    "print((\n",
    "    first_batch['input_ids'].shape, \n",
    "    first_batch['attention_mask'].shape, \n",
    "    first_batch['labels'].shape, \n",
    "))\n",
    "print()\n",
    "\n",
    "first_batch_text = tokenizer.batch_decode(first_batch['input_ids'], skip_special_tokens=True)[0]\n",
    "print(\"First batch text:\")\n",
    "print(first_batch_text[:100], \"...\")\n",
    "print()\n",
    "\n",
    "check_loss_and_grad_norm(nero_model, tokenizer, prompt=first_batch_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(nero_model, tokenizer=tokenizer, prompt=sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_689/3034244394.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malimtegar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddc3874a5bd4da58a99ad75520c0110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112705911111536, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250803_164522-nq069c2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alimtegar/Nero-XLT/runs/nq069c2f' target=\"_blank\">summer-flower-71</a></strong> to <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/nq069c2f' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/nq069c2f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_689/3034244394.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ea30d3ecba4ddeaa9839ff07ebac33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-0/adapter_model.bin:   0%|          | 1.00/5.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8874dc4593c4e70b54ac2efa94ad8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-0/scaler.pt:   0%|          | 1.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770a7144ce8d4bdbb35bcaca54e6a7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-0/training_state.pt:   0%|          | 1.00/892 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae82d9bf82d4e00b0d1ae78e6fabd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-0/scheduler.pt:   0%|          | 1.00/0.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce6d125b1fd4576b04bc1f6731396bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file checkpoint-0/optimizer.pt:   0%|          | 1.00/3.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/alxxtexxr/L3.1-8B-wikipedia-ja-5K-Nero-v20250803164454\n",
      "   e92510d..0d7f73d  main -> main\n",
      "\n",
      "/tmp/ipykernel_689/3034244394.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/1, step: 1/10, loss: 13.894257545471191, grad_norm: 0.5216511298905168, lr: 0.00019510565162951537\n",
      "epoch: 0/1, step: 3/10, loss: 13.740740776062012, grad_norm: 0.408808239260627, lr: 0.00018090169943749476\n",
      "epoch: 0/1, step: 5/10, loss: 13.746880531311035, grad_norm: 0.47036268693720007, lr: 0.00015877852522924732\n",
      "epoch: 0/1, step: 7/10, loss: 13.788190841674805, grad_norm: 0.45381971411593797, lr: 0.00013090169943749476\n",
      "epoch: 0/1, step: 9/10, loss: 13.611076354980469, grad_norm: 0.4550647138584545, lr: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c34b6382094b5a80851fb879e73f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>grad_norm</td><td>‚ñà‚ñÅ‚ñÖ‚ñÑ‚ñÑ</td></tr><tr><td>loss</td><td>‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÅ</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>grad_norm</td><td>0.45506</td></tr><tr><td>loss</td><td>13.61108</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>step</td><td>9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-flower-71</strong> at: <a href='https://wandb.ai/alimtegar/Nero-XLT/runs/nq069c2f' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT/runs/nq069c2f</a><br/> View project at: <a href='https://wandb.ai/alimtegar/Nero-XLT' target=\"_blank\">https://wandb.ai/alimtegar/Nero-XLT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250803_164522-nq069c2f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure device is set\n",
    "if device is None or device == 'auto':\n",
    "    device = next(iter(nero_model.parameters())).device\n",
    "\n",
    "# Setup optimizer and gradient scaler\n",
    "nero_params = [p for n, p in nero_model.named_parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(nero_params, lr=lr)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Setup LR scheduler\n",
    "max_global_steps = max_global_steps or len(train_loader) * num_epochs\n",
    "warmup_steps = int(warmup_ratio * max_global_steps)\n",
    "if warmup_ratio > 0:\n",
    "    # If `warmup_ratio` > 0, use cosine annealing scheduler with warm-up \n",
    "    from transformers import get_cosine_schedule_with_warmup # type: ignore\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=max_global_steps,\n",
    "    )\n",
    "else:\n",
    "    # If `warmup_ratio` is 0, use a dummy scheduler that returns constant LR\n",
    "    from torch.optim.lr_scheduler import LambdaLR # type: ignore\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 1.0)\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project='Nero-XLT',\n",
    "    reinit=True,\n",
    "    config=dict(\n",
    "        seed = seed,\n",
    "        target_lang=target_lang,\n",
    "        target_task=target_task,\n",
    "        device = device,\n",
    "        train_size = train_size,\n",
    "        test_size = test_size,\n",
    "        max_seq_length = max_seq_length,\n",
    "        batch_size = batch_size,\n",
    "        num_epochs = num_epochs,\n",
    "        max_global_steps = max_global_steps,\n",
    "        resume_step = resume_step,\n",
    "        lr = lr,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        clip_grad_norm = clip_grad_norm,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "global_step = resume_step\n",
    "start_epoch = 0\n",
    "if resume_step > 0:\n",
    "    checkpoint_path = os.path.join(nero_dir, f'checkpoint-{resume_step}')\n",
    "    print(f\"[INFO] Resuming training from {checkpoint_path}\")\n",
    "\n",
    "    # Load adapter state\n",
    "    model_path = os.path.join(checkpoint_path, 'adapter_model.bin')\n",
    "    nero_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_path, 'optimizer.pt')\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "\n",
    "    # Load scheduler state\n",
    "    scheduler_path = os.path.join(checkpoint_path, 'scheduler.pt')\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, map_location=device))\n",
    "\n",
    "    # Load scaler state\n",
    "    scaler_path = os.path.join(checkpoint_path, 'scaler.pt')\n",
    "    scaler.load_state_dict(torch.load(scaler_path, map_location=device))\n",
    "\n",
    "    # Load training state\n",
    "    training_state_path = os.path.join(checkpoint_path, 'training_state.pt')\n",
    "    if os.path.exists(training_state_path):\n",
    "        training_state = torch.load(training_state_path)\n",
    "        start_epoch = training_state.get('epoch', 0)\n",
    "        print(f\"[INFO] Resumed from epoch {start_epoch}, step {resume_step}\")\n",
    "\n",
    "# Set model to training mode\n",
    "nero_model.train()\n",
    "done = False\n",
    "\n",
    "# Safety: Zero gradients at the start of gradient accumulation cycle\n",
    "# This ensures there are no leftover gradients when resuming mid-cycle or after a previous cycle was interrupted\n",
    "if global_step % grad_accumulation_steps == 0:\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Stop training if `max_global_steps` reached\n",
    "        if global_step >= max_global_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            outputs = nero_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "\n",
    "                # Disable cache to avoid conflict with gradient checkpointing\n",
    "                use_cache=False, \n",
    "            )\n",
    "\n",
    "            # Compute loss\n",
    "            loss = outputs.loss / grad_accumulation_steps\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update parameters and log only at the end of gradient accumulation cycle\n",
    "        if (global_step + 1) % grad_accumulation_steps == 0:\n",
    "            # Unscale gradients before computing gradient norm and applying clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            # Compute gradient norm\n",
    "            grad_norm = 0.0\n",
    "            for p in nero_params:\n",
    "                p_grad_norm = p.grad.data.norm(2)\n",
    "                grad_norm += p_grad_norm.item() ** 2\n",
    "            grad_norm = grad_norm ** 0.5\n",
    "\n",
    "            # Clip gradients\n",
    "            if clip_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(nero_params, clip_grad_norm)\n",
    "\n",
    "            # Update parameters\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Zero gradients for the next gradient accumulation cycle\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Logging\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'step': global_step,\n",
    "                'lr': scheduler.get_last_lr()[0],\n",
    "                'loss': loss.item() * grad_accumulation_steps,\n",
    "                'grad_norm': grad_norm,\n",
    "            })\n",
    "            print(f\"epoch: {epoch}/{num_epochs}, \"\n",
    "                f\"step: {global_step}/{max_global_steps}, \"\n",
    "                f\"loss: {loss.item() * grad_accumulation_steps}, \"\n",
    "                f\"grad_norm: {grad_norm}, \"\n",
    "                f\"lr: {scheduler.get_last_lr()[0]}\")\n",
    "        \n",
    "        # Save and push checkpoint every `checkpoint_steps`\n",
    "        if global_step > 0 and global_step % checkpoint_steps == 0:\n",
    "            # Create checkpoint directory\n",
    "            checkpoint_path = os.path.join(nero_dir, f'checkpoint-{global_step}')\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "            # Save Nero adapter, optimizer, scheduler, and scaler states\n",
    "            torch.save(nero_model.state_dict(), os.path.join(checkpoint_path, 'adapter_model.bin'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(checkpoint_path, 'optimizer.pt'))\n",
    "            torch.save(scheduler.state_dict(), os.path.join(checkpoint_path, 'scheduler.pt'))\n",
    "            torch.save(scaler.state_dict(), os.path.join(checkpoint_path, 'scaler.pt'))\n",
    "\n",
    "            # Save training state for resuming training\n",
    "            training_state = {\n",
    "                'step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'lr': scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            torch.save(training_state, os.path.join(checkpoint_path, 'training_state.pt'))\n",
    "\n",
    "            # Commit and push checkpoint to Hugging Face\n",
    "            repo.git_add(pattern=f\"checkpoint-{global_step}\")\n",
    "            repo.git_commit(f\"Add checkpoint at step {global_step}\")\n",
    "            repo.git_push()\n",
    "        \n",
    "        # Check generated text every `generate_steps`\n",
    "        if global_step > 0 and global_step % generate_steps == 0:\n",
    "            generated = generate_text(nero_model, tokenizer, sample_prompt, device=device)\n",
    "            print()\n",
    "            print(\"================================\")\n",
    "            print(\"CHECK GENERATED TEXT\")\n",
    "            print(\"================================\")\n",
    "            print(f\"{'Prompt':<9}:\", sample_prompt)\n",
    "            print(f\"{'Generated':<9}:\", generated)\n",
    "            print()\n",
    "\n",
    "        global_step += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = f'L3.1-8B-{target_task}-{target_lang}-{train_size//1000}K-Nero-v{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "# hub_model_id = f'alxxtexxr/{project_name}'\n",
    "\n",
    "# print(\"Project name:\", project_name)\n",
    "# print(\"Hugging Face model ID:\", hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Nero parameters\n",
    "# nero_params_path = f\"nero_params_L1T1_to_{model_configs['target']['label']}.pth\"\n",
    "# lora_state_dict = {k: v for k, v in nero_model.state_dict().items() if 'nero_' in k}\n",
    "# torch.save(lora_state_dict, nero_params_path)\n",
    "# print(\"Nero parameters saved to: \", nero_params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nero_model.eval()\n",
    "# nero_model.set_return_nero_outputs(False)\n",
    "# generate_text(\n",
    "#     nero_model, \n",
    "#     tokenizer, \n",
    "#     prompt=\"Êµ∑„ÅØ\",\n",
    "# )\n",
    "# nero_model.set_return_nero_outputs(True)\n",
    "# nero_model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
